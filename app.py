"""
RSS Aggregator Web Application
Provides a web interface to filter, sort, and browse RSS feed articles with AI-generated metadata.
"""
import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections.abc import Iterable
from dataclasses import dataclass, asdict

import feedparser
from flask import Flask, render_template, jsonify, request
from bs4 import BeautifulSoup

app = Flask(__name__)

# Configuration
DOCS_DIR = Path(__file__).parent / "docs"
CACHE_DURATION = 300  # 5 minutes in seconds

# Categories from schema.json
FAILURE_MODES = ["human-misuse", "misalignment", "societal-disruption", "other", "non-applicable"]
PRIMARY_FOCUSES = ["interpretability", "alignment", "robustness", "control", "other"]

# Global cache
_cache: Dict[str, Any] = {
    "articles": [],
    "feeds": [],
    "last_refresh": 0
}


@dataclass
class Article:
    """Represents a parsed article with all metadata."""
    feed_name: str
    title: str
    link: str
    published: Optional[str]
    published_timestamp: Optional[float]
    summary_en: str
    summary_cn: str
    keywords: List[str]
    interpretability: int
    understanding: int
    safety: int
    technicality: int
    surprisal: int
    failure_mode: str
    primary_focus: str

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)


def normalize_category(value: Optional[str], allowed: Iterable[str], default: str) -> str:
    """Normalize category strings to slug format defined in schema."""
    if not value:
        return default

    slug = re.sub(r'[^a-z0-9]+', '-', value.strip().lower()).strip('-')
    return slug if slug in set(allowed) else default


def extract_acme_data(item_node: Optional[BeautifulSoup]) -> Dict[str, Any]:
    """Extract structured metadata from the <acme> JSON block when present."""
    if item_node is None:
        return {}

    acme_tag = item_node.find("acme")
    if not acme_tag or not acme_tag.string:
        return {}

    try:
        data = json.loads(acme_tag.string)
    except json.JSONDecodeError:
        return {}

    summary_en = data.get("summary", "") or ""
    summary_cn = data.get("summary_cn", "") or ""

    raw_keywords = data.get("keywords", [])
    if isinstance(raw_keywords, str):
        keywords = [k.strip() for k in re.split(r'[,;]', raw_keywords) if k.strip()]
    elif isinstance(raw_keywords, Iterable):
        keywords = [str(k).strip() for k in raw_keywords if str(k).strip()]
    else:
        keywords = []

    scores_raw = data.get("scoring", {}) or {}
    scores: Dict[str, int] = {}
    for key, value in scores_raw.items():
        if not isinstance(key, str):
            continue
        try:
            scores[key.strip().lower()] = int(value)
        except (ValueError, TypeError):
            continue

    category_raw = data.get("category", {}) or {}
    failure_mode = category_raw.get("failure_mode_addressed") or category_raw.get("failure_mode") or ""
    primary_focus = category_raw.get("primary_focus") or ""

    return {
        "summary_en": summary_en,
        "summary_cn": summary_cn,
        "keywords": keywords,
        "scores": scores,
        "categories": {
            "failure_mode": failure_mode,
            "primary_focus": primary_focus
        }
    }


def parse_summary_html(summary_html: str) -> Dict[str, Any]:
    """
    Parse the structured HTML summary generated by main.py.

    Example format:
    <strong>Summary:</strong> text<br>
    <strong>Summary (CN):</strong> text<br>
    <strong>Keywords:</strong> keyword1, keyword2<br>
    <strong>Scores:</strong> Interpretability: 8, Understanding: 7, ...<br>
    <strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment
    """
    result = {
        "summary_en": "",
        "summary_cn": "",
        "keywords": [],
        "scores": {},
        "categories": {}
    }

    if not summary_html:
        return result

    # Split by <br> or <br/>
    parts = re.split(r'<br\s*/?>', summary_html, flags=re.IGNORECASE)

    for part in parts:
        part = part.strip()
        if not part:
            continue

        # Remove HTML tags for parsing
        text = BeautifulSoup(part, "html.parser").get_text()

        if text.startswith("Summary:"):
            result["summary_en"] = text.replace("Summary:", "").strip()
        elif text.startswith("Summary (CN):"):
            result["summary_cn"] = text.replace("Summary (CN):", "").strip()
        elif text.startswith("Keywords:"):
            keywords_str = text.replace("Keywords:", "").strip()
            result["keywords"] = [k.strip() for k in keywords_str.split(",") if k.strip()]
        elif text.startswith("Scores:"):
            scores_str = text.replace("Scores:", "").strip()
            # Parse "Interpretability: 8, Understanding: 7, ..."
            for score_part in scores_str.split(","):
                if ":" in score_part:
                    key, value = score_part.split(":", 1)
                    key = key.strip().lower()
                    try:
                        result["scores"][key] = int(value.strip())
                    except ValueError:
                        pass
        elif text.startswith("Categories:"):
            categories_str = text.replace("Categories:", "").strip()
            # Parse "Failure mode - misalignment; Primary focus - alignment"
            for cat_part in categories_str.split(";"):
                if "Failure mode -" in cat_part:
                    result["categories"]["failure_mode"] = cat_part.split("-", 1)[1].strip()
                elif "Primary focus -" in cat_part:
                    result["categories"]["primary_focus"] = cat_part.split("-", 1)[1].strip()

    return result


def get_entry_metadata(entry: Any, item_node: Optional[BeautifulSoup]) -> Dict[str, Any]:
    """Obtain structured metadata for a feed entry, preferring ACME JSON."""
    acme_data = extract_acme_data(item_node)
    if acme_data:
        return acme_data

    summary_html = getattr(entry, 'summary', '')
    return parse_summary_html(summary_html)


def parse_xml_file(xml_path: Path) -> List[Article]:
    """Parse a single XML feed file and extract all articles."""
    articles = []
    feed_name = xml_path.stem  # filename without .xml extension

    try:
        with open(xml_path, 'r', encoding='utf-8') as f:
            content = f.read()

        feed = feedparser.parse(content)
        soup = BeautifulSoup(content, "xml")
        item_nodes = soup.find_all("item")

        for idx, entry in enumerate(feed.entries):
            item_node = item_nodes[idx] if idx < len(item_nodes) else None
            parsed = get_entry_metadata(entry, item_node)

            # Get publication date
            published = getattr(entry, 'published', None) or getattr(entry, 'updated', None)
            published_timestamp = None

            if published:
                try:
                    # Try to parse the published date
                    from email.utils import parsedate_to_datetime
                    dt = parsedate_to_datetime(published)
                    published_timestamp = dt.timestamp()
                except:
                    # If parsing fails, use current time
                    published_timestamp = time.time()

            # Extract scores with defaults
            scores = parsed.get("scores", {})

            # Extract categories with defaults
            categories = parsed.get("categories", {})
            failure_mode = normalize_category(
                categories.get("failure_mode", "other"),
                FAILURE_MODES,
                "other",
            )
            primary_focus = normalize_category(
                categories.get("primary_focus", "other"),
                PRIMARY_FOCUSES,
                "other",
            )

            article = Article(
                feed_name=feed_name,
                title=getattr(entry, 'title', 'Untitled'),
                link=getattr(entry, 'link', ''),
                published=published,
                published_timestamp=published_timestamp,
                summary_en=parsed.get("summary_en", ""),
                summary_cn=parsed.get("summary_cn", ""),
                keywords=parsed.get("keywords", []),
                interpretability=scores.get("interpretability", 0),
                understanding=scores.get("understanding", 0),
                safety=scores.get("safety", 0),
                technicality=scores.get("technicality", 0),
                surprisal=scores.get("surprisal", 0),
                failure_mode=failure_mode,
                primary_focus=primary_focus
            )

            articles.append(article)

    except Exception as e:
        print(f"Error parsing {xml_path}: {e}")

    return articles


def refresh_data():
    """Scan all XML files in docs/ and parse articles."""
    global _cache

    articles = []
    feeds = set()

    # Find all XML files in docs/
    if DOCS_DIR.exists():
        for xml_file in DOCS_DIR.glob("*.xml"):
            parsed_articles = parse_xml_file(xml_file)
            articles.extend(parsed_articles)
            if parsed_articles:
                feeds.add(xml_file.stem)

    _cache["articles"] = [article.to_dict() for article in articles]
    _cache["feeds"] = sorted(list(feeds))
    _cache["last_refresh"] = time.time()

    print(f"Refreshed data: {len(articles)} articles from {len(feeds)} feeds")


def get_cached_data():
    """Get cached data, refreshing if needed."""
    current_time = time.time()

    if current_time - _cache["last_refresh"] > CACHE_DURATION:
        refresh_data()

    return _cache


@app.route('/')
def index():
    """Render the main page."""
    return render_template('index.html')


def _filter_articles_by_date(articles: List[Dict[str, Any]], date_type: str,
                             start: Optional[str], end: Optional[str]) -> List[Dict[str, Any]]:
    """Filter cached articles by the selected date range."""
    if not date_type or date_type == 'all':
        return articles

    now = time.time()

    if date_type == 'custom':
        if not start and not end:
            return articles

        start_ts = 0.0
        end_ts = now

        if start:
            try:
                start_dt = datetime.strptime(start, "%Y-%m-%d")
                start_ts = start_dt.timestamp()
            except ValueError:
                pass

        if end:
            try:
                end_dt = datetime.strptime(end, "%Y-%m-%d")
                # Include entire day by moving to next day midnight minus epsilon
                end_ts = (end_dt.timestamp() + 86399.999)
            except ValueError:
                pass

        return [
            article for article in articles
            if not article.get("published_timestamp") or (
                start_ts <= article["published_timestamp"] <= end_ts
            )
        ]

    try:
        days = int(date_type)
    except ValueError:
        days = 7

    cutoff = now - (days * 24 * 60 * 60)
    return [
        article for article in articles
        if not article.get("published_timestamp") or article["published_timestamp"] >= cutoff
    ]


def _filter_articles_by_feeds(articles: List[Dict[str, Any]], feeds: Optional[List[str]]) -> List[Dict[str, Any]]:
    """Filter cached articles by selected feed names."""
    if not feeds:
        return articles

    feeds_set = set(feeds)
    return [article for article in articles if article.get("feed_name") in feeds_set]


@app.route('/api/data')
def api_data():
    """Get all articles and feeds."""
    data = get_cached_data()
    return jsonify({
        "articles": data["articles"],
        "feeds": data["feeds"],
        "categories": {
            "failure_modes": FAILURE_MODES,
            "primary_focuses": PRIMARY_FOCUSES
        }
    })


@app.route('/api/grid')
def api_grid():
    """Return counts for the category grid based on filters."""
    data = get_cached_data()
    articles = data["articles"]

    date_type = request.args.get('date', '7')
    date_start = request.args.get('dateStart')
    date_end = request.args.get('dateEnd')

    feeds_param = request.args.get('feeds')
    feeds = [f for f in feeds_param.split(',') if f] if feeds_param else None

    filtered = _filter_articles_by_date(articles, date_type, date_start, date_end)
    filtered = _filter_articles_by_feeds(filtered, feeds)

    counts = {
        f"{failure}:{focus}": 0
        for failure in FAILURE_MODES
        for focus in PRIMARY_FOCUSES
    }

    for article in filtered:
        failure_mode = article.get("failure_mode", "other")
        primary_focus = article.get("primary_focus", "other")
        key = f"{failure_mode}:{primary_focus}"
        if key in counts:
            counts[key] += 1

    return jsonify({"counts": counts})


@app.route('/api/refresh', methods=['POST'])
def api_refresh():
    """Force refresh the data."""
    refresh_data()
    return jsonify({"status": "ok", "message": "Data refreshed"})


if __name__ == '__main__':
    # Initial data load
    refresh_data()

    # Run the app
    app.run(debug=True, port=5000)
