"""
RSS Aggregator Web Application
Provides a web interface to filter, sort, and browse RSS feed articles with AI-generated metadata.
"""
import os
import re
import time
import json
import math
import calendar
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections.abc import Iterable
from dataclasses import dataclass, asdict

import feedparser
from flask import Flask, render_template, jsonify, request, send_from_directory
from bs4 import BeautifulSoup

app = Flask(__name__)

# Configuration
DOCS_DIR = Path(__file__).parent / "docs"
CACHE_DURATION = 300  # 5 minutes in seconds

# Categories from schema.json
FAILURE_MODES = ["human-misuse", "misalignment", "societal-disruption", "other", "non-applicable"]
PRIMARY_FOCUSES = ["interpretability", "alignment", "robustness", "control", "other"]

# Scoring configuration
WEIGHT_KEYS = [
    "interpretability",
    "understanding",
    "safety",
    "technicality",
    "nontechnicality",
    "surprisal",
]
DEFAULT_PAGE_SIZE = 100
MAX_PAGE_SIZE = 200

# Global cache
_cache: Dict[str, Any] = {
    "articles": [],
    "feeds": [],
    "last_refresh": 0
}


@dataclass
class Article:
    """Represents a parsed article with all metadata."""
    feed_name: str
    title: str
    link: str
    published: Optional[str]
    published_timestamp: Optional[float]
    summary_en: str
    summary_cn: str
    keywords: List[str]
    interpretability: int
    understanding: int
    safety: int
    technicality: int
    surprisal: int
    nontechnicality: int
    failure_mode: str
    primary_focus: str
    authors: Optional[List[str]]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)


def normalize_category(value: Optional[str], allowed: Iterable[str], default: str) -> str:
    """Normalize category strings to slug format defined in schema."""
    if not value:
        return default

    slug = re.sub(r'[^a-z0-9]+', '-', value.strip().lower()).strip('-')
    return slug if slug in set(allowed) else default


def extract_acme_data(item_node: Optional[BeautifulSoup]) -> Dict[str, Any]:
    """Extract structured metadata from the <acme> JSON block when present."""
    if item_node is None:
        return {}

    acme_tag = item_node.find("acme")
    if not acme_tag or not acme_tag.string:
        return {}

    try:
        data = json.loads(acme_tag.string)
    except json.JSONDecodeError:
        return {}

    summary_en = data.get("summary", "") or ""
    summary_cn = data.get("summary_cn", "") or ""

    raw_keywords = data.get("keywords", [])
    if isinstance(raw_keywords, str):
        keywords = [k.strip() for k in re.split(r'[,;]', raw_keywords) if k.strip()]
    elif isinstance(raw_keywords, Iterable):
        keywords = [str(k).strip() for k in raw_keywords if str(k).strip()]
    else:
        keywords = []

    scores_raw = data.get("scoring", {}) or {}
    scores: Dict[str, int] = {}
    for key, value in scores_raw.items():
        if not isinstance(key, str):
            continue
        try:
            scores[key.strip().lower()] = int(value)
        except (ValueError, TypeError):
            continue

    category_raw = data.get("category", {}) or {}
    failure_mode = category_raw.get("failure_mode_addressed") or category_raw.get("failure_mode") or ""
    primary_focus = category_raw.get("primary_focus") or ""

    authors = data.get("authors", [])
    if isinstance(authors, str):
        authors = [authors]
    elif isinstance(authors, Iterable):
        authors = [str(a).strip() for a in authors if str(a).strip()]
    else:
        authors = []

    return {
        "summary_en": summary_en,
        "summary_cn": summary_cn,
        "keywords": keywords,
        "scores": scores,
        "categories": {
            "failure_mode": failure_mode,
            "primary_focus": primary_focus
        },
        "authors": authors
    }


def parse_summary_html(summary_html: str) -> Dict[str, Any]:
    """
    Parse the structured HTML summary generated by main.py.

    Example format:
    <strong>Summary:</strong> text<br>
    <strong>Summary (CN):</strong> text<br>
    <strong>Keywords:</strong> keyword1, keyword2<br>
    <strong>Scores:</strong> Interpretability: 8, Understanding: 7, ...<br>
    <strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment
    """
    result = {
        "summary_en": "",
        "summary_cn": "",
        "keywords": [],
        "scores": {},
        "categories": {}
    }

    if not summary_html:
        return result

    # Split by <br> or <br/>
    parts = re.split(r'<br\s*/?>', summary_html, flags=re.IGNORECASE)

    for part in parts:
        part = part.strip()
        if not part:
            continue

        # Remove HTML tags for parsing
        text = BeautifulSoup(part, "html.parser").get_text()

        if text.startswith("Summary:"):
            result["summary_en"] = text.replace("Summary:", "").strip()
        elif text.startswith("Summary (CN):"):
            result["summary_cn"] = text.replace("Summary (CN):", "").strip()
        elif text.startswith("Keywords:"):
            keywords_str = text.replace("Keywords:", "").strip()
            result["keywords"] = [k.strip() for k in keywords_str.split(",") if k.strip()]
        elif text.startswith("Scores:"):
            scores_str = text.replace("Scores:", "").strip()
            # Parse "Interpretability: 8, Understanding: 7, ..."
            for score_part in scores_str.split(","):
                if ":" in score_part:
                    key, value = score_part.split(":", 1)
                    key = key.strip().lower()
                    try:
                        result["scores"][key] = int(value.strip())
                    except ValueError:
                        pass
        elif text.startswith("Categories:"):
            categories_str = text.replace("Categories:", "").strip()
            # Parse "Failure mode - misalignment; Primary focus - alignment"
            for cat_part in categories_str.split(";"):
                if "Failure mode -" in cat_part:
                    result["categories"]["failure_mode"] = cat_part.split("-", 1)[1].strip()
                elif "Primary focus -" in cat_part:
                    result["categories"]["primary_focus"] = cat_part.split("-", 1)[1].strip().split()[0]

    return result


def get_entry_metadata(entry: Any, item_node: Optional[BeautifulSoup]) -> Dict[str, Any]:
    """Obtain structured metadata for a feed entry, preferring ACME JSON."""
    acme_data = extract_acme_data(item_node)
    if acme_data:
        return acme_data

    summary_html = getattr(entry, 'summary', '')
    return parse_summary_html(summary_html)


def parse_xml_file(xml_path: Path) -> List[Article]:
    """Parse a single XML feed file and extract all articles."""
    articles = []
    feed_name = xml_path.stem  # filename without .xml extension

    try:
        with open(xml_path, 'r', encoding='utf-8') as f:
            content = f.read()

        feed = feedparser.parse(content)
        soup = BeautifulSoup(content, "xml")
        item_nodes = soup.find_all("item")

        for idx, entry in enumerate(feed.entries):
            item_node = item_nodes[idx] if idx < len(item_nodes) else None
            parsed = get_entry_metadata(entry, item_node)

            # Get publication date
            published = getattr(entry, 'published', None) or getattr(entry, 'updated', None)
            published_timestamp = None

            try:
                published_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
                if published_struct:
                    published_timestamp = float(calendar.timegm(published_struct))
            except (TypeError, ValueError, OverflowError):
                published_timestamp = None

            if published_timestamp is None and published:
                try:
                    from email.utils import parsedate_to_datetime
                    dt = parsedate_to_datetime(published)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    published_timestamp = float(dt.timestamp())
                except Exception:
                    published_timestamp = None

            # Extract scores with defaults
            scores = parsed.get("scores", {})

            # Extract categories with defaults
            categories = parsed.get("categories", {})
            failure_mode = normalize_category(
                categories.get("failure_mode", "other"),
                FAILURE_MODES,
                "other",
            )
            primary_focus = normalize_category(
                categories.get("primary_focus", "other"),
                PRIMARY_FOCUSES,
                "other",
            )

            technicality_score = int(scores.get("technicality", 0) or 0)
            nontechnicality_score = max(0, 11 - technicality_score)

            article = Article(
                feed_name=feed_name,
                title=getattr(entry, 'title', 'Untitled'),
                link=getattr(entry, 'link', ''),
                published=published,
                published_timestamp=published_timestamp,
                summary_en=parsed.get("summary_en", ""),
                summary_cn=parsed.get("summary_cn", ""),
                keywords=parsed.get("keywords", []),
                interpretability=scores.get("interpretability", 0),
                understanding=scores.get("understanding", 0),
                safety=scores.get("safety", 0),
                technicality=technicality_score,
                surprisal=scores.get("surprisal", 0),
                nontechnicality=nontechnicality_score,
                failure_mode=failure_mode,
                primary_focus=primary_focus,
                authors=parsed.get("authors", None)
            )

            articles.append(article)

    except Exception as e:
        print(f"Error parsing {xml_path}: {e}")

    return articles


def refresh_data():
    """Scan all XML files in docs/ and parse articles."""
    global _cache

    articles = []
    feeds = set()

    # Find all XML files in docs/
    if DOCS_DIR.exists():
        for xml_file in DOCS_DIR.glob("*.xml"):
            parsed_articles = parse_xml_file(xml_file)
            articles.extend(parsed_articles)
            if parsed_articles:
                feeds.add(xml_file.stem)

    _cache["articles"] = [article.to_dict() for article in articles]
    _cache["feeds"] = sorted(list(feeds))
    _cache["last_refresh"] = time.time()

    print(f"Refreshed data: {len(articles)} articles from {len(feeds)} feeds")


def get_cached_data():
    """Get cached data, refreshing if needed."""
    current_time = time.time()

    if current_time - _cache["last_refresh"] > CACHE_DURATION:
        refresh_data()

    return _cache


@app.route('/')
def index():
    """Render the main page."""
    return render_template('index.html')


@app.route('/docs/<path:filename>')
def serve_docs(filename: str):
    """Serve static files from the docs directory."""
    return send_from_directory(str(DOCS_DIR), filename, conditional=True)


def _filter_articles_by_date(articles: List[Dict[str, Any]], date_type: str,
                             start: Optional[str], end: Optional[str]) -> List[Dict[str, Any]]:
    """Filter cached articles by the selected date range."""
    if not date_type or date_type == 'all':
        return articles

    now = time.time()

    if date_type == 'custom':
        if not start and not end:
            return articles

        start_ts = 0.0
        end_ts = now

        if start:
            try:
                start_dt = datetime.strptime(start, "%Y-%m-%d")
                start_ts = start_dt.timestamp()
            except ValueError:
                pass

        if end:
            try:
                end_dt = datetime.strptime(end, "%Y-%m-%d")
                # Include entire day by moving to next day midnight minus epsilon
                end_ts = (end_dt.timestamp() + 86399.999)
            except ValueError:
                pass

        filtered_custom: List[Dict[str, Any]] = []
        for article in articles:
            ts_raw = article.get("published_timestamp")
            if ts_raw is None:
                continue
            try:
                ts = float(ts_raw)
            except (TypeError, ValueError):
                continue
            if start_ts <= ts <= end_ts:
                filtered_custom.append(article)
        return filtered_custom

    try:
        days = int(date_type)
    except ValueError:
        days = 7

    cutoff = now - (days * 24 * 60 * 60)
    filtered_recent: List[Dict[str, Any]] = []
    for article in articles:
        ts_raw = article.get("published_timestamp")
        if ts_raw is None:
            continue
        try:
            ts = float(ts_raw)
        except (TypeError, ValueError):
            continue
        if ts >= cutoff:
            filtered_recent.append(article)
    return filtered_recent


def _filter_articles_by_feeds(articles: List[Dict[str, Any]], feeds: Optional[List[str]]) -> List[Dict[str, Any]]:
    """Filter cached articles by selected feed names."""
    if not feeds:
        return articles

    feeds_set = set(feeds)
    return [article for article in articles if article.get("feed_name") in feeds_set]


def _filter_articles_by_categories(articles: List[Dict[str, Any]],
                                   failure_modes: Optional[List[str]],
                                   primary_focuses: Optional[List[str]]) -> List[Dict[str, Any]]:
    """Filter articles by failure modes and primary focuses."""
    if not failure_modes and not primary_focuses:
        return articles

    failure_set = set(failure_modes) if failure_modes else None
    focus_set = set(primary_focuses) if primary_focuses else None

    filtered: List[Dict[str, Any]] = []
    for article in articles:
        failure_value = article.get("failure_mode")
        focus_value = article.get("primary_focus")
        failure_match = True if failure_set is None else failure_value in failure_set
        focus_match = True if focus_set is None else focus_value in focus_set
        if failure_match and focus_match:
            filtered.append(article)
    return filtered


def _parse_csv_param(raw_value: Optional[str], allowed: Optional[Iterable[str]] = None) -> Optional[List[str]]:
    """Parse a comma-separated string into a validated list."""
    if not raw_value:
        return None

    items = [item.strip() for item in raw_value.split(',') if item.strip()]
    if allowed is not None:
        allowed_set = set(allowed)
        items = [item for item in items if item in allowed_set]
    return items or None


def _parse_weights(raw_value: Optional[str]) -> Dict[str, float]:
    """Parse slider weights into a normalized weight mapping."""
    weights = {key: 0.0 for key in WEIGHT_KEYS}

    if raw_value:
        parts = raw_value.split(',')
        for idx, part in enumerate(parts):
            if idx >= len(WEIGHT_KEYS):
                break
            try:
                value = float(part)
            except (TypeError, ValueError):
                continue
            weights[WEIGHT_KEYS[idx]] = max(0.0, value)

    total = sum(weights.values())
    if total <= 0:
        defaults: Dict[str, float] = {}
        baseline_keys = [key for key in WEIGHT_KEYS if key != "nontechnicality"]
        if baseline_keys:
            equal_weight = 1.0 / len(baseline_keys)
            for key in baseline_keys:
                defaults[key] = equal_weight
        else:
            # Fallback to equal distribution if nontechnicality is the only key
            equal_weight = 1.0 / len(WEIGHT_KEYS) if WEIGHT_KEYS else 0.0
            for key in WEIGHT_KEYS:
                defaults[key] = equal_weight
        defaults["nontechnicality"] = 0.0
        return defaults

    return {key: weight / total for key, weight in weights.items()}


def _calculate_score(article: Dict[str, Any], normalized_weights: Dict[str, float]) -> float:
    """Calculate a weighted score for an article."""
    score = 0.0
    for key in WEIGHT_KEYS:
        if key == "nontechnicality":
            value = article.get("nontechnicality")
            if value is None:
                tech = article.get("technicality", 0) or 0
                value = max(0, 11 - int(tech))
        else:
            value = article.get(key, 0) or 0
        score += float(value) * normalized_weights.get(key, 0.0)
    return score


def _apply_filters(articles: List[Dict[str, Any]],
                   date_type: str,
                   date_start: Optional[str],
                   date_end: Optional[str],
                   feeds: Optional[List[str]],
                   failure_modes: Optional[List[str]],
                   primary_focuses: Optional[List[str]]) -> List[Dict[str, Any]]:
    """Apply all supported filters to the article list."""
    filtered = _filter_articles_by_date(articles, date_type, date_start, date_end)
    filtered = _filter_articles_by_feeds(filtered, feeds)
    filtered = _filter_articles_by_categories(filtered, failure_modes, primary_focuses)
    return filtered


@app.route('/api/data')
def api_data():
    """Get available feeds and category metadata."""
    data = get_cached_data()
    return jsonify({
        "feeds": data["feeds"],
        "categories": {
            "failure_modes": FAILURE_MODES,
            "primary_focuses": PRIMARY_FOCUSES
        },
        "total_articles": len(data["articles"]),
        "last_refresh": data["last_refresh"]
    })


@app.route('/api/articles')
def api_articles():
    """Return paginated, filtered, and sorted articles."""
    data = get_cached_data()
    articles = data["articles"]

    date_type = request.args.get('date', '7')
    date_start = request.args.get('dateStart')
    date_end = request.args.get('dateEnd')

    feeds = _parse_csv_param(request.args.get('feeds'))
    failure_modes = _parse_csv_param(request.args.get('failureModes'), FAILURE_MODES)
    primary_focuses = _parse_csv_param(request.args.get('primaryFocuses'), PRIMARY_FOCUSES)

    normalized_weights = _parse_weights(request.args.get('weights'))

    filtered = _apply_filters(
        articles,
        date_type=date_type,
        date_start=date_start,
        date_end=date_end,
        feeds=feeds,
        failure_modes=failure_modes,
        primary_focuses=primary_focuses
    )

    scored_articles = [
        (_calculate_score(article, normalized_weights), article)
        for article in filtered
    ]
    scored_articles.sort(key=lambda item: item[0], reverse=True)

    try:
        page = int(request.args.get('page', 1))
    except (TypeError, ValueError):
        page = 1
    try:
        page_size = int(request.args.get('pageSize', DEFAULT_PAGE_SIZE))
    except (TypeError, ValueError):
        page_size = DEFAULT_PAGE_SIZE

    page = max(1, page)
    page_size = max(1, min(page_size, MAX_PAGE_SIZE))

    total = len(scored_articles)
    if total == 0:
        total_pages = 0
        page = 1
    else:
        total_pages = math.ceil(total / page_size)
        if page > total_pages:
            page = total_pages

    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size
    page_items = scored_articles[start_idx:end_idx]

    payload: List[Dict[str, Any]] = []
    for score, article in page_items:
        article_copy = dict(article)
        article_copy["score"] = round(score, 5)
        payload.append(article_copy)

    start_item = start_idx + 1 if total else 0
    end_item = min(end_idx, total) if total else 0

    return jsonify({
        "articles": payload,
        "pagination": {
            "page": page,
            "page_size": page_size,
            "total": total,
            "total_pages": total_pages,
            "has_next": page < total_pages if total_pages else False,
            "has_prev": page > 1 and total > 0,
            "start": start_item,
            "end": end_item
        }
    })


@app.route('/api/grid')
def api_grid():
    """Return counts for the category grid based on filters."""
    data = get_cached_data()
    articles = data["articles"]

    date_type = request.args.get('date', '7')
    date_start = request.args.get('dateStart')
    date_end = request.args.get('dateEnd')

    feeds = _parse_csv_param(request.args.get('feeds'))
    failure_modes = _parse_csv_param(request.args.get('failureModes'), FAILURE_MODES)
    primary_focuses = _parse_csv_param(request.args.get('primaryFocuses'), PRIMARY_FOCUSES)

    filtered = _apply_filters(
        articles,
        date_type=date_type,
        date_start=date_start,
        date_end=date_end,
        feeds=feeds,
        failure_modes=failure_modes,
        primary_focuses=primary_focuses
    )

    counts = {
        f"{failure}:{focus}": 0
        for failure in FAILURE_MODES
        for focus in PRIMARY_FOCUSES
    }

    for article in filtered:
        failure_mode = article.get("failure_mode", "other")
        primary_focus = article.get("primary_focus", "other")
        key = f"{failure_mode}:{primary_focus}"
        if key in counts:
            counts[key] += 1

    return jsonify({"counts": counts})


@app.route('/api/refresh', methods=['POST'])
def api_refresh():
    """Force refresh the data."""
    refresh_data()
    return jsonify({"status": "ok", "message": "Data refreshed"})


if __name__ == '__main__':
    # Initial data load
    refresh_data()

    # Run the app
    app.run(debug=True, port=5000)
