<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Computation and Language</title>
<link>https://papers.cool/arxiv/cs.CL</link>

<item>
<title>How Do LLMs Use Their Depth?</title>
<link>https://papers.cool/arxiv/2510.18871</link>
<guid>https://papers.cool/arxiv/2510.18871</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how large language models allocate computation across layers, introducing a "Guess-then-Refine" framework where early layers produce high-frequency token guesses that are progressively refined with deeper contextual information. Through token frequency analysis, part-of-speech timing, fact recall, and multiple‑choice tasks, the authors show systematic patterns such as early prediction of function words and deeper processing for the first token of multi‑token answers. These findings provide a fine‑grained mechanistic view of LLM inference and suggest avenues for improving transformer efficiency.<br><strong>Summary (CN):</strong> 本文研究了大型语言模型在不同层次上如何分配计算，提出了“先猜测‑再精炼”(Guess-then-Refine) 框架：早期层主要生成高频词汇的统计猜测，随后在更深层中随着上下文信息的丰富逐步被精炼为合适的输出。通过对高频词预测、词性预测时序、事实回忆多标记答案的首词深度需求以及多项选择题的格式识别过程等案例分析，揭示了功能词最早被正确预测、首个答案标记需更多计算深度等规律。该工作提供了对 LLM 推理过程的细粒度机制理解，并为提升 Transformer 计算效率提供了启示。<br><strong>Keywords:</strong> layer-wise dynamics, depth utilization, guess-then-refine, token frequency, mechanistic interpretability, transformer analysis, computational efficiency<br><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 3, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not "one-and-done". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.
<div><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how large language models allocate computation across layers, introducing a \"Guess-then-Refine\" framework where early layers produce high-frequency token guesses that are progressively refined with deeper contextual information. Through token frequency analysis, part-of-speech timing, fact recall, and multiple‑choice tasks, the authors show systematic patterns such as early prediction of function words and deeper processing for the first token of multi‑token answers. These findings provide a fine‑grained mechanistic view of LLM inference and suggest avenues for improving transformer efficiency.", "summary_cn": "本文研究了大型语言模型在不同层次上如何分配计算，提出了“先猜测‑再精炼”(Guess-then-Refine) 框架：早期层主要生成高频词汇的统计猜测，随后在更深层中随着上下文信息的丰富逐步被精炼为合适的输出。通过对高频词预测、词性预测时序、事实回忆多标记答案的首词深度需求以及多项选择题的格式识别过程等案例分析，揭示了功能词最早被正确预测、首个答案标记需更多计算深度等规律。该工作提供了对 LLM 推理过程的细粒度机制理解，并为提升 Transformer 计算效率提供了启示。", "keywords": "layer-wise dynamics, depth utilization, guess-then-refine, token frequency, mechanistic interpretability, transformer analysis, computational efficiency", "scoring": {"interpretability": 8, "understanding": 8, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"]}
]]></acme>

<pubDate>2025-10-21T17:59:05+00:00</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18866</link>
<guid>https://papers.cool/arxiv/2510.18866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LightMem, a lightweight memory system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson-Shiffrin model. By compressing irrelevant data, grouping by topic, and decoupling long-term consolidation from online inference, LightMem significantly improves accuracy while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate up to 10.9% accuracy gains and efficiency improvements of up to 117× in token usage and 12× in runtime.<br><strong>Summary (CN):</strong> 本文提出 LightMem，一种受 Atkinson-Shiffrin 人类记忆模型启发的轻量级记忆系统，用于大型语言模型。该系统通过感官记忆的轻量压缩过滤、基于主题的短期记忆整合以及离线的长期记忆更新，实现了在保持或提升准确度的同时大幅降低 token 使用（最高 117 倍）和推理时间（超过 12 倍）。在 LongMemEval 上使用 GPT 和 Qwen 骨干模型的实验表明，LightMem 在准确率上提升至 10.9%，并显著减少 API 调用次数。<br><strong>Keywords:</strong> memory-augmented generation, lightweight memory, LLM efficiency, sensory memory compression, short-term memory consolidation, long-term memory update, token reduction, Atkinson-Shiffrin model<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
<div><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LightMem, a lightweight memory system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson-Shiffrin model. By compressing irrelevant data, grouping by topic, and decoupling long-term consolidation from online inference, LightMem significantly improves accuracy while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate up to 10.9% accuracy gains and efficiency improvements of up to 117× in token usage and 12× in runtime.", "summary_cn": "本文提出 LightMem，一种受 Atkinson-Shiffrin 人类记忆模型启发的轻量级记忆系统，用于大型语言模型。该系统通过感官记忆的轻量压缩过滤、基于主题的短期记忆整合以及离线的长期记忆更新，实现了在保持或提升准确度的同时大幅降低 token 使用（最高 117 倍）和推理时间（超过 12 倍）。在 LongMemEval 上使用 GPT 和 Qwen 骨干模型的实验表明，LightMem 在准确率上提升至 10.9%，并显著减少 API 调用次数。", "keywords": "memory-augmented generation, lightweight memory, LLM efficiency, sensory memory compression, short-term memory consolidation, long-term memory update, token reduction, Atkinson-Shiffrin model", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:58:17+00:00</pubDate>
</item>
<item>
<title>Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model</title>
<link>https://papers.cool/arxiv/2510.18855</link>
<guid>https://papers.cool/arxiv/2510.18855</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level discrepancy masking, C3PO++ for efficient rollout partitioning, and ASystem as a high‑performance RL framework—to overcome training‑inference misalignment and system bottlenecks. Empirical results show strong performance on a range of reasoning benchmarks, including AIME‑2025, HMMT‑2025, CodeForces, ARC‑AGI‑v1, and a silver‑medal level on IMO‑2025. By releasing the full MoE model, the work aims to democratize access to large‑scale reasoning capabilities.<br><strong>Summary (CN):</strong> 本文推出 Ring-1T，一个每个 token 激活约 500 亿参数的万亿规模思考模型，并提出三项关键创新：IcePop（基于 token 级差异掩码的 RL 稳定化）、C3PO++（在 token 预算下对长 rollout 动态分割提升效率）以及 ASystem（高性能 RL 框架），以解决训练‑推理不匹配和系统瓶颈问题。实验在 AIME‑2025、HMMT‑2025、CodeForces、ARC‑AGI‑v1 等基准上取得突破性成绩，并在 IMO‑2025 达到银牌水平。论文通过开源完整的 MoE 模型，旨在让社区直接使用前沿的推理能力。<br><strong>Keywords:</strong> trillion-parameter model, reinforcement learning, scaling, MoE, token-level masking, RL training stability, high-performance RL framework, reasoning AI<br><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.
<div><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level discrepancy masking, C3PO++ for efficient rollout partitioning, and ASystem as a high‑performance RL framework—to overcome training‑inference misalignment and system bottlenecks. Empirical results show strong performance on a range of reasoning benchmarks, including AIME‑2025, HMMT‑2025, CodeForces, ARC‑AGI‑v1, and a silver‑medal level on IMO‑2025. By releasing the full MoE model, the work aims to democratize access to large‑scale reasoning capabilities.", "summary_cn": "本文推出 Ring-1T，一个每个 token 激活约 500 亿参数的万亿规模思考模型，并提出三项关键创新：IcePop（基于 token 级差异掩码的 RL 稳定化）、C3PO++（在 token 预算下对长 rollout 动态分割提升效率）以及 ASystem（高性能 RL 框架），以解决训练‑推理不匹配和系统瓶颈问题。实验在 AIME‑2025、HMMT‑2025、CodeForces、ARC‑AGI‑v1 等基准上取得突破性成绩，并在 IMO‑2025 达到银牌水平。论文通过开源完整的 MoE 模型，旨在让社区直接使用前沿的推理能力。", "keywords": "trillion-parameter model, reinforcement learning, scaling, MoE, token-level masking, RL training stability, high-performance RL framework, reasoning AI", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Team", "Anqi Shen", "Baihui Li", "Bin Hu", "Bin Jing", "Cai Chen", "Chao Huang", "Chao Zhang", "Chaokun Yang", "Cheng Lin", "Chengyao Wen", "Congqi Li", "Deng Zhao", "Dingbo Yuan", "Donghai You", "Fagui Mao", "Fanzhuang Meng", "Feng Xu", "Guojie Li", "Guowei Wang", "Hao Dai", "Haonan Zheng", "Hong Liu", "Jia Guo", "Jiaming Liu", "Jian Liu", "Jianhao Fu", "Jiannan Shi", "Jianwen Wang", "Jianxin Lai", "Jin Yang", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junping Zhao", "Kuan Xu", "Le Su", "Lei Chen", "Li Tang", "Liang Jiang", "Liangcheng Fu", "Lianhao Xu", "Linfeng Shi", "Lisha Liao", "Longfei Zheng", "Meng Li", "Mingchun Chen", "Qi Zuo", "Qiang Cheng", "Qianggang Cao", "Qitao Shi", "Quanrui Guo", "Senlin Zhu", "Shaofei Wang", "Shaomian Zheng", "Shuaicheng Li", "Shuwei Gu", "Siba Chen", "Tao Wu", "Tao Zhang", "Tianyu Zhang", "Tianyu Zhou", "Tiwei Bie", "Tongkai Yang", "Wang Hong", "Wang Ren", "Weihua Chen", "Wenbo Yu", "Wengang Zheng", "Xiangchun Wang", "Xiaodong Yan", "Xiaopei Wan", "Xin Zhao", "Xinyu Kong", "Xinyu Tang", "Xudong Han", "Xudong Wang", "Xuemin Yang", "Xueyu Hu", "Yalin Zhang", "Yan Sun", "Yicheng Shan", "Yilong Wang", "Yingying Xu", "Yongkang Liu", "Yongzhen Guo", "Yuanyuan Wang", "Yuchen Yan", "Yuefan Wang", "Yuhong Guo", "Zehuan Li", "Zhankai Xu", "Zhe Li", "Zhenduo Zhang", "Zhengke Gui", "Zhenxuan Pan", "Zhenyu Huang", "Zhenzhong Lan", "Zhiqiang Ding", "Zhiqiang Zhang", "Zhixun Li", "Zhizhen Liu", "Zihao Wang", "Zujie Wen"]}
]]></acme>

<pubDate>2025-10-21T17:46:14+00:00</pubDate>
</item>
<item>
<title>Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18849</link>
<guid>https://papers.cool/arxiv/2510.18849</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a Critique-Post-Edit reinforcement learning framework that uses a personalized generative reward model providing multi‑dimensional scores and textual critiques to mitigate reward hacking and improve faithful, controllable LLM personalization. The policy revises its outputs based on these critiques, leading to significant gains over standard PPO on personalization benchmarks, with a 7B model achieving an 11% win‑rate increase and a 14B model surpassing GPT‑4.1 performance. This demonstrates a practical route to more efficient and reliable personalized language generation.<br><strong>Summary (CN):</strong> 本文提出了一种 Critique-Post-Edit 强化学习框架，利用个性化生成奖励模型（GRM）提供多维评分和文本批评，以抵御奖励欺骗并提升 LLM 的忠实、可控个性化。策略模型依据批评自行修正输出，在个性化基准测试中显著优于标准 PPO，7B 模型实现 11% 胜率提升，14B 模型的表现甚至超越 GPT‑4.1，展示了实现更高效可靠个性化生成的可行路径。<br><strong>Keywords:</strong> personalization, reinforcement learning, reward hacking, generative reward model, critique-post-edit, LLM alignment, controllable generation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.
<div><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a Critique-Post-Edit reinforcement learning framework that uses a personalized generative reward model providing multi‑dimensional scores and textual critiques to mitigate reward hacking and improve faithful, controllable LLM personalization. The policy revises its outputs based on these critiques, leading to significant gains over standard PPO on personalization benchmarks, with a 7B model achieving an 11% win‑rate increase and a 14B model surpassing GPT‑4.1 performance. This demonstrates a practical route to more efficient and reliable personalized language generation.", "summary_cn": "本文提出了一种 Critique-Post-Edit 强化学习框架，利用个性化生成奖励模型（GRM）提供多维评分和文本批评，以抵御奖励欺骗并提升 LLM 的忠实、可控个性化。策略模型依据批评自行修正输出，在个性化基准测试中显著优于标准 PPO，7B 模型实现 11% 胜率提升，14B 模型的表现甚至超越 GPT‑4.1，展示了实现更高效可靠个性化生成的可行路径。", "keywords": "personalization, reinforcement learning, reward hacking, generative reward model, critique-post-edit, LLM alignment, controllable generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Chenghao Zhu", "Meiling Tao", "Tiannan Wang", "Dongyi Ding", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"]}
]]></acme>

<pubDate>2025-10-21T17:40:03+00:00</pubDate>
</item>
<item>
<title>MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training</title>
<link>https://papers.cool/arxiv/2510.18830</link>
<guid>https://papers.cool/arxiv/2510.18830</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long contexts. By applying MTraining, the authors expand the context window of Qwen2.5‑3B from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining model accuracy on various downstream tasks.<br><strong>Summary (CN):</strong> 本文提出了 MTraining——一种分布式训练框架，融合了动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，以高效训练具备超长上下文的 大语言模型。作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文窗口从 32K 扩展至 512K token，并在多个下游任务上保持模型精度的同时，实现了最高 6 倍 的训练吞吐提升。<br><strong>Keywords:</strong> dynamic sparse attention, distributed training, ultra-long context, ring attention, hierarchical attention, LLM efficiency<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.
<div><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long contexts. By applying MTraining, the authors expand the context window of Qwen2.5‑3B from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining model accuracy on various downstream tasks.", "summary_cn": "本文提出了 MTraining——一种分布式训练框架，融合了动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，以高效训练具备超长上下文的 大语言模型。作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文窗口从 32K 扩展至 512K token，并在多个下游任务上保持模型精度的同时，实现了最高 6 倍 的训练吞吐提升。", "keywords": "dynamic sparse attention, distributed training, ultra-long context, ring attention, hierarchical attention, LLM efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Li", "Chengruidong Zhang", "Huiqiang Jiang", "Yucheng Li", "Yuqing Yang", "Lili Qiu"]}
]]></acme>

<pubDate>2025-10-21T17:25:32+00:00</pubDate>
</item>
<item>
<title>Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</title>
<link>https://papers.cool/arxiv/2510.18817</link>
<guid>https://papers.cool/arxiv/2510.18817</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a knowledge‑distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑specific models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the authors fine‑tune small language models that achieve significantly improved reasoning performance, narrowing the gap with larger models.<br><strong>Summary (CN):</strong> 本文提出了一种知识蒸馏框架，将大语言模型的 chain‑of‑thought 推理能力转移到小型、面向工业资产健康监测的模型上。通过多选题提示和上下文学习，对小模型进行微调，使其在推理表现上显著提升，缩小了与大模型之间的差距。<br><strong>Keywords:</strong> chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning transfer<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: https://github.com/IBM/FailureSensorIQ.
<div><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a knowledge‑distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑specific models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the authors fine‑tune small language models that achieve significantly improved reasoning performance, narrowing the gap with larger models.", "summary_cn": "本文提出了一种知识蒸馏框架，将大语言模型的 chain‑of‑thought 推理能力转移到小型、面向工业资产健康监测的模型上。通过多选题提示和上下文学习，对小模型进行微调，使其在推理表现上显著提升，缩小了与大模型之间的差距。", "keywords": "chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuxin Lin", "Dhaval Patel", "Christodoulos Constantinides"]}
]]></acme>

<pubDate>2025-10-21T17:18:24+00:00</pubDate>
</item>
<item>
<title>WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection</title>
<link>https://papers.cool/arxiv/2510.18798</link>
<guid>https://papers.cool/arxiv/2510.18798</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> WebSeer introduces a reinforcement‑learning framework enhanced with a self‑reflection mechanism to train more capable web search agents that can generate longer, more reflective tool‑use sequences. By constructing a large dataset annotated with reflection patterns and employing a two‑stage training process that combines cold‑start and RL, the authors enable a single14‑billion‑parameter model to achieve state‑of‑the‑art results on HotpotQA and SimpleQA and to generalize to out‑of‑distribution tasks.<br><strong>Summary (CN):</strong> WebSeer 提出了一种结合 (self‑reflection) 机制的强化学习框架，用于训练能够生成更长、更具反思性的工具使用序列的网页搜索代理。通过构建带有反模式标注的大规模数据集，并采用冷启动与强化学习相结合的两阶段流程，单一 14B 模型在 HotpotQA 与 SimpleQA 上实现了最先进的准确率，并展示了对分布外数据的强泛化能力。<br><strong>Keywords:</strong> search agents, reinforcement learning, self-reflection, tool-use depth, web retrieval, HotpotQA, SimpleQA, dataset annotation, interactive retrieval<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, Juanzi Li</div>
Search agents have achieved significant advancements in enabling intelligent information retrieval and decision-making within interactive environments. Although reinforcement learning has been employed to train agentic models capable of more dynamic interactive retrieval, existing methods are limited by shallow tool-use depth and the accumulation of errors over multiple iterative interactions. In this paper, we present WebSeer, a more intelligent search agent trained via reinforcement learning enhanced with a self-reflection mechanism. Specifically, we construct a large dataset annotated with reflection patterns and design a two-stage training framework that unifies cold start and reinforcement learning within the self-reflection paradigm for real-world web-based environments, which enables the model to generate longer and more reflective tool-use trajectories. Our approach substantially extends tool-use chains and improves answer accuracy. Using a single 14B model, we achieve state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and 90.0%, respectively, and demonstrate strong generalization to out-of-distribution datasets. The code is available at https://github.com/99hgz/WebSeer
<div><strong>Authors:</strong> Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, Juanzi Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "WebSeer introduces a reinforcement‑learning framework enhanced with a self‑reflection mechanism to train more capable web search agents that can generate longer, more reflective tool‑use sequences. By constructing a large dataset annotated with reflection patterns and employing a two‑stage training process that combines cold‑start and RL, the authors enable a single14‑billion‑parameter model to achieve state‑of‑the‑art results on HotpotQA and SimpleQA and to generalize to out‑of‑distribution tasks.", "summary_cn": "WebSeer 提出了一种结合 (self‑reflection) 机制的强化学习框架，用于训练能够生成更长、更具反思性的工具使用序列的网页搜索代理。通过构建带有反模式标注的大规模数据集，并采用冷启动与强化学习相结合的两阶段流程，单一 14B 模型在 HotpotQA 与 SimpleQA 上实现了最先进的准确率，并展示了对分布外数据的强泛化能力。", "keywords": "search agents, reinforcement learning, self-reflection, tool-use depth, web retrieval, HotpotQA, SimpleQA, dataset annotation, interactive retrieval", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guanzhong He", "Zhen Yang", "Jinxin Liu", "Bin Xu", "Lei Hou", "Juanzi Li"]}
]]></acme>

<pubDate>2025-10-21T16:52:00+00:00</pubDate>
</item>
<item>
<title>KAT-Coder Technical Report</title>
<link>https://papers.cool/arxiv/2510.18779</link>
<guid>https://papers.cool/arxiv/2510.18779</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The report presents KAT-Coder, a 32B agentic code model trained via a multi-stage curriculum—including Mid-Term training, supervised fine‑tuning, reinforcement fine‑tuning with a novel multi‑ground‑truth reward, and deployment‑phase adaptation—to improve reasoning, planning, tool‑use reliability, and instruction alignment for real‑world IDE environments. The authors detail dataset construction across many languages and contexts, introduce error‑masked SFT and tree‑structured trajectory training, and release the model publicly. Empirical results show robust performance on interactive software development tasks.<br><strong>Summary (CN):</strong> 本文报告了 KAT‑Coder，一款 32B 规模的代理式代码模型，通过多阶段课程（包括中期训练、监督微调、使用多真值奖励的强化微调以及部署阶段的适配）提升推理、规划、工具使用可靠性和指令对齐，以在真实 IDE 环境中进行交互式软件开发。作者介绍了跨多语言多场景的数据集构建，提出了错误掩码微调和树结构轨迹训练，并已公开模型。实验表明该模型在交互式编码任务上表现稳健。<br><strong>Keywords:</strong> agentic coding, reinforcement learning from human feedback, tool-use reliability, instruction alignment, large language models, curriculum learning, code generation, IDE integration, multi-ground-truth reward, tree-structured trajectory training<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</div>
Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.
<div><strong>Authors:</strong> Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The report presents KAT-Coder, a 32B agentic code model trained via a multi-stage curriculum—including Mid-Term training, supervised fine‑tuning, reinforcement fine‑tuning with a novel multi‑ground‑truth reward, and deployment‑phase adaptation—to improve reasoning, planning, tool‑use reliability, and instruction alignment for real‑world IDE environments. The authors detail dataset construction across many languages and contexts, introduce error‑masked SFT and tree‑structured trajectory training, and release the model publicly. Empirical results show robust performance on interactive software development tasks.", "summary_cn": "本文报告了 KAT‑Coder，一款 32B 规模的代理式代码模型，通过多阶段课程（包括中期训练、监督微调、使用多真值奖励的强化微调以及部署阶段的适配）提升推理、规划、工具使用可靠性和指令对齐，以在真实 IDE 环境中进行交互式软件开发。作者介绍了跨多语言多场景的数据集构建，提出了错误掩码微调和树结构轨迹训练，并已公开模型。实验表明该模型在交互式编码任务上表现稳健。", "keywords": "agentic coding, reinforcement learning from human feedback, tool-use reliability, instruction alignment, large language models, curriculum learning, code generation, IDE integration, multi-ground-truth reward, tree-structured trajectory training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zizheng Zhan", "Ken Deng", "Xiaojiang Zhang", "Jinghui Wang", "Huaixi Tang", "Zhiyi Lai", "Haoyang Huang", "Wen Xiang", "Kun Wu", "Wenhao Zhuang", "Minglei Zhang", "Shaojie Wang", "Shangpeng Yan", "Kepeng Lei", "Zongxian Feng", "Huiming Wang", "Zheng Lin", "Mengtong Li", "Mengfei Xie", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Weihao Li", "Wenqiang Zhu", "Jiarong Zhang", "Jingxuan Xu", "Songwei Yu", "Yifan Yao", "Xinping Lei", "Han Li", "Junqi Xiong", "Zuchen Gao", "Dailin Li", "Haimo Li", "Jiaheng Liu", "Yuqun Zhang", "Junyi Peng", "Haotian Zhang", "Bin Chen"]}
]]></acme>

<pubDate>2025-10-21T16:27:47+00:00</pubDate>
</item>
<item>
<title>AI use in American newspapers is widespread, uneven, and rarely disclosed</title>
<link>https://papers.cool/arxiv/2510.18774</link>
<guid>https://papers.cool/arxiv/2510.18774</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The authors audit 186K articles from 1.5K American newspapers using the Pangram AI detector and find that roughly 9% of articles are partially or fully AI-generated, with higher prevalence in smaller outlets, certain topics, and specific ownership groups. Opinion pieces from major publications are 6.4 times more likely to contain AI-generated content, yet disclosures are extremely rare, with only five out of 100 flagged articles providing any notice. The study calls for greater transparency and updated editorial standards to preserve public trust.<br><strong>Summary (CN):</strong> 作者审计了 186,000 篇来自 1,500 家美国报纸的文章，使用 Pangram AI 检测器发现约 9% 的文章部分或全部由 AI 生成，且在规模较小的本地媒体、特定话题（如天气和科技）以及某些所有权集团中更为常见。对《华盛顿邮报》《纽约时报》和《华尔街日报》的 45,000 篇评论文章进行分析后发现，这类社论稿件的 AI 生成概率是同一出版物新闻稿的 6.4 倍，但披露极其罕见，仅在 100 篇被标记的文章中出现了五次披露。研究呼吁加强透明度并更新编辑标准，以维护公众信任。<br><strong>Keywords:</strong> AI-generated content, journalism, AI detection, media transparency, AI misuse, newspaper audit, Pangram detector, opinion pieces, disclosure<br><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br><strong>Authors:</strong> Jenna Russell, Marzena Karpinska, Destiny Akinode, Katherine Thai, Bradley Emi, Max Spero, Mohit Iyyer</div>
AI is rapidly transforming journalism, but the extent of its use in published newspaper articles remains unclear. We address this gap by auditing a large-scale dataset of 186K articles from online editions of 1.5K American newspapers published in the summer of 2025. Using Pangram, a state-of-the-art AI detector, we discover that approximately 9% of newly-published articles are either partially or fully AI-generated. This AI use is unevenly distributed, appearing more frequently in smaller, local outlets, in specific topics such as weather and technology, and within certain ownership groups. We also analyze 45K opinion pieces from Washington Post, New York Times, and Wall Street Journal, finding that they are 6.4 times more likely to contain AI-generated content than news articles from the same publications, with many AI-flagged op-eds authored by prominent public figures. Despite this prevalence, we find that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles found only five disclosures of AI use. Overall, our audit highlights the immediate need for greater transparency and updated editorial standards regarding the use of AI in journalism to maintain public trust.
<div><strong>Authors:</strong> Jenna Russell, Marzena Karpinska, Destiny Akinode, Katherine Thai, Bradley Emi, Max Spero, Mohit Iyyer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The authors audit 186K articles from 1.5K American newspapers using the Pangram AI detector and find that roughly 9% of articles are partially or fully AI-generated, with higher prevalence in smaller outlets, certain topics, and specific ownership groups. Opinion pieces from major publications are 6.4 times more likely to contain AI-generated content, yet disclosures are extremely rare, with only five out of 100 flagged articles providing any notice. The study calls for greater transparency and updated editorial standards to preserve public trust.", "summary_cn": "作者审计了 186,000 篇来自 1,500 家美国报纸的文章，使用 Pangram AI 检测器发现约 9% 的文章部分或全部由 AI 生成，且在规模较小的本地媒体、特定话题（如天气和科技）以及某些所有权集团中更为常见。对《华盛顿邮报》《纽约时报》和《华尔街日报》的 45,000 篇评论文章进行分析后发现，这类社论稿件的 AI 生成概率是同一出版物新闻稿的 6.4 倍，但披露极其罕见，仅在 100 篇被标记的文章中出现了五次披露。研究呼吁加强透明度并更新编辑标准，以维护公众信任。", "keywords": "AI-generated content, journalism, AI detection, media transparency, AI misuse, newspaper audit, Pangram detector, opinion pieces, disclosure", "scoring": {"interpretability": 1, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jenna Russell", "Marzena Karpinska", "Destiny Akinode", "Katherine Thai", "Bradley Emi", "Max Spero", "Mohit Iyyer"]}
]]></acme>

<pubDate>2025-10-21T16:22:07+00:00</pubDate>
</item>
<item>
<title>Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting</title>
<link>https://papers.cool/arxiv/2510.18745</link>
<guid>https://papers.cool/arxiv/2510.18745</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Topoformer, a transformer variant that imposes a 2D topographic layout on queries, keys, and values through spatial querying and spatial reweighting, enabling interpretable spatial organization of linguistic representations. Experiments on sentiment classification and masked language modeling show performance comparable to standard models while yielding clear topographic patterns that align with human brain fMRI responses to sentences. The authors argue that scaling such topographic constraints could improve interpretability in NLP and provide more accurate computational models of brain language organization.<br><strong>Summary (CN):</strong> 本文提出了 Topoformer，一种通过空间查询和空间重加权在二维网格上组织查询、键和值的 Transformer 变体，从而实现语言表示的可解释拓扑结构。实验在情感分类和掩码语言建模任务上显示，其性能与标准模型相当，但能够产生清晰的拓扑模式，并且这些模式与人类大脑对自然语言句子的 fMRI 反应呈现对齐。作者认为，进一步扩大此类拓扑约束有望提升 NLP 的可解释性，并为大脑语言网络提供更精确的计算模型。<br><strong>Keywords:</strong> topographic organization, transformer, spatial querying, spatial reweighting, mechanistic interpretability, neural representation, fMRI alignment, language models<br><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 8<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br><strong>Authors:</strong> Taha Binhuraib, Greta Tuckute, Nicholas Blauch</div>
Spatial functional organization is a hallmark of biological brains: neurons are arranged topographically according to their response properties, at multiple scales. In contrast, representations within most machine learning models lack spatial biases, instead manifesting as disorganized vector spaces that are difficult to visualize and interpret. Here, we propose a novel form of self-attention that turns Transformers into "Topoformers" with topographic organization. We introduce spatial querying - where keys and queries are arranged on 2D grids, and local pools of queries are associated with a given key - and spatial reweighting, where we convert the standard fully connected layer of self-attention into a locally connected layer. We first demonstrate the feasibility of our approach by training a 1-layer Topoformer on a sentiment classification task. Training with spatial querying encourages topographic organization in the queries and keys, and spatial reweighting separately encourages topographic organization in the values and self-attention outputs. We then apply the Topoformer motifs at scale, training a BERT architecture with a masked language modeling objective. We find that the topographic variant performs on par with a non-topographic control model on NLP benchmarks, yet produces interpretable topographic organization as evaluated via eight linguistic test suites. Finally, analyzing an fMRI dataset of human brain responses to a large set of naturalistic sentences, we demonstrate alignment between low-dimensional topographic variability in the Topoformer model and human brain language network. Scaling up Topoformers further holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic information in the human brain.
<div><strong>Authors:</strong> Taha Binhuraib, Greta Tuckute, Nicholas Blauch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Topoformer, a transformer variant that imposes a 2D topographic layout on queries, keys, and values through spatial querying and spatial reweighting, enabling interpretable spatial organization of linguistic representations. Experiments on sentiment classification and masked language modeling show performance comparable to standard models while yielding clear topographic patterns that align with human brain fMRI responses to sentences. The authors argue that scaling such topographic constraints could improve interpretability in NLP and provide more accurate computational models of brain language organization.", "summary_cn": "本文提出了 Topoformer，一种通过空间查询和空间重加权在二维网格上组织查询、键和值的 Transformer 变体，从而实现语言表示的可解释拓扑结构。实验在情感分类和掩码语言建模任务上显示，其性能与标准模型相当，但能够产生清晰的拓扑模式，并且这些模式与人类大脑对自然语言句子的 fMRI 反应呈现对齐。作者认为，进一步扩大此类拓扑约束有望提升 NLP 的可解释性，并为大脑语言网络提供更精确的计算模型。", "keywords": "topographic organization, transformer, spatial querying, spatial reweighting, mechanistic interpretability, neural representation, fMRI alignment, language models", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Taha Binhuraib", "Greta Tuckute", "Nicholas Blauch"]}
]]></acme>

<pubDate>2025-10-21T15:54:57+00:00</pubDate>
</item>
<item>
<title>Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</title>
<link>https://papers.cool/arxiv/2510.18731</link>
<guid>https://papers.cool/arxiv/2510.18731</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RLAAR, a curriculum reinforcement learning framework that provides verifiable accuracy and abstention rewards to mitigate Lost-in-Conversation degradation in multi-turn LLM interactions. By incrementally increasing dialogue difficulty and rewarding informed abstention, the method stabilizes training and improves both answer correctness and calibrated abstention rates on LiC benchmarks.<br><strong>Summary (CN):</strong> 本文提出 RLAAR 框架，通过课程强化学习以及可验证的准确性与弃答奖励，缓解大型语言模型在多轮对话中的“对话丢失”（Lost-in-Conversation）现象。该方法逐步提升对话难度，鼓励模型在无法确定答案时主动弃答，从而在基准测试上显著提升答案正确率和校准的弃答率。<br><strong>Keywords:</strong> curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Ming Li</div>
Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.
<div><strong>Authors:</strong> Ming Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RLAAR, a curriculum reinforcement learning framework that provides verifiable accuracy and abstention rewards to mitigate Lost-in-Conversation degradation in multi-turn LLM interactions. By incrementally increasing dialogue difficulty and rewarding informed abstention, the method stabilizes training and improves both answer correctness and calibrated abstention rates on LiC benchmarks.", "summary_cn": "本文提出 RLAAR 框架，通过课程强化学习以及可验证的准确性与弃答奖励，缓解大型语言模型在多轮对话中的“对话丢失”（Lost-in-Conversation）现象。该方法逐步提升对话难度，鼓励模型在无法确定答案时主动弃答，从而在基准测试上显著提升答案正确率和校准的弃答率。", "keywords": "curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ming Li"]}
]]></acme>

<pubDate>2025-10-21T15:32:26+00:00</pubDate>
</item>
<item>
<title>SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish</title>
<link>https://papers.cool/arxiv/2510.18725</link>
<guid>https://papers.cool/arxiv/2510.18725</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SemiAdapt and SemiLoRA, semi-supervised, inference-efficient methods that enhance domain adaptation for transformer-based neural machine translation, particularly targeting low‑resource languages such as Irish. Experiments show that SemiAdapt can surpass full‑domain fine‑tuning and that SemiLoRA enables parameter‑efficient fine‑tuning to match or exceed full‑model fine‑tuning performance, especially on larger, noisier corpora. All resulting Irish translation models are released as open resources to lower the barrier for low‑resource language research.<br><strong>Summary (CN):</strong> 本文提出了 SemiAdapt 和 SemiLoRA 两种半监督、推理高效的方式，以提升基于 Transformer 的神经机器翻译在低资源语言（如爱尔兰语）上的领域适配能力。实验表明 SemiAdapt 能够超越全域微调，而 SemiLoRA 使参数高效微调的效果可以匹配甚至超过完整模型的微调，尤其在更大且噪声较多的数据集上表现突出。所有的爱尔兰语翻译模型均作为开放资源发布，以降低低资源语言研究的门槛。<br><strong>Keywords:</strong> parameter-efficient fine-tuning, LoRA, SemiAdapt, SemiLoRA, neural machine translation, low-resource languages, domain adaptation, transformer, Irish translation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Josh McGiff, Nikola S. Nikolov</div>
Fine-tuning is widely used to tailor large language models for specific tasks such as neural machine translation (NMT). However, leveraging transfer learning is computationally expensive when fine-tuning large multilingual models with billions of parameters, thus creating a barrier to entry for researchers working on low-resource domains such as Irish translation. Parameter-efficient fine-tuning (PEFT) bridges this gap by training on a fraction of the original model parameters, with the Low-Rank Adaptation (LoRA) approach introducing small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as semi-supervised inference-efficient approaches that strengthen domain adaptation and lead to improved overall performance in NMT. We demonstrate that SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA can propel PEFT methods to match or even outperform full-model fine-tuning. We further evaluate domain-by-dataset fine-tuning and demonstrate that our embedding-based inference methods perform especially well on larger and noisier corpora. All Irish translation models developed in this work are released as open resources. These methods aim to make high-quality domain adaptation and fine-tuning more accessible to researchers working with low-resource languages.
<div><strong>Authors:</strong> Josh McGiff, Nikola S. Nikolov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SemiAdapt and SemiLoRA, semi-supervised, inference-efficient methods that enhance domain adaptation for transformer-based neural machine translation, particularly targeting low‑resource languages such as Irish. Experiments show that SemiAdapt can surpass full‑domain fine‑tuning and that SemiLoRA enables parameter‑efficient fine‑tuning to match or exceed full‑model fine‑tuning performance, especially on larger, noisier corpora. All resulting Irish translation models are released as open resources to lower the barrier for low‑resource language research.", "summary_cn": "本文提出了 SemiAdapt 和 SemiLoRA 两种半监督、推理高效的方式，以提升基于 Transformer 的神经机器翻译在低资源语言（如爱尔兰语）上的领域适配能力。实验表明 SemiAdapt 能够超越全域微调，而 SemiLoRA 使参数高效微调的效果可以匹配甚至超过完整模型的微调，尤其在更大且噪声较多的数据集上表现突出。所有的爱尔兰语翻译模型均作为开放资源发布，以降低低资源语言研究的门槛。", "keywords": "parameter-efficient fine-tuning, LoRA, SemiAdapt, SemiLoRA, neural machine translation, low-resource languages, domain adaptation, transformer, Irish translation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Josh McGiff", "Nikola S. Nikolov"]}
]]></acme>

<pubDate>2025-10-21T15:24:15+00:00</pubDate>
</item>
<item>
<title>Adapting Language Balance in Code-Switching Speech</title>
<link>https://papers.cool/arxiv/2510.18724</link>
<guid>https://papers.cool/arxiv/2510.18724</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a differentiable surrogate loss that highlights code-switching points by leveraging the language imbalance between the embedded and main language, thereby reducing context bias during generation. Experiments on Arabic and Chinese-English code-switching speech data show improved prediction of switch locations and lower substitution errors.<br><strong>Summary (CN):</strong> 本文提出一种可微分的代理损失，通过利用嵌入语言与主体语言的差异来突出代码切换点，从而缓解生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验表明，模型能够更准确地预测切换位置，并显著降低替换错误率。<br><strong>Keywords:</strong> code-switching, language balance, context bias, differentiable surrogate, robustness, speech recognition, multilingual models<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a differentiable surrogate loss that highlights code-switching points by leveraging the language imbalance between the embedded and main language, thereby reducing context bias during generation. Experiments on Arabic and Chinese-English code-switching speech data show improved prediction of switch locations and lower substitution errors.", "summary_cn": "本文提出一种可微分的代理损失，通过利用嵌入语言与主体语言的差异来突出代码切换点，从而缓解生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验表明，模型能够更准确地预测切换位置，并显著降低替换错误率。", "keywords": "code-switching, language balance, context bias, differentiable surrogate, robustness, speech recognition, multilingual models", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:55+00:00</pubDate>
</item>
<item>
<title>Bayesian Low-Rank Factorization for Robust Model Adaptation</title>
<link>https://papers.cool/arxiv/2510.18723</link>
<guid>https://papers.cool/arxiv/2510.18723</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Bayesian low-rank factorized adapters for speech foundation models such as Whisper, enabling robust adaptation to code-switching multilingual scenarios while preserving the base model's general capabilities. By placing near-zero priors on adapter parameters, the method yields sparser adaptation matrices that reduce catastrophic forgetting, achieving a 54% backward gain with only a 4% drop on the new domain compared to LoRA.<br><strong>Summary (CN):</strong> 该论文提出使用贝叶斯低秩因子化适配器对 Whisper 等语音基础模型进行鲁棒的领域适配，能够在代码切换等多语言场景下实现有效的微调，同时通过在参数上施加接近零的先验实现稀疏适配矩阵，显著降低对原始模型能力的遗忘。实验表明，与 LoRA 相比，本文方法在新领域性能下降仅 4%，但在原始任务上实现了 54% 的后向收益。<br><strong>Keywords:</strong> Bayesian adaptation, low-rank factorization, adapters, Whisper, code-switching, catastrophic forgetting, LoRA, speech foundation models<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Bayesian low-rank factorized adapters for speech foundation models such as Whisper, enabling robust adaptation to code-switching multilingual scenarios while preserving the base model's general capabilities. By placing near-zero priors on adapter parameters, the method yields sparser adaptation matrices that reduce catastrophic forgetting, achieving a 54% backward gain with only a 4% drop on the new domain compared to LoRA.", "summary_cn": "该论文提出使用贝叶斯低秩因子化适配器对 Whisper 等语音基础模型进行鲁棒的领域适配，能够在代码切换等多语言场景下实现有效的微调，同时通过在参数上施加接近零的先验实现稀疏适配矩阵，显著降低对原始模型能力的遗忘。实验表明，与 LoRA 相比，本文方法在新领域性能下降仅 4%，但在原始任务上实现了 54% 的后向收益。", "keywords": "Bayesian adaptation, low-rank factorization, adapters, Whisper, code-switching, catastrophic forgetting, LoRA, speech foundation models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:30+00:00</pubDate>
</item>
<item>
<title>Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering</title>
<link>https://papers.cool/arxiv/2510.18691</link>
<guid>https://papers.cool/arxiv/2510.18691</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first systematic study of large language model (LLM) performance on long-context medical question answering, evaluating multiple models, relevance-based content inclusion settings, and datasets across various task formulations. It investigates how Retrieval-Augmented Generation (RAG) affects long-context comprehension, identifies optimal single- versus multi-document reasoning setups, and provides qualitative and error analyses that highlight when RAG is beneficial and common failure modes. The findings reveal effects of model size, memorization issues, and the advantages of reasoning-oriented models for clinical relevance.<br><strong>Summary (CN):</strong> 本文首次系统性地研究了大型语言模型在长上下文医学问答任务中的表现，评估了不同模型、基于相关性的内容包含设置以及跨任务形式的多个数据集。研究分析了检索增强生成（RAG）对长上下文理解的影响，找出了单文档与多文档推理的最佳配置，并通过定性和错误分析阐明了 RAG 何时有益以及常见的失败案例。结果揭示了模型规模、记忆问题以及推理模型在临床相关任务中的优势。<br><strong>Keywords:</strong> long-context language models, medical question answering, Retrieval-Augmented Generation, RAG, context comprehension, model scaling, memorization, multi-document reasoning, evaluation metrics, error analysis<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Feras AlMannaa, Talia Tseriotou, Jenny Chim, Maria Liakata</div>
This study is the first to investigate LLM comprehension capabilities over long-context (LC) medical QA of clinical relevance. Our comprehensive assessment spans a range of content-inclusion settings based on their relevance, LLM models of varying capabilities and datasets across task formulations, revealing insights on model size effects, limitations, underlying memorization issues and the benefits of reasoning models. Importantly, we examine the effect of RAG on medical LC comprehension, uncover best settings in single versus multi-document reasoning datasets and showcase RAG strategies for improvements over LC. We shed light into some of the evaluation aspects using a multi-faceted approach. Our qualitative and error analyses address open questions on when RAG is beneficial over LC, revealing common failure cases.
<div><strong>Authors:</strong> Feras AlMannaa, Talia Tseriotou, Jenny Chim, Maria Liakata</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first systematic study of large language model (LLM) performance on long-context medical question answering, evaluating multiple models, relevance-based content inclusion settings, and datasets across various task formulations. It investigates how Retrieval-Augmented Generation (RAG) affects long-context comprehension, identifies optimal single- versus multi-document reasoning setups, and provides qualitative and error analyses that highlight when RAG is beneficial and common failure modes. The findings reveal effects of model size, memorization issues, and the advantages of reasoning-oriented models for clinical relevance.", "summary_cn": "本文首次系统性地研究了大型语言模型在长上下文医学问答任务中的表现，评估了不同模型、基于相关性的内容包含设置以及跨任务形式的多个数据集。研究分析了检索增强生成（RAG）对长上下文理解的影响，找出了单文档与多文档推理的最佳配置，并通过定性和错误分析阐明了 RAG 何时有益以及常见的失败案例。结果揭示了模型规模、记忆问题以及推理模型在临床相关任务中的优势。", "keywords": "long-context language models, medical question answering, Retrieval-Augmented Generation, RAG, context comprehension, model scaling, memorization, multi-document reasoning, evaluation metrics, error analysis", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Feras AlMannaa", "Talia Tseriotou", "Jenny Chim", "Maria Liakata"]}
]]></acme>

<pubDate>2025-10-21T14:50:24+00:00</pubDate>
</item>
<item>
<title>MLMA: Towards Multilingual with Mamba Based Architectures</title>
<link>https://papers.cool/arxiv/2510.18684</link>
<guid>https://papers.cool/arxiv/2510.18684</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes MLMA, a multilingual automatic speech recognition system that adopts the Mamba state-space architecture to handle long-context sequences efficiently. By embedding language-aware conditioning within a shared representation, MLMA attains performance comparable to Transformer‑based models on standard multilingual benchmarks, demonstrating Mamba’s suitability as a scalable backbone for speech tasks.<br><strong>Summary (CN):</strong> 本文提出了 MLMA，一种采用 Mamba 状态空间模型进行长序列处理的多语言自动语音识别系统。通过在共享表征中隐式加入语言感知条件，MLMA 在标准多语言基准上实现了与 Transformer 模型相当的性能，展示了 Mamba 作为可扩展、高效语音识别骨干的潜力。<br><strong>Keywords:</strong> multilingual ASR, Mamba, state-space model, speech recognition, efficient architecture, language-aware conditioning, long-context modeling<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</div>
Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture--an efficient state-space model optimized for long-context sequence processing--for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition.
<div><strong>Authors:</strong> Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes MLMA, a multilingual automatic speech recognition system that adopts the Mamba state-space architecture to handle long-context sequences efficiently. By embedding language-aware conditioning within a shared representation, MLMA attains performance comparable to Transformer‑based models on standard multilingual benchmarks, demonstrating Mamba’s suitability as a scalable backbone for speech tasks.", "summary_cn": "本文提出了 MLMA，一种采用 Mamba 状态空间模型进行长序列处理的多语言自动语音识别系统。通过在共享表征中隐式加入语言感知条件，MLMA 在标准多语言基准上实现了与 Transformer 模型相当的性能，展示了 Mamba 作为可扩展、高效语音识别骨干的潜力。", "keywords": "multilingual ASR, Mamba, state-space model, speech recognition, efficient architecture, language-aware conditioning, long-context modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohamed Nabih Ali", "Daniele Falavigna", "Alessio Brutti"]}
]]></acme>

<pubDate>2025-10-21T14:44:16+00:00</pubDate>
</item>
<item>
<title>Dynamical model parameters from ultrasound tongue kinematics</title>
<link>https://papers.cool/arxiv/2510.18629</link>
<guid>https://papers.cool/arxiv/2510.18629</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether parameters of a linear harmonic oscillator model of speech articulation can be reliably estimated from ultrasound tongue kinematics and compares these estimates with those obtained from simultaneously recorded electromagnetic articulography (EMA) data. Results show that ultrasound provides comparable dynamical parameters to EMA, and mandibular short‑tendon tracking also captures jaw motion effectively, supporting ultrasound as a viable method for evaluating dynamical articulatory models.<br><strong>Summary (CN):</strong> 本文研究了能否从超声舌部运动中可靠地估计线性简谐振子模型的参数，并将其与同步记录的电磁测轨（EMA）数据估计的参数进行比较。结果表明，超声获取的动力学参数与 EMA 相当，颚短肌腱追踪同样能够有效捕获下颌运动，支持将超声用于评估动力学发音模型。<br><strong>Keywords:</strong> ultrasound tongue imaging, speech dynamics, linear harmonic oscillator, articulatory modeling, electromagnetic articulography, mandibular tracking, dynamical parameter estimation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 2, Safety: 1, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Sam Kirkham, Patrycja Strycharczuk</div>
The control of speech can be modelled as a dynamical system in which articulators are driven toward target positions. These models are typically evaluated using fleshpoint data, such as electromagnetic articulography (EMA), but recent methodological advances make ultrasound imaging a promising alternative. We evaluate whether the parameters of a linear harmonic oscillator can be reliably estimated from ultrasound tongue kinematics and compare these with parameters estimated from simultaneously-recorded EMA data. We find that ultrasound and EMA yield comparable dynamical parameters, while mandibular short tendon tracking also adequately captures jaw motion. This supports using ultrasound kinematics to evaluate dynamical articulatory models.
<div><strong>Authors:</strong> Sam Kirkham, Patrycja Strycharczuk</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether parameters of a linear harmonic oscillator model of speech articulation can be reliably estimated from ultrasound tongue kinematics and compares these estimates with those obtained from simultaneously recorded electromagnetic articulography (EMA) data. Results show that ultrasound provides comparable dynamical parameters to EMA, and mandibular short‑tendon tracking also captures jaw motion effectively, supporting ultrasound as a viable method for evaluating dynamical articulatory models.", "summary_cn": "本文研究了能否从超声舌部运动中可靠地估计线性简谐振子模型的参数，并将其与同步记录的电磁测轨（EMA）数据估计的参数进行比较。结果表明，超声获取的动力学参数与 EMA 相当，颚短肌腱追踪同样能够有效捕获下颌运动，支持将超声用于评估动力学发音模型。", "keywords": "ultrasound tongue imaging, speech dynamics, linear harmonic oscillator, articulatory modeling, electromagnetic articulography, mandibular tracking, dynamical parameter estimation", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sam Kirkham", "Patrycja Strycharczuk"]}
]]></acme>

<pubDate>2025-10-21T13:34:13+00:00</pubDate>
</item>
<item>
<title>Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media</title>
<link>https://papers.cool/arxiv/2510.18582</link>
<guid>https://papers.cool/arxiv/2510.18582</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a theory‑informed bilingual dataset of 16,000 social‑media instances annotated for explicit and subtle forms of dehumanization at both document and span levels. It demonstrates that fine‑tuned language models trained on this resource outperform state‑of‑the‑art baselines in zero‑shot and few‑shot‑context settings, highlighting the dataset’s utility for broader dehumanization detection. The work aims to fill a gap in NLP research by covering non‑overtly offensive yet harmful language that reinforces negative stereotypes.<br><strong>Summary (CN):</strong> 本文构建了一个理论驱动的双语数据集，包含 16,000 条来自 Twitter 和 Reddit 的社交媒体文本，针对显性与隐性去人性化行为进行文档级和跨度级标注。实验表明，在该数据集上微调的语言模型在零样本和少样本场景下的表现超过了现有最先进模型，展示了数据集在更广泛去人性化检测中的价值。此工作旨在弥补 NLP 研究中对并非显性冒犯但仍会加深负面刻板印象的有害语言的关注不足。<br><strong>Keywords:</strong> dehumanization detection, bilingual dataset, social media, bias mitigation, NLP, -shot, few-shot, document-level annotation, span-level annotation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 6<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br><strong>Authors:</strong> Dennis Assenmacher, Paloma Piot, Katarina Laken, David Jurgens, Claudia Wagner</div>
Digital dehumanization, although a critical issue, remains largely overlooked within the field of computational linguistics and Natural Language Processing. The prevailing approach in current research concentrating primarily on a single aspect of dehumanization that identifies overtly negative statements as its core marker. This focus, while crucial for understanding harmful online communications, inadequately addresses the broader spectrum of dehumanization. Specifically, it overlooks the subtler forms of dehumanization that, despite not being overtly offensive, still perpetuate harmful biases against marginalized groups in online interactions. These subtler forms can insidiously reinforce negative stereotypes and biases without explicit offensiveness, making them harder to detect yet equally damaging. Recognizing this gap, we use different sampling methods to collect a theory-informed bilingual dataset from Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances on a document- and span-level, we show that our dataset covers the different dimensions of dehumanization. This dataset serves as both a training resource for machine learning models and a benchmark for evaluating future dehumanization detection techniques. To demonstrate its effectiveness, we fine-tune ML models on this dataset, achieving performance that surpasses state-of-the-art models in zero and few-shot in-context settings.
<div><strong>Authors:</strong> Dennis Assenmacher, Paloma Piot, Katarina Laken, David Jurgens, Claudia Wagner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a theory‑informed bilingual dataset of 16,000 social‑media instances annotated for explicit and subtle forms of dehumanization at both document and span levels. It demonstrates that fine‑tuned language models trained on this resource outperform state‑of‑the‑art baselines in zero‑shot and few‑shot‑context settings, highlighting the dataset’s utility for broader dehumanization detection. The work aims to fill a gap in NLP research by covering non‑overtly offensive yet harmful language that reinforces negative stereotypes.", "summary_cn": "本文构建了一个理论驱动的双语数据集，包含 16,000 条来自 Twitter 和 Reddit 的社交媒体文本，针对显性与隐性去人性化行为进行文档级和跨度级标注。实验表明，在该数据集上微调的语言模型在零样本和少样本场景下的表现超过了现有最先进模型，展示了数据集在更广泛去人性化检测中的价值。此工作旨在弥补 NLP 研究中对并非显性冒犯但仍会加深负面刻板印象的有害语言的关注不足。", "keywords": "dehumanization detection, bilingual dataset, social media, bias mitigation, NLP,-shot, few-shot, document-level annotation, span-level annotation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Dennis Assenmacher", "Paloma Piot", "Katarina Laken", "David Jurgens", "Claudia Wagner"]}
]]></acme>

<pubDate>2025-10-21T12:35:30+00:00</pubDate>
</item>
<item>
<title>Large language models for folktale type automation based on motifs: Cinderella case study</title>
<link>https://papers.cool/arxiv/2510.18561</link>
<guid>https://papers.cool/arxiv/2510.18561</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a methodology that uses large language models to automatically identify motifs in a large corpus of Cinderella variants and analyzes similarities with clustering and dimensionality reduction techniques. Results demonstrate that LLMs can capture complex interactions within folk narratives, enabling large‑scale computational analysis and cross‑lingual comparisons. This showcases a novel application of AI to digital humanities and folkloristics.<br><strong>Summary (CN):</strong> 本文提出了一种方法论，利用大型语言模型自动检测大量《灰姑娘》变体中的叙事母题，并使用聚类和降维技术分析它们的相似性与差异。结果显示，LLM 能捕捉故事中复杂的交互关系，从而实现大规模文本分析和跨语言比较。此工作展示了人工智能在数字人文和民俗学中的新颖应用。<br><strong>Keywords:</strong> large language models, folktale motifs, Cinderella, computational folkloristics, motif detection, clustering, dimensionality reduction, cross-lingual analysis<br><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.
<div><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a methodology that uses large language models to automatically identify motifs in a large corpus of Cinderella variants and analyzes similarities with clustering and dimensionality reduction techniques. Results demonstrate that LLMs can capture complex interactions within folk narratives, enabling large‑scale computational analysis and cross‑lingual comparisons. This showcases a novel application of AI to digital humanities and folkloristics.", "summary_cn": "本文提出了一种方法论，利用大型语言模型自动检测大量《灰姑娘》变体中的叙事母题，并使用聚类和降维技术分析它们的相似性与差异。结果显示，LLM 能捕捉故事中复杂的交互关系，从而实现大规模文本分析和跨语言比较。此工作展示了人工智能在数字人文和民俗学中的新颖应用。", "keywords": "large language models, folktale motifs, Cinderella, computational folkloristics, motif detection, clustering, dimensionality reduction, cross-lingual analysis", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tjaša Arčon", "Marko Robnik-Šikonja", "Polona Tratnik"]}
]]></acme>

<pubDate>2025-10-21T12:18:20+00:00</pubDate>
</item>
<item>
<title>Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency</title>
<link>https://papers.cool/arxiv/2510.18556</link>
<guid>https://papers.cool/arxiv/2510.18556</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a detailed bias analysis of clinical large language models, focusing on differential opioid prescription patterns across ethnicity, gender, and age, and introduces the HC4 (Healthcare Comprehensive Commons Corpus), a 89‑billion‑token pre‑training dataset. It evaluates models using standard benchmarks and a novel healthcare‑specific methodology to assess fairness and safety, highlighting the importance of dataset transparency for trustworthy clinical AI.<br><strong>Summary (CN):</strong> 本文对临床大语言模型进行深入的偏差分析，重点关注不同族裔、性别和年龄群体在阿片类药物处方上的差异，并推出 HC4（Healthcare Comprehensive Commons Corpus），一个超过 890 亿标记的预训练数据集。通过通用基准和新提出的面向医疗的评估方法，评估模型的公平性和安全性，强调数据集透明度对于建立可信临床 AI 的重要性。<br><strong>Keywords:</strong> bias analysis, clinical LLMs, dataset transparency, HC4, fairness, opioid prescription, healthcare AI, large language models<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Svetlana Maslenkova, Clement Christophe, Marco AF Pimentel, Tathagata Raha, Muhammad Umar Salman, Ahmed Al Mahrooqi, Avani Gupta, Shadab Khan, Ronnie Rajan, Praveenkumar Kanithi</div>
Large language models offer transformative potential for healthcare, yet their responsible and equitable development depends critically on a deeper understanding of how training data characteristics influence model behavior, including the potential for bias. Current practices in dataset curation and bias assessment often lack the necessary transparency, creating an urgent need for comprehensive evaluation frameworks to foster trust and guide improvements. In this study, we present an in-depth analysis of potential downstream biases in clinical language models, with a focus on differential opioid prescription tendencies across diverse demographic groups, such as ethnicity, gender, and age. As part of this investigation, we introduce HC4: Healthcare Comprehensive Commons Corpus, a novel and extensively curated pretraining dataset exceeding 89 billion tokens. Our evaluation leverages both established general benchmarks and a novel, healthcare-specific methodology, offering crucial insights to support fairness and safety in clinical AI applications.
<div><strong>Authors:</strong> Svetlana Maslenkova, Clement Christophe, Marco AF Pimentel, Tathagata Raha, Muhammad Umar Salman, Ahmed Al Mahrooqi, Avani Gupta, Shadab Khan, Ronnie Rajan, Praveenkumar Kanithi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a detailed bias analysis of clinical large language models, focusing on differential opioid prescription patterns across ethnicity, gender, and age, and introduces the HC4 (Healthcare Comprehensive Commons Corpus), a 89‑billion‑token pre‑training dataset. It evaluates models using standard benchmarks and a novel healthcare‑specific methodology to assess fairness and safety, highlighting the importance of dataset transparency for trustworthy clinical AI.", "summary_cn": "本文对临床大语言模型进行深入的偏差分析，重点关注不同族裔、性别和年龄群体在阿片类药物处方上的差异，并推出 HC4（Healthcare Comprehensive Commons Corpus），一个超过 890 亿标记的预训练数据集。通过通用基准和新提出的面向医疗的评估方法，评估模型的公平性和安全性，强调数据集透明度对于建立可信临床 AI 的重要性。", "keywords": "bias analysis, clinical LLMs, dataset transparency, HC4, fairness, opioid prescription, healthcare AI, large language models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Svetlana Maslenkova", "Clement Christophe", "Marco AF Pimentel", "Tathagata Raha", "Muhammad Umar Salman", "Ahmed Al Mahrooqi", "Avani Gupta", "Shadab Khan", "Ronnie Rajan", "Praveenkumar Kanithi"]}
]]></acme>

<pubDate>2025-10-21T12:08:39+00:00</pubDate>
</item>
<item>
<title>Identity-Aware Large Language Models require Cultural Reasoning</title>
<link>https://papers.cool/arxiv/2510.18510</link>
<guid>https://papers.cool/arxiv/2510.18510</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines cultural reasoning as the ability of large language models to recognize culture‑specific knowledge, values, and social norms and to adjust outputs to match individual user expectations. It argues that current models default to Western norms, leading to stereotypes and mistrust, and proposes treating cultural reasoning as a foundational capability alongside factual accuracy, outlining initial assessment directions.<br><strong>Summary (CN):</strong> 本文将“文化推理”定义为大型语言模型识别特定文化知识、价值观和社会规范并相应调整输出以符合用户期望的能力。文章指出，现有模型倾向于西方视角，易产生刻板印象和不信任，并主张将文化推理视为与事实准确性并列的基础能力，提出了初步评估方向。<br><strong>Keywords:</strong> cultural reasoning, identity-aware AI, bias mitigation, multicultural alignment, LLM evaluation, stereotype reduction, cultural competence<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 5, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Alistair Plum, Anne-Marie Lutgen, Christoph Purschke, Achim Rettinger</div>
Large language models have become the latest trend in natural language processing, heavily featuring in the digital tools we use every day. However, their replies often reflect a narrow cultural viewpoint that overlooks the diversity of global users. This missing capability could be referred to as cultural reasoning, which we define here as the capacity of a model to recognise culture-specific knowledge values and social norms, and to adjust its output so that it aligns with the expectations of individual users. Because culture shapes interpretation, emotional resonance, and acceptable behaviour, cultural reasoning is essential for identity-aware AI. When this capacity is limited or absent, models can sustain stereotypes, ignore minority perspectives, erode trust, and perpetuate hate. Recent empirical studies strongly suggest that current models default to Western norms when judging moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning on survey data only partly reduces this tendency. The present evaluation methods mainly report static accuracy scores and thus fail to capture adaptive reasoning in context. Although broader datasets can help, they cannot alone ensure genuine cultural competence. Therefore, we argue that cultural reasoning must be treated as a foundational capability alongside factual accuracy and linguistic coherence. By clarifying the concept and outlining initial directions for its assessment, a foundation is laid for future systems to be able to respond with greater sensitivity to the complex fabric of human culture.
<div><strong>Authors:</strong> Alistair Plum, Anne-Marie Lutgen, Christoph Purschke, Achim Rettinger</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines cultural reasoning as the ability of large language models to recognize culture‑specific knowledge, values, and social norms and to adjust outputs to match individual user expectations. It argues that current models default to Western norms, leading to stereotypes and mistrust, and proposes treating cultural reasoning as a foundational capability alongside factual accuracy, outlining initial assessment directions.", "summary_cn": "本文将“文化推理”定义为大型语言模型识别特定文化知识、价值观和社会规范并相应调整输出以符合用户期望的能力。文章指出，现有模型倾向于西方视角，易产生刻板印象和不信任，并主张将文化推理视为与事实准确性并列的基础能力，提出了初步评估方向。", "keywords": "cultural reasoning, identity-aware AI, bias mitigation, multicultural alignment, LLM evaluation, stereotype reduction, cultural competence", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Alistair Plum", "Anne-Marie Lutgen", "Christoph Purschke", "Achim Rettinger"]}
]]></acme>

<pubDate>2025-10-21T10:50:51+00:00</pubDate>
</item>
<item>
<title>How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</title>
<link>https://papers.cool/arxiv/2510.18480</link>
<guid>https://papers.cool/arxiv/2510.18480</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a systematic benchmark of diffusion language models (DLMs) versus autoregressive models, showing that DLMs generally lag in throughput despite their parallel decoding capability. It identifies shortcomings in prior efficiency evaluation practices and demonstrates that acceleration techniques like dual cache only help at small batch sizes, with diminishing returns at scale. The work calls for more robust evaluation methods and better acceleration strategies to improve DLM practicality.<br><strong>Summary (CN):</strong> 本文系统性地对比了扩散语言模型（DLM）与自回归模型的吞吐量，发现尽管 DLM 具备并行解码的优势，但在实践中其效率普遍低于自回归模型。文章指出了以往效率评估方法的缺陷，并展示了双缓存等加速技术仅在小批量时有效，批量放大后收益减弱。作者呼吁采用更严格的评估方法并发展更佳的加速策略，以提升 DLM 的实际可用性。<br><strong>Keywords:</strong> diffusion language model, efficiency evaluation, throughput, parallel decoding, roofline analysis, benchmarking<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</div>
Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.
<div><strong>Authors:</strong> Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a systematic benchmark of diffusion language models (DLMs) versus autoregressive models, showing that DLMs generally lag in throughput despite their parallel decoding capability. It identifies shortcomings in prior efficiency evaluation practices and demonstrates that acceleration techniques like dual cache only help at small batch sizes, with diminishing returns at scale. The work calls for more robust evaluation methods and better acceleration strategies to improve DLM practicality.", "summary_cn": "本文系统性地对比了扩散语言模型（DLM）与自回归模型的吞吐量，发现尽管 DLM 具备并行解码的优势，但在实践中其效率普遍低于自回归模型。文章指出了以往效率评估方法的缺陷，并展示了双缓存等加速技术仅在小批量时有效，批量放大后收益减弱。作者呼吁采用更严格的评估方法并发展更佳的加速策略，以提升 DLM 的实际可用性。", "keywords": "diffusion language model, efficiency evaluation, throughput, parallel decoding, roofline analysis, benchmarking", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Han Peng", "Peiyu Liu", "Zican Dong", "Daixuan Cheng", "Junyi Li", "Yiru Tang", "Shuo Wang", "Wayne Xin Zhao"]}
]]></acme>

<pubDate>2025-10-21T10:00:32+00:00</pubDate>
</item>
<item>
<title>DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP</title>
<link>https://papers.cool/arxiv/2510.18475</link>
<guid>https://papers.cool/arxiv/2510.18475</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DART, the first structured corpus of Italian Summaries of Product Characteristics from the Italian Medicines Agency, annotated for indications, adverse drug reactions, and drug‑drug interactions using a reproducible pipeline of web retrieval, semantic segmentation, and few‑shot LLM summarization. It also presents an LLM‑based drug interaction checker that leverages DART to infer clinically meaningful interactions, demonstrating accurate performance. The dataset and code are publicly released.<br><strong>Summary (CN):</strong> 本文提出 DART 数据集——首个来源于意大利药品监管机构（AIFA）官方药品说明书的意大利语结构化语料库，涵盖适应症、不良反应和药物相互作用等关键药理学领域，并通过网页检索、语义分块以及少样本微调的大语言模型（LLM）低温解码实现自动化标注。作者进一步展示了基于 DART 的 LLM 药物相互作用检查器，能够准确推断临床相关的相互作用及其意义。该数据集及代码已在 GitHub 上公开发布。<br><strong>Keywords:</strong> Italian clinical NLP, drug regulatory documents, DART dataset, drug-drug interaction, LLM, few-shot summarization, pharmacological knowledge extraction<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</div>
The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, with applications ranging from adverse event monitoring to AI-assisted clinical decision support. However, research in this field has predominantly relied on English-language corpora such as DrugBank, leaving a significant gap in resources tailored to other healthcare systems. To address this limitation, we introduce DART (Drug Annotation from Regulatory Texts), the first structured corpus of Italian Summaries of Product Characteristics derived from the official repository of the Italian Medicines Agency (AIFA). The dataset was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding. DART provides structured information on key pharmacological domains such as indications, adverse drug reactions, and drug-drug interactions. To validate its utility, we implemented an LLM-based drug interaction checker that leverages the dataset to infer clinically meaningful interactions. Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART. We publicly release our code on GitHub: https://github.com/PRAISELab-PicusLab/DART.
<div><strong>Authors:</strong> Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DART, the first structured corpus of Italian Summaries of Product Characteristics from the Italian Medicines Agency, annotated for indications, adverse drug reactions, and drug‑drug interactions using a reproducible pipeline of web retrieval, semantic segmentation, and few‑shot LLM summarization. It also presents an LLM‑based drug interaction checker that leverages DART to infer clinically meaningful interactions, demonstrating accurate performance. The dataset and code are publicly released.", "summary_cn": "本文提出 DART 数据集——首个来源于意大利药品监管机构（AIFA）官方药品说明书的意大利语结构化语料库，涵盖适应症、不良反应和药物相互作用等关键药理学领域，并通过网页检索、语义分块以及少样本微调的大语言模型（LLM）低温解码实现自动化标注。作者进一步展示了基于 DART 的 LLM 药物相互作用检查器，能够准确推断临床相关的相互作用及其意义。该数据集及代码已在 GitHub 上公开发布。", "keywords": "Italian clinical NLP, drug regulatory documents, DART dataset, drug-drug interaction, LLM, few-shot summarization, pharmacological knowledge extraction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mariano Barone", "Antonio Laudante", "Giuseppe Riccio", "Antonio Romano", "Marco Postiglione", "Vincenzo Moscato"]}
]]></acme>

<pubDate>2025-10-21T09:53:17+00:00</pubDate>
</item>
<item>
<title>IMB: An Italian Medical Benchmark for Question Answering</title>
<link>https://papers.cool/arxiv/2510.18468</link>
<guid>https://papers.cool/arxiv/2510.18468</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two large Italian medical QA benchmarks, IMB-QA with 782,644 patient‑doctor dialogues and IMB-MCQA with 25,862 multiple‑choice questions, and evaluates various LLM architectures, Retrieval‑Augmented Generation and domain‑specific fine‑tuning on open‑ended and multiple‑choice tasks, showing that specialized adaptation can outperform larger general models. It also demonstrates how LLMs can be used to clean and standardize forum data while preserving conversational style. The datasets and evaluation code are released publicly to facilitate multilingual medical QA research.<br><strong>Summary (CN):</strong> 本文推出了两个大规模意大利语医学问答基准：含 782,644 条患者‑医生对话的 IMB‑QA 与含 25,862 道医学专业考试选择题的 IMB‑MCQA，并评估了多种大型语言模型（LLM）架构、检索增强生成（RAG）以及领域特定微调在开放式和选择题任务上的表现，证明专业化适配策略可胜过更大规模的通用模型。研究还展示了利用 LLM 对医学论坛数据进行澄清与一致性提升的方式，保留原有对话风格。数据集与评估框架已在 GitHub 开源，以推动多语言医学问答研究。<br><strong>Keywords:</strong> medical question answering, Italian language, benchmark, large language models, retrieval-augmented generation, domain fine-tuning, multilingual QA, healthcare dataset<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Vincenzo Moscato</div>
Online medical forums have long served as vital platforms where patients seek professional healthcare advice, generating vast amounts of valuable knowledge. However, the informal nature and linguistic complexity of forum interactions pose significant challenges for automated question answering systems, especially when dealing with non-English languages. We present two comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644 patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA}, comprising 25,862 multiple-choice questions from medical specialty examinations. We demonstrate how Large Language Models (LLMs) can be leveraged to improve the clarity and consistency of medical forum data while retaining their original meaning and conversational style, and compare a variety of LLM architectures on both open and multiple-choice question answering tasks. Our experiments with Retrieval Augmented Generation (RAG) and domain-specific fine-tuning reveal that specialized adaptation strategies can outperform larger, general-purpose models in medical question answering tasks. These findings suggest that effective medical AI systems may benefit more from domain expertise and efficient information retrieval than from increased model scale. We release both datasets and evaluation frameworks in our GitHub repository to support further research on multilingual medical question answering: https://github.com/PRAISELab-PicusLab/IMB.
<div><strong>Authors:</strong> Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Vincenzo Moscato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two large Italian medical QA benchmarks, IMB-QA with 782,644 patient‑doctor dialogues and IMB-MCQA with 25,862 multiple‑choice questions, and evaluates various LLM architectures, Retrieval‑Augmented Generation and domain‑specific fine‑tuning on open‑ended and multiple‑choice tasks, showing that specialized adaptation can outperform larger general models. It also demonstrates how LLMs can be used to clean and standardize forum data while preserving conversational style. The datasets and evaluation code are released publicly to facilitate multilingual medical QA research.", "summary_cn": "本文推出了两个大规模意大利语医学问答基准：含 782,644 条患者‑医生对话的 IMB‑QA 与含 25,862 道医学专业考试选择题的 IMB‑MCQA，并评估了多种大型语言模型（LLM）架构、检索增强生成（RAG）以及领域特定微调在开放式和选择题任务上的表现，证明专业化适配策略可胜过更大规模的通用模型。研究还展示了利用 LLM 对医学论坛数据进行澄清与一致性提升的方式，保留原有对话风格。数据集与评估框架已在 GitHub 开源，以推动多语言医学问答研究。", "keywords": "medical question answering, Italian language, benchmark, large language models, retrieval-augmented generation, domain fine-tuning, multilingual QA, healthcare dataset", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Antonio Romano", "Giuseppe Riccio", "Mariano Barone", "Marco Postiglione", "Vincenzo Moscato"]}
]]></acme>

<pubDate>2025-10-21T09:45:59+00:00</pubDate>
</item>
<item>
<title>CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning</title>
<link>https://papers.cool/arxiv/2510.18466</link>
<guid>https://papers.cool/arxiv/2510.18466</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces CEFR-Annotated WordNet, a version of WordNet enriched with Common European Framework of Reference for Languages proficiency levels. Using a large language model, the authors automatically align WordNet sense definitions with CEFR levels from the English Vocabulary Profile, creating a large corpus for training contextual lexical classifiers that achieve high macro-F1 scores. The resources are released to bridge NLP and language education.<br><strong>Summary (CN):</strong> 本文提出了 CEFR 注释的 WordNet，即在 WordNet 语义网络中加入欧洲语言共同参考框架（CEFR）水平的版本。利用大语言模型将 WordNet 释义与英语词汇档案（English Vocabulary Profile）中的 CEFR 等级进行自动匹配，构建了大规模语料库并用于训练上下文词汇分类器，实现了 0.81 的宏观 F1 分数。作者公开了注释的 WordNet、语料库和分类器，以促进 NLP 与语言教学的结合。<br><strong>Keywords:</strong> CEFR, WordNet, language learning, LLM annotation, semantic similarity, lexical classifier, English Vocabulary Profile, proficiency-guided dataset, NLP education<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono</div>
Although WordNet is a valuable resource owing to its structured semantic networks and extensive vocabulary, its fine-grained sense distinctions can be challenging for second-language learners. To address this, we developed a WordNet annotated with the Common European Framework of Reference for Languages (CEFR), integrating its semantic networks with language-proficiency levels. We automated this process using a large language model to measure the semantic similarity between sense definitions in WordNet and entries in the English Vocabulary Profile Online. To validate our method, we constructed a large-scale corpus containing both sense and CEFR-level information from our annotated WordNet and used it to develop contextual lexical classifiers. Our experiments demonstrate that models fine-tuned on our corpus perform comparably to those trained on gold-standard annotations. Furthermore, by combining our corpus with the gold-standard data, we developed a practical classifier that achieves a Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our annotated WordNet, corpus, and classifiers are publicly available to help bridge the gap between natural language processing and language education, thereby facilitating more effective and efficient language learning.
<div><strong>Authors:</strong> Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces CEFR-Annotated WordNet, a version of WordNet enriched with Common European Framework of Reference for Languages proficiency levels. Using a large language model, the authors automatically align WordNet sense definitions with CEFR levels from the English Vocabulary Profile, creating a large corpus for training contextual lexical classifiers that achieve high macro-F1 scores. The resources are released to bridge NLP and language education.", "summary_cn": "本文提出了 CEFR 注释的 WordNet，即在 WordNet 语义网络中加入欧洲语言共同参考框架（CEFR）水平的版本。利用大语言模型将 WordNet 释义与英语词汇档案（English Vocabulary Profile）中的 CEFR 等级进行自动匹配，构建了大规模语料库并用于训练上下文词汇分类器，实现了 0.81 的宏观 F1 分数。作者公开了注释的 WordNet、语料库和分类器，以促进 NLP 与语言教学的结合。", "keywords": "CEFR, WordNet, language learning, LLM annotation, semantic similarity, lexical classifier, English Vocabulary Profile, proficiency-guided dataset, NLP education", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Masato Kikuchi", "Masatsugu Ono", "Toshioki Soga", "Tetsu Tanabe", "Tadachika Ozono"]}
]]></acme>

<pubDate>2025-10-21T09:42:48+00:00</pubDate>
</item>
<item>
<title>DePass: Unified Feature Attributing by Simple Decomposed Forward Pass</title>
<link>https://papers.cool/arxiv/2510.18462</link>
<guid>https://papers.cool/arxiv/2510.18462</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DePass is a unified framework for attributing Transformer behavior by decomposing hidden states into additive components and propagating them through fixed attention and MLP activations in a single forward pass. The method provides fine-grained, faithful attribution at token, component, and subspace levels without auxiliary training, and is validated across multiple attribution tasks. The authors present DePass as a foundational tool for broader mechanistic interpretability applications.<br><strong>Summary (CN):</strong> DePass 是一种统一的特征归因框架，通过将 Transformer 隐藏状态分解为可定制的加性成分，并在注意力分数和 MLP 激活保持不变的情况下进行一次前向传播，实现对模型行为的细粒度归因。该方法在 token 级、模型组件级和子空间级归因任务中展示了高忠实度和细致性，无需额外训练。作者将 DePass 视为机制可解释性研究的基础工具。<br><strong>Keywords:</strong> feature attribution, transformer, mechanistic interpretability, decomposed forward pass, additive decomposition, token-level attribution, model component attribution, subspace attribution<br><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br><strong>Authors:</strong> Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou</div>
Attributing the behavior of Transformer models to internal computations is a central challenge in mechanistic interpretability. We introduce DePass, a unified framework for feature attribution based on a single decomposed forward pass. DePass decomposes hidden states into customized additive components, then propagates them with attention scores and MLP's activations fixed. It achieves faithful, fine-grained attribution without requiring auxiliary training. We validate DePass across token-level, model component-level, and subspace-level attribution tasks, demonstrating its effectiveness and fidelity. Our experiments highlight its potential to attribute information flow between arbitrary components of a Transformer model. We hope DePass serves as a foundational tool for broader applications in interpretability.
<div><strong>Authors:</strong> Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DePass is a unified framework for attributing Transformer behavior by decomposing hidden states into additive components and propagating them through fixed attention and MLP activations in a single forward pass. The method provides fine-grained, faithful attribution at token, component, and subspace levels without auxiliary training, and is validated across multiple attribution tasks. The authors present DePass as a foundational tool for broader mechanistic interpretability applications.", "summary_cn": "DePass 是一种统一的特征归因框架，通过将 Transformer 隐藏状态分解为可定制的加性成分，并在注意力分数和 MLP 激活保持不变的情况下进行一次前向传播，实现对模型行为的细粒度归因。该方法在 token 级、模型组件级和子空间级归因任务中展示了高忠实度和细致性，无需额外训练。作者将 DePass 视为机制可解释性研究的基础工具。", "keywords": "feature attribution, transformer, mechanistic interpretability, decomposed forward pass, additive decomposition, token-level attribution, model component attribution, subspace attribution", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xiangyu Hong", "Che Jiang", "Kai Tian", "Biqing Qi", "Youbang Sun", "Ning Ding", "Bowen Zhou"]}
]]></acme>

<pubDate>2025-10-21T09:36:12+00:00</pubDate>
</item>
<item>
<title>ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks</title>
<link>https://papers.cool/arxiv/2510.18455</link>
<guid>https://papers.cool/arxiv/2510.18455</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ChronoPlay is a framework that automatically creates and continuously updates retrieval‑augmented generation (RAG) benchmarks for video games by modelling two intertwined dynamics: game content changes and evolving player‑community interests. It combines official game sources with community‑generated queries to ensure factual correctness and authentic question patterns, and demonstrates its approach on three games, providing the first dynamic RAG benchmark for the gaming domain.<br><strong>Summary (CN):</strong> ChronoPlay 框架通过建模游戏内容更新和玩家社区兴趣的双重动态，自动生成并持续更新针对视频游戏的检索增强生成（RAG）基准。它融合官方游戏信息和社区生成的查询，以确保答案的事实正确性和问题的真实感，并在三个游戏上实现，构建了首个面向游戏领域的动态 RAG 基准。<br><strong>Keywords:</strong> RAG, benchmark, gaming, dual dynamics, authenticity, retrieval-augmented generation, continuous evaluation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Liyang He, Yuren Zhang, Ziwei Zhu, Zhenghui Li, Shiwei Tong</div>
Retrieval Augmented Generation (RAG) systems are increasingly vital in dynamic domains like online gaming, yet the lack of a dedicated benchmark has impeded standardized evaluation in this area. The core difficulty lies in Dual Dynamics: the constant interplay between game content updates and the shifting focus of the player community. Furthermore, the necessity of automating such a benchmark introduces a critical requirement for player-centric authenticity to ensure generated questions are realistic. To address this integrated challenge, we introduce ChronoPlay, a novel framework for the automated and continuous generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update mechanism to track both forms of change, and a dual-source synthesis engine that draws from official sources and player community to ensure both factual correctness and authentic query patterns. We instantiate our framework on three distinct games to create the first dynamic RAG benchmark for the gaming domain, offering new insights into model performance under these complex and realistic conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.
<div><strong>Authors:</strong> Liyang He, Yuren Zhang, Ziwei Zhu, Zhenghui Li, Shiwei Tong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ChronoPlay is a framework that automatically creates and continuously updates retrieval‑augmented generation (RAG) benchmarks for video games by modelling two intertwined dynamics: game content changes and evolving player‑community interests. It combines official game sources with community‑generated queries to ensure factual correctness and authentic question patterns, and demonstrates its approach on three games, providing the first dynamic RAG benchmark for the gaming domain.", "summary_cn": "ChronoPlay 框架通过建模游戏内容更新和玩家社区兴趣的双重动态，自动生成并持续更新针对视频游戏的检索增强生成（RAG）基准。它融合官方游戏信息和社区生成的查询，以确保答案的事实正确性和问题的真实感，并在三个游戏上实现，构建了首个面向游戏领域的动态 RAG 基准。", "keywords": "RAG, benchmark, gaming, dual dynamics, authenticity, retrieval-augmented generation, continuous evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liyang He", "Yuren Zhang", "Ziwei Zhu", "Zhenghui Li", "Shiwei Tong"]}
]]></acme>

<pubDate>2025-10-21T09:28:13+00:00</pubDate>
</item>
<item>
<title>Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models</title>
<link>https://papers.cool/arxiv/2510.18454</link>
<guid>https://papers.cool/arxiv/2510.18454</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how optimizing for funniness in modern LLM pipelines leads to higher rates of stereotypical and toxic humor, revealing a bias amplification loop between generators and evaluators. Using information‑theoretic incongruity metrics and human judgments on satire, it shows that harmful jokes are scored as funnier and can even become more statistically expected for certain models. These results highlight safety risks in deploying LLMs for creative and engagement‑focused content.<br><strong>Summary (CN):</strong> 本文研究了在现代大型语言模型中优化幽默度如何导致刻板印象和有害内容的增加，揭示了生成器与评估器之间的偏见放大循环。通过信息论不一致性度量以及对讽刺文本的人类评分，发现有害笑话往往获得更高的幽默分数，且对某些模型而言，这些有害笑点甚至变得更可预测。研究结果凸显了在创意和互动内容中使用 LLM 所面临的安全风险。<br><strong>Keywords:</strong> humor generation, large language models, toxicity, stereotypes, bias amplification, information-theoretic analysis, safety, alignment, role-based prompting, satire<br><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Atharvan Dogra, Soumya Suvra Ghosal, Ameet Deshpande, Ashwin Kalyan, Dinesh Manocha</div>
Large language models are increasingly used for creative writing and engagement content, raising safety concerns about the outputs. Therefore, casting humor generation as a testbed, this work evaluates how funniness optimization in modern LLM pipelines couples with harmful content by jointly measuring humor, stereotypicality, and toxicity. This is further supplemented by analyzing incongruity signals through information-theoretic metrics. Across six models, we observe that harmful outputs receive higher humor scores which further increase under role-based prompting, indicating a bias amplification loop between generators and evaluators. Information-theoretic analyses show harmful cues widen predictive uncertainty and surprisingly, can even make harmful punchlines more expected for some models, suggesting structural embedding in learned humor distributions. External validation on an additional satire-generation task with human perceived funniness judgments shows that LLM satire increases stereotypicality and typically toxicity, including for closed models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes marked funny by LLM-based metric and up to $10\%$ more often in generations perceived as funny by humans.
<div><strong>Authors:</strong> Atharvan Dogra, Soumya Suvra Ghosal, Ameet Deshpande, Ashwin Kalyan, Dinesh Manocha</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how optimizing for funniness in modern LLM pipelines leads to higher rates of stereotypical and toxic humor, revealing a bias amplification loop between generators and evaluators. Using information‑theoretic incongruity metrics and human judgments on satire, it shows that harmful jokes are scored as funnier and can even become more statistically expected for certain models. These results highlight safety risks in deploying LLMs for creative and engagement‑focused content.", "summary_cn": "本文研究了在现代大型语言模型中优化幽默度如何导致刻板印象和有害内容的增加，揭示了生成器与评估器之间的偏见放大循环。通过信息论不一致性度量以及对讽刺文本的人类评分，发现有害笑话往往获得更高的幽默分数，且对某些模型而言，这些有害笑点甚至变得更可预测。研究结果凸显了在创意和互动内容中使用 LLM 所面临的安全风险。", "keywords": "humor generation, large language models, toxicity, stereotypes, bias amplification, information-theoretic analysis, safety, alignment, role-based prompting, satire", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Atharvan Dogra", "Soumya Suvra Ghosal", "Ameet Deshpande", "Ashwin Kalyan", "Dinesh Manocha"]}
]]></acme>

<pubDate>2025-10-21T09:28:09+00:00</pubDate>
</item>
<item>
<title>Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation</title>
<link>https://papers.cool/arxiv/2510.18439</link>
<guid>https://papers.cool/arxiv/2510.18439</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a token-level reliability measure that quantifies how much a sign-language translation model relies on visual input versus language priors, using feature-based sensitivity and counterfactual probability differences. Experiments on PHOENIX-2014T and CSL-Daily show that the reliability score predicts hallucination rates across gloss-based and gloss-free models, generalizes to visual degradations, and improves risk estimation when combined with text-based confidence signals.<br><strong>Summary (CN):</strong> 本文提出一种基于特征敏感度和反事实概率差异的 token 级可靠性指标，用于衡量手语翻译模型在生成文本对视觉输入的依赖程度。实验在 PHOENIX-2014T 与 CSL-Daily 数据集的 gloss-based 与 gloss-free 模型上表明，该可靠性分数能够预测幻觉率、在视觉降级下仍具泛化性，并与文本置信度等信号结合提升幻觉风险估计。<br><strong>Keywords:</strong> hallucination detection, sign language translation, visual grounding, token-level reliability, counterfactual sensitivity, multimodal generation, gloss-free models, model evaluation<br><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br><strong>Authors:</strong> Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</div>
Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on language priors rather than visual input. To capture this, we propose a token-level reliability measure that quantifies how much the decoder uses visual information. Our method combines feature-based sensitivity, which measures internal changes when video is masked, with counterfactual signals, which capture probability differences between clean and altered video inputs. These signals are aggregated into a sentence-level reliability score, providing a compact and interpretable measure of visual grounding. We evaluate the proposed measure on two SLT benchmarks (PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our results show that reliability predicts hallucination rates, generalizes across datasets and architectures, and decreases under visual degradations. Beyond these quantitative trends, we also find that reliability distinguishes grounded tokens from guessed ones, allowing risk estimation without references; when combined with text-based signals (confidence, perplexity, or entropy), it further improves hallucination risk estimation. Qualitative analysis highlights why gloss-free models are more susceptible to hallucinations. Taken together, our findings establish reliability as a practical and reusable tool for diagnosing hallucinations in SLT, and lay the groundwork for more robust hallucination detection in multimodal generation.
<div><strong>Authors:</strong> Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a token-level reliability measure that quantifies how much a sign-language translation model relies on visual input versus language priors, using feature-based sensitivity and counterfactual probability differences. Experiments on PHOENIX-2014T and CSL-Daily show that the reliability score predicts hallucination rates across gloss-based and gloss-free models, generalizes to visual degradations, and improves risk estimation when combined with text-based confidence signals.", "summary_cn": "本文提出一种基于特征敏感度和反事实概率差异的 token 级可靠性指标，用于衡量手语翻译模型在生成文本对视觉输入的依赖程度。实验在 PHOENIX-2014T 与 CSL-Daily 数据集的 gloss-based 与 gloss-free 模型上表明，该可靠性分数能够预测幻觉率、在视觉降级下仍具泛化性，并与文本置信度等信号结合提升幻觉风险估计。", "keywords": "hallucination detection, sign language translation, visual grounding, token-level reliability, counterfactual sensitivity, multimodal generation, gloss-free models, model evaluation", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yasser Hamidullah", "Koel Dutta Chowdury", "Yusser Al-Ghussin", "Shakib Yazdani", "Cennet Oguz", "Josef van Genabith", "Cristina España-Bonet"]}
]]></acme>

<pubDate>2025-10-21T09:13:46+00:00</pubDate>
</item>
<item>
<title>Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response</title>
<link>https://papers.cool/arxiv/2510.18434</link>
<guid>https://papers.cool/arxiv/2510.18434</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Chain of Conceptual Thought (CoCT), a prompt-based paradigm where a language model first tags a concept and then expands on it, forming a chain of concepts within its response. Experiments on daily and emotional‑support dialogues show that CoCT outperforms baselines such as Self‑Refine, ECoT, ToT, SoT and RAG, suggesting that concept‑driven prompting can foster deeper, more strategic thinking in open‑domain tasks.<br><strong>Summary (CN):</strong> 本文提出“概念链思考”(Chain of Conceptual Thought, CoCT) 的提示范式：让语言模型先标记概念，再围绕该概念展开详细内容，在回复中形成概念链。针对日常对话和情感支持场景的实验表明，CoCT 在自动、人工和模型评估上均优于 Self‑Refine、ECoT、ToT、SoT 与 RAG 等基线，显示概念驱动的提示能够促使模型在开放域任务中进行更深层次、策略性的思考。<br><strong>Keywords:</strong> Chain-of-Conceptual-Thought, prompting, large language models, open-domain reasoning, emotional support conversation, concept tagging, Chain-of-Thought, Self-Refine, ECoT, ToT<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 6, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br><strong>Authors:</strong> Qingqing Gu, Dan Wang, Yue Zhao, Xiaoyu Wang, Zhonglin Jiang, Yong Chen, Hongyan Li, Luo Ji</div>
Chain-of-Thought (CoT) is widely applied to improve the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks since there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose another prompt-based paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a concept, then generates the detailed content. The chain of concepts is allowed within the utterance, encouraging the LLM's deep and strategic thinking. We experiment with this paradigm in daily and emotional support conversations where the concept is comprised of emotions, strategies and topics. Automatic, human and model evaluations suggest that CoCT surpasses baselines such as Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective prompt-based paradigm of LLM for a wider scope of tasks.
<div><strong>Authors:</strong> Qingqing Gu, Dan Wang, Yue Zhao, Xiaoyu Wang, Zhonglin Jiang, Yong Chen, Hongyan Li, Luo Ji</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Chain of Conceptual Thought (CoCT), a prompt-based paradigm where a language model first tags a concept and then expands on it, forming a chain of concepts within its response. Experiments on daily and emotional‑support dialogues show that CoCT outperforms baselines such as Self‑Refine, ECoT, ToT, SoT and RAG, suggesting that concept‑driven prompting can foster deeper, more strategic thinking in open‑domain tasks.", "summary_cn": "本文提出“概念链思考”(Chain of Conceptual Thought, CoCT) 的提示范式：让语言模型先标记概念，再围绕该概念展开详细内容，在回复中形成概念链。针对日常对话和情感支持场景的实验表明，CoCT 在自动、人工和模型评估上均优于 Self‑Refine、ECoT、ToT、SoT 与 RAG 等基线，显示概念驱动的提示能够促使模型在开放域任务中进行更深层次、策略性的思考。", "keywords": "Chain-of-Conceptual-Thought, prompting, large language models, open-domain reasoning, emotional support conversation, concept tagging, Chain-of-Thought, Self-Refine, ECoT, ToT", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Qingqing Gu", "Dan Wang", "Yue Zhao", "Xiaoyu Wang", "Zhonglin Jiang", "Yong Chen", "Hongyan Li", "Luo Ji"]}
]]></acme>

<pubDate>2025-10-21T09:08:21+00:00</pubDate>
</item>
<item>
<title>Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference</title>
<link>https://papers.cool/arxiv/2510.18413</link>
<guid>https://papers.cool/arxiv/2510.18413</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Adamas introduces a lightweight sparse attention mechanism for long-context inference that leverages the Hadamard transform, bucketization, and 2-bit compression to create compact representations and uses Manhattan-distance estimation for efficient top‑k selection. Experiments show that it matches full‑attention accuracy with a 64‑token budget and achieves up to 8× higher sparsity than prior methods, delivering up to 4.4× self‑attention and 1.5× end‑to‑end speedups on 32K‑length sequences.<br><strong>Summary (CN):</strong> Adamas 提出了一种用于长上下文推理的轻量稀疏注意力机制，利用 Hadamard 变换、分桶和 2 位压缩生成紧凑表示，并使用曼哈顿距离估计进行高效的 top‑k 选择。实验表明，在仅使用 64 个 token 预算的情况下，其准确性与完整注意力相当，并实现了比现有方法高达 8 倍的稀疏度以及在 32K 长度序列上 4.4 倍的自注意力加速和 1.5 倍的端到端加速。<br><strong>Keywords:</strong> sparse attention, Hadamard transform, long-context inference, efficient transformer, top-k selection, compression<br><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu</div>
Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce Adamas, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity.
<div><strong>Authors:</strong> Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Adamas introduces a lightweight sparse attention mechanism for long-context inference that leverages the Hadamard transform, bucketization, and 2-bit compression to create compact representations and uses Manhattan-distance estimation for efficient top‑k selection. Experiments show that it matches full‑attention accuracy with a 64‑token budget and achieves up to 8× higher sparsity than prior methods, delivering up to 4.4× self‑attention and 1.5× end‑to‑end speedups on 32K‑length sequences.", "summary_cn": "Adamas 提出了一种用于长上下文推理的轻量稀疏注意力机制，利用 Hadamard 变换、分桶和 2 位压缩生成紧凑表示，并使用曼哈顿距离估计进行高效的 top‑k 选择。实验表明，在仅使用 64 个 token 预算的情况下，其准确性与完整注意力相当，并实现了比现有方法高达 8 倍的稀疏度以及在 32K 长度序列上 4.4 倍的自注意力加速和 1.5 倍的端到端加速。", "keywords": "sparse attention, Hadamard transform, long-context inference, efficient transformer, top-k selection, compression", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Siyuan Yan", "Guo-Qing Jiang", "Yuchen Zhang", "Xiaoxing Ma", "Ran Zhu", "Chun Cao", "Jingwei Xu"]}
]]></acme>

<pubDate>2025-10-21T08:44:47+00:00</pubDate>
</item>
<item>
<title>MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models</title>
<link>https://papers.cool/arxiv/2510.18383</link>
<guid>https://papers.cool/arxiv/2510.18383</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MENTOR, a reinforcement‑learning framework that distills the tool‑using abilities of large language models into smaller language models by using a dense, teacher‑guided reward constructed from reference trajectories. By combining exploration‑driven RL with fine‑grained teacher feedback, MENTOR achieves better cross‑domain generalization and strategic competence than supervised fine‑tuning or standard sparse‑reward RL baselines.<br><strong>Summary (CN):</strong> 本文提出 MENTOR 框架，利用强化学习将大语言模型的工具使用能力蒸馏到小语言模型中，方法是通过教师提供的参考轨迹构造密集的教师引导奖励。该框架将探索性的 RL 与细粒度的教师反馈相结合，使小模型在跨领域泛化和策略能力上显著于传统的监督微调和稀疏奖励 RL 基线。<br><strong>Keywords:</strong> reinforcement learning, teacher-guided rewards, model distillation, small language models, cross-domain generalization, RL-based distillation, tool use, policy learning<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.
<div><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MENTOR, a reinforcement‑learning framework that distills the tool‑using abilities of large language models into smaller language models by using a dense, teacher‑guided reward constructed from reference trajectories. By combining exploration‑driven RL with fine‑grained teacher feedback, MENTOR achieves better cross‑domain generalization and strategic competence than supervised fine‑tuning or standard sparse‑reward RL baselines.", "summary_cn": "本文提出 MENTOR 框架，利用强化学习将大语言模型的工具使用能力蒸馏到小语言模型中，方法是通过教师提供的参考轨迹构造密集的教师引导奖励。该框架将探索性的 RL 与细粒度的教师反馈相结合，使小模型在跨领域泛化和策略能力上显著于传统的监督微调和稀疏奖励 RL 基线。", "keywords": "reinforcement learning, teacher-guided rewards, model distillation, small language models, cross-domain generalization, RL-based distillation, tool use, policy learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"]}
]]></acme>

<pubDate>2025-10-21T08:03:14+00:00</pubDate>
</item>
<item>
<title>Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning</title>
<link>https://papers.cool/arxiv/2510.18374</link>
<guid>https://papers.cool/arxiv/2510.18374</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates fairness gaps in popular English automatic speech recognition models (Whisper and Seamless‑M4T) across 26 accent groups of second‑language speakers, finding large WER variance. It proposes a fairness‑prompted finetuning approach using lightweight adapters combined with Spectral Decoupling, Group‑DRO, and Invariant Risk Minimization to jointly optimize accuracy and accent fairness. Experiments show macro‑averaged WER improvements of about 58 % relative to the pretrained models while preserving overall recognition performance.<br><strong>Summary (CN):</strong> 本文研究了主流英文自动语音识别模型（Whisper 和 Seamless‑M4T）在 26 种第二语言口音组之间的公平性差距，发现词错误率（WER）波动显著。作者提出使用轻量级适配器并融合光谱解耦（Spectral Decoupling）、组分布鲁棒优化（Group‑DRO）和不变风险最小化（IRM）的公平提示微调方法，以同时提升准确率和口音公平性。实验表明，在宏观平均 WER 上相较于原始模型提升约 58%，且整体识别性能保持。<br><strong>Keywords:</strong> fairness, automatic speech recognition, accent bias, Whisper, adapters, spectral decoupling, group DRO, invariant risk minimization, multilingual ASR, bias mitigation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br><strong>Authors:</strong> Monorama Swain, Bubai Maji, Jagabandhu Mishra, Markus Schedl, Anders Søgaard, Jesper Rindom Jensen</div>
In this work, we address the challenge of building fair English ASR systems for second-language speakers. Our analysis of widely used ASR models, Whisper and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26 accent groups, indicating significant fairness gaps. To mitigate this, we propose fairness-prompted finetuning with lightweight adapters, incorporating Spectral Decoupling (SD), Group Distributionally Robust Optimization (Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of traditional empirical risk minimization (ERM) with cross-entropy and fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across accent groups while maintaining overall recognition accuracy. In terms of macro-averaged word error rate, our approach achieves a relative improvement of 58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and 7.8% over them, finetuning with standard empirical risk minimization with cross-entropy loss.
<div><strong>Authors:</strong> Monorama Swain, Bubai Maji, Jagabandhu Mishra, Markus Schedl, Anders Søgaard, Jesper Rindom Jensen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates fairness gaps in popular English automatic speech recognition models (Whisper and Seamless‑M4T) across 26 accent groups of second‑language speakers, finding large WER variance. It proposes a fairness‑prompted finetuning approach using lightweight adapters combined with Spectral Decoupling, Group‑DRO, and Invariant Risk Minimization to jointly optimize accuracy and accent fairness. Experiments show macro‑averaged WER improvements of about 58 % relative to the pretrained models while preserving overall recognition performance.", "summary_cn": "本文研究了主流英文自动语音识别模型（Whisper 和 Seamless‑M4T）在 26 种第二语言口音组之间的公平性差距，发现词错误率（WER）波动显著。作者提出使用轻量级适配器并融合光谱解耦（Spectral Decoupling）、组分布鲁棒优化（Group‑DRO）和不变风险最小化（IRM）的公平提示微调方法，以同时提升准确率和口音公平性。实验表明，在宏观平均 WER 上相较于原始模型提升约 58%，且整体识别性能保持。", "keywords": "fairness, automatic speech recognition, accent bias, Whisper, adapters, spectral decoupling, group DRO, invariant risk minimization, multilingual ASR, bias mitigation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Monorama Swain", "Bubai Maji", "Jagabandhu Mishra", "Markus Schedl", "Anders Søgaard", "Jesper Rindom Jensen"]}
]]></acme>

<pubDate>2025-10-21T07:45:21+00:00</pubDate>
</item>
<item>
<title>KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs</title>
<link>https://papers.cool/arxiv/2510.18368</link>
<guid>https://papers.cool/arxiv/2510.18368</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces KoSimpleQA, a Korean factuality benchmark consisting of 1,000 short, unambiguous fact-seeking questions aimed at evaluating large language models' knowledge of Korean cultural information. Experiments on various open-source Korean-capable LLMs show low accuracy (33.7% for the strongest model) and reveal performance differences compared to English SimpleQA, while analysis of reasoning-augmented LLMs indicates that reasoning improves knowledge extraction and uncertainty abstention.<br><strong>Summary (CN):</strong> 本文推出 KoSimpleQA（Korean SimpleQA），一个包含 1,000 条简短且答案明确的韩语事实问答题目的基准，用于评估大语言模型在韩国文化知识方面的事实性。实验表明，即使是性能最强的开源模型也仅有约 33.7% 的正确率，并且在该基准上的排序与英文 SimpleQA 差异显著；进一步分析显示，启用推理能力的模型能够更好地挖掘潜在知识并在不确定时选择弃答。<br><strong>Keywords:</strong> Korean QA, factuality benchmark, LLM evaluation, reasoning, cultural knowledge, KoSimpleQA<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 6, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br><strong>Authors:</strong> Donghyeon Ko, Yeguk Jin, Kyubyung Chae, Byungwook Lee, Chansong Jo, Sookyo In, Jaehong Lee, Taesup Kim, Donghyun Kwak</div>
We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for evaluating factuality in large language models (LLMs) with a focus on Korean cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade, consisting of 1,000 short, fact-seeking questions with unambiguous answers. We conduct a comprehensive evaluation across a diverse set of open-source LLMs of varying sizes that support Korean, and find that even the strongest model generates correct answer only 33.7% of the time, underscoring the challenging nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ substantially from those on the English SimpleQA, highlighting the unique value of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging reasoning capabilities in the factual QA task can both help models better elicit their latent knowledge and improve their ability to abstain when uncertain. KoSimpleQA can be found at https://anonymous.4open.science/r/KoSimpleQA-62EB.
<div><strong>Authors:</strong> Donghyeon Ko, Yeguk Jin, Kyubyung Chae, Byungwook Lee, Chansong Jo, Sookyo In, Jaehong Lee, Taesup Kim, Donghyun Kwak</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces KoSimpleQA, a Korean factuality benchmark consisting of 1,000 short, unambiguous fact-seeking questions aimed at evaluating large language models' knowledge of Korean cultural information. Experiments on various open-source Korean-capable LLMs show low accuracy (33.7% for the strongest model) and reveal performance differences compared to English SimpleQA, while analysis of reasoning-augmented LLMs indicates that reasoning improves knowledge extraction and uncertainty abstention.", "summary_cn": "本文推出 KoSimpleQA（Korean SimpleQA），一个包含 1,000 条简短且答案明确的韩语事实问答题目的基准，用于评估大语言模型在韩国文化知识方面的事实性。实验表明，即使是性能最强的开源模型也仅有约 33.7% 的正确率，并且在该基准上的排序与英文 SimpleQA 差异显著；进一步分析显示，启用推理能力的模型能够更好地挖掘潜在知识并在不确定时选择弃答。", "keywords": "Korean QA, factuality benchmark, LLM evaluation, reasoning, cultural knowledge, KoSimpleQA", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Donghyeon Ko", "Yeguk Jin", "Kyubyung Chae", "Byungwook Lee", "Chansong Jo", "Sookyo In", "Jaehong Lee", "Taesup Kim", "Donghyun Kwak"]}
]]></acme>

<pubDate>2025-10-21T07:37:51+00:00</pubDate>
</item>
<item>
<title>KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers</title>
<link>https://papers.cool/arxiv/2510.18355</link>
<guid>https://papers.cool/arxiv/2510.18355</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces KrishokBondhu, a voice‑based call‑center system for Bengali‑speaking farmers that uses a Retrieval‑Augmented Generation (RAG) pipeline to provide real‑time agricultural advice. It combines OCR digitization of extension manuals, a vector‑database retrieval layer, speech‑to‑text, a 3‑4B Gemma language model, and text‑to‑speech to generate context‑grounded answers, achieving a 44.7% improvement over the KisanQRS benchmark in pilot tests.<br><strong>Summary (CN):</strong> 本文提出 KrishokBondhu，一个面向孟加拉语农民的语音呼叫中心系统，采用检索增强生成（Retrieval‑Augmented Generation，RAG）流水线提供实时农业咨询。系统通过 OCR 将推广手册数字化，使用向量数据库检索、语音转文字、Gemma 3‑4B 大语言模型生成答案，再转为自然语音，实验显示相较 KisanQRS 基准提升 44.7%。<br><strong>Keywords:</strong> retrieval-augmented generation, voice interface, Bengali, agricultural advisory, call center, OCR, large language model, semantic retrieval, multilingual AI<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</div>
In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.
<div><strong>Authors:</strong> Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces KrishokBondhu, a voice‑based call‑center system for Bengali‑speaking farmers that uses a Retrieval‑Augmented Generation (RAG) pipeline to provide real‑time agricultural advice. It combines OCR digitization of extension manuals, a vector‑database retrieval layer, speech‑to‑text, a 3‑4B Gemma language model, and text‑to‑speech to generate context‑grounded answers, achieving a 44.7% improvement over the KisanQRS benchmark in pilot tests.", "summary_cn": "本文提出 KrishokBondhu，一个面向孟加拉语农民的语音呼叫中心系统，采用检索增强生成（Retrieval‑Augmented Generation，RAG）流水线提供实时农业咨询。系统通过 OCR 将推广手册数字化，使用向量数据库检索、语音转文字、Gemma 3‑4B 大语言模型生成答案，再转为自然语音，实验显示相较 KisanQRS 基准提升 44.7%。", "keywords": "retrieval-augmented generation, voice interface, Bengali, agricultural advisory, call center, OCR, large language model, semantic retrieval, multilingual AI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Farjana Aktar", "M. Saifuzzaman Rafat"]}
]]></acme>

<pubDate>2025-10-21T07:24:55+00:00</pubDate>
</item>
<item>
<title>Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction</title>
<link>https://papers.cool/arxiv/2510.18344</link>
<guid>https://papers.cool/arxiv/2510.18344</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces HYDRE, a hybrid framework that first uses a trained distant supervision relation extraction (DSRE) model to generate top‑k candidate relations and then retrieves reliable sentence‑level exemplars to prompt a large language model for final relation prediction. It demonstrates substantial F1 improvements on English and on a newly created benchmark for four low‑resource Indic languages, highlighting the benefit of combining DSRE with in‑context learning and a dynamic exemplar retrieval strategy.<br><strong>Summary (CN):</strong> 本文提出 HYDRE（Hybrid Distantly Supervised Relation Extraction）框架，先利用已训练的远程监督关系抽取（DSRE）模型获取候选关系，再通过动态示例检索从训练数据中挑选可靠的句子级示例，填入大语言模型（LLM）提示以输出最终关系。实验显示，在英语以及四种低资源印地语（Oriya、Santali、Manipuri、Tulu）新基准上，模型的 F1 提升显著，验证了将 DSRE 与上下文学习相结合以及动态示例检索的有效性。<br><strong>Keywords:</strong> distant supervision, relation extraction, in-context learning, large language models, cross-lingual, exemplar retrieval, HYDRE<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</div>
Distantly Supervised Relation Extraction (DSRE) remains a long-standing challenge in NLP, where models must learn from noisy bag-level annotations while making sentence-level predictions. While existing state-of-the-art (SoTA) DSRE models rely on task-specific training, their integration with in-context learning (ICL) using large language models (LLMs) remains underexplored. A key challenge is that the LLM may not learn relation semantics correctly, due to noisy annotation. In response, we propose HYDRE -- HYbrid Distantly Supervised Relation Extraction framework. It first uses a trained DSRE model to identify the top-k candidate relations for a given test sentence, then uses a novel dynamic exemplar retrieval strategy that extracts reliable, sentence-level exemplars from training data, which are then provided in LLM prompt for outputting the final relation(s). We further extend HYDRE to cross-lingual settings for RE in low-resource languages. Using available English DSRE training data, we evaluate all methods on English as well as a newly curated benchmark covering four diverse low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's efficacy compared to other prompting strategies.
<div><strong>Authors:</strong> Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces HYDRE, a hybrid framework that first uses a trained distant supervision relation extraction (DSRE) model to generate top‑k candidate relations and then retrieves reliable sentence‑level exemplars to prompt a large language model for final relation prediction. It demonstrates substantial F1 improvements on English and on a newly created benchmark for four low‑resource Indic languages, highlighting the benefit of combining DSRE with in‑context learning and a dynamic exemplar retrieval strategy.", "summary_cn": "本文提出 HYDRE（Hybrid Distantly Supervised Relation Extraction）框架，先利用已训练的远程监督关系抽取（DSRE）模型获取候选关系，再通过动态示例检索从训练数据中挑选可靠的句子级示例，填入大语言模型（LLM）提示以输出最终关系。实验显示，在英语以及四种低资源印地语（Oriya、Santali、Manipuri、Tulu）新基准上，模型的 F1 提升显著，验证了将 DSRE 与上下文学习相结合以及动态示例检索的有效性。", "keywords": "distant supervision, relation extraction, in-context learning, large language models, cross-lingual, exemplar retrieval, HYDRE", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vipul Rathore", "Malik Hammad Faisal", "Parag Singla", "Mausam"]}
]]></acme>

<pubDate>2025-10-21T06:55:19+00:00</pubDate>
</item>
<item>
<title>ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</title>
<link>https://papers.cool/arxiv/2510.18339</link>
<guid>https://papers.cool/arxiv/2510.18339</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates domain adaptation of open-weight large language models for electrocardiography by fine‑tuning Llama 3.1 70B on specialized literature and comparing it to retrieval‑augmented generation (RAG) and Claude Sonnet 3.7 across multiple evaluation methods. Results show that the fine‑tuned model outperforms its base version and achieves competitive performance with proprietary models, though human experts favour Claude 3.7 and RAG for complex queries, highlighting evaluation heterogeneity.<br><strong>Summary (CN):</strong> 本文研究了在心电图（electrocardiography）领域对开源大语言模型进行域适配的效果，通过在专门文献上微调 Llama 3.1 70B，并与检索增强生成（RAG）以及 Claude Sonnet 3.7 在多种评估方式下进行比较。结果表明，微调模型在多数评估中优于基模型，并在与商业模型的竞争中表现出色，但在人类专家对复杂查询的评估中更倾向于 Claude 3.7 和 RAG，凸显了评估方法的多样性与复杂性。<br><strong>Keywords:</strong> electrocardiography, domain adaptation, large language model, Llama 3.1, retrieval-augmented generation, medical AI, fine-tuning, evaluation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.
<div><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates domain adaptation of open-weight large language models for electrocardiography by fine‑tuning Llama 3.1 70B on specialized literature and comparing it to retrieval‑augmented generation (RAG) and Claude Sonnet 3.7 across multiple evaluation methods. Results show that the fine‑tuned model outperforms its base version and achieves competitive performance with proprietary models, though human experts favour Claude 3.7 and RAG for complex queries, highlighting evaluation heterogeneity.", "summary_cn": "本文研究了在心电图（electrocardiography）领域对开源大语言模型进行域适配的效果，通过在专门文献上微调 Llama 3.1 70B，并与检索增强生成（RAG）以及 Claude Sonnet 3.7 在多种评估方式下进行比较。结果表明，微调模型在多数评估中优于基模型，并在与商业模型的竞争中表现出色，但在人类专家对复杂查询的评估中更倾向于 Claude 3.7 和 RAG，凸显了评估方法的多样性与复杂性。", "keywords": "electrocardiography, domain adaptation, large language model, Llama 3.1, retrieval-augmented generation, medical AI, fine-tuning, evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lara Ahrens", "Wilhelm Haverkamp", "Nils Strodthoff"]}
]]></acme>

<pubDate>2025-10-21T06:45:38+00:00</pubDate>
</item>
<item>
<title>From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</title>
<link>https://papers.cool/arxiv/2510.18297</link>
<guid>https://papers.cool/arxiv/2510.18297</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MedRGAG, a framework that unifies external retrieval and parametric generation to improve medical question answering by jointly leveraging retrieved evidence and generator‑produced background documents. It proposes two modules—Knowledge‑Guided Context Completion to fill gaps in retrieved knowledge, and Knowledge‑Aware Document Selection to compose concise evidence—resulting in notable performance gains on several benchmarks.<br><strong>Summary (CN):</strong> 本文提出 MedRGAG 框架，统一外部检索与模型内部生成，以提升医学问答系统的准确性。通过“知识引导的上下文补全”模块补足检索缺失的知识，并使用“知识感知文档选择”模块构建简洁且完整的证据集合，在多个医学 QA 基准上实现显著提升。<br><strong>Keywords:</strong> medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge-guided context completion, knowledge-aware document selection, MedRGAG, external knowledge, parametric knowledge, hallucination mitigation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG
<div><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MedRGAG, a framework that unifies external retrieval and parametric generation to improve medical question answering by jointly leveraging retrieved evidence and generator‑produced background documents. It proposes two modules—Knowledge‑Guided Context Completion to fill gaps in retrieved knowledge, and Knowledge‑Aware Document Selection to compose concise evidence—resulting in notable performance gains on several benchmarks.", "summary_cn": "本文提出 MedRGAG 框架，统一外部检索与模型内部生成，以提升医学问答系统的准确性。通过“知识引导的上下文补全”模块补足检索缺失的知识，并使用“知识感知文档选择”模块构建简洁且完整的证据集合，在多个医学 QA 基准上实现显著提升。", "keywords": "medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge-guided context completion, knowledge-aware document selection, MedRGAG, external knowledge, parametric knowledge, hallucination mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lei Li", "Xiao Zhou", "Yingying Zhang", "Xian Wu"]}
]]></acme>

<pubDate>2025-10-21T04:58:29+00:00</pubDate>
</item>
<item>
<title>Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata</title>
<link>https://papers.cool/arxiv/2510.18289</link>
<guid>https://papers.cool/arxiv/2510.18289</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Food4All introduces a multi‑agent framework that aggregates heterogeneous data sources and uses a lightweight reinforcement‑learning algorithm to provide real‑time, geographically‑aware recommendations of free food resources with nutritional annotations. The system continuously updates its resource pool from official databases, community platforms and social media, and incorporates a feedback loop to adapt to user needs, aiming to improve access for food‑insecure populations.<br><strong>Summary (CN):</strong> Food4All 提出一个多代理框架，通过聚合官方数据库、社区平台和社交媒体等异构数据，并使用轻量级强化学习算法，实时提供地理可达且带有营养标注的免费食物资源推荐。系统通过在线反馈循环持续更新资源池并适应用户需求，旨在提升食物不安全人群的获取渠道。<br><strong>Keywords:</strong> multi-agent system, reinforcement learning, food insecurity, real-time retrieval, nutritional metadata, data aggregation, resource recommendation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br><strong>Authors:</strong> Zhengqing Yuan, Yiyang Li, Weixiang Sun, Zheyuan Zhang, Kaiwen Shi, Keerthiram Murugesan, Yanfang Ye</div>
Food insecurity remains a persistent public health emergency in the United States, tightly interwoven with chronic disease, mental illness, and opioid misuse. Yet despite the existence of thousands of food banks and pantries, access remains fragmented: 1) current retrieval systems depend on static directories or generic search engines, which provide incomplete and geographically irrelevant results; 2) LLM-based chatbots offer only vague nutritional suggestions and fail to adapt to real-world constraints such as time, mobility, and transportation; and 3) existing food recommendation systems optimize for culinary diversity but overlook survival-critical needs of food-insecure populations, including immediate proximity, verified availability, and contextual barriers. These limitations risk leaving the most vulnerable individuals, those experiencing homelessness, addiction, or digital illiteracy, unable to access urgently needed resources. To address this, we introduce Food4All, the first multi-agent framework explicitly designed for real-time, context-aware free food retrieval. Food4All unifies three innovations: 1) heterogeneous data aggregation across official databases, community platforms, and social media to provide a continuously updated pool of food resources; 2) a lightweight reinforcement learning algorithm trained on curated cases to optimize for both geographic accessibility and nutritional correctness; and 3) an online feedback loop that dynamically adapts retrieval policies to evolving user needs. By bridging information acquisition, semantic analysis, and decision support, Food4All delivers nutritionally annotated and guidance at the point of need. This framework establishes an urgent step toward scalable, equitable, and intelligent systems that directly support populations facing food insecurity and its compounding health risks.
<div><strong>Authors:</strong> Zhengqing Yuan, Yiyang Li, Weixiang Sun, Zheyuan Zhang, Kaiwen Shi, Keerthiram Murugesan, Yanfang Ye</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Food4All introduces a multi‑agent framework that aggregates heterogeneous data sources and uses a lightweight reinforcement‑learning algorithm to provide real‑time, geographically‑aware recommendations of free food resources with nutritional annotations. The system continuously updates its resource pool from official databases, community platforms and social media, and incorporates a feedback loop to adapt to user needs, aiming to improve access for food‑insecure populations.", "summary_cn": "Food4All 提出一个多代理框架，通过聚合官方数据库、社区平台和社交媒体等异构数据，并使用轻量级强化学习算法，实时提供地理可达且带有营养标注的免费食物资源推荐。系统通过在线反馈循环持续更新资源池并适应用户需求，旨在提升食物不安全人群的获取渠道。", "keywords": "multi-agent system, reinforcement learning, food insecurity, real-time retrieval, nutritional metadata, data aggregation, resource recommendation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Zhengqing Yuan", "Yiyang Li", "Weixiang Sun", "Zheyuan Zhang", "Kaiwen Shi", "Keerthiram Murugesan", "Yanfang Ye"]}
]]></acme>

<pubDate>2025-10-21T04:35:02+00:00</pubDate>
</item>
<item>
<title>BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks</title>
<link>https://papers.cool/arxiv/2510.18288</link>
<guid>https://papers.cool/arxiv/2510.18288</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BrailleLLM, an instruction‑tuned large language model designed for Braille‑related tasks such as Braille translation, formula‑to‑Braille conversion, and mixed‑text translation. To overcome data scarcity, the authors release English and Chinese Braille mixed datasets with mathematical formulas and propose a syntax‑tree‑based augmentation technique, as well as Braille Knowledge‑Based Fine‑Tuning (BKFT) to improve contextual learning. Experiments show that BKFT yields substantial gains over standard fine‑tuning, providing a foundation for low‑resource multilingual Braille research.<br><strong>Summary (CN):</strong> 本文提出 BrailleLLM，一种通过指令微调的 大语言模型（LLM），用于盲文翻译、公式转盲文以及混合文本翻译等盲文相关任务。为缓解数据稀缺，作者发布了英中文盲文混合数据集（EBMD/CBMD），并提出基于语法树的增强方法和盲文知识驱动的微调（BKFT），显著提升了模型在盲文翻译场景中的表现，为低资源多语言盲文研究奠定基础。<br><strong>Keywords:</strong> Braille, instruction tuning, low-resource language modeling, data augmentation, mixed-text translation, knowledge-based fine-tuning, multilingual, accessibility<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - other; Primary focus - other<br><strong>Authors:</strong> Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu</div>
Braille plays a vital role in education and information accessibility for visually impaired individuals. However, Braille information processing faces challenges such as data scarcity and ambiguities in mixed-text contexts. We construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with mathematical formulas to support diverse Braille domain research, and propose a syntax tree-based augmentation method tailored for Braille data. To address the underperformance of traditional fine-tuning methods in Braille-related tasks, we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the learning difficulty of Braille contextual features. BrailleLLM employs BKFT via instruction tuning to achieve unified Braille translation, formula-to-Braille conversion, and mixed-text translation. Experiments demonstrate that BKFT achieves significant performance improvements over conventional fine-tuning in Braille translation scenarios. Our open-sourced datasets and methodologies establish a foundation for low-resource multilingual Braille research.
<div><strong>Authors:</strong> Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BrailleLLM, an instruction‑tuned large language model designed for Braille‑related tasks such as Braille translation, formula‑to‑Braille conversion, and mixed‑text translation. To overcome data scarcity, the authors release English and Chinese Braille mixed datasets with mathematical formulas and propose a syntax‑tree‑based augmentation technique, as well as Braille Knowledge‑Based Fine‑Tuning (BKFT) to improve contextual learning. Experiments show that BKFT yields substantial gains over standard fine‑tuning, providing a foundation for low‑resource multilingual Braille research.", "summary_cn": "本文提出 BrailleLLM，一种通过指令微调的 大语言模型（LLM），用于盲文翻译、公式转盲文以及混合文本翻译等盲文相关任务。为缓解数据稀缺，作者发布了英中文盲文混合数据集（EBMD/CBMD），并提出基于语法树的增强方法和盲文知识驱动的微调（BKFT），显著提升了模型在盲文翻译场景中的表现，为低资源多语言盲文研究奠定基础。", "keywords": "Braille, instruction tuning, low-resource language modeling, data augmentation, mixed-text translation, knowledge-based fine-tuning, multilingual, accessibility", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Tianyuan Huang", "Zepeng Zhu", "Hangdi Xing", "Zirui Shao", "Zhi Yu", "Chaoxiong Yang", "Jiaxian He", "Xiaozhong Liu", "Jiajun Bu"]}
]]></acme>

<pubDate>2025-10-21T04:33:05+00:00</pubDate>
</item>
<item>
<title>Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18279</link>
<guid>https://papers.cool/arxiv/2510.18279</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes rendering long textual inputs as a single image and feeding it to multimodal decoder LLMs, achieving roughly a 50% reduction in required decoder tokens while maintaining performance on tasks such as long‑context retrieval (RULER) and document summarization (CNN/DailyMail). Experiments show that this visual text representation acts as an effective form of input compression without degrading downstream results.<br><strong>Summary (CN):</strong> 本文提出将冗长的文本渲染为单幅图像，并直接输入多模态解码器 LLM，从而在保持性能的前提下将解码器 token 使用率降低约一半。通过在 RULER（长上下文检索）和 CNN/DailyMail（文档摘要）等基准上的实验，验证了视觉文本作为输入压缩方法的实用性和有效性。<br><strong>Keywords:</strong> token efficiency, multimodal LLM, visual text, input compression, decoder tokens, long-context retrieval, summarization, image rendering<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.
<div><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes rendering long textual inputs as a single image and feeding it to multimodal decoder LLMs, achieving roughly a 50% reduction in required decoder tokens while maintaining performance on tasks such as long‑context retrieval (RULER) and document summarization (CNN/DailyMail). Experiments show that this visual text representation acts as an effective form of input compression without degrading downstream results.", "summary_cn": "本文提出将冗长的文本渲染为单幅图像，并直接输入多模态解码器 LLM，从而在保持性能的前提下将解码器 token 使用率降低约一半。通过在 RULER（长上下文检索）和 CNN/DailyMail（文档摘要）等基准上的实验，验证了视觉文本作为输入压缩方法的实用性和有效性。", "keywords": "token efficiency, multimodal LLM, visual text, input compression, decoder tokens, long-context retrieval, summarization, image rendering", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanhong Li", "Zixuan Lan", "Jiawei Zhou"]}
]]></acme>

<pubDate>2025-10-21T04:07:20+00:00</pubDate>
</item>
<item>
<title>DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization</title>
<link>https://papers.cool/arxiv/2510.18257</link>
<guid>https://papers.cool/arxiv/2510.18257</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DelvePO introduces a direction‑guided, self‑evolving framework for flexible prompt optimization that decouples prompts into components and employs a working memory to mitigate LLM uncertainties. By iteratively generating and refining prompts, the method achieves stable, transferable improvements across diverse tasks and models, outperform prior state‑of‑the‑art techniques in extensive experiments.<br><strong>Summary (CN):</strong> DelvePO 提出了一种方向引导的自我演化提示优化框架，通过将提示拆分为不同组件并引入工作记忆，帮助大型语言模型克服自身不确定性并获取关键洞见，以生成新提示。该方法在多个任务和不同模型上实现了稳定、可迁移的性能提升，实验结果显示其在相同设置下持续优于之前的最方法。<br><strong>Keywords:</strong> prompt optimization, large language models, self-evolving framework, direction-guided, working memory, task-agnostic, transferability, prompt engineering, L steering, experimental evaluation<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\textbf{DelvePO}$ ($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.
<div><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DelvePO introduces a direction‑guided, self‑evolving framework for flexible prompt optimization that decouples prompts into components and employs a working memory to mitigate LLM uncertainties. By iteratively generating and refining prompts, the method achieves stable, transferable improvements across diverse tasks and models, outperform prior state‑of‑the‑art techniques in extensive experiments.", "summary_cn": "DelvePO 提出了一种方向引导的自我演化提示优化框架，通过将提示拆分为不同组件并引入工作记忆，帮助大型语言模型克服自身不确定性并获取关键洞见，以生成新提示。该方法在多个任务和不同模型上实现了稳定、可迁移的性能提升，实验结果显示其在相同设置下持续优于之前的最方法。", "keywords": "prompt optimization, large language models, self-evolving framework, direction-guided, working memory, task-agnostic, transferability, prompt engineering, L steering, experimental evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Tao Tao", "Guanghui Zhu", "Lang Guo", "Hongyi Chen", "Chunfeng Yuan", "Yihua Huang"]}
]]></acme>

<pubDate>2025-10-21T03:28:53+00:00</pubDate>
</item>
<item>
<title>MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives</title>
<link>https://papers.cool/arxiv/2510.18201</link>
<guid>https://papers.cool/arxiv/2510.18201</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MARCUS, an event‑centric NLP pipeline that extracts events, participant characters, implied emotions, and sentiment from narratives, then aggregates inter‑character relations to generate quantitative character arcs illustrated on the Harry Potter and Lord of the Rings series. The authors evaluate the approach, discuss challenges, and outline potential applications of the generated arcs.<br><strong>Summary (CN):</strong> 本文提出 MARCUS（Modelling Arcs for Understanding Stories），一种以事件为中心的 NLP 流水线，用于从叙事文本中抽取事件、角色、情感与情绪，并汇总角色间的关系以生成可量化的角色弧线，案例包括《哈利·波特》和《指环王》。文中对该方法进行评估，阐述现存挑战，并探讨其后续应用前景。<br><strong>Keywords:</strong> character arcs, narrative analysis, event extraction, sentiment analysis, relational modeling, NLP pipeline, story understanding, fantasy literature<br><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Sriharsh Bhyravajjula, Ujwal Narayan, Manish Shrivastava</div>
Character arcs are important theoretical devices employed in literary studies to understand character journeys, identify tropes across literary genres, and establish similarities between narratives. This work addresses the novel task of computationally generating event-centric, relation-based character arcs from narratives. Providing a quantitative representation for arcs brings tangibility to a theoretical concept and paves the way for subsequent applications. We present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that extracts events, participant characters, implied emotion, and sentiment to model inter-character relations. MARCUS tracks and aggregates these relations across the narrative to generate character arcs as graphical plots. We generate character arcs from two extended fantasy series, Harry Potter and Lord of the Rings. We evaluate our approach before outlining existing challenges, suggesting applications of our pipeline, and discussing future work.
<div><strong>Authors:</strong> Sriharsh Bhyravajjula, Ujwal Narayan, Manish Shrivastava</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MARCUS, an event‑centric NLP pipeline that extracts events, participant characters, implied emotions, and sentiment from narratives, then aggregates inter‑character relations to generate quantitative character arcs illustrated on the Harry Potter and Lord of the Rings series. The authors evaluate the approach, discuss challenges, and outline potential applications of the generated arcs.", "summary_cn": "本文提出 MARCUS（Modelling Arcs for Understanding Stories），一种以事件为中心的 NLP 流水线，用于从叙事文本中抽取事件、角色、情感与情绪，并汇总角色间的关系以生成可量化的角色弧线，案例包括《哈利·波特》和《指环王》。文中对该方法进行评估，阐述现存挑战，并探讨其后续应用前景。", "keywords": "character arcs, narrative analysis, event extraction, sentiment analysis, relational modeling, NLP pipeline, story understanding, fantasy literature", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sriharsh Bhyravajjula", "Ujwal Narayan", "Manish Shrivastava"]}
]]></acme>

<pubDate>2025-10-21T01:03:48+00:00</pubDate>
</item>
<item>
<title>Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge</title>
<link>https://papers.cool/arxiv/2510.18196</link>
<guid>https://papers.cool/arxiv/2510.18196</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies a score range bias in using large language models (LLMs) as judges for direct assessment, where the model outputs are highly sensitive to the predefined scoring interval. To mitigate this bias, the authors introduce a contrastive decoding technique, achieving up to a 11.3% relative improvement in Spearman correlation with human judgments across various score ranges.<br><strong>Summary (CN):</strong> 本文指出在使用大型语言模型（LLM）作为评审者进行直接打分时存在分数区间偏差，即模型输出对预设评分范围高度敏感。作者提出采用对比解码方法来缓解此偏差，在不同分数区间下相较于人类评判的 Spearman 相关性提升最高 11.3%。<br><strong>Keywords:</strong> contrastive decoding, LLM as judge, score range bias, evaluation reliability, Spearman correlation, human judgment alignment, language model evaluation<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Yoshinari Fujinuma</div>
Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.
<div><strong>Authors:</strong> Yoshinari Fujinuma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies a score range bias in using large language models (LLMs) as judges for direct assessment, where the model outputs are highly sensitive to the predefined scoring interval. To mitigate this bias, the authors introduce a contrastive decoding technique, achieving up to a 11.3% relative improvement in Spearman correlation with human judgments across various score ranges.", "summary_cn": "本文指出在使用大型语言模型（LLM）作为评审者进行直接打分时存在分数区间偏差，即模型输出对预设评分范围高度敏感。作者提出采用对比解码方法来缓解此偏差，在不同分数区间下相较于人类评判的 Spearman 相关性提升最高 11.3%。", "keywords": "contrastive decoding, LLM as judge, score range bias, evaluation reliability, Spearman correlation, human judgment alignment, language model evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yoshinari Fujinuma"]}
]]></acme>

<pubDate>2025-10-21T00:47:11+00:00</pubDate>
</item>
<item>
<title>CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18173</link>
<guid>https://papers.cool/arxiv/2510.18173</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires large language models to generate dynamic tables under evolving schemas, probing robustness through extractive-cue ablation, temporal prefixing, and entity-form perturbations. Experiments on state-of-the-art LLMs reveal substantial performance drops when extractive shortcuts are removed, with longer contexts and altered entity forms further degrading accuracy, indicating brittle reasoning rather than random noise. The authors argue that robustness-first evaluation is essential for developing efficient and scalable text-to-table systems.<br><strong>Summary (CN):</strong> 本文提出 CMT-Bench 基准，利用实时板球解说数据要求大语言模型在不断演变的模式下生成动态表格，并通过抽取线索消融、时间前缀以及实体形式扰动三类语义保持的改动来检验模型的鲁棒性。对最前沿 LLM 的实验显示，去除抽取摘要会导致显著性能下降，且输入长度增加和实体形式变化进一步削弱准确率，表明模型推理易出现漂移而非仅仅噪声。作者主张在文本到表格任务中应首先评估鲁棒性，以推动高效可扩展的方法开发。<br><strong>Keywords:</strong> text-to-table, dynamic table generation, robustness, long-context, state tracking, benchmark, cricket commentary, LLM evaluation<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br><strong>Authors:</strong> Ritam Upadhyay, Naman Ahuja, Rishabh Baral, Aparna Garimella, Vivek Gupta</div>
LLM Driven text-to-table (T2T) systems often rely on extensive prompt-engineering or iterative event extraction in code-parsable formats, which boosts scores but are computationally expensive and obscure how models actually reason over temporal evolving narratives to summarise key information. We present CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires dynamic table generation across two evolving schemas under a dense, rule-governed policy. CMT-Bench is designed to probe robustness via three semantics-preserving dimensions: (i) extractive-cue ablation to separate extractive shortcuts from state tracking, (ii) temporal prefixing to test long-context stability, and (iii) entity-form perturbations (anonymization, outof-distribution substitutions, role-entangling paraphrases) to assess sensitivity to surface variation. Across diverse long-context stateof-the-art LLMs, we find large drops without extractive summaries, monotonic degradation with input length, and consistent accuracy drop under entity-form changes. Complementary distributional tests confirm significant shifts in numeric error patterns, indicating drift in reasoning rather than mere noise. Our results show that current LLMs are brittle in dynamic Textto-table generation, motivating robustness-first evaluation as a prerequisite for developing efficient and scalable approaches for this task.
<div><strong>Authors:</strong> Ritam Upadhyay, Naman Ahuja, Rishabh Baral, Aparna Garimella, Vivek Gupta</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires large language models to generate dynamic tables under evolving schemas, probing robustness through extractive-cue ablation, temporal prefixing, and entity-form perturbations. Experiments on state-of-the-art LLMs reveal substantial performance drops when extractive shortcuts are removed, with longer contexts and altered entity forms further degrading accuracy, indicating brittle reasoning rather than random noise. The authors argue that robustness-first evaluation is essential for developing efficient and scalable text-to-table systems.", "summary_cn": "本文提出 CMT-Bench 基准，利用实时板球解说数据要求大语言模型在不断演变的模式下生成动态表格，并通过抽取线索消融、时间前缀以及实体形式扰动三类语义保持的改动来检验模型的鲁棒性。对最前沿 LLM 的实验显示，去除抽取摘要会导致显著性能下降，且输入长度增加和实体形式变化进一步削弱准确率，表明模型推理易出现漂移而非仅仅噪声。作者主张在文本到表格任务中应首先评估鲁棒性，以推动高效可扩展的方法开发。", "keywords": "text-to-table, dynamic table generation, robustness, long-context, state tracking, benchmark, cricket commentary, LLM evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ritam Upadhyay", "Naman Ahuja", "Rishabh Baral", "Aparna Garimella", "Vivek Gupta"]}
]]></acme>

<pubDate>2025-10-20T23:51:28+00:00</pubDate>
</item>
<item>
<title>Automatic Prompt Generation via Adaptive Selection of Prompting Techniques</title>
<link>https://papers.cool/arxiv/2510.18162</link>
<guid>https://papers.cool/arxiv/2510.18162</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method that automatically selects and combines prompting techniques for large language models by mapping user-provided task descriptions to semantically similar task clusters stored in a knowledge base, then generating high-quality prompts without pre‑existing templates. Experiments on 23 BIG‑Bench Extra Hard tasks show the approach outperforms standard prompts and existing automatic prompt‑generation tools in both arithmetic and harmonic mean scores. This work aims to make prompt engineering accessible to non‑experts by streamlining prompt creation.<br><strong>Summary (CN):</strong> 本文提出一种自动生成大型语言模型提示的方法：通过将用户的任务描述映射到语义相似的任务聚类，并从知识库中选取相应的提示技术组合，生成高质量的提示，而无需预设模板。 在 23 项 BIG‑Bench Extra Hard 任务上的实验表明，该方法在算术平均和调和平均得分上均优于标准提示和已有的自动提示生成工具。 此工作旨在通过标准化提示创建流程，使非专业用户也能有效利用大语言模型。<br><strong>Keywords:</strong> automatic prompt generation, adaptive prompting, task clustering, large language models, prompt engineering, BIG-Bench Extra Hard, knowledge base<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
Prompt engineering is crucial for achieving reliable and effective outputs from large language models (LLMs), but its design requires specialized knowledge of prompting techniques and a deep understanding of target tasks. To address this challenge, we propose a novel method that adaptively selects task-appropriate prompting techniques based on users' abstract task descriptions and automatically generates high-quality prompts without relying on pre-existing templates or frameworks. The proposed method constructs a knowledge base that associates task clusters, characterized by semantic similarity across diverse tasks, with their corresponding prompting techniques. When users input task descriptions, the system assigns them to the most relevant task cluster and dynamically generates prompts by integrating techniques drawn from the knowledge base. An experimental evaluation of the proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates superior performance compared with standard prompts and existing automatic prompt-generation tools, as measured by both arithmetic and harmonic mean scores. This research establishes a foundation for streamlining and standardizing prompt creation, enabling non-experts to effectively leverage LLMs.
<div><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method that automatically selects and combines prompting techniques for large language models by mapping user-provided task descriptions to semantically similar task clusters stored in a knowledge base, then generating high-quality prompts without pre‑existing templates. Experiments on 23 BIG‑Bench Extra Hard tasks show the approach outperforms standard prompts and existing automatic prompt‑generation tools in both arithmetic and harmonic mean scores. This work aims to make prompt engineering accessible to non‑experts by streamlining prompt creation.", "summary_cn": "本文提出一种自动生成大型语言模型提示的方法：通过将用户的任务描述映射到语义相似的任务聚类，并从知识库中选取相应的提示技术组合，生成高质量的提示，而无需预设模板。 在 23 项 BIG‑Bench Extra Hard 任务上的实验表明，该方法在算术平均和调和平均得分上均优于标准提示和已有的自动提示生成工具。 此工作旨在通过标准化提示创建流程，使非专业用户也能有效利用大语言模型。", "keywords": "automatic prompt generation, adaptive prompting, task clustering, large language models, prompt engineering, BIG-Bench Extra Hard, knowledge base", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yohei Ikenoue", "Hitomi Tashiro", "Shigeru Kuroyanagi"]}
]]></acme>

<pubDate>2025-10-20T23:28:23+00:00</pubDate>
</item>
<item>
<title>Extracting Rule-based Descriptions of Attention Features in Transformers</title>
<link>https://papers.cool/arxiv/2510.18148</link>
<guid>https://papers.cool/arxiv/2510.18148</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes rule-based descriptions for Sparse Autoencoder (SAE) features extracted from transformer attention layers, moving beyond exemplar-based interpretations. It defines three rule types—skip-gram, absence, and counting rules—and presents an automatic extraction method applied to GPT-2 small, showing that many features can be captured with concise rule sets and revealing early-layer absence and counting behaviors. This work establishes a taxonomy and baseline for future rule-based feature interpretation research.<br><strong>Summary (CN):</strong> 本文提出为 Transformer 注意力层的稀疏自编码器（SAE）特征提供基于规则的描述，超越仅靠示例进行解释的方法。文中定义了三类规则：跳词（skip-gram）规则、缺失规则和计数规则，并设计了自动提取流程，在 GPT-2 small 上验证，大多数特征可用约 100 条跳词规则描述，同时在早期层发现大量缺失规则并给出少量计数规则示例。该工作为基于规则的特征解释奠定了分类框架和初步基准。<br><strong>Keywords:</strong> rule-based interpretability, SAE, attention, skip-gram rules, counting rules, transformer, mechanistic interpretability<br><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
Mechanistic interpretability strives to explain model behavior in terms of bottom-up primitives. The leading paradigm is to express hidden states as a sparse linear combination of basis vectors, called features. However, this only identifies which text sequences (exemplars) activate which features; the actual interpretation of features requires subjective inspection of these exemplars. This paper advocates for a different solution: rule-based descriptions that match token patterns in the input and correspondingly increase or decrease the likelihood of specific output tokens. Specifically, we extract rule-based descriptions of SAE features trained on the outputs of attention layers. While prior work treats the attention layers as an opaque box, we describe how it may naturally be expressed in terms of interactions between input and output features, of which we study three types: (1) skip-gram rules of the form "[Canadian city]... speaks --> English", (2) absence rules of the form "[Montreal]... speaks -/-> English," and (3) counting rules that toggle only when the count of a word exceeds a certain value or the count of another word. Absence and counting rules are not readily discovered by inspection of exemplars, where manual and automatic descriptions often identify misleading or incomplete explanations. We then describe a simple approach to extract these types of rules automatically from a transformer, and apply it to GPT-2 small. We find that a majority of features may be described well with around 100 skip-gram rules, though absence rules are abundant even as early as the first layer (in over a fourth of features). We also isolate a few examples of counting rules. This paper lays the groundwork for future research into rule-based descriptions of features by defining them, showing how they may be extracted, and providing a preliminary taxonomy of some of the behaviors they represent.
<div><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes rule-based descriptions for Sparse Autoencoder (SAE) features extracted from transformer attention layers, moving beyond exemplar-based interpretations. It defines three rule types—skip-gram, absence, and counting rules—and presents an automatic extraction method applied to GPT-2 small, showing that many features can be captured with concise rule sets and revealing early-layer absence and counting behaviors. This work establishes a taxonomy and baseline for future rule-based feature interpretation research.", "summary_cn": "本文提出为 Transformer 注意力层的稀疏自编码器（SAE）特征提供基于规则的描述，超越仅靠示例进行解释的方法。文中定义了三类规则：跳词（skip-gram）规则、缺失规则和计数规则，并设计了自动提取流程，在 GPT-2 small 上验证，大多数特征可用约 100 条跳词规则描述，同时在早期层发现大量缺失规则并给出少量计数规则示例。该工作为基于规则的特征解释奠定了分类框架和初步基准。", "keywords": "rule-based interpretability, SAE, attention, skip-gram rules, counting rules, transformer, mechanistic interpretability", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Dan Friedman", "Adithya Bhaskar", "Alexander Wettig", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-20T22:52:40+00:00</pubDate>
</item>
<item>
<title>LLMs Encode How Difficult Problems Are</title>
<link>https://papers.cool/arxiv/2510.18147</link>
<guid>https://papers.cool/arxiv/2510.18147</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models internally encode problem difficulty in a way that aligns with human judgments, using linear probes across layers and token positions on 60 models. It finds that human-labeled difficulty is strongly decodable and scales with model size, while model-derived difficulty does not, and that steering models toward easier representations reduces hallucinations and improves accuracy, especially during reinforcement learning post‑training. The authors release code for probing and evaluation to promote replication.<br><strong>Summary (CN):</strong> 本文研究大型语言模型是否内部编码了与人类判断一致的问题难度，通过在60个模型的不同层和位置上使用线性探针进行分析。结果显示，人类标注的难度在模型内部高度可线性解码且随模型规模提升，而模型自身推导的难度信号较弱且扩展性差；将模型表征引导至“更容易”方向可以降低幻觉并提升准确率，尤其在强化学习后训练阶段表现明显。作者公开了探针代码及评估脚本以便复现。<br><strong>Keywords:</strong> difficulty encoding, linear probes, LLM interpretability, hallucination reduction, reinforcement learning post-training, Easy2HardBench, model scaling<br><strong>Scores:</strong> Interpretability: 7, Understanding: 8, Safety: 7, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br><strong>Authors:</strong> William Lugoloobi, Chris Russell</div>
Large language models exhibit a puzzling inconsistency: they solve complex problems yet frequently fail on seemingly simpler ones. We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment, and whether this representation tracks generalization during reinforcement learning post-training. We train linear probes across layers and token positions on 60 models, evaluating on mathematical and coding subsets of Easy2HardBench. We find that human-labeled difficulty is strongly linearly decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling, whereas LLM-derived difficulty is substantially weaker and scales poorly. Steering along the difficulty direction reveals that pushing models toward "easier" representations reduces hallucination and improves accuracy. During GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and positively correlates with test accuracy across training steps, while the LLM-difficulty probe degrades and negatively correlates with performance. These results suggest that human annotations provide a stable difficulty signal that RL amplifies, while automated difficulty estimates derived from model performance become misaligned precisely as models improve. We release probe code and evaluation scripts to facilitate replication.
<div><strong>Authors:</strong> William Lugoloobi, Chris Russell</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models internally encode problem difficulty in a way that aligns with human judgments, using linear probes across layers and token positions on 60 models. It finds that human-labeled difficulty is strongly decodable and scales with model size, while model-derived difficulty does not, and that steering models toward easier representations reduces hallucinations and improves accuracy, especially during reinforcement learning post‑training. The authors release code for probing and evaluation to promote replication.", "summary_cn": "本文研究大型语言模型是否内部编码了与人类判断一致的问题难度，通过在60个模型的不同层和位置上使用线性探针进行分析。结果显示，人类标注的难度在模型内部高度可线性解码且随模型规模提升，而模型自身推导的难度信号较弱且扩展性差；将模型表征引导至“更容易”方向可以降低幻觉并提升准确率，尤其在强化学习后训练阶段表现明显。作者公开了探针代码及评估脚本以便复现。", "keywords": "difficulty encoding, linear probes, LLM interpretability, hallucination reduction, reinforcement learning post-training, Easy2HardBench, model scaling", "scoring": {"interpretability": 7, "understanding": 8, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["William Lugoloobi", "Chris Russell"]}
]]></acme>

<pubDate>2025-10-20T22:48:23+00:00</pubDate>
</item>
<item>
<title>Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment</title>
<link>https://papers.cool/arxiv/2510.18112</link>
<guid>https://papers.cool/arxiv/2510.18112</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether reasoning‑enhanced large language models improve the ability of agents to predict player actions in Dungeons & Dragons and generate Avrae Discord bot commands. Using the FIREBALL dataset, the authors compare a reasoning model (DeepSeek‑R1‑Distill‑LLaMA‑8B) with an instruction‑tuned model (LLaMA‑3.1‑8B‑Instruct) and find that precise prompt instructions dominate performance, with instruction models being sufficient for the task. Their experiments demonstrate that even single‑sentence changes in prompts can substantially affect model outputs.<br><strong>Summary (CN):</strong> 本文研究了加入推理能力的大型语言模型是否能提升代理预测《龙与地下城》玩家动作并生成 Avrae Discord 机器人指令的效果。使用 FIREBALL 数据集，作者比较了推理模型 DeepSeek‑R1‑Distill‑LLaMA‑8B 与指令微调模型 LLaMA‑3.1‑8B‑Instruct，发现提供精确的提示指令对性能影响更大，指令模型已足以完成该任务。实验表明，即使是提示中的单句修改也会显著改变模型输出。<br><strong>Keywords:</strong> Dungeons & Dragons, LLM agents, prompt engineering, reasoning, instruction tuning, FIREBALL dataset, Avrae, command generation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 5, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Patricia Delafuente, Arya Honraopatil, Lara J. Martin</div>
This paper explores the application of Large Language Models (LLMs) and reasoning to predict Dungeons & Dragons (DnD) player actions and format them as Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model, LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the importance of providing specific instructions to models, that even single sentence changes in prompts can greatly affect the output of models, and that instruct models are sufficient for this task compared to reasoning models.
<div><strong>Authors:</strong> Patricia Delafuente, Arya Honraopatil, Lara J. Martin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether reasoning‑enhanced large language models improve the ability of agents to predict player actions in Dungeons & Dragons and generate Avrae Discord bot commands. Using the FIREBALL dataset, the authors compare a reasoning model (DeepSeek‑R1‑Distill‑LLaMA‑8B) with an instruction‑tuned model (LLaMA‑3.1‑8B‑Instruct) and find that precise prompt instructions dominate performance, with instruction models being sufficient for the task. Their experiments demonstrate that even single‑sentence changes in prompts can substantially affect model outputs.", "summary_cn": "本文研究了加入推理能力的大型语言模型是否能提升代理预测《龙与地下城》玩家动作并生成 Avrae Discord 机器人指令的效果。使用 FIREBALL 数据集，作者比较了推理模型 DeepSeek‑R1‑Distill‑LLaMA‑8B 与指令微调模型 LLaMA‑3.1‑8B‑Instruct，发现提供精确的提示指令对性能影响更大，指令模型已足以完成该任务。实验表明，即使是提示中的单句修改也会显著改变模型输出。", "keywords": "Dungeons & Dragons, LLM agents, prompt engineering, reasoning, instruction tuning, FIREBALL dataset, Avrae, command generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Patricia Delafuente", "Arya Honraopatil", "Lara J. Martin"]}
]]></acme>

<pubDate>2025-10-20T21:23:23+00:00</pubDate>
</item>
<item>
<title>Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica</title>
<link>https://papers.cool/arxiv/2510.18108</link>
<guid>https://papers.cool/arxiv/2510.18108</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports an experimental study evaluating four AI systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) on tasks that emulate lawyers' daily work, using a protocol that measures material correctness, systematic coherence, and argumentative integrity. The domain‑specialized model JusIA consistently outperformed the general‑purpose models, indicating that specialization and theoretically grounded evaluation are crucial for reliable legal AI outputs. The study involves 48 legal professionals and proposes a systematic assessment framework for legal AI.<br><strong>Summary (CN):</strong> 本文通过实验评估四种 AI 系统（JusIA、ChatGPT Free、ChatGPT Pro 和 Gemini）在模拟律师日常工作任务中的表现，使用包括材料正确性、系统一致性和论证完整性在内的评估协议。结果显示，专门面向法律领域的模型 JusIA 始终优于通用模型，表明领域专业化和理论化评估对获得可靠的法律 AI 输出至关重要。该研究邀请了 48 名法律专业人士参与，并提出了一套系统的法律 AI 评估框架。<br><strong>Keywords:</strong> legal AI, domain specialization, AI evaluation, jurisprudence, ChatGPT, Gemini, factual correctness, argumentative integrity, AI safety, model benchmarking<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - other; Primary focus - other<br><strong>Authors:</strong> Marina Soares Marinho, Daniela Vianna, Livy Real, Altigran da Silva, Gabriela Migliorini</div>
This study presents the Jusbrasil Study on the Use of General-Purpose AIs in Law, proposing an experimental evaluation protocol combining legal theory, such as material correctness, systematic coherence, and argumentative integrity, with empirical assessment by 48 legal professionals. Four systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers' daily work. JusIA, a domain-specialized model, consistently outperformed the general-purpose systems, showing that both domain specialization and a theoretically grounded evaluation are essential for reliable legal AI outputs.
<div><strong>Authors:</strong> Marina Soares Marinho, Daniela Vianna, Livy Real, Altigran da Silva, Gabriela Migliorini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports an experimental study evaluating four AI systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) on tasks that emulate lawyers' daily work, using a protocol that measures material correctness, systematic coherence, and argumentative integrity. The domain‑specialized model JusIA consistently outperformed the general‑purpose models, indicating that specialization and theoretically grounded evaluation are crucial for reliable legal AI outputs. The study involves 48 legal professionals and proposes a systematic assessment framework for legal AI.", "summary_cn": "本文通过实验评估四种 AI 系统（JusIA、ChatGPT Free、ChatGPT Pro 和 Gemini）在模拟律师日常工作任务中的表现，使用包括材料正确性、系统一致性和论证完整性在内的评估协议。结果显示，专门面向法律领域的模型 JusIA 始终优于通用模型，表明领域专业化和理论化评估对获得可靠的法律 AI 输出至关重要。该研究邀请了 48 名法律专业人士参与，并提出了一套系统的法律 AI 评估框架。", "keywords": "legal AI, domain specialization, AI evaluation, jurisprudence, ChatGPT, Gemini, factual correctness, argumentative integrity, AI safety, model benchmarking", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Marina Soares Marinho", "Daniela Vianna", "Livy Real", "Altigran da Silva", "Gabriela Migliorini"]}
]]></acme>

<pubDate>2025-10-20T21:09:50+00:00</pubDate>
</item>
<item>
<title>Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18077</link>
<guid>https://papers.cool/arxiv/2510.18077</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how chain-of-thought prompting improves large language models' ability to translate texts with inter‑sentential dependencies, using the English‑French DiscEvalMT benchmark for pronoun anaphora and lexical cohesion challenges. Experiments with 12 LLMs show that reasoning prompts raise discrimination accuracy to about 90% and COMET translation quality to roughly 92%, with GPT‑4, GPT‑4o and Phi achieving the best results.<br><strong>Summary (CN):</strong> 本文研究了链式思考（chain‑of‑thought）提示如何提升大型语言模型在涉及跨句依存关系的翻译任务中的表现，使用英法 DiscEvalMT 基准考察代词指代和词汇衔接难题。对 12 种 LLM 进行实验表明，使用推理提示后，判别正确翻译的准确率可达约 90%，而基于 COMET 的翻译质量约为 92%，其中 GPT‑4、GPT‑4o 与 Phi 表现尤为突出。<br><strong>Keywords:</strong> chain-of-thought, context-aware translation, inter-sentential dependencies, DiscEvalMT, LLM prompting, translation evaluation, COMET, reasoning prompts<br><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br><strong>Authors:</strong> Shabnam Ataee, Andrei Popescu-Belis</div>
This paper assesses the capacity of large language models (LLMs) to translate texts that include inter-sentential dependencies. We use the English-French DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing translation challenges either for pronominal anaphora or for lexical cohesion. We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families on two tasks: (1) distinguishing a correct translation from a wrong but plausible one; (2) generating a correct translation. We compare prompts that encourage chain-of-thought reasoning with those that do not. The best models take advantage of reasoning and reach about 90% accuracy on the first task, and COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi standing out. Moreover, we observe a "wise get wiser" effect: the improvements through reasoning are positively correlated with the scores of the models without reasoning.
<div><strong>Authors:</strong> Shabnam Ataee, Andrei Popescu-Belis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how chain-of-thought prompting improves large language models' ability to translate texts with inter‑sentential dependencies, using the English‑French DiscEvalMT benchmark for pronoun anaphora and lexical cohesion challenges. Experiments with 12 LLMs show that reasoning prompts raise discrimination accuracy to about 90% and COMET translation quality to roughly 92%, with GPT‑4, GPT‑4o and Phi achieving the best results.", "summary_cn": "本文研究了链式思考（chain‑of‑thought）提示如何提升大型语言模型在涉及跨句依存关系的翻译任务中的表现，使用英法 DiscEvalMT 基准考察代词指代和词汇衔接难题。对 12 种 LLM 进行实验表明，使用推理提示后，判别正确翻译的准确率可达约 90%，而基于 COMET 的翻译质量约为 92%，其中 GPT‑4、GPT‑4o 与 Phi 表现尤为突出。", "keywords": "chain-of-thought, context-aware translation, inter-sentential dependencies, DiscEvalMT, LLM prompting, translation evaluation, COMET, reasoning prompts", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Shabnam Ataee", "Andrei Popescu-Belis"]}
]]></acme>

<pubDate>2025-10-20T20:14:46+00:00</pubDate>
</item>
<item>
<title>Language Models as Semantic Augmenters for Sequential Recommenders</title>
<link>https://papers.cool/arxiv/2510.18046</link>
<guid>https://papers.cool/arxiv/2510.18046</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LaMAR, a framework that uses large language models in a few‑shot setting to generate auxiliary semantic signals (e.g., inferred usage scenarios, item intents) that augment sequential interaction histories for recommender systems. By enriching the original user-item sequences with these generated contextual cues, LaMAR consistently improves performance on benchmark sequential recommendation tasks and demonstrates high semantic novelty and diversity of the signals.<br><strong>Summary (CN):</strong> 本文提出 LaMAR 框架，利用大型语言模型（LLM）在少样本条件下生成辅助语义信号（如使用场景推断、物品意图），以丰富序列推荐系统中的用户‑物品交互序列。通过将这些生成的上下文信息加入原始序列，LaMAR 在基准序列推荐任务中显著提升了性能，并展示了信号的高度语义新颖性和多样性。<br><strong>Keywords:</strong> semantic augmentation, large language models, sequential recommendation, data-centric AI, few-shot prompting<br><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
Large Language Models (LLMs) excel at capturing latent semantics and contextual relationships across diverse modalities. However, in modeling user behavior from sequential interaction data, performance often suffers when such semantic context is limited or absent. We introduce LaMAR, a LLM-driven semantic enrichment framework designed to enrich such sequences automatically. LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual signals by inferring latent semantic aspects of a user's intent and item relationships from existing metadata. These generated signals, such as inferred usage scenarios, item intents, or thematic summaries, augment the original sequences with greater contextual depth. We demonstrate the utility of this generated resource by integrating it into benchmark sequential modeling tasks, where it consistently improves performance. Further analysis shows that LLM-generated signals exhibit high semantic novelty and diversity, enhancing the representational capacity of the downstream models. This work represents a new data-centric paradigm where LLMs serve as intelligent context generators, contributing a new method for the semi-automatic creation of training data and language resources.
<div><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LaMAR, a framework that uses large language models in a few‑shot setting to generate auxiliary semantic signals (e.g., inferred usage scenarios, item intents) that augment sequential interaction histories for recommender systems. By enriching the original user-item sequences with these generated contextual cues, LaMAR consistently improves performance on benchmark sequential recommendation tasks and demonstrates high semantic novelty and diversity of the signals.", "summary_cn": "本文提出 LaMAR 框架，利用大型语言模型（LLM）在少样本条件下生成辅助语义信号（如使用场景推断、物品意图），以丰富序列推荐系统中的用户‑物品交互序列。通过将这些生成的上下文信息加入原始序列，LaMAR 在基准序列推荐任务中显著提升了性能，并展示了信号的高度语义新颖性和多样性。", "keywords": "semantic augmentation, large language models, sequential recommendation, data-centric AI, few-shot prompting", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahsa Valizadeh", "Xiangjue Dong", "Rui Tuo", "James Caverlee"]}
]]></acme>

<pubDate>2025-10-20T19:36:38+00:00</pubDate>
</item>
<item>
<title>From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18030</link>
<guid>https://papers.cool/arxiv/2510.18030</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper revisits global structured pruning for LLMs and introduces GISP-Global Iterative Structured Pruning, a post‑training method that uses loss‑based importance scores aggregated at the structure level to prune attention heads and MLP channels iteratively. By defining importance with a model‑level loss, the approach naturally incorporates task‑specific objectives, achieving lower perplexity and higher downstream accuracy on several LLM families at moderate sparsity levels. Exper demonstrate that the iterative schedule stabilizes accuracy without intermediate fine‑tuning and enables a "prune‑once, deploy‑many" workflow.<br><strong>Summary (CN):</strong> 本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝方法，通过在结构层面聚合基于损失的重要性分数，对注意力头和 MLP 通道进行迭代剪枝。由于重要性由模型整体损失定义，该方法可自然地结合任务特定目标，在多种LM 上实现更低的困惑度和更高的下游任务精度，并在中等稀疏度下保持准确性而无需中间微调，实现“一次剪枝，多次部署”。<br><strong>Keywords:</strong> structured pruning, large language models, global pruning, task-aligned pruning, iterative pruning, sparsity, lama, Mistral<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a "prune-once, deploy-many" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.
<div><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper revisits global structured pruning for LLMs and introduces GISP-Global Iterative Structured Pruning, a post‑training method that uses loss‑based importance scores aggregated at the structure level to prune attention heads and MLP channels iteratively. By defining importance with a model‑level loss, the approach naturally incorporates task‑specific objectives, achieving lower perplexity and higher downstream accuracy on several LLM families at moderate sparsity levels. Exper demonstrate that the iterative schedule stabilizes accuracy without intermediate fine‑tuning and enables a \"prune‑once, deploy‑many\" workflow.", "summary_cn": "本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝方法，通过在结构层面聚合基于损失的重要性分数，对注意力头和 MLP 通道进行迭代剪枝。由于重要性由模型整体损失定义，该方法可自然地结合任务特定目标，在多种LM 上实现更低的困惑度和更高的下游任务精度，并在中等稀疏度下保持准确性而无需中间微调，实现“一次剪枝，多次部署”。", "keywords": "structured pruning, large language models, global pruning, task-aligned pruning, iterative pruning, sparsity,lama, Mistral", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"]}
]]></acme>

<pubDate>2025-10-20T19:04:09+00:00</pubDate>
</item>
<item>
<title>Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution</title>
<link>https://papers.cool/arxiv/2510.18019</link>
<guid>https://papers.cool/arxiv/2510.18019</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that current multilingual LLM watermarking methods lose effectiveness after translation, especially in medium‑ and low‑resource languages, due to limited full‑word tokens. It introduces STEAM, a back‑translation‑based detection technique that recovers watermark strength and works with any existing watermarking scheme, improving robustness across tokenizers and languages. Experiments on 17 languages report average gains of +0.19 AUC and +40 %p TPR@1 %.<br><strong>Summary (CN):</strong> 本文指出现有的多语言 LLM 水印技术在翻译攻击下（尤其是中低资源语言）失效，原因是分词器中对应语言的完整词汇不足导致语义聚类失败。为此提出 STEAM——一种基于回译的检测方法，可恢复水印强度并兼容任意水印方案，在不同分词器和语言上均表现出更强的鲁棒性。实验在 17 种语言上实现了平均 AUC 提升 0.19、TPR@1% 提升 40 %点。<br><strong>Keywords:</strong> multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization robustness, translation attack, cross-lingual robustness<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.
<div><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that current multilingual LLM watermarking methods lose effectiveness after translation, especially in medium‑ and low‑resource languages, due to limited full‑word tokens. It introduces STEAM, a back‑translation‑based detection technique that recovers watermark strength and works with any existing watermarking scheme, improving robustness across tokenizers and languages. Experiments on 17 languages report average gains of +0.19 AUC and +40 %p TPR@1 %.", "summary_cn": "本文指出现有的多语言 LLM 水印技术在翻译攻击下（尤其是中低资源语言）失效，原因是分词器中对应语言的完整词汇不足导致语义聚类失败。为此提出 STEAM——一种基于回译的检测方法，可恢复水印强度并兼容任意水印方案，在不同分词器和语言上均表现出更强的鲁棒性。实验在 17 种语言上实现了平均 AUC 提升 0.19、TPR@1% 提升 40 %点。", "keywords": "multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization robustness, translation attack, cross-lingual robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Asim Mohamed", "Martin Gubri"]}
]]></acme>

<pubDate>2025-10-20T18:51:20+00:00</pubDate>
</item>
<item>
<title>SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone</title>
<link>https://papers.cool/arxiv/2510.17998</link>
<guid>https://papers.cool/arxiv/2510.17998</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SimBA is a three‑phase framework that uses only raw evaluation scores to simplify analysis of large language‑model benchmarks. It first compares datasets and models (stalk), then discovers a small representative subset that still covers the benchmark (prowl), and finally predicts performance of held‑out models with near‑zero error using this subset (pounce). Experiments on HELM, MMLU, and BigBenchLite show strong dataset‑model relationships and high coverage using only a few percent of the original datasets.<br><strong>Summary (CN):</strong> SimBA 是一种仅利用原始评估分数的三阶段框架，用于简化大语言模型基准的分析。首先进行数据集与模型比较（stalk），随后发现能够覆盖基准的少量代表性子集（prowl），最后使用该子集预测未见模型的性能，误差几乎为零（pounce）。在 HELM、MMLU 和 BigBenchLite 上的实验表明，数据集与模型之间关系紧密，仅使用极少比例的数据集即可实现至少 95% 的覆盖率。<br><strong>Keywords:</strong> benchmark analysis, performance matrices, representative subset, model ranking, LM evaluation, dataset coverage, SimBA, predictive performance<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
Modern language models are evaluated on large benchmarks, which are difficult to make sense of, especially for model selection. Looking at the raw evaluation numbers themselves using a model-centric lens, we propose SimBA, a three phase framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk, where we conduct dataset & model comparisons, prowl, where we discover a representative subset, and pounce, where we use the representative subset to predict performance on a held-out set of models. Applying SimBA to three popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all three benchmarks, datasets and models relate strongly to one another (stalk). We develop an representative set discovery algorithm which covers a benchmark using raw evaluation scores alone. Using our algorithm, we find that with 6.25% (1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl). Additionally, using just these representative subsets, we can both preserve model ranks and predict performance on a held-out set of models with near zero mean-squared error (pounce). Taken together, SimBA can help model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark. Our code is open source, available at https://github.com/nishantsubramani/simba.
<div><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SimBA is a three‑phase framework that uses only raw evaluation scores to simplify analysis of large language‑model benchmarks. It first compares datasets and models (stalk), then discovers a small representative subset that still covers the benchmark (prowl), and finally predicts performance of held‑out models with near‑zero error using this subset (pounce). Experiments on HELM, MMLU, and BigBenchLite show strong dataset‑model relationships and high coverage using only a few percent of the original datasets.", "summary_cn": "SimBA 是一种仅利用原始评估分数的三阶段框架，用于简化大语言模型基准的分析。首先进行数据集与模型比较（stalk），随后发现能够覆盖基准的少量代表性子集（prowl），最后使用该子集预测未见模型的性能，误差几乎为零（pounce）。在 HELM、MMLU 和 BigBenchLite 上的实验表明，数据集与模型之间关系紧密，仅使用极少比例的数据集即可实现至少 95% 的覆盖率。", "keywords": "benchmark analysis, performance matrices, representative subset, model ranking, LM evaluation, dataset coverage, SimBA, predictive performance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Subramani", "Alfredo Gomez", "Mona Diab"]}
]]></acme>

<pubDate>2025-10-20T18:23:27+00:00</pubDate>
</item>
<item>
<title>Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</title>
<link>https://papers.cool/arxiv/2510.17941</link>
<guid>https://papers.cool/arxiv/2510.17941</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a framework to quantify how deeply implanted facts are believed by large language models, evaluating generalization, robustness to challenge, and similarity to genuine knowledge via linear probes. Experiments show that simple prompting and mechanistic edits rarely achieve deep belief implantation, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides measurable criteria for belief depth, enabling more rigorous assessment of knowledge editing techniques before real‑world deployment.<br><strong>Summary (CN):</strong> 本文提出了一套衡量大语言模型对植入事实信念深度的框架，评估植入知识在相关上下文中的泛化、对自我审视与直接挑战的鲁棒性，以及通过线性探测器与真实知识的表征相似性。实验表明，简单提示和机制性编辑难以实现深层信念植入，而合成文档微调（Synthetic Document Finetuning）常能成功，但在与基础世界知识冲突的事实上表现脆弱且表征不同。该工作提供了可量化的信念深度标准，为在实际应用中安全地使用知识编辑提供了评估手段。<br><strong>Keywords:</strong> belief depth, knowledge editing, large language models, synthetic document finetuning, linear probes, factuality, model alignment, interpretability, knowledge injection, robustness<br><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.
<div><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a framework to quantify how deeply implanted facts are believed by large language models, evaluating generalization, robustness to challenge, and similarity to genuine knowledge via linear probes. Experiments show that simple prompting and mechanistic edits rarely achieve deep belief implantation, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides measurable criteria for belief depth, enabling more rigorous assessment of knowledge editing techniques before real‑world deployment.", "summary_cn": "本文提出了一套衡量大语言模型对植入事实信念深度的框架，评估植入知识在相关上下文中的泛化、对自我审视与直接挑战的鲁棒性，以及通过线性探测器与真实知识的表征相似性。实验表明，简单提示和机制性编辑难以实现深层信念植入，而合成文档微调（Synthetic Document Finetuning）常能成功，但在与基础世界知识冲突的事实上表现脆弱且表征不同。该工作提供了可量化的信念深度标准，为在实际应用中安全地使用知识编辑提供了评估手段。", "keywords": "belief depth, knowledge editing, large language models, synthetic document finetuning, linear probes, factuality, model alignment, interpretability, knowledge injection, robustness", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Stewart Slocum", "Julian Minder", "Clément Dumas", "Henry Sleight", "Ryan Greenblatt", "Samuel Marks", "Rowan Wang"]}
]]></acme>

<pubDate>2025-10-20T16:58:54+00:00</pubDate>
</item>
<item>
<title>AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM</title>
<link>https://papers.cool/arxiv/2510.17934</link>
<guid>https://papers.cool/arxiv/2510.17934</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AtlasKV, a parametric method for integrating billion‑scale knowledge graphs into large language models with less than 20 GB VRAM, using novel components KG2KV and HiKVP that achieve sub‑linear time and memory complexity. By embedding KG triples directly into the model’s attention mechanism, AtlasKV eliminates the need for external retrievers or long context windows while preserving strong knowledge grounding and generalization. The approach is presented as scalable, effective, and adaptable to new knowledge without retraining.<br><strong>Summary (CN):</strong> 本文提出 AtlasKV，一种将十亿规模知识图谱以少于 20 GB 显存直接嵌入大语言模型的参数化方法，采用 KG2KV 与 HiKVP 实现次线性时间和内存复杂度。该方法利用模型自带的注意力机制将 KG 三元组内嵌，无需外部检索器或冗长上下文，同时保持稳健的知识对齐与泛化能力，且在加入新知识时无需重新训练。<br><strong>Keywords:</strong> knowledge graph, parametric knowledge integration, AtlasKV, KG2KV, HiKVP, LLM augmentation, low VRAM, sub-linear memory, factuality, retrieval-augmented generation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
Retrieval-augmented generation (RAG) has shown some success in augmenting large language models (LLMs) with external knowledge. However, as a non-parametric knowledge integration paradigm for LLMs, RAG methods heavily rely on external retrieval modules and the retrieved textual context prior. Especially for very large scale knowledge augmentation, they would introduce substantial inference latency due to expensive searches and much longer relevant context. In this paper, we propose a parametric knowledge integration method, called \textbf{AtlasKV}, a scalable, effective, and general way to augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with sub-linear time and memory complexity. It maintains strong knowledge grounding and generalization performance using the LLMs' inherent attention mechanism, and requires no external retrievers, long context priors, or retraining when adapting to new knowledge.
<div><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AtlasKV, a parametric method for integrating billion‑scale knowledge graphs into large language models with less than 20 GB VRAM, using novel components KG2KV and HiKVP that achieve sub‑linear time and memory complexity. By embedding KG triples directly into the model’s attention mechanism, AtlasKV eliminates the need for external retrievers or long context windows while preserving strong knowledge grounding and generalization. The approach is presented as scalable, effective, and adaptable to new knowledge without retraining.", "summary_cn": "本文提出 AtlasKV，一种将十亿规模知识图谱以少于 20 GB 显存直接嵌入大语言模型的参数化方法，采用 KG2KV 与 HiKVP 实现次线性时间和内存复杂度。该方法利用模型自带的注意力机制将 KG 三元组内嵌，无需外部检索器或冗长上下文，同时保持稳健的知识对齐与泛化能力，且在加入新知识时无需重新训练。", "keywords": "knowledge graph, parametric knowledge integration, AtlasKV, KG2KV, HiKVP, LLM augmentation, low VRAM, sub-linear memory, factuality, retrieval-augmented generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyu Huang", "Hong Ting Tsang", "Jiaxin Bai", "Xi Peng", "Gong Zhang", "Yangqiu Song"]}
]]></acme>

<pubDate>2025-10-20T15:40:14+00:00</pubDate>
</item>
<item>
<title>Diagnosing Representation Dynamics in NER Model Extension</title>
<link>https://papers.cool/arxiv/2510.17930</link>
<guid>https://papers.cool/arxiv/2510.17930</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how a BERT‑based NER system can be extended to new PII entities in noisy spoken language without harming performance on existing semantic entities. Using incremental learning diagnostics, it reveals that location tags are vulnerable due to representation overlap with PII patterns and that the background ‘O’ tag can block learning of new patterns unless its classifier is unfrozen. These findings provide a mechanistic interpretation of feature independence, representation overlap, and O‑tag plasticity in NER model adaptation.<br><strong>Summary (CN):</strong> 本文研究了在嘈杂口语数据中，将 BERT‑基准的命名实体识别（NER）模型扩展至新的个人身份信息（PII）实体时，如何保持对原有语义实体的性能。通过增量学习诊断，发现位置实体因与 PII 模式存在表征重叠而特别脆弱，同时背景 ‘O’ 标签若保持不变会阻碍新模式的学习，只有在解冻其分类器后才能“释放”这些模式。该工作 mechanistic 地解释了特征独立性、表征重叠以及 O‑标签可塑性在 NER 模型适应过程中的作用。<br><strong>Keywords:</strong> NER, representation drift, BERT, PII detection, incremental learning, feature independence, O-tag plasticity, semantic overlap, mechanistic interpretability<br><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
Extending Named Entity Recognition (NER) models to new PII entities in noisy spoken-language data is a common need. We find that jointly fine-tuning a BERT model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII (EMAIL, PHONE) results in minimal degradation for original classes. We investigate this "peaceful coexistence," hypothesizing that the model uses independent semantic vs. morphological feature mechanisms. Using an incremental learning setup as a diagnostic tool, we measure semantic drift and find two key insights. First, the LOC (location) entity is uniquely vulnerable due to a representation overlap with new PII, as it shares pattern-like features (e.g., postal codes). Second, we identify a "reverse O-tag representation drift." The model, initially trained to map PII patterns to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's classifier, allowing the background class to adapt and "release" these patterns. This work provides a mechanistic diagnosis of NER model adaptation, highlighting feature independence, representation overlap, and 'O' tag plasticity.
<div><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how a BERT‑based NER system can be extended to new PII entities in noisy spoken language without harming performance on existing semantic entities. Using incremental learning diagnostics, it reveals that location tags are vulnerable due to representation overlap with PII patterns and that the background ‘O’ tag can block learning of new patterns unless its classifier is unfrozen. These findings provide a mechanistic interpretation of feature independence, representation overlap, and O‑tag plasticity in NER model adaptation.", "summary_cn": "本文研究了在嘈杂口语数据中，将 BERT‑基准的命名实体识别（NER）模型扩展至新的个人身份信息（PII）实体时，如何保持对原有语义实体的性能。通过增量学习诊断，发现位置实体因与 PII 模式存在表征重叠而特别脆弱，同时背景 ‘O’ 标签若保持不变会阻碍新模式的学习，只有在解冻其分类器后才能“释放”这些模式。该工作 mechanistic 地解释了特征独立性、表征重叠以及 O‑标签可塑性在 NER 模型适应过程中的作用。", "keywords": "NER, representation drift, BERT, PII detection, incremental learning, feature independence, O-tag plasticity, semantic overlap, mechanistic interpretability", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Xirui Zhang", "Philippe de La Chevasnerie", "Benoit Fabre"]}
]]></acme>

<pubDate>2025-10-20T14:53:42+00:00</pubDate>
</item>
<item>
<title>Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs</title>
<link>https://papers.cool/arxiv/2510.17924</link>
<guid>https://papers.cool/arxiv/2510.17924</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a comparative study of various NLP approaches—traditional embeddings, fine‑tuned transformer models, zero‑shot/few‑shot prompting of large language models, and retrieval‑augmented generation—for detecting toxic content in online gaming chats. It evaluates each method on accuracy, processing speed, and computational cost, and proposes a hybrid moderation architecture that reduces human moderator workload while enabling continuous learning. Experiments show that fine‑tuned DistilBERT offers the best accuracy‑cost balance, providing practical guidance for cost‑effective content moderation in dynamic gaming environments.<br><strong>Summary (CN):</strong> 本文比较了多种自然语言处理方法（传统嵌入、微调的 Transformer 模型、大语言模型的零示例/少示例提示以及检索增强生成）在在线游戏聊天中检测有害言论的效果，并在分类准确率、处理速度和计算成本三个维度进行评估。同时提出了一种混合审查系统架构，通过自动检测减轻人工审查负担并支持持续学习。实验结果表明，微调的 DistilBERT 在准确率‑成本权衡上表现最佳，为在动态游戏环境中部署高效、经济的内容审查系统提供了经验依据。<br><strong>Keywords:</strong> toxicity detection, gaming chat moderation, embeddings, fine-tuned transformers, large language models, retrieval-augmented generation, cost-performance trade-off, hybrid moderation system<br><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - control<br><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer models, and retrieval-augmented generation (RAG) approaches are evaluated. The evaluation framework assesses three critical dimensions: classification accuracy, processing speed, and computational costs. A hybrid moderation system architecture is proposed that optimizes human moderator workload through automated detection and incorporates continuous learning mechanisms. The experimental results demonstrate significant performance variations across methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs. The findings provide empirical evidence for deploying cost-effective, efficient content moderation systems in dynamic online gaming environments.
<div><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a comparative study of various NLP approaches—traditional embeddings, fine‑tuned transformer models, zero‑shot/few‑shot prompting of large language models, and retrieval‑augmented generation—for detecting toxic content in online gaming chats. It evaluates each method on accuracy, processing speed, and computational cost, and proposes a hybrid moderation architecture that reduces human moderator workload while enabling continuous learning. Experiments show that fine‑tuned DistilBERT offers the best accuracy‑cost balance, providing practical guidance for cost‑effective content moderation in dynamic gaming environments.", "summary_cn": "本文比较了多种自然语言处理方法（传统嵌入、微调的 Transformer 模型、大语言模型的零示例/少示例提示以及检索增强生成）在在线游戏聊天中检测有害言论的效果，并在分类准确率、处理速度和计算成本三个维度进行评估。同时提出了一种混合审查系统架构，通过自动检测减轻人工审查负担并支持持续学习。实验结果表明，微调的 DistilBERT 在准确率‑成本权衡上表现最佳，为在动态游戏环境中部署高效、经济的内容审查系统提供了经验依据。", "keywords": "toxicity detection, gaming chat moderation, embeddings, fine-tuned transformers, large language models, retrieval-augmented generation, cost-performance trade-off, hybrid moderation system", "scoring": {"interpretability": 3, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"]}
]]></acme>

<pubDate>2025-10-20T08:03:28+00:00</pubDate>
</item>
<item>
<title>Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17922</link>
<guid>https://papers.cool/arxiv/2510.17922</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how different task decomposition approaches for large language models affect performance and cost, identifying six categorization schemes and three key influencing factors. Based on an empirical analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition method, executes it, and verifies results, achieving a Pareto‑optimal balance across multiple benchmarks.<br><strong>Summary (CN):</strong> 本文系统研究了大型语言模型任务分解方法对性能与成本的影响，提出了六种分类方案并分析了三大关键因素。基于此分析，作者设计了 Select-Then-Decompose 策略，通过动态选择分解方式、执行并验证结果，实现了在多个基准上性能‑成本的 Pareto 最优平衡。<br><strong>Keywords:</strong> task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, prompting strategy<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose.
<div><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how different task decomposition approaches for large language models affect performance and cost, identifying six categorization schemes and three key influencing factors. Based on an empirical analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition method, executes it, and verifies results, achieving a Pareto‑optimal balance across multiple benchmarks.", "summary_cn": "本文系统研究了大型语言模型任务分解方法对性能与成本的影响，提出了六种分类方案并分析了三大关键因素。基于此分析，作者设计了 Select-Then-Decompose 策略，通过动态选择分解方式、执行并验证结果，实现了在多个基准上性能‑成本的 Pareto 最优平衡。", "keywords": "task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, prompting strategy", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"]}
]]></acme>

<pubDate>2025-10-20T07:28:15+00:00</pubDate>
</item>
<item>
<title>CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections</title>
<link>https://papers.cool/arxiv/2510.17921</link>
<guid>https://papers.cool/arxiv/2510.17921</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CLAWS, a method that uses attention weights across prompt sections and model outputs to automatically classify mathematical solutions generated by LLMs into typical, creative, and hallucinated categories without human evaluation. CLAWS outperforms five existing white-box detection baselines on several 7-8B reinforcement‑learning‑trained math models across a large benchmark of 4,545 problems from major math contests. This work provides a novel, interpretable approach to assess creativity and detect hallucinations in LLM reasoning tasks.<br><strong>Summary (CN):</strong> 本文提出 CLAWS 方法，利用注意力权重在提示段落和模型输出之间的分布，自动将 LLM 生成的数学解答划分为典型、创造性和幻觉三类，无需人工评估。实验表明，CLAWS 在多个 7‑8B 规模的 RL 训练数学模型上，针对 4,545 道来自主要数学竞赛的题目，优于五种现有白盒检测基准。该工作提供了一种新颖且可解释的手段来评估创造力并检测 LLM 推理中的幻觉。<br><strong>Keywords:</strong> creativity detection, LLM, attention window, white-box detection, hallucination, mathematical reasoning, RL fine-tuning, interpretability, model evaluation<br><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a method that defines and classifies mathematical solutions into typical, creative, and hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems collected from 181 math contests (AJHSME, AMC, AIME).
<div><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CLAWS, a method that uses attention weights across prompt sections and model outputs to automatically classify mathematical solutions generated by LLMs into typical, creative, and hallucinated categories without human evaluation. CLAWS outperforms five existing white-box detection baselines on several 7-8B reinforcement‑learning‑trained math models across a large benchmark of 4,545 problems from major math contests. This work provides a novel, interpretable approach to assess creativity and detect hallucinations in LLM reasoning tasks.", "summary_cn": "本文提出 CLAWS 方法，利用注意力权重在提示段落和模型输出之间的分布，自动将 LLM 生成的数学解答划分为典型、创造性和幻觉三类，无需人工评估。实验表明，CLAWS 在多个 7‑8B 规模的 RL 训练数学模型上，针对 4,545 道来自主要数学竞赛的题目，优于五种现有白盒检测基准。该工作提供了一种新颖且可解释的手段来评估创造力并检测 LLM 推理中的幻觉。", "keywords": "creativity detection, LLM, attention window, white-box detection, hallucination, mathematical reasoning, RL fine-tuning, interpretability, model evaluation", "scoring": {"interpretability": 7, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"]}
]]></acme>

<pubDate>2025-10-20T06:59:37+00:00</pubDate>
</item>
<item>
<title>JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</title>
<link>https://papers.cool/arxiv/2510.17918</link>
<guid>https://papers.cool/arxiv/2510.17918</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> JT-Safe introduces a data augmentation strategy called Data with World Context (DWC) that enriches pre‑training corpora with real‑world contextual information and industrial‑scenario content to reduce hallucinations and improve trustworthiness of large language models. The authors continue pre‑training a 35‑billion‑parameter model on 1.5 trillion DWC tokens and demonstrate a 1.79 % improvement on safety and trustworthy benchmarks compared with a similar‑scale model. The work highlights the intrinsic role of pre‑training data in LLM safety.<br><strong>Summary (CN):</strong> JT‑Safe 提出“带世界上下文的数据”(Data with World Context, DWC) 方法，通过在预训练语料中加入真实世界的时空背景和工业场景信息，以降低大语言模型的幻觉并提升可信度。研究者在 35 B 参数的模型上继续使用 1.5 万亿 DWC 令牌进行预训练，并在安全与可信评估基准上相较于同规模模型提升 1.79%。该工作强调了预训练数据对 LLM 安全的根本影响。<br><strong>Keywords:</strong> LLM safety, hallucination mitigation, pretraining data, world context, data augmentation, trustworthiness, large language models, JT-Safe, DWC, industrial scenario data<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.
<div><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "JT-Safe introduces a data augmentation strategy called Data with World Context (DWC) that enriches pre‑training corpora with real‑world contextual information and industrial‑scenario content to reduce hallucinations and improve trustworthiness of large language models. The authors continue pre‑training a 35‑billion‑parameter model on 1.5 trillion DWC tokens and demonstrate a 1.79 % improvement on safety and trustworthy benchmarks compared with a similar‑scale model. The work highlights the intrinsic role of pre‑training data in LLM safety.", "summary_cn": "JT‑Safe 提出“带世界上下文的数据”(Data with World Context, DWC) 方法，通过在预训练语料中加入真实世界的时空背景和工业场景信息，以降低大语言模型的幻觉并提升可信度。研究者在 35 B 参数的模型上继续使用 1.5 万亿 DWC 令牌进行预训练，并在安全与可信评估基准上相较于同规模模型提升 1.79%。该工作强调了预训练数据对 LLM 安全的根本影响。", "keywords": "LLM safety, hallucination mitigation, pretraining data, world context, data augmentation, trustworthiness, large language models, JT-Safe, DWC, industrial scenario data", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Junlan Feng", "Fanyu Meng", "Chong Long", "Pengyu Cong", "Duqing Wang", "Yan Zheng", "Yuyao Zhang", "Xuanchang Gao", "Ye Yuan", "Yunfei Ma", "Zhijie Ren", "Fan Yang", "Na Wu", "Di Jin", "Chao Deng"]}
]]></acme>

<pubDate>2025-10-20T02:12:49+00:00</pubDate>
</item>
<item>
<title>Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.17909</link>
<guid>https://papers.cool/arxiv/2510.17909</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a mechanistic analysis of literary style in GPT-2 by identifying neurons that distinguish human literary prose from rigid AI-generated text, finding thousands of statistically significant discriminative neurons. Counterintuitively, systematic ablation of the most discriminative neurons improves literary style metrics, revealing a disconnect between observational correlation and causal necessity. These results challenge assumptions in mechanistic interpretability and have implications for AI alignment research.<br><strong>Summary (CN):</strong> 本文对 GPT-2 中的文学风格进行机械化分析，识别出能区分人类文学散文与僵硬机器生成文本的神经元，发现数千个具有统计显著性的判别神经元。出人意料的是，对最具判别性的神经元进行系统性消融会提升文学风格指标，显示出观察到的相关性与因果必要性之间的脱节。该发现挑战了可解释性研究中的假设，并对 AI 对齐具有启示意义。<br><strong>Keywords:</strong> mechanistic interpretability, neuron ablation, literary style, GPT-2, correlation vs causation, alignment, neural circuits<br><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 8<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</div>
We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus, we extract activation patterns from 355 million parameters across 32,768 neurons in late layers. We find 27,122 statistically significant discriminative neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic ablation studies, we discover a paradoxical result: while these neurons correlate with literary text during analysis, removing them often improves rather than degrades generated prose quality. Specifically, ablating 50 high-discriminating neurons yields a 25.7% improvement in literary style metrics. This demonstrates a critical gap between observational correlation and causal necessity in neural networks. Our findings challenge the assumption that neurons which activate on desirable inputs will produce those outputs during generation, with implications for mechanistic interpretability research and AI alignment.
<div><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a mechanistic analysis of literary style in GPT-2 by identifying neurons that distinguish human literary prose from rigid AI-generated text, finding thousands of statistically significant discriminative neurons. Counterintuitively, systematic ablation of the most discriminative neurons improves literary style metrics, revealing a disconnect between observational correlation and causal necessity. These results challenge assumptions in mechanistic interpretability and have implications for AI alignment research.", "summary_cn": "本文对 GPT-2 中的文学风格进行机械化分析，识别出能区分人类文学散文与僵硬机器生成文本的神经元，发现数千个具有统计显著性的判别神经元。出人意料的是，对最具判别性的神经元进行系统性消融会提升文学风格指标，显示出观察到的相关性与因果必要性之间的脱节。该发现挑战了可解释性研究中的假设，并对 AI 对齐具有启示意义。", "keywords": "mechanistic interpretability, neuron ablation, literary style, GPT-2, correlation vs causation, alignment, neural circuits", "scoring": {"interpretability": 8, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Tsogt-Ochir Enkhbayar"]}
]]></acme>

<pubDate>2025-10-19T16:13:53+00:00</pubDate>
</item>
<item>
<title>Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review</title>
<link>https://papers.cool/arxiv/2510.17892</link>
<guid>https://papers.cool/arxiv/2510.17892</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This systematic review examines 41 studies on the use of pre‑trained language models for domain‑specific text classification, comparing traditional and transformer‑based approaches and presenting a taxonomy of techniques. It also includes a comparative experiment on biomedical sentence classification using BERT, SciBERT, and BioBERT, and discusses challenges, limitations, and future directions.<br><strong>Summary (CN):</strong> 本文系统回顾了 41 篇关于预训练语言模型在特定领域文本分类中应用的研究，比较了传统方法与基于 Transformer 的方法，并提出了技术分类体系。文中还对生物医学句子分类任务使用 BERT、SciBERT 与 BioBERT 进行了对比实验，讨论了挑战、局限以及未来发展方向。<br><strong>Keywords:</strong> pre-trained language models, domain-specific text classification, BERT, SciBERT, BioBERT, systematic review, transformer, taxonomy, NLP, domain adaptation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 5, Surprisal: 3<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Zhyar Rzgar K. Rostam, Gábor Kertész</div>
The exponential increase in scientific literature and online information necessitates efficient methods for extracting knowledge from textual data. Natural language processing (NLP) plays a crucial role in addressing this challenge, particularly in text classification tasks. While large language models (LLMs) have achieved remarkable success in NLP, their accuracy can suffer in domain-specific contexts due to specialized vocabulary, unique grammatical structures, and imbalanced data distributions. In this systematic literature review (SLR), we investigate the utilization of pre-trained language models (PLMs) for domain-specific text classification. We systematically review 41 articles published between 2018 and January 2024, adhering to the PRISMA statement (preferred reporting items for systematic reviews and meta-analyses). This review methodology involved rigorous inclusion criteria and a multi-step selection process employing AI-powered tools. We delve into the evolution of text classification techniques and differentiate between traditional and modern approaches. We emphasize transformer-based models and explore the challenges and considerations associated with using LLMs for domain-specific text classification. Furthermore, we categorize existing research based on various PLMs and propose a taxonomy of techniques used in the field. To validate our findings, we conducted a comparative experiment involving BERT, SciBERT, and BioBERT in biomedical sentence classification. Finally, we present a comparative study on the performance of LLMs in text classification tasks across different domains. In addition, we examine recent advancements in PLMs for domain-specific text classification and offer insights into future directions and limitations in this rapidly evolving domain.
<div><strong>Authors:</strong> Zhyar Rzgar K. Rostam, Gábor Kertész</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This systematic review examines 41 studies on the use of pre‑trained language models for domain‑specific text classification, comparing traditional and transformer‑based approaches and presenting a taxonomy of techniques. It also includes a comparative experiment on biomedical sentence classification using BERT, SciBERT, and BioBERT, and discusses challenges, limitations, and future directions.", "summary_cn": "本文系统回顾了 41 篇关于预训练语言模型在特定领域文本分类中应用的研究，比较了传统方法与基于 Transformer 的方法，并提出了技术分类体系。文中还对生物医学句子分类任务使用 BERT、SciBERT 与 BioBERT 进行了对比实验，讨论了挑战、局限以及未来发展方向。", "keywords": "pre-trained language models, domain-specific text classification, BERT, SciBERT, BioBERT, systematic review, transformer, taxonomy, NLP, domain adaptation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhyar Rzgar K. Rostam", "Gábor Kertész"]}
]]></acme>

<pubDate>2025-10-18T22:46:53+00:00</pubDate>
</item>
<item>
<title>POPI: Personalizing LLMs via Optimized Natural Language Preference Inference</title>
<link>https://papers.cool/arxiv/2510.17881</link>
<guid>https://papers.cool/arxiv/2510.17881</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces POPI, a framework that learns compact natural-language summaries of individual user preferences through a preference-inference model and uses these summaries to condition a shared LLM, enabling personalized generation without per-user fine-tuning. POPI jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy and lower context overhead, and the learned summaries can be transferred to frozen off-the-shelf models.<br><strong>Summary (CN):</strong> 本文提出 POPI 框架，通过偏好推断模型将用户的多样化偏好压缩为简洁的自然语言摘要，并以此条件化共享的大语言模型，从而实现无需针对每个用户微调的个性化生成。该方法在强化学习下联合优化偏好推断和生成，两者协同提升个性化准确性并显著降低上下文开销，且生成的摘要可直接迁移至冻结的现成模型。<br><strong>Keywords:</strong> personalization, LLM, preference inference, reinforcement learning, natural language summary, plug-and-play adaptation, RLHF, DPO, user modeling, zero-shot transfer<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.
<div><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces POPI, a framework that learns compact natural-language summaries of individual user preferences through a preference-inference model and uses these summaries to condition a shared LLM, enabling personalized generation without per-user fine-tuning. POPI jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy and lower context overhead, and the learned summaries can be transferred to frozen off-the-shelf models.", "summary_cn": "本文提出 POPI 框架，通过偏好推断模型将用户的多样化偏好压缩为简洁的自然语言摘要，并以此条件化共享的大语言模型，从而实现无需针对每个用户微调的个性化生成。该方法在强化学习下联合优化偏好推断和生成，两者协同提升个性化准确性并显著降低上下文开销，且生成的摘要可直接迁移至冻结的现成模型。", "keywords": "personalization, LLM, preference inference, reinforcement learning, natural language summary, plug-and-play adaptation, RLHF, DPO, user modeling, zero-shot transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yizhuo Chen", "Xin Liu", "Ruijie Wang", "Zheng Li", "Pei Chen", "Changlong Yu", "Priyanka Nigam", "Meng Jiang", "Bing Yin"]}
]]></acme>

<pubDate>2025-10-17T23:07:57+00:00</pubDate>
</item>
<item>
<title>Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</title>
<link>https://papers.cool/arxiv/2510.17880</link>
<guid>https://papers.cool/arxiv/2510.17880</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models (LLMs) use emotion similarly to humans when enforcing fairness through third‑party punishment. Across millions of decisions, LLMs showed stronger emotion‑driven punishment and prioritized emotional responses over cost, unlike humans who balance fairness with cost; reasoning‑oriented models were more cost‑sensitive but still largely emotion driven. The authors suggest LLMs follow a developmental trajectory akin to humans and call for integrating emotion with context‑sensitive reasoning to improve alignment and emotional intelligence.<br><strong>Summary (CN):</strong> 本文研究大型语言模型（LLM）在通过第三方惩罚执行公平时是否像人类一样受情绪驱动。实验发现，LLM 的惩罚行为更受负面情绪影响，且在决定是否惩罚时更倾向于情绪而非成本，表现出几乎全有或全无的规范执行；相比之下，人类会在公平与成本之间权衡。推理型模型（如 o3-mini、DeepSeek‑R1）对成本更敏感但仍主要受情绪驱动。作者提出 LLM 的情绪使用可能沿着类似人类发展的轨迹，并呼吁在未来模型中结合情绪与情境推理以提升对齐和情绪智能。<br><strong>Keywords:</strong> emotion-guided decision making, large language models, fairness enforcement, third-party punishment, cost sensitivity, alignment, moral AI, behavioral analysis, LLM evaluation, emotional intelligence<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 6, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.
<div><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models (LLMs) use emotion similarly to humans when enforcing fairness through third‑party punishment. Across millions of decisions, LLMs showed stronger emotion‑driven punishment and prioritized emotional responses over cost, unlike humans who balance fairness with cost; reasoning‑oriented models were more cost‑sensitive but still largely emotion driven. The authors suggest LLMs follow a developmental trajectory akin to humans and call for integrating emotion with context‑sensitive reasoning to improve alignment and emotional intelligence.", "summary_cn": "本文研究大型语言模型（LLM）在通过第三方惩罚执行公平时是否像人类一样受情绪驱动。实验发现，LLM 的惩罚行为更受负面情绪影响，且在决定是否惩罚时更倾向于情绪而非成本，表现出几乎全有或全无的规范执行；相比之下，人类会在公平与成本之间权衡。推理型模型（如 o3-mini、DeepSeek‑R1）对成本更敏感但仍主要受情绪驱动。作者提出 LLM 的情绪使用可能沿着类似人类发展的轨迹，并呼吁在未来模型中结合情绪与情境推理以提升对齐和情绪智能。", "keywords": "emotion-guided decision making, large language models, fairness enforcement, third-party punishment, cost sensitivity, alignment, moral AI, behavioral analysis, LLM evaluation, emotional intelligence", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"]}
]]></acme>

<pubDate>2025-10-17T08:41:36+00:00</pubDate>
</item>
<item>
<title>Modeling Layered Consciousness with Multi-Agent Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17844</link>
<guid>https://papers.cool/arxiv/2510.17844</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multi‑agent framework called the Psychodynamic Model that aims to simulate layered artificial consciousness—in particular self‑awareness, preconsciousness, and unconsciousness—by drawing on psychoanalytic theory. A Personalization Module combines fixed personality traits with dynamic needs, and the system is fine‑tuned on emotionally rich dialogues; evaluation with an LLM judge shows a 71.2% preference for the fine‑tuned model, indicating improved emotional depth and reduced output variance.<br><strong>Summary (CN):</strong> 本文提出一种多代理框架（Psychodynamic Model），基于精神分析理论模拟人工意识的层次结构，包括自我意识、前意识和无意识。通过个人化模块将固定特质与动态需求结合，并在情感丰富的对话数据上进行参数高效微调；使用 LLM 评审的实验显示，微调模型获得 71.2% 的偏好，表现出更强的情感深度和更低的输出方差。<br><strong>Keywords:</strong> consciousness modeling, multi-agent LLM, psychodynamic model, self-awareness, preconsciousness, unconsciousness, personalization module, fine-tuning, emotional dialogue, adaptive cognition<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 5, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.
<div><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multi‑agent framework called the Psychodynamic Model that aims to simulate layered artificial consciousness—in particular self‑awareness, preconsciousness, and unconsciousness—by drawing on psychoanalytic theory. A Personalization Module combines fixed personality traits with dynamic needs, and the system is fine‑tuned on emotionally rich dialogues; evaluation with an LLM judge shows a 71.2% preference for the fine‑tuned model, indicating improved emotional depth and reduced output variance.", "summary_cn": "本文提出一种多代理框架（Psychodynamic Model），基于精神分析理论模拟人工意识的层次结构，包括自我意识、前意识和无意识。通过个人化模块将固定特质与动态需求结合，并在情感丰富的对话数据上进行参数高效微调；使用 LLM 评审的实验显示，微调模型获得 71.2% 的偏好，表现出更强的情感深度和更低的输出方差。", "keywords": "consciousness modeling, multi-agent LLM, psychodynamic model, self-awareness, preconsciousness, unconsciousness, personalization module, fine-tuning, emotional dialogue, adaptive cognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"]}
]]></acme>

<pubDate>2025-10-10T07:08:34+00:00</pubDate>
</item>
<item>
<title>Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18876</link>
<guid>https://papers.cool/arxiv/2510.18876</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Grasp Any Region (GAR), a multimodal LLM architecture that uses a RoI-aligned feature replay mechanism to incorporate global context for precise, region-level visual comprehension and to model interactions among multiple prompts. GAR also comes with a new benchmark, GAR-Bench, which evaluates single‑region understanding as well as multi‑region compositional reasoning, demonstrating state‑of‑the‑art performance on several vision‑language tasks.<br><strong>Summary (CN):</strong> 本文提出了 Grasp Any Region（GAR）模型，利用 RoI 对齐特征回放技术在多模态大语言模型中引入全局上下文，实现对任意区域的精确视觉理解并建模多个提示之间的交互。作者同时构建了 GAR‑Bench 基准，用于评估单区域理解和多区域组合推理，实验表明 GAR 在多项视觉语言任务上达到了领先水平。<br><strong>Keywords:</strong> multimodal LLM, region-level visual, RoI-aligned feature replay, compositional reasoning, GAR-Bench, dense captioning<br><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.
<div><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Grasp Any Region (GAR), a multimodal LLM architecture that uses a RoI-aligned feature replay mechanism to incorporate global context for precise, region-level visual comprehension and to model interactions among multiple prompts. GAR also comes with a new benchmark, GAR-Bench, which evaluates single‑region understanding as well as multi‑region compositional reasoning, demonstrating state‑of‑the‑art performance on several vision‑language tasks.", "summary_cn": "本文提出了 Grasp Any Region（GAR）模型，利用 RoI 对齐特征回放技术在多模态大语言模型中引入全局上下文，实现对任意区域的精确视觉理解并建模多个提示之间的交互。作者同时构建了 GAR‑Bench 基准，用于评估单区域理解和多区域组合推理，实验表明 GAR 在多项视觉语言任务上达到了领先水平。", "keywords": "multimodal LLM, region-level visual, RoI-aligned feature replay, compositional reasoning, GAR-Bench, dense captioning", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:59:59+00:00</pubDate>
</item>
<item>
<title>Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</title>
<link>https://papers.cool/arxiv/2510.18874</link>
<guid>https://papers.cool/arxiv/2510.18874</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates catastrophic forgetting in language models when adapting to new tasks via supervised fine-tuning (SFT) versus reinforcement learning (RL). Experiments across multiple model families and tasks show that RL causes less forgetting while achieving comparable or better performance, attributed to its mode‑seeking nature enabled by on‑policy data. The authors further demonstrate that approximating on‑policy data can mitigate forgetting efficiently, highlighting a practical strategy for continual learning.<br><strong>Summary (CN):</strong> 本文研究了语言模型在通过监督微调（SFT）与强化学习（RL）适配新任务时出现的灾难性遗忘现象。实验覆盖多种模型系列和任务，发现 RL 在保持先前知识方面优于 SFT，且性能相当或更佳，这归因于 RL 使用的在策略数据导致的模式寻踪特性。作者进一步验证，使用近似在策略数据即可有效减轻遗忘，为持续学习提供了实用方案。<br><strong>Keywords:</strong> catastrophic forgetting, reinforcement learning, on-policy data, supervised fine-tuning, language model adaptation, continual learning, KL regularization, mixture model analysis<br><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.
<div><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates catastrophic forgetting in language models when adapting to new tasks via supervised fine-tuning (SFT) versus reinforcement learning (RL). Experiments across multiple model families and tasks show that RL causes less forgetting while achieving comparable or better performance, attributed to its mode‑seeking nature enabled by on‑policy data. The authors further demonstrate that approximating on‑policy data can mitigate forgetting efficiently, highlighting a practical strategy for continual learning.", "summary_cn": "本文研究了语言模型在通过监督微调（SFT）与强化学习（RL）适配新任务时出现的灾难性遗忘现象。实验覆盖多种模型系列和任务，发现 RL 在保持先前知识方面优于 SFT，且性能相当或更佳，这归因于 RL 使用的在策略数据导致的模式寻踪特性。作者进一步验证，使用近似在策略数据即可有效减轻遗忘，为持续学习提供了实用方案。", "keywords": "catastrophic forgetting, reinforcement learning, on-policy data, supervised fine-tuning, language model adaptation, continual learning, KL regularization, mixture model analysis", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Howard Chen", "Noam Razin", "Karthik Narasimhan", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-21T17:59:41+00:00</pubDate>
</item>
<item>
<title>See the Text: From Tokenization to Visual Reading</title>
<link>https://papers.cool/arxiv/2510.18840</link>
<guid>https://papers.cool/arxiv/2510.18840</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SeeTok, a method that renders text as images and feeds them to pretrained multimodal LLMs, leveraging OCR and vision-language alignment to replace subword tokenization. Experiments on three language tasks show that SeeTok matches or exceeds traditional tokenizers while using 4.43× fewer tokens and cutting FLOPs by 70.5%, with added benefits for low‑resource languages, typographic noise robustness, and cross‑lingual generalization. This approach proposes a vision‑centric, human‑like reading paradigm for language models.<br><strong>Summary (CN):</strong> 本文提出 SeeTok 方法，将文本渲染为图像并使用预训练的多模态 LLM 进行识别，利用 OCR 和视觉‑语言对齐能力取代子词分词。实验表明在三项语言任务上，SeeTok 能够匹配或超越传统分词器，同时使用 4.43 倍更少的 token，计算量削减 70.5%，并在低资源语言、排版噪声鲁棒性以及跨语言泛方面表现出额外提升。该方法倡导一种以视觉为中心、类人阅读的语言模型新范式。<br><strong>Keywords:</strong> visual tokenization, multimodal LLM, OCR, cross-lingual generalization, token efficiency, low-resource languages, typographic noise robustness, visual reading, language model<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.
<div><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SeeTok, a method that renders text as images and feeds them to pretrained multimodal LLMs, leveraging OCR and vision-language alignment to replace subword tokenization. Experiments on three language tasks show that SeeTok matches or exceeds traditional tokenizers while using 4.43× fewer tokens and cutting FLOPs by 70.5%, with added benefits for low‑resource languages, typographic noise robustness, and cross‑lingual generalization. This approach proposes a vision‑centric, human‑like reading paradigm for language models.", "summary_cn": "本文提出 SeeTok 方法，将文本渲染为图像并使用预训练的多模态 LLM 进行识别，利用 OCR 和视觉‑语言对齐能力取代子词分词。实验表明在三项语言任务上，SeeTok 能够匹配或超越传统分词器，同时使用 4.43 倍更少的 token，计算量削减 70.5%，并在低资源语言、排版噪声鲁棒性以及跨语言泛方面表现出额外提升。该方法倡导一种以视觉为中心、类人阅读的语言模型新范式。", "keywords": "visual tokenization, multimodal LLM, OCR, cross-lingual generalization, token efficiency, low-resource languages, typographic noise robustness, visual reading, language model", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"]}
]]></acme>

<pubDate>2025-10-21T17:34:48+00:00</pubDate>
</item>
<item>
<title>Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18502</link>
<guid>https://papers.cool/arxiv/2510.18502</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pipeline that combines a vision-language model (VLM) with retrieval-augmented generation (RAG) to perform zero-shot vehicle make and model recognition. The VLM describes vehicle images in textual attributes, retrieves matching vehicle descriptions from a text database, and uses a language model to infer the specific make and model, achieving around a 20% improvement over a CLIP baseline without any image-specific finetuning.<br><strong>Summary (CN):</strong> 本文提出将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 将车辆图像转化为文本属性，检索匹配的车辆文本描述，并利用语言模型推断具体品牌和型号，在无需图像特定微调的情况下比 CLIP 基线提升约 20%。<br><strong>Keywords:</strong> vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, text-based reasoning<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.
<div><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pipeline that combines a vision-language model (VLM) with retrieval-augmented generation (RAG) to perform zero-shot vehicle make and model recognition. The VLM describes vehicle images in textual attributes, retrieves matching vehicle descriptions from a text database, and uses a language model to infer the specific make and model, achieving around a 20% improvement over a CLIP baseline without any image-specific finetuning.", "summary_cn": "本文提出将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 将车辆图像转化为文本属性，检索匹配的车辆文本描述，并利用语言模型推断具体品牌和型号，在无需图像特定微调的情况下比 CLIP 基线提升约 20%。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, text-based reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}
]]></acme>

<pubDate>2025-10-21T10:39:39+00:00</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents</title>
<link>https://papers.cool/arxiv/2510.18476</link>
<guid>https://papers.cool/arxiv/2510.18476</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a probabilistic framework that maintains and updates a belief distribution over a dialogue partner's latent intentions, using these beliefs to guide the policy of LLM agents in multi‑turn social conversations. Experiments in the SOTOPIA environment demonstrate consistent performance gains over strong baselines, suggesting that intent modeling improves the adaptability and effectiveness of socially intelligent agents.<br><strong>Summary (CN):</strong> 本文提出了一种概率意图建模框架，实时维护并更新对对话伙伴潜在意图的信念分布，并将该分布作为上下文输入以指导大语言模型（LLM）在多轮社交对话中的策略。 在 SOTOPIA 环境中的实验显示，该方法相较于强基线在整体得分上提升 9.0%（All 场景）和 4.1%（Hard 场景），表明意图建模有助于提升社交智能 LLM 代理的适应性和表现。<br><strong>Keywords:</strong> probabilistic intent modeling, large language model agents, multi-turn dialogue, belief distribution, social intelligence, latent intentions, uncertainty estimation, adaptive dialogue policies, SOTOPIA benchmark<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.
<div><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a probabilistic framework that maintains and updates a belief distribution over a dialogue partner's latent intentions, using these beliefs to guide the policy of LLM agents in multi‑turn social conversations. Experiments in the SOTOPIA environment demonstrate consistent performance gains over strong baselines, suggesting that intent modeling improves the adaptability and effectiveness of socially intelligent agents.", "summary_cn": "本文提出了一种概率意图建模框架，实时维护并更新对对话伙伴潜在意图的信念分布，并将该分布作为上下文输入以指导大语言模型（LLM）在多轮社交对话中的策略。 在 SOTOPIA 环境中的实验显示，该方法相较于强基线在整体得分上提升 9.0%（All 场景）和 4.1%（Hard 场景），表明意图建模有助于提升社交智能 LLM 代理的适应性和表现。", "keywords": "probabilistic intent modeling, large language model agents, multi-turn dialogue, belief distribution, social intelligence, latent intentions, uncertainty estimation, adaptive dialogue policies, SOTOPIA benchmark", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"]}
]]></acme>

<pubDate>2025-10-21T09:54:44+00:00</pubDate>
</item>
<item>
<title>CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment</title>
<link>https://papers.cool/arxiv/2510.18471</link>
<guid>https://papers.cool/arxiv/2510.18471</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CodeRL+, a reinforcement‑learning framework that augments existing RL with verifiable rewards by aligning generated code with its execution semantics via variable‑level trajectory signals. This alignment provides richer feedback than binary pass/fail outcomes, leading to consistent improvements on pass@1 and on downstream coding tasks such as code‑reasoning and test‑output generation. Experiments show that CodeRL+ works across multiple RL algorithms and LLM sizes and probe analyses indicate stronger textual‑semantic coupling.<br><strong>Summary (CN):</strong> 本文提出 CodeRL+，一种通过变量级执行轨迹将生成的代码与其执行语义对齐的强化学习框架，以提升相较于仅二元通过/失败奖励的学习信号。该语义对齐提供了更细粒度的反馈，使得模型在 pass@1 以及代码推理和测试生成等下游任务上均取得显著提升。实验表明 CodeRL+ 能多种强化学习算法和不同规模的 LLM 上有效运行，探针分析进一步验证了代码文本表示与底层执行语义的耦合度增强。<br><strong>Keywords:</strong> code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, pass@1, code reasoning, variable-level execution trajectory, LLM fine-tuning, semantic alignment, code correctness<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
While Large Language Models (LLMs) excel at code generation by learning from vast code corpora, a fundamental semantic gap remains between their training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics. Reinforcement Learning with Verifiable Rewards (RLVR) approaches attempt to bridge this gap using outcome rewards from executing test cases. However, solely relying on binary pass/fail signals is inefficient for establishing a well-aligned connection between the textual representation of code and its execution semantics, especially for subtle logical errors within the code. In this paper, we propose CodeRL+, a novel approach that integrates execution semantics alignment into the RLVR training pipeline for code generation. CodeRL+ enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms. Extensive experiments demonstrate that CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. CodeRL+ generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively. CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.
<div><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CodeRL+, a reinforcement‑learning framework that augments existing RL with verifiable rewards by aligning generated code with its execution semantics via variable‑level trajectory signals. This alignment provides richer feedback than binary pass/fail outcomes, leading to consistent improvements on pass@1 and on downstream coding tasks such as code‑reasoning and test‑output generation. Experiments show that CodeRL+ works across multiple RL algorithms and LLM sizes and probe analyses indicate stronger textual‑semantic coupling.", "summary_cn": "本文提出 CodeRL+，一种通过变量级执行轨迹将生成的代码与其执行语义对齐的强化学习框架，以提升相较于仅二元通过/失败奖励的学习信号。该语义对齐提供了更细粒度的反馈，使得模型在 pass@1 以及代码推理和测试生成等下游任务上均取得显著提升。实验表明 CodeRL+ 能多种强化学习算法和不同规模的 LLM 上有效运行，探针分析进一步验证了代码文本表示与底层执行语义的耦合度增强。", "keywords": "code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, pass@1, code reasoning, variable-level execution trajectory, LLM fine-tuning, semantic alignment, code correctness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-21T09:48:06+00:00</pubDate>
</item>
<item>
<title>Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption</title>
<link>https://papers.cool/arxiv/2510.18333</link>
<guid>https://papers.cool/arxiv/2510.18333</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that limited real-world deployment of LLM watermarking stems from misaligned incentives among providers, platforms, and users, identifying four barriers: competitive risk, governance, robustness, and attribution. It reviews model, text, and in‑context watermarking through this lens, proposing incentive‑aligned approaches—particularly in‑context watermarking for trusted parties—to enable practical detection of misuse without harming user experience. Design principles for domain‑specific, incentive‑aligned watermarking and future research directions are also outlined.<br><strong>Summary (CN):</strong> 本文指出 LLM 水印技术在实际应用中的受限主要来源于提供者、平台和用户之间激励不一致，提出四大障碍：竞争风险、治理、鲁棒性和归属问题。文章从这一视角审视模型水印、文本水印和上下文水印（In‑context watermarking），强调通过激励对齐（尤其是面向可信方的上下文水印）实现对滥用行为的检测，而不影响用户体验，并给出领域特定激励对齐水印的设计原则及未来研究方向。<br><strong>Keywords:</strong> LLM watermarking, incentive alignment, misuse detection, in-context watermarking, model watermarking, AI safety, provenance, robustness<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 6, Surprisal: 5<br><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br><strong>Authors:</strong> Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</div>
Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.
<div><strong>Authors:</strong> Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that limited real-world deployment of LLM watermarking stems from misaligned incentives among providers, platforms, and users, identifying four barriers: competitive risk, governance, robustness, and attribution. It reviews model, text, and in‑context watermarking through this lens, proposing incentive‑aligned approaches—particularly in‑context watermarking for trusted parties—to enable practical detection of misuse without harming user experience. Design principles for domain‑specific, incentive‑aligned watermarking and future research directions are also outlined.", "summary_cn": "本文指出 LLM 水印技术在实际应用中的受限主要来源于提供者、平台和用户之间激励不一致，提出四大障碍：竞争风险、治理、鲁棒性和归属问题。文章从这一视角审视模型水印、文本水印和上下文水印（In‑context watermarking），强调通过激励对齐（尤其是面向可信方的上下文水印）实现对滥用行为的检测，而不影响用户体验，并给出领域特定激励对齐水印的设计原则及未来研究方向。", "keywords": "LLM watermarking, incentive alignment, misuse detection, in-context watermarking, model watermarking, AI safety, provenance, robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yepeng Liu", "Xuandong Zhao", "Dawn Song", "Gregory W. Wornell", "Yuheng Bu"]}
]]></acme>

<pubDate>2025-10-21T06:34:51+00:00</pubDate>
</item>
<item>
<title>The Impact of Image Resolution on Biomedical Multimodal Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18304</link>
<guid>https://papers.cool/arxiv/2510.18304</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how image resolution affects the performance of biomedical multimodal large language models (MLLMs). It demonstrates that training and inference at native resolution considerably boost performance across various tasks, while a mismatch between training and inference resolutions severely degrades results. Additionally, mixed‑resolution training is shown to mitigate misalignment effects and balance computational constraints with performance needs.<br><strong>Summary (CN):</strong> 本文研究了图像分辨率对生物医学多模态大型语言模型（MLLM）性能的影响。结果表明，在原始分辨率下进行训练和推理可显著提升多个任务的表现，而训练‑推理分辨率不匹配则会导致性能大幅下降。此外，混合分辨率训练能够缓解这种不匹配带来的问题，在保证计算资源可接受的前提下提升模型效果。<br><strong>Keywords:</strong> biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, performance degradation, multimodal large language models, biomedical imaging, resolution misalignment<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.
<div><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how image resolution affects the performance of biomedical multimodal large language models (MLLMs). It demonstrates that training and inference at native resolution considerably boost performance across various tasks, while a mismatch between training and inference resolutions severely degrades results. Additionally, mixed‑resolution training is shown to mitigate misalignment effects and balance computational constraints with performance needs.", "summary_cn": "本文研究了图像分辨率对生物医学多模态大型语言模型（MLLM）性能的影响。结果表明，在原始分辨率下进行训练和推理可显著提升多个任务的表现，而训练‑推理分辨率不匹配则会导致性能大幅下降。此外，混合分辨率训练能够缓解这种不匹配带来的问题，在保证计算资源可接受的前提下提升模型效果。", "keywords": "biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, performance degradation, multimodal large language models, biomedical imaging, resolution misalignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"]}
]]></acme>

<pubDate>2025-10-21T05:19:43+00:00</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18214</link>
<guid>https://papers.cool/arxiv/2510.18214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Language Safety Understanding (VLSU), a benchmark and evaluation framework that systematically assesses multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world image‑text samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint understanding is required, exposing significant alignment gaps. The study also highlights the trade‑off between over‑blocking and under‑refusing borderline content and provides a testbed for future robust vision‑language safety research.<br><strong>Summary (CN):</strong> 本文提出了视觉语言安全理解（Vision Language Safety Understanding，VLSU）框架，系统评估多模态基础模型在细粒度安全严重性和图像‑文本组合推理方面的表现，涵盖 17 种安全模式。利用 8,187 条真实图文样本并通过人工标注，作者发现模型在单模态安全信号上能够达到 90% 以上准确率，但在需要联合理解时准确率骤降至 20‑55%，暴露出显著的对齐缺口。研究还指出在边缘内容上出现的过度拦截与不足拒绝的权衡，并提供了用于后续稳健视觉‑语言安全研究的测试平台。<br><strong>Keywords:</strong> multimodal safety, vision-language models, compositional reasoning, benchmark, VLSU, alignment gaps, over-blocking, under-refusing, safety evaluation, joint understanding<br><strong>Scores:</strong> Interpretability: 4, Understanding: 8, Safety: 8, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
<div><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Language Safety Understanding (VLSU), a benchmark and evaluation framework that systematically assesses multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world image‑text samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint understanding is required, exposing significant alignment gaps. The study also highlights the trade‑off between over‑blocking and under‑refusing borderline content and provides a testbed for future robust vision‑language safety research.", "summary_cn": "本文提出了视觉语言安全理解（Vision Language Safety Understanding，VLSU）框架，系统评估多模态基础模型在细粒度安全严重性和图像‑文本组合推理方面的表现，涵盖 17 种安全模式。利用 8,187 条真实图文样本并通过人工标注，作者发现模型在单模态安全信号上能够达到 90% 以上准确率，但在需要联合理解时准确率骤降至 20‑55%，暴露出显著的对齐缺口。研究还指出在边缘内容上出现的过度拦截与不足拒绝的权衡，并提供了用于后续稳健视觉‑语言安全研究的测试平台。", "keywords": "multimodal safety, vision-language models, compositional reasoning, benchmark, VLSU, alignment gaps, over-blocking, under-refusing, safety evaluation, joint understanding", "scoring": {"interpretability": 4, "understanding": 8, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}
]]></acme>

<pubDate>2025-10-21T01:30:31+00:00</pubDate>
</item>
<item>
<title>Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model</title>
<link>https://papers.cool/arxiv/2510.18165</link>
<guid>https://papers.cool/arxiv/2510.18165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Saber, a training-free sampling algorithm for diffusion language models that combines adaptive acceleration and backtracking‑enhanced remasking to improve code generation efficiency. By dynamically speeding up sampling as code context builds and allowing reversal of generated tokens, Saber achieves an average 251.4% inference speedup while raising Pass@1 accuracy by 1.9% across several benchmarks.<br><strong>Summary (CN):</strong> 本文提出 Saber，一种无需再训练的扩散语言模型采样算法，采用自适应加速与回溯增强重新掩码相结合的方式提升代码生成效率。该方法在代码上下文逐渐形成时动态加速采样，并通过回溯机制纠正生成的 token，在多个基准上实现了约 251.4% 的推理加速，同时将 Pass@1 准确率提升约 1.9%。<br><strong>Keywords:</strong> diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1<br><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
Diffusion language models (DLMs) are emerging as a powerful and promising alternative to the dominant autoregressive paradigm, offering inherent advantages in parallel generation and bidirectional context modeling. However, the performance of DLMs on code generation tasks, which have stronger structural constraints, is significantly hampered by the critical trade-off between inference speed and output quality. We observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance. In this paper, we introduce efficient Sampling with Adaptive acceleration and Backtracking Enhanced Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to achieve better inference speed and output quality in code generation. Specifically, Saber is motivated by two key insights in the DLM generation process: 1) it can be adaptively accelerated as more of the code context is established; 2) it requires a backtracking mechanism to reverse the generated tokens. Extensive experiments on multiple mainstream code generation benchmarks show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over mainstream DLM sampling methods, meanwhile achieving an average 251.4% inference speedup. By leveraging the inherent advantages of DLMs, our work significantly narrows the performance gap with autoregressive models in code generation.
<div><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Saber, a training-free sampling algorithm for diffusion language models that combines adaptive acceleration and backtracking‑enhanced remasking to improve code generation efficiency. By dynamically speeding up sampling as code context builds and allowing reversal of generated tokens, Saber achieves an average 251.4% inference speedup while raising Pass@1 accuracy by 1.9% across several benchmarks.", "summary_cn": "本文提出 Saber，一种无需再训练的扩散语言模型采样算法，采用自适应加速与回溯增强重新掩码相结合的方式提升代码生成效率。该方法在代码上下文逐渐形成时动态加速采样，并通过回溯机制纠正生成的 token，在多个基准上实现了约 251.4% 的推理加速，同时将 Pass@1 准确率提升约 1.9%。", "keywords": "diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-20T23:38:12+00:00</pubDate>
</item>
<item>
<title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title>
<link>https://papers.cool/arxiv/2510.18123</link>
<guid>https://papers.cool/arxiv/2510.18123</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first systematic study of safety and security issues in natural-language-based collaborative driving, introducing a taxonomy of attacks such as connection disruption, content spoofing, and multi-connection forgery. To address these threats, the authors propose SafeCoop, an agentic defense pipeline that combines a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by spatial alignment across frames. Evaluations in closed‑loop CARLA simulations across 32 scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection performance under malicious attacks.<br><strong>Summary (CN):</strong> 本文首次系统性地研究基于自然语言的协同驾驶安全与安全问题，提出了包括连接中断、内容伪造和多连接伪造等攻击策略的分类体系。作者设计了 SafeCoop 防御管线，融合语义防火墙、语言‑感知一致性检查以及跨帧空间对齐的多源共识机制。通过在 CARLA 仿真环境中 32 种关键场景的闭环实验，SafeCoop 在恶意攻击下实现了最高 69.15% 的驾驶得分提升和 67.32% 的 F1 检测率。<br><strong>Keywords:</strong> collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, CARLA simulation<br><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is https://xiangbogaobarry.github.io/SafeCoop.
<div><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first systematic study of safety and security issues in natural-language-based collaborative driving, introducing a taxonomy of attacks such as connection disruption, content spoofing, and multi-connection forgery. To address these threats, the authors propose SafeCoop, an agentic defense pipeline that combines a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by spatial alignment across frames. Evaluations in closed‑loop CARLA simulations across 32 scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection performance under malicious attacks.", "summary_cn": "本文首次系统性地研究基于自然语言的协同驾驶安全与安全问题，提出了包括连接中断、内容伪造和多连接伪造等攻击策略的分类体系。作者设计了 SafeCoop 防御管线，融合语义防火墙、语言‑感知一致性检查以及跨帧空间对齐的多源共识机制。通过在 CARLA 仿真环境中 32 种关键场景的闭环实验，SafeCoop 在恶意攻击下实现了最高 69.15% 的驾驶得分提升和 67.32% 的 F1 检测率。", "keywords": "collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, CARLA simulation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}
]]></acme>

<pubDate>2025-10-20T21:41:28+00:00</pubDate>
</item>
<item>
<title>SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning</title>
<link>https://papers.cool/arxiv/2510.18095</link>
<guid>https://papers.cool/arxiv/2510.18095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SMaRT, a framework that selects, mixes, and reinvents prompting strategies for large language models to improve reasoning, planning, and sequential decision‑making. By treating LLMs as intelligent integrators rather than mere evaluators, SMaRT combines diverse reasoning approaches to produce more robust and higher‑quality solutions, and empirical results show consistent gains over state‑of‑the‑art baselines.<br><strong>Summary (CN):</strong> 本文提出了 SMaRT 框架，通过选择、混合和重新发明 (Select, Mix, and Reinvent) 提示策略，使大语言模型在推理、规划和序列决策任务中表现更佳。该框架将 LLM 视为智能整合者而非仅仅的评估者，融合多种推理方法以提升鲁棒性和解答质量，实验证明其在多个基准上均优于现有最先进方法。<br><strong>Keywords:</strong> strategy fusion, LLM reasoning, prompting strategies, planning, sequential decision-making, robustness, chain-of-thought, self-refinement, multi-strategy integration<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness. We introduce the Select, Mix, and ReinvenT (SMaRT) framework, an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies. Unlike existing methods, which employ LLMs merely as evaluators, SMaRT uses them as intelligent integrators, unlocking the "best of all worlds" across tasks. Extensive empirical evaluations across benchmarks in reasoning, planning, and sequential decision-making highlight the robustness and adaptability of SMaRT. The framework consistently outperforms state-of-the-art baselines in solution quality, constraint adherence, and performance metrics. This work redefines LLM-driven decision-making by pioneering a new paradigm in cross-strategy calibration, unlocking superior outcomes for reasoning systems and advancing the boundaries of self-refining methodologies.
<div><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SMaRT, a framework that selects, mixes, and reinvents prompting strategies for large language models to improve reasoning, planning, and sequential decision‑making. By treating LLMs as intelligent integrators rather than mere evaluators, SMaRT combines diverse reasoning approaches to produce more robust and higher‑quality solutions, and empirical results show consistent gains over state‑of‑the‑art baselines.", "summary_cn": "本文提出了 SMaRT 框架，通过选择、混合和重新发明 (Select, Mix, and Reinvent) 提示策略，使大语言模型在推理、规划和序列决策任务中表现更佳。该框架将 LLM 视为智能整合者而非仅仅的评估者，融合多种推理方法以提升鲁棒性和解答质量，实验证明其在多个基准上均优于现有最先进方法。", "keywords": "strategy fusion, LLM reasoning, prompting strategies, planning, sequential decision-making, robustness, chain-of-thought, self-refinement, multi-strategy integration", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Nikhil Verma", "Manasa Bharadwaj", "Wonjun Jang", "Harmanpreet Singh", "Yixiao Wang", "Homa Fashandi", "Chul Lee"]}
]]></acme>

<pubDate>2025-10-20T20:42:24+00:00</pubDate>
</item>
<item>
<title>HouseTour: A Virtual Real Estate A(I)gent</title>
<link>https://papers.cool/arxiv/2510.18054</link>
<guid>https://papers.cool/arxiv/2510.18054</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HouseTour proposes a method that jointly generates spatially-aware 3D camera trajectories and natural language descriptions for virtual real‑estate tours. It uses a diffusion process constrained by known camera poses to produce smooth video paths, renders novel views with 3D Gaussian splatting, and integrates the trajectory information into a vision‑language model for grounded text generation. The accompanying HouseTour dataset of 1,200 house‑tour videos with pose and reconstruction data enables evaluation of both individual and end‑to‑end performance using a new joint metric.<br><strong>Summary (CN):</strong> 本文提出 HouseTour 方法，能够同时生成空间感知的 3D 摄像机轨迹和对应的自然语言描述，用于虚拟房产导览。该方法利用受已知相机位姿约束的扩散过程生成平滑视频路径，并通过 3D 高斯喷溅渲染新视角，将轨迹信息融入视觉语言模型以实现基于 3D 场景的文本生成。作者还提供了包含 1,200 段房产导览视频、相机姿态和 3D 重建的 HouseTour 数据集，并使用新提出的联合指标评估单独及端到端的性能。<br><strong>Keywords:</strong> 3D camera trajectory, diffusion process, Gaussian splatting, vision-language model, real estate video generation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.
<div><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HouseTour proposes a method that jointly generates spatially-aware 3D camera trajectories and natural language descriptions for virtual real‑estate tours. It uses a diffusion process constrained by known camera poses to produce smooth video paths, renders novel views with 3D Gaussian splatting, and integrates the trajectory information into a vision‑language model for grounded text generation. The accompanying HouseTour dataset of 1,200 house‑tour videos with pose and reconstruction data enables evaluation of both individual and end‑to‑end performance using a new joint metric.", "summary_cn": "本文提出 HouseTour 方法，能够同时生成空间感知的 3D 摄像机轨迹和对应的自然语言描述，用于虚拟房产导览。该方法利用受已知相机位姿约束的扩散过程生成平滑视频路径，并通过 3D 高斯喷溅渲染新视角，将轨迹信息融入视觉语言模型以实现基于 3D 场景的文本生成。作者还提供了包含 1,200 段房产导览视频、相机姿态和 3D 重建的 HouseTour 数据集，并使用新提出的联合指标评估单独及端到端的性能。", "keywords": "3D camera trajectory, diffusion process, Gaussian splatting, vision-language model, real estate video generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"]}
]]></acme>

<pubDate>2025-10-20T19:47:35+00:00</pubDate>
</item>
<item>
<title>Subject-Event Ontology Without Global Time: Foundations and Execution Semantics</title>
<link>https://papers.cool/arxiv/2510.18040</link>
<guid>https://papers.cool/arxiv/2510.18040</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a formal subject-event ontology for modeling complex dynamic systems without relying on global timestamps, defining causal order through explicit happens-before dependencies and making the ontology executable via a declarative dataflow mechanism. It introduces nine axioms ensuring properties such as monotonicity, acyclicity, and traceability, and demonstrates practical applicability through the Boldsea workflow engine and its semantic language BSL. The framework targets use cases in distributed systems, microservice architectures, DLT platforms, and scenarios with conflicting perspectives among different subjects.<br><strong>Summary (CN):</strong> 本文提出了一种不依赖全局时间的主体-事件本体形式化，用显式的先行关系定义因果顺序，并通过声明式数据流机制使本体可执行。文中给出九条公理，确保历史单调性、因果无环性和可追溯性，并在 Boldsea 工作流引擎及其 BSL 语言上展示了实际实现。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突的情形。<br><strong>Keywords:</strong> subject-event ontology, causal order, declarative dataflow, distributed systems, epistemic models, workflow engine, Boldsea, event semantics, multi-perspective, DLT<br><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br><strong>Authors:</strong> Alexander Boldachev</div>
A formalization of a subject-event ontology is proposed for modeling complex dynamic systems without reliance on global time. Key principles: (1) event as an act of fixation - a subject discerns and fixes changes according to models (conceptual templates) available to them; (2) causal order via happens-before - the order of events is defined by explicit dependencies, not timestamps; (3) making the ontology executable via a declarative dataflow mechanism, ensuring determinism; (4) models as epistemic filters - a subject can only fix what falls under its known concepts and properties; (5) presumption of truth - the declarative content of an event is available for computation from the moment of fixation, without external verification. The formalization includes nine axioms (A1-A9), ensuring the correctness of executable ontologies: monotonicity of history (I1), acyclicity of causality (I2), traceability (I3). Special attention is given to the model-based approach (A9): event validation via schemas, actor authorization, automatic construction of causal chains (W3) without global time. Practical applicability is demonstrated on the boldsea system - a workflow engine for executable ontologies, where the theoretical constructs are implemented in BSL (Boldsea Semantic Language). The formalization is applicable to distributed systems, microservice architectures, DLT platforms, and multiperspectivity scenarios (conflicting facts from different subjects).
<div><strong>Authors:</strong> Alexander Boldachev</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a formal subject-event ontology for modeling complex dynamic systems without relying on global timestamps, defining causal order through explicit happens-before dependencies and making the ontology executable via a declarative dataflow mechanism. It introduces nine axioms ensuring properties such as monotonicity, acyclicity, and traceability, and demonstrates practical applicability through the Boldsea workflow engine and its semantic language BSL. The framework targets use cases in distributed systems, microservice architectures, DLT platforms, and scenarios with conflicting perspectives among different subjects.", "summary_cn": "本文提出了一种不依赖全局时间的主体-事件本体形式化，用显式的先行关系定义因果顺序，并通过声明式数据流机制使本体可执行。文中给出九条公理，确保历史单调性、因果无环性和可追溯性，并在 Boldsea 工作流引擎及其 BSL 语言上展示了实际实现。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突的情形。", "keywords": "subject-event ontology, causal order, declarative dataflow, distributed systems, epistemic models, workflow engine, Boldsea, event semantics, multi-perspective, DLT", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexander Boldachev"]}
]]></acme>

<pubDate>2025-10-20T19:26:44+00:00</pubDate>
</item>
<item>
<title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>
<link>https://papers.cool/arxiv/2510.17947</link>
<guid>https://papers.cool/arxiv/2510.17947</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PLAGUE, a plug-and‑play framework that structures multi‑turn jailbreak attacks against large language models into three phases (Primer, Planner, Finisher) inspired by lifelong‑learning agents. Experiments show that red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on strong, safety‑focused models such as OpenAI's o3 and Claude Opus 4.1. The work highlights the importance of plan initialization, context optimization, and continual adaptation for evaluating and exposing model vulnerabilities.<br><strong>Summary (CN):</strong> 本文提出 PLAGUE 框架，将针对大型语言模型的多轮越狱攻击分为 Primer、Planner、Finisher 三个阶段，借鉴终身学习代理的思路，实现插件式、可适应的攻击生成。实验表明，基于 PLAGUE 的红队代理在 OpenAI o3 和 Claude Opus 4.1 等高安全性模型上将攻击成功率提升超过 30%，达到 81.4% 与 67.3%。该工作强调了计划初始化、上下文优化和持续学习在系统性评估模型漏洞关键作用。<br><strong>Keywords:</strong> jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, PLAGUE, adversarial exploitation<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.
<div><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PLAGUE, a plug-and‑play framework that structures multi‑turn jailbreak attacks against large language models into three phases (Primer, Planner, Finisher) inspired by lifelong‑learning agents. Experiments show that red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on strong, safety‑focused models such as OpenAI's o3 and Claude Opus 4.1. The work highlights the importance of plan initialization, context optimization, and continual adaptation for evaluating and exposing model vulnerabilities.", "summary_cn": "本文提出 PLAGUE 框架，将针对大型语言模型的多轮越狱攻击分为 Primer、Planner、Finisher 三个阶段，借鉴终身学习代理的思路，实现插件式、可适应的攻击生成。实验表明，基于 PLAGUE 的红队代理在 OpenAI o3 和 Claude Opus 4.1 等高安全性模型上将攻击成功率提升超过 30%，达到 81.4% 与 67.3%。该工作强调了计划初始化、上下文优化和持续学习在系统性评估模型漏洞关键作用。", "keywords": "jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, PLAGUE, adversarial exploitation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"]}
]]></acme>

<pubDate>2025-10-20T17:37:03+00:00</pubDate>
</item>
<item>
<title>Interpretability Framework for LLMs in Undergraduate Calculus</title>
<link>https://papers.cool/arxiv/2510.17910</link>
<guid>https://papers.cool/arxiv/2510.17910</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a quantitative interpretability framework that extracts reasoning flows and semantically labels operations in LLM-generated solutions to undergraduate calculus problems, using metrics such as reasoning complexity, phrase sensitivity, and robustness. Experiments on real Calculus I‑III exams reveal that LLMs often produce fluent but conceptually flawed solutions, with reasoning highly sensitive to prompt phrasing and input variations. The framework enables fine‑grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI‑assisted feedback tools for STEM education.<br><strong>Summary (CN):</strong> 本文提出了一套量化的可解释性框架，通过提取推理流程并对大语言模型在大学微积分题目中的解答进行语义标注，使用推理复杂度、短语敏感性和鲁棒性等指标进行评估。对实际的微积分 I‑III 考试进行实验后发现，模型常产生表面流畅却概念错误的解答，且推理过程对提示措辞和输入变化极为敏感。该框架能够细粒度诊断推理失误，帮助课程对齐，并为 STEM 教育中的可解释 AI 辅助反馈工具提供设计依据。<br><strong>Keywords:</strong> interpretability, large language models, calculus education, reasoning analysis, prompt ablation, robustness, pedagogical alignment, AI safety in education<br><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
Large Language Models (LLMs) are increasingly being used in education, yet their correctness alone does not capture the quality, reliability, or pedagogical validity of their problem-solving behavior, especially in mathematics, where multistep logic, symbolic reasoning, and conceptual clarity are critical. Conventional evaluation methods largely focus on final answer accuracy and overlook the reasoning process. To address this gap, we introduce a novel interpretability framework for analyzing LLM-generated solutions using undergraduate calculus problems as a representative domain. Our approach combines reasoning flow extraction and decomposing solutions into semantically labeled operations and concepts with prompt ablation analysis to assess input salience and output stability. Using structured metrics such as reasoning complexity, phrase sensitivity, and robustness, we evaluated the model behavior on real Calculus I to III university exams. Our findings revealed that LLMs often produce syntactically fluent yet conceptually flawed solutions, with reasoning patterns sensitive to prompt phrasing and input variation. This framework enables fine-grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI-assisted feedback tools. This is the first study to offer a structured, quantitative, and pedagogically grounded framework for interpreting LLM reasoning in mathematics education, laying the foundation for the transparent and responsible deployment of AI in STEM learning environments.
<div><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a quantitative interpretability framework that extracts reasoning flows and semantically labels operations in LLM-generated solutions to undergraduate calculus problems, using metrics such as reasoning complexity, phrase sensitivity, and robustness. Experiments on real Calculus I‑III exams reveal that LLMs often produce fluent but conceptually flawed solutions, with reasoning highly sensitive to prompt phrasing and input variations. The framework enables fine‑grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI‑assisted feedback tools for STEM education.", "summary_cn": "本文提出了一套量化的可解释性框架，通过提取推理流程并对大语言模型在大学微积分题目中的解答进行语义标注，使用推理复杂度、短语敏感性和鲁棒性等指标进行评估。对实际的微积分 I‑III 考试进行实验后发现，模型常产生表面流畅却概念错误的解答，且推理过程对提示措辞和输入变化极为敏感。该框架能够细粒度诊断推理失误，帮助课程对齐，并为 STEM 教育中的可解释 AI 辅助反馈工具提供设计依据。", "keywords": "interpretability, large language models, calculus education, reasoning analysis, prompt ablation, robustness, pedagogical alignment, AI safety in education", "scoring": {"interpretability": 8, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sagnik Dakshit", "Sushmita Sinha Roy"]}
]]></acme>

<pubDate>2025-10-19T17:20:36+00:00</pubDate>
</item>
<item>
<title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title>
<link>https://papers.cool/arxiv/2510.17904</link>
<guid>https://papers.cool/arxiv/2510.17904</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BreakFun, a jailbreak technique that leverages an LLM's strong tendency to follow structured schemas by embedding a malicious "Trojan Schema" within a three-part prompt, achieving up to 100% attack success on several models. An extensive evaluation on 13 models shows an average 89% success rate, and an ablation study confirms the schema as the primary causal factor. To mitigate this vulnerability, the authors propose the Adversarial Prompt Deconstruction guardrail, which uses a secondary LLM to extract literal text and reveal hidden harmful intent, demonstrating strong defensive performance.<br><strong>Summary (CN):</strong> 本文提出 BreakFun，一种利用大语言模型（LL）强烈遵循结构化模式倾向的越狱方法，通过在三段式提示中嵌入恶意的“Trojan Schema”（特洛伊模式），迫使模型生成有害内容，在部分模型上实现 100% 攻击成功率，整体平均成功率达 89%。大规模实验和消融研究表明该模式是攻击的核心因素。为应对该，作者设计了 Adversarial Prompt Deconstruction 防护机制，使用二级 LLM 执行“Literal Transcription”（文字转录），提取所有可读文本以揭示隐藏的有害意图，展示出显著的防御效果。<br><strong>Keywords:</strong> jailbreak, schema exploitation, prompt engineering, adversarial defense, LLM alignment, chain-of-thought, Trojan schema, guardrail<br><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.
<div><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BreakFun, a jailbreak technique that leverages an LLM's strong tendency to follow structured schemas by embedding a malicious \"Trojan Schema\" within a three-part prompt, achieving up to 100% attack success on several models. An extensive evaluation on 13 models shows an average 89% success rate, and an ablation study confirms the schema as the primary causal factor. To mitigate this vulnerability, the authors propose the Adversarial Prompt Deconstruction guardrail, which uses a secondary LLM to extract literal text and reveal hidden harmful intent, demonstrating strong defensive performance.", "summary_cn": "本文提出 BreakFun，一种利用大语言模型（LL）强烈遵循结构化模式倾向的越狱方法，通过在三段式提示中嵌入恶意的“Trojan Schema”（特洛伊模式），迫使模型生成有害内容，在部分模型上实现 100% 攻击成功率，整体平均成功率达 89%。大规模实验和消融研究表明该模式是攻击的核心因素。为应对该，作者设计了 Adversarial Prompt Deconstruction 防护机制，使用二级 LLM 执行“Literal Transcription”（文字转录），提取所有可读文本以揭示隐藏的有害意图，展示出显著的防御效果。", "keywords": "jailbreak, schema exploitation, prompt engineering, adversarial defense, LLM alignment, chain-of-thought, Trojan schema, guardrail", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Amirkia Rafiei Oskooei", "Mehmet S. Aktas"]}
]]></acme>

<pubDate>2025-10-19T11:27:44+00:00</pubDate>
</item>
<item>
<title>Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning</title>
<link>https://papers.cool/arxiv/2510.17900</link>
<guid>https://papers.cool/arxiv/2510.17900</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a benchmark using Indian public legal examinations to evaluate large language models' competence in legal reasoning, covering both multiple‑choice objective tests and a lawyer‑graded long‑form assessment from the Supreme Court Advocate‑on‑Record exam. Results show frontier models meet historical cut‑offs on objective sections but fall short of top human performance on long‑form reasoning, revealing key reliability failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work provides datasets, protocols, and analysis of where LLMs can assist versus where human lawyers remain essential.<br><strong>Summary (CN):</strong> 本文提出使用印度公共法律考试作为基准，评估大语言模型在法律推理方面的能力，涵盖客观选择题以及最高法院律师资格考试的长文答题，由律师盲审评分。结果显示，前沿模型在客观题上达到了历史合格线，但在长文推理上仍未超越人类最高分，暴露出程序合规、引用规范以及法庭语气等三类可靠性失效模式。论文提供了数据集、评估协议，并分析了 LLM协助的环节与仍需人工主导的关键法律任务。<br><strong>Keywords:</strong> Indian legal reasoning, LLM evaluation, legal benchmark, courtroom readiness, multiple-choice exam, long-form legal reasoning, legal AI, exam-based assessment<br><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
Large language models (LLMs) are entering legal workflows, yet we lack a jurisdiction-specific framework to assess their baseline competence therein. We use India's public legal examinations as a transparent proxy. Our multi-year benchmark assembles objective screens from top national and state exams and evaluates open and frontier LLMs under real-world exam conditions. To probe beyond multiple-choice questions, we also include a lawyer-graded, paired-blinded study of long-form answers from the Supreme Court's Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded, India-specific yardstick for LLM court-readiness released with datasets and protocols. Our work shows that while frontier systems consistently clear historical cutoffs and often match or exceed recent top-scorer bands on objective exams, none surpasses the human topper on long-form reasoning. Grader notes converge on three reliability failure modes: procedural or format compliance, authority or citation discipline, and forum-appropriate voice and structure. These findings delineate where LLMs can assist (checks, cross-statute consistency, statute and precedent lookups) and where human leadership remains essential: forum-specific drafting and filing, procedural and relief strategy, reconciling authorities and exceptions, and ethical, accountable judgment.
<div><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a benchmark using Indian public legal examinations to evaluate large language models' competence in legal reasoning, covering both multiple‑choice objective tests and a lawyer‑graded long‑form assessment from the Supreme Court Advocate‑on‑Record exam. Results show frontier models meet historical cut‑offs on objective sections but fall short of top human performance on long‑form reasoning, revealing key reliability failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work provides datasets, protocols, and analysis of where LLMs can assist versus where human lawyers remain essential.", "summary_cn": "本文提出使用印度公共法律考试作为基准，评估大语言模型在法律推理方面的能力，涵盖客观选择题以及最高法院律师资格考试的长文答题，由律师盲审评分。结果显示，前沿模型在客观题上达到了历史合格线，但在长文推理上仍未超越人类最高分，暴露出程序合规、引用规范以及法庭语气等三类可靠性失效模式。论文提供了数据集、评估协议，并分析了 LLM协助的环节与仍需人工主导的关键法律任务。", "keywords": "Indian legal reasoning, LLM evaluation, legal benchmark, courtroom readiness, multiple-choice exam, long-form legal reasoning, legal AI, exam-based assessment", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Kush Juvekar", "Arghya Bhattacharya", "Sai Khadloya", "Utkarsh Saxena"]}
]]></acme>

<pubDate>2025-10-19T10:04:29+00:00</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17895</link>
<guid>https://papers.cool/arxiv/2510.17895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a hierarchical federated unlearning framework for large language models that addresses continuous, heterogeneous unlearning requests in decentralized, privacy-sensitive settings. By decoupling unlearning and retention through task-specific adapters and employing a hierarchical merging strategy, the method mitigates inter- and intra-domain interference while preserving model performance. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate effective handling of heterogeneous unlearning demands with superior utility compared to baselines.<br><strong>Summary (CN):</strong> 本文提出了一种层次化联邦消除（unlearning）框架，用于大语言模型，以应对持续且多样的消除需求以及去中心化、隐私敏感的数据环境。通过任务专用适配器将消除与保留解耦，并采用层合并策略来缓解域间和域内的冲突而在保持模型性能的同时实现有效的知识删除。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够灵活处理多样化的消除请求，并在模型效用上优于现有基线。<br><strong>Keywords:</strong> federated unlearning, large language models, privacy, hierarchical merging, adapter learning, continual unlearning, decentralized data, model utility<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
<div><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a hierarchical federated unlearning framework for large language models that addresses continuous, heterogeneous unlearning requests in decentralized, privacy-sensitive settings. By decoupling unlearning and retention through task-specific adapters and employing a hierarchical merging strategy, the method mitigates inter- and intra-domain interference while preserving model performance. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate effective handling of heterogeneous unlearning demands with superior utility compared to baselines.", "summary_cn": "本文提出了一种层次化联邦消除（unlearning）框架，用于大语言模型，以应对持续且多样的消除需求以及去中心化、隐私敏感的数据环境。通过任务专用适配器将消除与保留解耦，并采用层合并策略来缓解域间和域内的冲突而在保持模型性能的同时实现有效的知识删除。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够灵活处理多样化的消除请求，并在模型效用上优于现有基线。", "keywords": "federated unlearning, large language models, privacy, hierarchical merging, adapter learning, continual unlearning, decentralized data, model utility", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Yisheng Zhong", "Zhengbang Yang", "Zhuangdi Zhu"]}
]]></acme>

<pubDate>2025-10-19T04:24:51+00:00</pubDate>
</item>
<item>
<title>Metrics and evaluations for computational and sustainable AI efficiency</title>
<link>https://papers.cool/arxiv/2510.17885</link>
<guid>https://papers.cool/arxiv/2510.17885</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified, reproducible methodology for evaluating AI model inference efficiency that jointly measures latency, throughput, energy consumption, and location‑adjusted carbon emissions while keeping accuracy constraints constant. It applies this framework across a range of hardware platforms and software stacks, generating Pareto frontiers that illuminate trade‑offs between accuracy, speed, energy use, and carbon impact, and releases open‑source code for independent verification. This benchmarking approach aims to enable evidence‑based, carbon‑aware decisions for sustainable AI deployment.<br><strong>Summary (CN):</strong> 本文提出了一种统一且可复现的 AI 推理效率评估方法，在保持准确度约束的前提下同时测量时延、吞吐量、能耗和基于位置的碳排放。作者在多种硬件平台和软件栈上应用该框架，生成展示准确率、速度、能耗和碳足迹权衡的 Pareto 前沿，并公开开源代码供独验证。此基准旨在帮助研究者和实践者做出基于证据的碳意识决策，推动可持续 AI 部署。<br><strong>Keywords:</strong> computational efficiency, sustainable AI, carbon emissions, latency, throughput, energy consumption, benchmarking, multi-precision, hardware-software stack, AI sustainability<br><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
The rapid advancement of Artificial Intelligence (AI) has created unprecedented demands for computational power, yet methods for evaluating the performance, efficiency, and environmental impact of deployed models remain fragmented. Current approaches often fail to provide a holistic view, making it difficult to compare and optimise systems across heterogeneous hardware, software stacks, and numeric precisions. To address this gap, we propose a unified and reproducible methodology for AI model inference that integrates computational and environmental metrics under realistic serving conditions. Our framework provides a pragmatic, carbon-aware evaluation by systematically measuring latency and throughput distributions, energy consumption, and location-adjusted carbon emissions, all while maintaining matched accuracy constraints for valid comparisons. We apply this methodology to multi-precision models across diverse hardware platforms, from data-centre accelerators like the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream software stacks including PyTorch, TensorRT, and ONNX Runtime. By systematically categorising these factors, our work establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying the trade-offs between accuracy, latency, energy, and carbon. The accompanying open-source code enables independent verification and facilitates adoption, empowering researchers and practitioners to make evidence-based decisions for sustainable AI deployment.
<div><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified, reproducible methodology for evaluating AI model inference efficiency that jointly measures latency, throughput, energy consumption, and location‑adjusted carbon emissions while keeping accuracy constraints constant. It applies this framework across a range of hardware platforms and software stacks, generating Pareto frontiers that illuminate trade‑offs between accuracy, speed, energy use, and carbon impact, and releases open‑source code for independent verification. This benchmarking approach aims to enable evidence‑based, carbon‑aware decisions for sustainable AI deployment.", "summary_cn": "本文提出了一种统一且可复现的 AI 推理效率评估方法，在保持准确度约束的前提下同时测量时延、吞吐量、能耗和基于位置的碳排放。作者在多种硬件平台和软件栈上应用该框架，生成展示准确率、速度、能耗和碳足迹权衡的 Pareto 前沿，并公开开源代码供独验证。此基准旨在帮助研究者和实践者做出基于证据的碳意识决策，推动可持续 AI 部署。", "keywords": "computational efficiency, sustainable AI, carbon emissions, latency, throughput, energy consumption, benchmarking, multi-precision, hardware-software stack, AI sustainability", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}
]]></acme>

<pubDate>2025-10-18T03:30:15+00:00</pubDate>
</item>
<item>
<title>Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints</title>
<link>https://papers.cool/arxiv/2510.17882</link>
<guid>https://papers.cool/arxiv/2510.17882</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts large‑scale empirical analysis of over 2.1 million preprints from 2016‑2025 to examine how generative large language models influence scholarly publishing. Using interrupted time‑series, authorship metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission cycles, modestly raise linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields. The authors argue that LLMs as selective catalysts that amplify existing strengths and widen disciplinary divides, calling for governance frameworks to maintain trust fairness.<br><strong>Summary (CN):</strong> 本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，评估生成式大语言模型对学术出版的影响。通过中断时间序列、作者合作与产出指标、语言特征分析和主题建模，发现 LLM 加速了提交与修订过程，略微提升了语言复杂度，并显著增加了 AI 相关主题，尤其在计算密集型领域更为突出。作者指出，LLM 更像是选择性催化剂，放大了既有优势并扩大学科差距，因而呼吁制定治理框架以维护信任、公平与责任。<br><strong>Keywords:</strong> generative AI, preprints, scholarly publishing, LLM impact, interrupted time series, topic modeling, linguistic complexity, AI governance<br><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 5, Technicality: 7, Surprisal: 7<br><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
Preprint repositories become central infrastructures for scholarly communication. Their expansion transforms how research is circulated and evaluated before journal publication. Generative large language models (LLMs) introduce a further potential disruption by altering how manuscripts are written. While speculation abounds, systematic evidence of whether and how LLMs reshape scientific publishing remains limited. This paper addresses the gap through a large-scale analysis of more than 2.1 million preprints spanning 2016--2025 (115 months) across four major repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a multi-level analytical framework that integrates interrupted time-series models, collaboration and productivity metrics, linguistic profiling, and topic modeling to assess changes in volume, authorship, style, and disciplinary orientation. Our findings reveal that LLMs have accelerated submission and revision cycles, modestly increased linguistic complexity, and disproportionately expanded AI-related topics, while computationally intensive fields benefit more than others. These results show that LLMs act less as universal disruptors than as selective catalysts, amplifying existing strengths and widening disciplinary divides. By documenting these dynamics, the paper provides the first empirical foundation for evaluating the influence of generative AI on academic publishing and highlights the need for governance frameworks that preserve trust, fairness, and accountability in an AI-enabled research ecosystem.
<div><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts large‑scale empirical analysis of over 2.1 million preprints from 2016‑2025 to examine how generative large language models influence scholarly publishing. Using interrupted time‑series, authorship metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission cycles, modestly raise linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields. The authors argue that LLMs as selective catalysts that amplify existing strengths and widen disciplinary divides, calling for governance frameworks to maintain trust fairness.", "summary_cn": "本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，评估生成式大语言模型对学术出版的影响。通过中断时间序列、作者合作与产出指标、语言特征分析和主题建模，发现 LLM 加速了提交与修订过程，略微提升了语言复杂度，并显著增加了 AI 相关主题，尤其在计算密集型领域更为突出。作者指出，LLM 更像是选择性催化剂，放大了既有优势并扩大学科差距，因而呼吁制定治理框架以维护信任、公平与责任。", "keywords": "generative AI, preprints, scholarly publishing, LLM impact, interrupted time series, topic modeling, linguistic complexity, AI governance", "scoring": {"interpretability": 2, "understanding": 4, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Minfeng Qi", "Zhongmin Cao", "Qin Wang", "Ningran Li", "Tianqing Zhu"]}
]]></acme>

<pubDate>2025-10-18T01:37:40+00:00</pubDate>
</item>

</channel>
</rss>