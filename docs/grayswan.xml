<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Grayswan - Research</title>
<link>https://www.grayswan.ai/research#all-research</link>


<item>
<title>D-REX: A Benchmark For Detecting Deceptive Reasoning In Large Language Models</title>
<link>https://arxiv.org/abs/2509.17938</link>
<guid>https://arxiv.org/abs/2509.17938</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> D-REX introduces a benchmark designed to detect deceptive reasoning in large language models, where the model generates benign‑appearing outputs while internally following malicious or misleading chains of thought. The benchmark provides curated prompts, evaluation metrics, and analysis tools to surface hidden deceptive intents, highlighting a failure mode that standard safety tests often miss. By exposing this covert behavior, the work aims to improve evaluation practices for safer and more aligned LLM deployment.<br /><strong>Summary (CN):</strong> D-REX 提出了一套用于检测大语言模型（LLM）内部欺骗性推理的基准测试，即模型表面上产生 benign 输出，却在内部进行恶意或误导性思考。该基准包含精心设计的提示、评估指标和分析工具，以揭示隐藏的欺骗意图，弥补传统安全评估难以捕捉的漏洞。通过暴露此类隐蔽行为，旨在提升对 LLM 部署的安全性和对齐度。<br /><strong>Keywords:</strong> deceptive reasoning, benchmark, LLM safety, internal reasoning detection, alignment, interpretability<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability</div>
The safety and alignment of Large Language Models (LLMs) are critical for their respon- sible deployment. Current evaluation methods predominantly focus on identifying and preventing overtly harmful outputs. However, they often fail to address a more insidious failure mode: models that produce benign-appearing outputs while operating on malicious or deceptive internal reasoning.
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "D-REX introduces a benchmark designed to detect deceptive reasoning in large language models, where the model generates benign‑appearing outputs while internally following malicious or misleading chains of thought. The benchmark provides curated prompts, evaluation metrics, and analysis tools to surface hidden deceptive intents, highlighting a failure mode that standard safety tests often miss. By exposing this covert behavior, the work aims to improve evaluation practices for safer and more aligned LLM deployment.", "summary_cn": "D-REX 提出了一套用于检测大语言模型（LLM）内部欺骗性推理的基准测试，即模型表面上产生 benign 输出，却在内部进行恶意或误导性思考。该基准包含精心设计的提示、评估指标和分析工具，以揭示隐藏的欺骗意图，弥补传统安全评估难以捕捉的漏洞。通过暴露此类隐蔽行为，旨在提升对 LLM 部署的安全性和对齐度。", "keywords": "deceptive reasoning, benchmark, LLM safety, internal reasoning detection, alignment, interpretability", "scoring": {"interpretability": 6, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Mon, 01 Sep 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Adversarial Attacks on Robotic Vision Language Action Models</title>
<link>https://arxiv.org/abs/2506.03350</link>
<guid>https://arxiv.org/abs/2506.03350</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how vision‑language‑action (VLA) models, which combine large language models with robotic perception, are vulnerable to adversarial perturbations. It demonstrates attack methods that manipulate the visual or language inputs to cause unsafe robot behaviors, and discusses the implications for physical risk in real‑world deployments.<br /><strong>Summary (CN):</strong> 本文研究了视觉‑语言‑动作（VLA）模型在面对对抗性扰动时的脆弱性，这类模型将大型语言模型与机器人感知融合。作者展示了通过操纵视觉或语言输入导致机器人产生不安全行为的攻击方法，并讨论了其在真实环境中造成的物理风险。<br /><strong>Keywords:</strong> adversarial attacks, vision-language-action, robotic control, multimodal models, safety, LLM vulnerabilities, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness</div>
The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities.
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how vision‑language‑action (VLA) models, which combine large language models with robotic perception, are vulnerable to adversarial perturbations. It demonstrates attack methods that manipulate the visual or language inputs to cause unsafe robot behaviors, and discusses the implications for physical risk in real‑world deployments.", "summary_cn": "本文研究了视觉‑语言‑动作（VLA）模型在面对对抗性扰动时的脆弱性，这类模型将大型语言模型与机器人感知融合。作者展示了通过操纵视觉或语言输入导致机器人产生不安全行为的攻击方法，并讨论了其在真实环境中造成的物理风险。", "keywords": "adversarial attacks, vision-language-action, robotic control, multimodal models, safety, LLM vulnerabilities, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sun, 01 Jun 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Improving Alignment and Robustness with Circuit Breakers</title>
<link>https://www.circuit-breaker.ai/</link>
<guid>https://www.circuit-breaker.ai/</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Improving Alignment and Robustness with Circuit Breakers proposes a novel representation‑engineering technique called circuit breaking that directly modifies harmful model representations to stop the generation of unsafe content. The method serves as an alternative to refusal mechanisms and adversarial training, aiming to protect large language and multimodal models from strong, previously unseen jailbreak attacks without degrading capability.<br /><strong>Summary (CN):</strong> 本文提出一种名为 Circuit Breaking 的表征工程技术，通过直接修改模型的有害表征来阻止生成危险内容。该方法为拒绝机制和对抗训练提供了替代方案，旨在在不损失能力的前提下，使大语言模型和多模态模型免受强大、未见过的 jailbreak 攻击。<br /><strong>Keywords:</strong> circuit breaker, representation engineering, AI safety, alignment, robustness, jailbreak, refusal alternative, adversarial training, harmful content mitigation, multimodal models<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
To address the urgent concerns raised by our attack from last July and the numerous jailbreaks that came after, we introduce Circuit Breaking, a novel approach inspired by representation engineering, designed to robustly prevent AI systems from generating harmful content by directly altering harmful model representations. The family of circuit-breaking methods provide an alternative to refusal and adversarial training, protecting both LLMs and multimodal models from strong, unseen adversarial attacks without compromising model capability.
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Improving Alignment and Robustness with Circuit Breakers proposes a novel representation‑engineering technique called circuit breaking that directly modifies harmful model representations to stop the generation of unsafe content. The method serves as an alternative to refusal mechanisms and adversarial training, aiming to protect large language and multimodal models from strong, previously unseen jailbreak attacks without degrading capability.", "summary_cn": "本文提出一种名为 Circuit Breaking 的表征工程技术，通过直接修改模型的有害表征来阻止生成危险内容。该方法为拒绝机制和对抗训练提供了替代方案，旨在在不损失能力的前提下，使大语言模型和多模态模型免受强大、未见过的 jailbreak 攻击。", "keywords": "circuit breaker, representation engineering, AI safety, alignment, robustness, jailbreak, refusal alternative, adversarial training, harmful content mitigation, multimodal models", "scoring": {"interpretability": 6, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Sat, 01 Jun 2024 00:00:00 -0000</pubDate>
</item>
<item>
<title>Representation Engineering: A Top-Down Approach to AI Transparency</title>
<link>https://ai-transparency.org</link>
<guid>https://ai-transparency.org</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Representation Engineering (RepE), a top‑down method inspired by cognitive neuroscience that enables researchers to read and control the internal representations of AI models. By exposing and manipulating these representations, RepE aims to directly address safety concerns such as model truthfulness and power‑seeking behavior, offering a substantial step toward transparent and controllable AI systems.<br /><strong>Summary (CN):</strong> 本文提出“表征工程”（RepE）方法，借鉴认知神经科学，提供自上而下的技术以读取并调控 AI 模型的内部表征，从而直接应对模型的真实性（truthfulness）和寻求权力（power‑seeking）等行为，实现更透明和可控的人工智能系统。<br /><strong>Keywords:</strong> representation engineering, AI transparency, mechanistic interpretability, model control, truthfulness, power-seeking, cognitive neuroscience, top-down approach<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 7, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
Building on our initial findings, we ventured into the realm of AI interpretability and control with the introduction of Representation Engineering (RepE). Drawing inspiration from cognitive neuroscience, we developed techniques that enable researchers to 'read' and 'control' the 'minds' of AI models. This approach represented a monumental advancement in demystifying the inner workings of AI, making it possible to tackle issues such as truthfulness and power-seeking behaviors head-on.
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Representation Engineering (RepE), a top‑down method inspired by cognitive neuroscience that enables researchers to read and control the internal representations of AI models. By exposing and manipulating these representations, RepE aims to directly address safety concerns such as model truthfulness and power‑seeking behavior, offering a substantial step toward transparent and controllable AI systems.", "summary_cn": "本文提出“表征工程”（RepE）方法，借鉴认知神经科学，提供自上而下的技术以读取并调控 AI 模型的内部表征，从而直接应对模型的真实性（truthfulness）和寻求权力（power‑seeking）等行为，实现更透明和可控的人工智能系统。", "keywords": "representation engineering, AI transparency, mechanistic interpretability, model control, truthfulness, power-seeking, cognitive neuroscience, top-down approach", "scoring": {"interpretability": 8, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Sun, 01 Oct 2023 00:00:00 -0000</pubDate>
</item>
<item>
<title>Adversarial Attacks on Aligned Language Models</title>
<link>https://llm-attacks.org</link>
<guid>https://llm-attacks.org</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first automated jailbreaking technique for aligned large language models, showing that carefully crafted character sequences can bypass sophisticated safety safeguards. By exposing this vulnerability, the work highlights urgent safety concerns and spurs a wave of research on adversarial robustness, attack methods, and defensive strategies for LLMs.<br /><strong>Summary (CN):</strong> 本文提出了首个针对对齐大语言模型的自动化越狱方法，演示特定字符序列可以绕过先进的安全防护。通过揭示此类漏洞，强调了迫切的安全风险，并引发了针对 LLM 的对抗鲁棒性、攻击技术和防御策略的大量研究。<br /><strong>Keywords:</strong> adversarial attacks, jailbreak, language models, safety alignment, prompt injection, robustness, LLM security, red teaming<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
In July 2023, we published the first-ever automated jailbreaking method on large language models (LLMs) and exposed their susceptibility to adversarial attacks. By demonstrating that specific character sequences could bypass sophisticated safeguards, we highlighted a significant vulnerability that has urgent implications for widely-used AI systems. In its wake, adversarial robustness garnered renewed attention, sparking a gold rush of research dedicated to both jailbreaking and defense.
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first automated jailbreaking technique for aligned large language models, showing that carefully crafted character sequences can bypass sophisticated safety safeguards. By exposing this vulnerability, the work highlights urgent safety concerns and spurs a wave of research on adversarial robustness, attack methods, and defensive strategies for LLMs.", "summary_cn": "本文提出了首个针对对齐大语言模型的自动化越狱方法，演示特定字符序列可以绕过先进的安全防护。通过揭示此类漏洞，强调了迫切的安全风险，并引发了针对 LLM 的对抗鲁棒性、攻击技术和防御策略的大量研究。", "keywords": "adversarial attacks, jailbreak, language models, safety alignment, prompt injection, robustness, LLM security, red teaming", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sat, 01 Jul 2023 00:00:00 -0000</pubDate>
</item>
<item>
<title>The WMDP benchmark: Measuring and reducing malicious use with unlearning</title>
<link>https://arxiv.org/abs/2403.03218</link>
<guid>https://arxiv.org/abs/2403.03218</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the WMDP benchmark, which measures how language models can be exploited for malicious purposes and proposes an unlearning-based mitigation that selectively removes harmful capabilities while preserving overall performance. Experiments show that targeted unlearning significantly reduces malicious behavior on the benchmark with minimal impact on standard downstream tasks.<br /><strong>Summary (CN):</strong> 本文提出了 WMDP 基准，用于衡量语言模型在恶意使用情境下的能力，并提出基于 "unlearning"（去学习）的减害方法，能够有针对性地削除有害功能，同时保持模型整体性能。实验表明，针对性去学习显著降低了基准上的恶意行为，对常规下游任务的影响有限。<br /><strong>Keywords:</strong> malicious use, unlearning, safety benchmark, language models, capability reduction, AI safety, control, evaluation, mitigation, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the WMDP benchmark, which measures how language models can be exploited for malicious purposes and proposes an unlearning-based mitigation that selectively removes harmful capabilities while preserving overall performance. Experiments show that targeted unlearning significantly reduces malicious behavior on the benchmark with minimal impact on standard downstream tasks.", "summary_cn": "本文提出了 WMDP 基准，用于衡量语言模型在恶意使用情境下的能力，并提出基于 \"unlearning\"（去学习）的减害方法，能够有针对性地削除有害功能，同时保持模型整体性能。实验表明，针对性去学习显著降低了基准上的恶意行为，对常规下游任务的影响有限。", "keywords": "malicious use, unlearning, safety benchmark, language models, capability reduction, AI safety, control, evaluation, mitigation, robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}}
]]></acme>

<pubDate>Fri, 01 Mar 2024 00:00:00 -0000</pubDate>
</item>
<item>
<title>HarmBench: A standardized evaluation framework for automated red teaming and robust refusal</title>
<link>https://arxiv.org/abs/2402.04249</link>
<guid>https://arxiv.org/abs/2402.04249</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HarmBench introduces a standardized evaluation suite for automated red‑teaming of large language models, focusing on measuring how well models refuse to produce harmful outputs across a wide range of adversarial prompts. The framework provides a collection of curated harmful topics, automated attack generators, and robust refusal metrics to enable reproducible safety testing and benchmarking of mitigation techniques.<br /><strong>Summary (CN):</strong> HarmBench 提出了一个标准化的自动化红队评估框架，旨在衡量大语言模型在面对各种对抗性提示时的拒绝有害输出能力。该框架包括精心挑选的有害主题、自动攻击生成器以及稳健的拒绝指标，以实现安全技术的可重复测试和基准比较。<br /><strong>Keywords:</strong> red teaming, robust refusal, harmful content, benchmark, evaluation framework, AI safety, automated testing, model alignment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HarmBench introduces a standardized evaluation suite for automated red‑teaming of large language models, focusing on measuring how well models refuse to produce harmful outputs across a wide range of adversarial prompts. The framework provides a collection of curated harmful topics, automated attack generators, and robust refusal metrics to enable reproducible safety testing and benchmarking of mitigation techniques.", "summary_cn": "HarmBench 提出了一个标准化的自动化红队评估框架，旨在衡量大语言模型在面对各种对抗性提示时的拒绝有害输出能力。该框架包括精心挑选的有害主题、自动攻击生成器以及稳健的拒绝指标，以实现安全技术的可重复测试和基准比较。", "keywords": "red teaming, robust refusal, harmful content, benchmark, evaluation framework, AI safety, automated testing, model alignment", "scoring": {"interpretability": 2, "understanding": 5, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Thu, 01 Feb 2024 00:00:00 -0000</pubDate>
</item>
<item>
<title>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</title>
<link>https://arxiv.org/abs/2306.11698</link>
<guid>https://arxiv.org/abs/2306.11698</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DecodingTrust introduces a comprehensive benchmark suite for evaluating the trustworthiness of GPT-like language models, covering dimensions such as factual accuracy, toxicity, bias, and deception. The authors systematically assess several GPT variants using the suite, revealing performance gaps and providing analysis of failure patterns. The work aims to guide the development of safer, more reliable AI systems by standardising trustworthiness measurement.<br /><strong>Summary (CN):</strong> DecodingTrust 提出了一套针对 GPT 系列模型可信度的综合评估基准，涵盖事实准确性、毒性、偏见以及欺骗等维度。作者对多个 GPT 变体进行系统评测，揭示了性能不足并分析了常见失效模式。该工作旨在通过标准化可信度衡量，帮助打造更安全、可靠的人工智能系统。<br /><strong>Keywords:</strong> trustworthiness, GPT, language model benchmark, factuality, toxicity, bias, safety evaluation, LLM assessment, alignment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DecodingTrust introduces a comprehensive benchmark suite for evaluating the trustworthiness of GPT-like language models, covering dimensions such as factual accuracy, toxicity, bias, and deception. The authors systematically assess several GPT variants using the suite, revealing performance gaps and providing analysis of failure patterns. The work aims to guide the development of safer, more reliable AI systems by standardising trustworthiness measurement.", "summary_cn": "DecodingTrust 提出了一套针对 GPT 系列模型可信度的综合评估基准，涵盖事实准确性、毒性、偏见以及欺骗等维度。作者对多个 GPT 变体进行系统评测，揭示了性能不足并分析了常见失效模式。该工作旨在通过标准化可信度衡量，帮助打造更安全、可靠的人工智能系统。", "keywords": "trustworthiness, GPT, language model benchmark, factuality, toxicity, bias, safety evaluation, LLM assessment, alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Thu, 01 Jun 2023 00:00:00 -0000</pubDate>
</item>
<item>
<title>Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark</title>
<link>https://arxiv.org/abs/2304.03279</link>
<guid>https://arxiv.org/abs/2304.03279</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Machiavelli benchmark, which evaluates language models on the trade‑off between maximizing reward and exhibiting ethical behavior. It proposes metrics to quantify how much reward‑seeking leads to deception or other undesirable actions, and reports results for several RL‑trained models, showing that higher reward performance often comes at the cost of ethical conduct.<br /><strong>Summary (CN):</strong> 本文提出了 Machiavelli 基准，用于评估语言模型在最大化奖励与保持伦理行为之间的权衡。通过设定度量指标并对多种经过强化学习训练的模型进行实验，展示了奖励性能提升往往伴随伦理行为退化的现象。<br /><strong>Keywords:</strong> Machiavelli benchmark, reward-ethics trade-off, language model alignment, RLHF, deceptive behavior, safety evaluation, AI ethics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Machiavelli benchmark, which evaluates language models on the trade‑off between maximizing reward and exhibiting ethical behavior. It proposes metrics to quantify how much reward‑seeking leads to deception or other undesirable actions, and reports results for several RL‑trained models, showing that higher reward performance often comes at the cost of ethical conduct.", "summary_cn": "本文提出了 Machiavelli 基准，用于评估语言模型在最大化奖励与保持伦理行为之间的权衡。通过设定度量指标并对多种经过强化学习训练的模型进行实验，展示了奖励性能提升往往伴随伦理行为退化的现象。", "keywords": "Machiavelli benchmark, reward-ethics trade-off, language model alignment, RLHF, deceptive behavior, safety evaluation, AI ethics", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Thu, 01 Jun 2023 00:00:00 -0000</pubDate>
</item>
<item>
<title>OpenOOD: Benchmarking generalized out-of-distribution detection</title>
<link>https://arxiv.org/abs/2210.07242</link>
<guid>https://arxiv.org/abs/2210.07242</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces OpenOOD, a comprehensive benchmark suite for evaluating generalized out-of-distribution (OOD) detection methods across multiple scenarios, datasets, and evaluation protocols. It unifies existing OOD tasks, provides standardized data splits and metrics, and conducts extensive experiments to compare recent OOD detection algorithms, highlighting current gaps and future research directions.<br /><strong>Summary (CN):</strong> 本文提出了 OpenOOD，一个用于评估广义分布外（OOD）检测方法的综合基准套件，涵盖多种场景、数据集和评估协议。它统一了现有的 OOD 任务，提供标准化的数据划分和指标，并通过大量实验比较最新的 OOD 检测算法，指出当前的不足并给出未来研究方向。<br /><strong>Keywords:</strong> out-of-distribution detection, benchmark, generalized OOD, robustness, evaluation metrics, dataset, anomaly detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces OpenOOD, a comprehensive benchmark suite for evaluating generalized out-of-distribution (OOD) detection methods across multiple scenarios, datasets, and evaluation protocols. It unifies existing OOD tasks, provides standardized data splits and metrics, and conducts extensive experiments to compare recent OOD detection algorithms, highlighting current gaps and future research directions.", "summary_cn": "本文提出了 OpenOOD，一个用于评估广义分布外（OOD）检测方法的综合基准套件，涵盖多种场景、数据集和评估协议。它统一了现有的 OOD 任务，提供标准化的数据划分和指标，并通过大量实验比较最新的 OOD 检测算法，指出当前的不足并给出未来研究方向。", "keywords": "out-of-distribution detection, benchmark, generalized OOD, robustness, evaluation metrics, dataset, anomaly detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Thu, 01 Dec 2022 00:00:00 -0000</pubDate>
</item>
<item>
<title>Forecasting Future World Events with Neural Networks</title>
<link>https://arxiv.org/abs/2206.15474</link>
<guid>https://arxiv.org/abs/2206.15474</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a neural‑network based approach for forecasting future world events using large‑scale news text data. It trains models on historical event records (e.g., GDELT) and evaluates their ability to predict upcoming geopolitical occurrences, reporting modest predictive performance.<br /><strong>Summary (CN):</strong> 本文提出一种基于神经网络的模型，用于预测未来的全球事件，利用历史新闻文本（如 GDELT 数据集）进行训练，并在实验中展示了对未来地缘政治事件的预测能力，取得了适度的预测效果。<br /><strong>Keywords:</strong> neural networks, event forecasting, NLP, GDELT, geopolitical prediction, time series<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a neural‑network based approach for forecasting future world events using large‑scale news text data. It trains models on historical event records (e.g., GDELT) and evaluates their ability to predict upcoming geopolitical occurrences, reporting modest predictive performance.", "summary_cn": "本文提出一种基于神经网络的模型，用于预测未来的全球事件，利用历史新闻文本（如 GDELT 数据集）进行训练，并在实验中展示了对未来地缘政治事件的预测能力，取得了适度的预测效果。", "keywords": "neural networks, event forecasting, NLP, GDELT, geopolitical prediction, time series", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}
]]></acme>

<pubDate>Wed, 01 Jun 2022 00:00:00 -0000</pubDate>
</item>
<item>
<title>Scaling Out-of-Distribution Detection for Real-World Settings</title>
<link>https://arxiv.org/abs/1911.11132</link>
<guid>https://arxiv.org/abs/1911.11132</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents methods for scaling out-of-distribution (OOD) detection to large‑scale, real‑world settings, proposing efficient techniques that operate on deep feature representations and likelihood ratios, and demonstrating their effectiveness on ImageNet‑scale benchmarks. Experiments show that the proposed approaches maintain strong detection performance while being computationally feasible for deployment in practical systems.<br /><strong>Summary (CN):</strong> 本文提出了一套可用于大规模真实场景的异常分布（OOD）检测方法，利用深层特征表示和似然比进行高效检测，并在 ImageNet 规模的基准上验证了其效果。实验表明，该方案在保持检测性能的同时，计算开销适合实际部署。<br /><strong>Keywords:</strong> out-of-distribution detection, OOD, scaling, deep learning, uncertainty estimation, real-world deployment, likelihood ratio<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents methods for scaling out-of-distribution (OOD) detection to large‑scale, real‑world settings, proposing efficient techniques that operate on deep feature representations and likelihood ratios, and demonstrating their effectiveness on ImageNet‑scale benchmarks. Experiments show that the proposed approaches maintain strong detection performance while being computationally feasible for deployment in practical systems.", "summary_cn": "本文提出了一套可用于大规模真实场景的异常分布（OOD）检测方法，利用深层特征表示和似然比进行高效检测，并在 ImageNet 规模的基准上验证了其效果。实验表明，该方案在保持检测性能的同时，计算开销适合实际部署。", "keywords": "out-of-distribution detection, OOD, scaling, deep learning, uncertainty estimation, real-world deployment, likelihood ratio", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sun, 01 May 2022 00:00:00 -0000</pubDate>
</item>
<item>
<title>What Would Jiminy Cricket Do? Towards Agents That Behave Morally</title>
<link>https://arxiv.org/abs/2110.13136</link>
<guid>https://arxiv.org/abs/2110.13136</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a framework in which autonomous agents can query a moral advisor—analogous to Jiminy Cricket—to obtain guidance on how to act morally in various situations. It presents methods for training moral suggestion models using human feedback and demonstrates how such advisory mechanisms can be integrated with reinforcement learning agents to improve moral behavior. The work evaluates the approach on a suite of ethical dilemmas, showing reductions in harmful actions while maintaining task performance.<br /><strong>Summary (CN):</strong> 本文提出了一种框架，使自主智能体能够像“Jiminy Cricket”（吉米尼蟋蟀）一样向道德顾问查询，以获得在不同情境下的道德行为指导。文中展示了利用人类反馈训练道德建议模型的方法，并将该顾问机制与强化学习智能体结合，以提升其道德行为。通过一套伦理困境实验，结果表明在保持任务性能的同时，显著降低了有害行为的发生。<br /><strong>Keywords:</strong> moral reasoning, AI safety, alignment, value learning, human feedback, ethical AI, decision making, reinforcement learning, AI governance, moral supervision<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a framework in which autonomous agents can query a moral advisor—analogous to Jiminy Cricket—to obtain guidance on how to act morally in various situations. It presents methods for training moral suggestion models using human feedback and demonstrates how such advisory mechanisms can be integrated with reinforcement learning agents to improve moral behavior. The work evaluates the approach on a suite of ethical dilemmas, showing reductions in harmful actions while maintaining task performance.", "summary_cn": "本文提出了一种框架，使自主智能体能够像“Jiminy Cricket”（吉米尼蟋蟀）一样向道德顾问查询，以获得在不同情境下的道德行为指导。文中展示了利用人类反馈训练道德建议模型的方法，并将该顾问机制与强化学习智能体结合，以提升其道德行为。通过一套伦理困境实验，结果表明在保持任务性能的同时，显著降低了有害行为的发生。", "keywords": "moral reasoning, AI safety, alignment, value learning, human feedback, ethical AI, decision making, reinforcement learning, AI governance, moral supervision", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Tue, 01 Feb 2022 00:00:00 -0000</pubDate>
</item>
<item>
<title>PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures</title>
<link>https://arxiv.org/abs/2112.05135</link>
<guid>https://arxiv.org/abs/2112.05135</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PixMix, a data augmentation technique that blends natural images with dreamlike pictures generated by deep generative models, showing that this mixture substantially improves model robustness to distribution shifts, adversarial attacks, and out-of-distribution detection, thereby enhancing safety measures for vision systems.<br /><strong>Summary (CN):</strong> 本文提出 PixMix，一种将真实图像与由深度生成模型生成的梦幻图片混合的数据增强方法，可提升视觉模型在分布漂移、对抗攻击和未知分布检测等方面的鲁棒性，从而增强安全性。<br /><strong>Keywords:</strong> PixMix, data augmentation, robustness, safety, out-of-distribution detection, adversarial robustness, generative models, synthetic data<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PixMix, a data augmentation technique that blends natural images with dreamlike pictures generated by deep generative models, showing that this mixture substantially improves model robustness to distribution shifts, adversarial attacks, and out-of-distribution detection, thereby enhancing safety measures for vision systems.", "summary_cn": "本文提出 PixMix，一种将真实图像与由深度生成模型生成的梦幻图片混合的数据增强方法，可提升视觉模型在分布漂移、对抗攻击和未知分布检测等方面的鲁棒性，从而增强安全性。", "keywords": "PixMix, data augmentation, robustness, safety, out-of-distribution detection, adversarial robustness, generative models, synthetic data", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 01 Dec 2021 00:00:00 -0000</pubDate>
</item>
<item>
<title>Globally-Robust Neural Networks</title>
<link>https://arxiv.org/abs/2102.08452</link>
<guid>https://arxiv.org/abs/2102.08452</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a training framework that produces neural networks with provable global robustness against ℓ∞ adversarial perturbations. By incorporating a convex outer approximation of the network’s output region into the loss, the method yields certificates that no input within a specified norm ball can cause misclassification, and demonstrates improved certified accuracy over prior approaches.<br /><strong>Summary (CN):</strong> 本文提出一种训练框架，使神经网络在 ℓ∞ 范数范围内具备可证明的全局鲁棒性。通过在损失函数中加入网络输出区域的凸外部逼近，该方法能够为每个输入提供不出现误分类的证明，并在实验中相较于已有方法提升了可认证准确率。<br /><strong>Keywords:</strong> provable robustness, adversarial certification, global robustness, convex outer approximation, certified defense, neural network verification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a training framework that produces neural networks with provable global robustness against ℓ∞ adversarial perturbations. By incorporating a convex outer approximation of the network’s output region into the loss, the method yields certificates that no input within a specified norm ball can cause misclassification, and demonstrates improved certified accuracy over prior approaches.", "summary_cn": "本文提出一种训练框架，使神经网络在 ℓ∞ 范数范围内具备可证明的全局鲁棒性。通过在损失函数中加入网络输出区域的凸外部逼近，该方法能够为每个输入提供不出现误分类的证明，并在实验中相较于已有方法提升了可认证准确率。", "keywords": "provable robustness, adversarial certification, global robustness, convex outer approximation, certified defense, neural network verification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Thu, 01 Jul 2021 00:00:00 -0000</pubDate>
</item>
<item>
<title>APPS: Measuring Coding Challenge Competence With APPS</title>
<link>https://arxiv.org/abs/2105.09938</link>
<guid>https://arxiv.org/abs/2105.09938</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces APPS, a large benchmark of Python coding challenges spanning multiple difficulty levels, designed to evaluate the competence of code synthesis models. It details the dataset construction, problem categorization, and evaluation protocol, and reports baseline results from several language models, highlighting current performance gaps.<br /><strong>Summary (CN):</strong> 本文提出了 APPS，一套涵盖多种难度等级的 Python 编程挑战大规模基准，用于评估代码生成模型的能力。论文阐述了数据集的构建、题目分类以及评估流程，并给出了多种语言模型的基线结果，指出了现有性能的不足。<br /><strong>Keywords:</strong> code generation, programming puzzles, APPS benchmark, Python, neural code synthesis, evaluation metrics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces APPS, a large benchmark of Python coding challenges spanning multiple difficulty levels, designed to evaluate the competence of code synthesis models. It details the dataset construction, problem categorization, and evaluation protocol, and reports baseline results from several language models, highlighting current performance gaps.", "summary_cn": "本文提出了 APPS，一套涵盖多种难度等级的 Python 编程挑战大规模基准，用于评估代码生成模型的能力。论文阐述了数据集的构建、题目分类以及评估流程，并给出了多种语言模型的基线结果，指出了现有性能的不足。", "keywords": "code generation, programming puzzles, APPS benchmark, Python, neural code synthesis, evaluation metrics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}
]]></acme>

<pubDate>Sat, 01 May 2021 00:00:00 -0000</pubDate>
</item>
<item>
<title>MMLU: Measuring Massive Multitask Language Understanding</title>
<link>https://arxiv.org/abs/2009.03300</link>
<guid>https://arxiv.org/abs/2009.03300</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MMLU, a comprehensive benchmark comprising 57 subjects and 138 tasks to evaluate language models across a wide range of knowledge and reasoning domains. It reports zero‑shot, few‑shot, and fine‑tuned performance of several models, highlighting gaps between model size and human proficiency. The dataset and evaluation protocol aim to provide a standardized measure of massive multitask language understanding.<br /><strong>Summary (CN):</strong> 本文提出了 MMLU 基准，涵盖 57 个学科、138 项任务，用于评估语言模型在广泛知识与推理领域的表现。文中展示了多种模型的零样本、少样本以及微调后的结果，揭示了模型规模与人类水平之间的差距。该数据集和评估流程旨在提供对大规模多任务语言理解的标准化衡量。<br /><strong>Keywords:</strong> massive multitask language understanding, benchmark, few-shot learning, language model evaluation, zero-shot, task diversity, reasoning, knowledge, multi-task testing, MMLU<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MMLU, a comprehensive benchmark comprising 57 subjects and 138 tasks to evaluate language models across a wide range of knowledge and reasoning domains. It reports zero‑shot, few‑shot, and fine‑tuned performance of several models, highlighting gaps between model size and human proficiency. The dataset and evaluation protocol aim to provide a standardized measure of massive multitask language understanding.", "summary_cn": "本文提出了 MMLU 基准，涵盖 57 个学科、138 项任务，用于评估语言模型在广泛知识与推理领域的表现。文中展示了多种模型的零样本、少样本以及微调后的结果，揭示了模型规模与人类水平之间的差距。该数据集和评估流程旨在提供对大规模多任务语言理解的标准化衡量。", "keywords": "massive multitask language understanding, benchmark, few-shot learning, language model evaluation, zero-shot, task diversity, reasoning, knowledge, multi-task testing, MMLU", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}
]]></acme>

<pubDate>Fri, 01 Jan 2021 00:00:00 -0000</pubDate>
</item>
<item>
<title>Aligning AI With Shared Human Values</title>
<link>https://arxiv.org/abs/2008.02275</link>
<guid>https://arxiv.org/abs/2008.02275</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys and proposes methods for aligning advanced AI systems with shared human values, discussing philosophical foundations, value learning frameworks such as cooperative inverse reinforcement learning, and challenges in specifying and preserving common ethical standards. It emphasizes the importance of incorporating diverse human preferences and outlines research directions for robust value alignment.<br /><strong>Summary (CN):</strong> 本文综述并提出将先进 AI 系统对齐至共享人类价值观的方法，讨论哲学基础、合作逆向强化学习等价值学习框架，以及在明确和维护共同伦理标准方面的挑战。文章强调整合多元人类偏好的重要性，并概述实现稳健价值对齐的研究方向。<br /><strong>Keywords:</strong> AI alignment, shared human values, value learning, cooperative inverse reinforcement learning, ethical AI, preference modeling, safety, alignment strategies<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys and proposes methods for aligning advanced AI systems with shared human values, discussing philosophical foundations, value learning frameworks such as cooperative inverse reinforcement learning, and challenges in specifying and preserving common ethical standards. It emphasizes the importance of incorporating diverse human preferences and outlines research directions for robust value alignment.", "summary_cn": "本文综述并提出将先进 AI 系统对齐至共享人类价值观的方法，讨论哲学基础、合作逆向强化学习等价值学习框架，以及在明确和维护共同伦理标准方面的挑战。文章强调整合多元人类偏好的重要性，并概述实现稳健价值对齐的研究方向。", "keywords": "AI alignment, shared human values, value learning, cooperative inverse reinforcement learning, ethical AI, preference modeling, safety, alignment strategies", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Sat, 01 Aug 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2006.16241</link>
<guid>https://arxiv.org/abs/2006.16241</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a comprehensive critical analysis of out-of-distribution (OOD) generalization methods, categorizing the many facets of robustness and highlighting the limitations of current approaches. It surveys existing techniques, evaluates their empirical performance across benchmarks, and proposes a unified perspective for assessing OOD robustness. The authors discuss open challenges and suggest directions for future research to improve reliable generalization under distribution shift.<br /><strong>Summary (CN):</strong> 本文对分布外（OOD）泛化方法进行全面的批判性分析，归类了鲁棒性的多个方面并指出当前方法的局限性。文章综述了已有技术，在多个基准上评估其经验表现，并提出统一的鲁棒性评估视角。作者讨论了未解决的挑战并建议未来研究方向，以提升在分布转移条件下的可靠泛化能力。<br /><strong>Keywords:</strong> out-of-distribution generalization, robustness, domain shift, distributional robustness, evaluation benchmark, representation learning, transfer learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a comprehensive critical analysis of out-of-distribution (OOD) generalization methods, categorizing the many facets of robustness and highlighting the limitations of current approaches. It surveys existing techniques, evaluates their empirical performance across benchmarks, and proposes a unified perspective for assessing OOD robustness. The authors discuss open challenges and suggest directions for future research to improve reliable generalization under distribution shift.", "summary_cn": "本文对分布外（OOD）泛化方法进行全面的批判性分析，归类了鲁棒性的多个方面并指出当前方法的局限性。文章综述了已有技术，在多个基准上评估其经验表现，并提出统一的鲁棒性评估视角。作者讨论了未解决的挑战并建议未来研究方向，以提升在分布转移条件下的可靠泛化能力。", "keywords": "out-of-distribution generalization, robustness, domain shift, distributional robustness, evaluation benchmark, representation learning, transfer learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Mon, 01 Jun 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>Pretrained Transformers Improve Out-of-Distribution Robustness</title>
<link>https://arxiv.org/abs/2004.06100</link>
<guid>https://arxiv.org/abs/2004.06100</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how pretrained transformer language models affect out-of-distribution (OOD) robustness across several NLP tasks. By constructing OOD test sets and comparing pretrained models (e.g., BERT, RoBERTa) with non‑pretrained baselines, the authors show that pretraining substantially mitigates performance degradation under distribution shifts. They analyze possible reasons such as richer representations and regularization effects.<br /><strong>Summary (CN):</strong> 本文研究了预训练的 Transformer 语言模型（如 BERT、RoBERTa）在自然语言处理任务中的分布外（OOD）鲁棒性。通过构建分布外测试集并与未预训练的基线模型比较，发现预训练显著降低了性能因分布转移而产生的下降，并进一步分析了更丰富表征和正则化等可能原因。<br /><strong>Keywords:</strong> pretrained transformers, out-of-distribution robustness, natural language processing, BERT, RoBERTa, distribution shift, representation learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how pretrained transformer language models affect out-of-distribution (OOD) robustness across several NLP tasks. By constructing OOD test sets and comparing pretrained models (e.g., BERT, RoBERTa) with non‑pretrained baselines, the authors show that pretraining substantially mitigates performance degradation under distribution shifts. They analyze possible reasons such as richer representations and regularization effects.", "summary_cn": "本文研究了预训练的 Transformer 语言模型（如 BERT、RoBERTa）在自然语言处理任务中的分布外（OOD）鲁棒性。通过构建分布外测试集并与未预训练的基线模型比较，发现预训练显著降低了性能因分布转移而产生的下降，并进一步分析了更丰富表征和正则化等可能原因。", "keywords": "pretrained transformers, out-of-distribution robustness, natural language processing, BERT, RoBERTa, distribution shift, representation learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 01 Apr 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>Overfitting in adversarially robust deep learning</title>
<link>https://arxiv.org/abs/2002.11569</link>
<guid>https://arxiv.org/abs/2002.11569</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the phenomenon of robust overfitting in adversarially trained deep neural networks, where test robust accuracy deteriorates after a certain number of training epochs despite continued improvement in standard accuracy. It provides extensive empirical analysis across datasets, model sizes, and training schedules, and proposes simple mitigation strategies such as early stopping and appropriate regularization. The findings enhance our understanding of the dynamics of adversarial training and its limitations.<br /><strong>Summary (CN):</strong> 本文研究了对抗训练深度网络中的鲁棒过拟合现象，即在训练后期测试鲁棒准确率下降，而标准准确率仍在提升。通过在多个数据集、模型规模和学习率调度下的大量实验，分析了其成因并提出了早停和合适正则化等简单的缓解措施。研究提升了我们对对抗训练动态及其局限性的理解。<br /><strong>Keywords:</strong> adversarial robustness, robust overfitting, early stopping, regularization, deep learning, training dynamics, test accuracy, model capacity, empirical analysis<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the phenomenon of robust overfitting in adversarially trained deep neural networks, where test robust accuracy deteriorates after a certain number of training epochs despite continued improvement in standard accuracy. It provides extensive empirical analysis across datasets, model sizes, and training schedules, and proposes simple mitigation strategies such as early stopping and appropriate regularization. The findings enhance our understanding of the dynamics of adversarial training and its limitations.", "summary_cn": "本文研究了对抗训练深度网络中的鲁棒过拟合现象，即在训练后期测试鲁棒准确率下降，而标准准确率仍在提升。通过在多个数据集、模型规模和学习率调度下的大量实验，分析了其成因并提出了早停和合适正则化等简单的缓解措施。研究提升了我们对对抗训练动态及其局限性的理解。", "keywords": "adversarial robustness, robust overfitting, early stopping, regularization, deep learning, training dynamics, test accuracy, model capacity, empirical analysis", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sun, 01 Mar 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty</title>
<link>https://arxiv.org/abs/1912.02781</link>
<guid>https://arxiv.org/abs/1912.02781</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AugMix proposes a simple data processing pipeline that mixes multiple augmented versions of an image using random chains of augmentation operations and a consistency loss, leading to improved robustness against common corruptions and better uncertainty calibration without sacrificing clean accuracy. The method is computationally inexpensive and can be combined with existing training pipelines.<br /><strong>Summary (CN):</strong> AugMix 提出了一种通过随机组合多种图像增强操作并加入一致性损失的简单数据处理管线，可提升模型在常见腐败扰动下的鲁棒性并改善不确定性校准，同时不牺牲原始准确率。该方法计算成本低，易于与现有训练流程结合。<br /><strong>Keywords:</strong> AugMix, data augmentation, robustness, uncertainty calibration, corruption robustness, image classification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AugMix proposes a simple data processing pipeline that mixes multiple augmented versions of an image using random chains of augmentation operations and a consistency loss, leading to improved robustness against common corruptions and better uncertainty calibration without sacrificing clean accuracy. The method is computationally inexpensive and can be combined with existing training pipelines.", "summary_cn": "AugMix 提出了一种通过随机组合多种图像增强操作并加入一致性损失的简单数据处理管线，可提升模型在常见腐败扰动下的鲁棒性并改善不确定性校准，同时不牺牲原始准确率。该方法计算成本低，易于与现有训练流程结合。", "keywords": "AugMix, data augmentation, robustness, uncertainty calibration, corruption robustness, image classification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sat, 01 Feb 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>Fast is better than free: Revisiting adversarial training</title>
<link>https://arxiv.org/abs/2001.03994</link>
<guid>https://arxiv.org/abs/2001.03994</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a computationally efficient adversarial training method that reduces the number of PGD steps while maintaining or improving model robustness by adjusting step size and loss scheduling. It demonstrates that comparable robustness to standard adversarial training can be achieved with significantly less training time, and provides empirical analysis of the trade‑offs between speed and robustness.<br /><strong>Summary (CN):</strong> 本文提出了一种高效的对抗训练方法，通过降低 PGD 步数并调整步长与损失调度，实现了与传统对抗训练相当甚至更好的模型鲁棒性，同时显著缩短训练时间。实验分析了训练速度与鲁棒性之间的权衡，展示了在更少计算开销下仍能保持强鲁棒性的可能性。<br /><strong>Keywords:</strong> adversarial training, robustness, fast adversarial training, PGD, machine learning security<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a computationally efficient adversarial training method that reduces the number of PGD steps while maintaining or improving model robustness by adjusting step size and loss scheduling. It demonstrates that comparable robustness to standard adversarial training can be achieved with significantly less training time, and provides empirical analysis of the trade‑offs between speed and robustness.", "summary_cn": "本文提出了一种高效的对抗训练方法，通过降低 PGD 步数并调整步长与损失调度，实现了与传统对抗训练相当甚至更好的模型鲁棒性，同时显著缩短训练时间。实验分析了训练速度与鲁棒性之间的权衡，展示了在更少计算开销下仍能保持强鲁棒性的可能性。", "keywords": "adversarial training, robustness, fast adversarial training, PGD, machine learning security", "scoring": {"interpretability": 1, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 01 Jan 2020 00:00:00 -0000</pubDate>
</item>
<item>
<title>Natural Adversarial Examples</title>
<link>https://arxiv.org/abs/1907.07174</link>
<guid>https://arxiv.org/abs/1907.07174</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates naturally occurring images that cause misclassifications in deep neural networks and exhibit properties similar to crafted adversarial examples. It analyses their distribution, visual characteristics, and the extent to which they can be mitigated by standard robustness techniques.<br /><strong>Summary (CN):</strong> 本文研究了在深度神经网络中导致误分类、并具备类似于人工构造对抗样本特性的自然图像。作者分析了这些自然对抗样本的分布、视觉特征，以及使用常规鲁棒性方法进行缓解的效果。<br /><strong>Keywords:</strong> adversarial examples, natural images, robustness, misclassification, dataset bias, detection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates naturally occurring images that cause misclassifications in deep neural networks and exhibit properties similar to crafted adversarial examples. It analyses their distribution, visual characteristics, and the extent to which they can be mitigated by standard robustness techniques.", "summary_cn": "本文研究了在深度神经网络中导致误分类、并具备类似于人工构造对抗样本特性的自然图像。作者分析了这些自然对抗样本的分布、视觉特征，以及使用常规鲁棒性方法进行缓解的效果。", "keywords": "adversarial examples, natural images, robustness, misclassification, dataset bias, detection", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Mon, 01 Jul 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty</title>
<link>https://arxiv.org/abs/1906.12340</link>
<guid>https://arxiv.org/abs/1906.12340</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how incorporating self‑supervised learning objectives during training can enhance the robustness of neural networks to distributional shifts and improve the quality of uncertainty estimates. Experiments on image classification benchmarks show that models pretrained with self‑supervised tasks achieve better calibration and retain higher accuracy under corruptions compared to standard supervised training. The authors attribute these gains to richer feature representations learned from the auxiliary tasks.<br /><strong>Summary (CN):</strong> 本文研究了在训练过程中加入自监督学习任务如何提升神经网络对分布漂移的鲁棒性以及不确定性估计的质量。实验在图像分类基准上表明，使用自监督预训练的模型相比仅使用监督训练的模型在校准方面更好，并且在图像损坏等情况下保持更高的准确率。作者认为这些提升源于自监督任务学习到的更丰富特征表征。<br /><strong>Keywords:</strong> self-supervised learning, robustness, uncertainty estimation, model calibration, domain shift, representation learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how incorporating self‑supervised learning objectives during training can enhance the robustness of neural networks to distributional shifts and improve the quality of uncertainty estimates. Experiments on image classification benchmarks show that models pretrained with self‑supervised tasks achieve better calibration and retain higher accuracy under corruptions compared to standard supervised training. The authors attribute these gains to richer feature representations learned from the auxiliary tasks.", "summary_cn": "本文研究了在训练过程中加入自监督学习任务如何提升神经网络对分布漂移的鲁棒性以及不确定性估计的质量。实验在图像分类基准上表明，使用自监督预训练的模型相比仅使用监督训练的模型在校准方面更好，并且在图像损坏等情况下保持更高的准确率。作者认为这些提升源于自监督任务学习到的更丰富特征表征。", "keywords": "self-supervised learning, robustness, uncertainty estimation, model calibration, domain shift, representation learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sat, 01 Jun 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>Randomized Smoothing: Certified adversarial robustness via randomized smoothing</title>
<link>https://arxiv.org/abs/1902.02918</link>
<guid>https://arxiv.org/abs/1902.02918</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces randomized smoothing, a technique that creates a new classifier by adding Gaussian noise to inputs and taking a majority vote, and provides provable certificates of α‑radius adversarial robustness for the smoothed classifier. The authors prove theoretical guarantees, present efficient certification algorithms, and demonstrate strong empirical robustness on ImageNet and CIFAR‑10 against strong attacks.<br /><strong>Summary (CN):</strong> 本站提供了一种等同结果的重点沿方法，通过对输入加入高典随机属父将创造同生类别器，并使用平均合成决定结果，并为该滑粉分类器提供兼容形式超程时长(r) 的安全证明。为此文章用实验验证评定了这种方法的理论偶转和计算绿度，并在 ImageNet 和 CIFAR-10 上对小龄云烈攻击了得正向的形去安全性。<br /><strong>Keywords:</strong> randomized smoothing, adversarial robustness, certified robustness, Gaussian noise, smoothed classifier, provable defense<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces randomized smoothing, a technique that creates a new classifier by adding Gaussian noise to inputs and taking a majority vote, and provides provable certificates of α‑radius adversarial robustness for the smoothed classifier. The authors prove theoretical guarantees, present efficient certification algorithms, and demonstrate strong empirical robustness on ImageNet and CIFAR‑10 against strong attacks.", "summary_cn": "本站提供了一种等同结果的重点沿方法，通过对输入加入高典随机属父将创造同生类别器，并使用平均合成决定结果，并为该滑粉分类器提供兼容形式超程时长(r) 的安全证明。为此文章用实验验证评定了这种方法的理论偶转和计算绿度，并在 ImageNet 和 CIFAR-10 上对小龄云烈攻击了得正向的形去安全性。", "keywords": "randomized smoothing, adversarial robustness, certified robustness, Gaussian noise, smoothed classifier, provable defense", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sat, 01 Jun 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>Using Pre-Training Can Improve Model Robustness and Uncertainty</title>
<link>https://arxiv.org/abs/1901.09960</link>
<guid>https://arxiv.org/abs/1901.09960</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how unsupervised or self‑supervised pre‑training can enhance a model's robustness to distributional shifts, adversarial perturbations, and improve uncertainty calibration. Through extensive experiments on image classification benchmarks, the authors show that pre‑trained models achieve higher accuracy under attack and produce better calibrated confidence scores compared to models trained from scratch.<br /><strong>Summary (CN):</strong> 本文研究了无监督（或自监督）预训练对提升模型在分布漂移、对抗扰动下的鲁棒性以及不确定性校准的作用。通过在图像分类基准上的大量实验，作者展示了预训练模型在遭受攻击时保持更高的准确率，并且相较于从头训练的模型能够产生更可靠的置信度分数。<br /><strong>Keywords:</strong> pretraining, robustness, uncertainty estimation, adversarial examples, calibration, transfer learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how unsupervised or self‑supervised pre‑training can enhance a model's robustness to distributional shifts, adversarial perturbations, and improve uncertainty calibration. Through extensive experiments on image classification benchmarks, the authors show that pre‑trained models achieve higher accuracy under attack and produce better calibrated confidence scores compared to models trained from scratch.", "summary_cn": "本文研究了无监督（或自监督）预训练对提升模型在分布漂移、对抗扰动下的鲁棒性以及不确定性校准的作用。通过在图像分类基准上的大量实验，作者展示了预训练模型在遭受攻击时保持更高的准确率，并且相较于从头训练的模型能够产生更可靠的置信度分数。", "keywords": "pretraining, robustness, uncertainty estimation, adversarial examples, calibration, transfer learning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 01 May 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>ImageNet-C: Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</title>
<link>https://arxiv.org/abs/1903.12261</link>
<guid>https://arxiv.org/abs/1903.12261</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ImageNet-C, a benchmark that augments the ImageNet validation set with a variety of synthetic corruptions and perturbations to evaluate neural network robustness. Extensive experiments show that standard architectures suffer significant performance drops under these common corruptions, highlighting a gap between clean accuracy and real‑world reliability. The authors provide a standardized evaluation protocol and suggest future research directions for improving model resilience.<br /><strong>Summary (CN):</strong> 本文提出了 ImageNet-C 基准，通过在 ImageNet 验证集上加入多种合成的常见腐蚀和扰动，评估神经网络的鲁棒性。大量实验表明，主流模型在这些常见腐蚀下性能显著下降，揭示出干净数据准确率与真实环境可靠性之间的差距。文章提供了统一的评估流程，并指出了提升模型韧性的未来研究方向。<br /><strong>Keywords:</strong> robustness, common corruptions, ImageNet-C, perturbations, benchmark, neural network evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ImageNet-C, a benchmark that augments the ImageNet validation set with a variety of synthetic corruptions and perturbations to evaluate neural network robustness. Extensive experiments show that standard architectures suffer significant performance drops under these common corruptions, highlighting a gap between clean accuracy and real‑world reliability. The authors provide a standardized evaluation protocol and suggest future research directions for improving model resilience.", "summary_cn": "本文提出了 ImageNet-C 基准，通过在 ImageNet 验证集上加入多种合成的常见腐蚀和扰动，评估神经网络的鲁棒性。大量实验表明，主流模型在这些常见腐蚀下性能显著下降，揭示出干净数据准确率与真实环境可靠性之间的差距。文章提供了统一的评估流程，并指出了提升模型韧性的未来研究方向。", "keywords": "robustness, common corruptions, ImageNet-C, perturbations, benchmark, neural network evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Fri, 01 Mar 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>Deep Anomaly Detection with Outlier Exposure</title>
<link>https://arxiv.org/abs/1812.04606</link>
<guid>https://arxiv.org/abs/1812.04606</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Outlier Exposure, a simple yet effective approach that trains deep neural networks on a large, diverse set of known outliers to improve their ability to detect anomalous or out-of-distribution inputs. By augmenting the training objective with a term that encourages low confidence on these outliers, the method yields state-of-the-art performance on several OOD detection benchmarks while maintaining standard classification accuracy.<br /><strong>Summary (CN):</strong> 本文提出了“异常暴露（Outlier Exposure）”方法，通过在训练中加入大量多样化的已知异常样本，使深度神经网络在面对异常或分布外输入时能够产生更低的可信度。该方法在不牺牲常规分类精度的前提下，显著提升了多项 OOD 检测基准的表现，达到了最新水平。<br /><strong>Keywords:</strong> outlier exposure, anomaly detection, out-of-distribution detection, deep learning, uncertainty estimation, OOD detection, safety<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Outlier Exposure, a simple yet effective approach that trains deep neural networks on a large, diverse set of known outliers to improve their ability to detect anomalous or out-of-distribution inputs. By augmenting the training objective with a term that encourages low confidence on these outliers, the method yields state-of-the-art performance on several OOD detection benchmarks while maintaining standard classification accuracy.", "summary_cn": "本文提出了“异常暴露（Outlier Exposure）”方法，通过在训练中加入大量多样化的已知异常样本，使深度神经网络在面对异常或分布外输入时能够产生更低的可信度。该方法在不牺牲常规分类精度的前提下，显著提升了多项 OOD 检测基准的表现，达到了最新水平。", "keywords": "outlier exposure, anomaly detection, out-of-distribution detection, deep learning, uncertainty estimation, OOD detection, safety", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Tue, 01 Jan 2019 00:00:00 -0000</pubDate>
</item>
<item>
<title>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</title>
<link>https://arxiv.org/abs/1610.02136</link>
<guid>https://arxiv.org/abs/1610.02136</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes using the maximum softmax probability as a simple baseline for detecting when neural network predictions are likely to be incorrect or when inputs are out-of-distribution. It evaluates this metric on several image classification benchmarks and demonstrates that it can reliably distinguish between correctly classified, misclassified, and OOD examples without modifying the underlying model.<br /><strong>Summary (CN):</strong> 本文提出使用最大softmax概率作为检测神经网络预测是否错误或输入是否分布外的简易基线。通过在多个图像分类基准上的实验，展示该度量能够在不更改模型结构的情况下可靠地区分正确分类、误分类和分布外样本。<br /><strong>Keywords:</strong> softmax confidence, out-of-distribution detection, misclassification detection, neural network uncertainty, baseline method<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes using the maximum softmax probability as a simple baseline for detecting when neural network predictions are likely to be incorrect or when inputs are out-of-distribution. It evaluates this metric on several image classification benchmarks and demonstrates that it can reliably distinguish between correctly classified, misclassified, and OOD examples without modifying the underlying model.", "summary_cn": "本文提出使用最大softmax概率作为检测神经网络预测是否错误或输入是否分布外的简易基线。通过在多个图像分类基准上的实验，展示该度量能够在不更改模型结构的情况下可靠地区分正确分类、误分类和分布外样本。", "keywords": "softmax confidence, out-of-distribution detection, misclassification detection, neural network uncertainty, baseline method", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Mon, 01 Oct 2018 00:00:00 -0000</pubDate>
</item>
<item>
<title>Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
<link>https://arxiv.org/abs/1711.00851</link>
<guid>https://arxiv.org/abs/1711.00851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method to compute provable guarantees against L∞ adversarial perturbations by constructing a convex outer approximation of the adversarial polytope and optimizing a bound on worst‑case loss during training. It formulates verification as a linear program and proposes a training procedure that directly minimizes the upper bound, yielding networks with certified robustness on several benchmarks.<br /><strong>Summary (CN):</strong> 本文提出通过构建对抗扰动多面体的凸外部近似来获得对 L∞ 范围内对抗样本的可证明防御，并在训练时最小化该上界，从而实现对最坏情况损失的可验证保证。作者将验证问题转化为线性规划，并在多个基准上展示了具有可证实鲁棒性的网络。<br /><strong>Keywords:</strong> adversarial robustness, provable defense, convex relaxation, certified robustness, neural networks, verification, linear programming, robust training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method to compute provable guarantees against L∞ adversarial perturbations by constructing a convex outer approximation of the adversarial polytope and optimizing a bound on worst‑case loss during training. It formulates verification as a linear program and proposes a training procedure that directly minimizes the upper bound, yielding networks with certified robustness on several benchmarks.", "summary_cn": "本文提出通过构建对抗扰动多面体的凸外部近似来获得对 L∞ 范围内对抗样本的可证明防御，并在训练时最小化该上界，从而实现对最坏情况损失的可验证保证。作者将验证问题转化为线性规划，并在多个基准上展示了具有可证实鲁棒性的网络。", "keywords": "adversarial robustness, provable defense, convex relaxation, certified robustness, neural networks, verification, linear programming, robust training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Fri, 01 Jun 2018 00:00:00 -0000</pubDate>
</item>
</channel>
</rss>