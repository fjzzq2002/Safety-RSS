<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Machine Learning</title>
<link>https://papers.cool/arxiv/cs.LG</link>


<item>
<title>KL-Regularized Reinforcement Learning is Designed to Mode Collapse</title>
<link>https://papers.cool/arxiv/2510.20817</link>
<guid>https://papers.cool/arxiv/2510.20817</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that the common belief that reverse KL leads to mode seeking and forward KL to mass covering does not reliably extend to KL-regularized reinforcement learning, where mode coverage is mainly driven by regularization strength and reward scaling. It derives the family of optimal target distributions, highlights that typical low-regularization settings produce unimodal targets, and proposes a simple adjustment to reward magnitudes that yields diverse, high-quality samples for large language and chemical language models with both forward and reverse KL.<br /><strong>Summary (CN):</strong> 本文指出，逆 KL 导致模式寻找、正 KL 覆盖整体的直觉并不适用于 KL 正则化的强化学习；模式覆盖主要取决于正则化强度和奖励与参考概率的相对尺度。研究了最优目标分布的形式，发现常用的低正则化设置会产生单峰目标，并提出通过轻微调节奖励大小即可在正向和逆向 KL 下实现高质量且多样的样本，适用于大语言模型和化学语言模型。<br /><strong>Keywords:</strong> KL regularization, reinforcement learning, mode collapse, forward KL, reverse KL, diversity, large language models, chemical language models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath</div>
It is commonly believed that optimizing the reverse KL divergence results in "mode seeking", while optimizing forward KL results in "mass covering", with the latter being preferred if the goal is to sample from multiple diverse modes. We show -- mathematically and empirically -- that this intuition does not necessarily transfer well to doing reinforcement learning with reverse/forward KL regularization (e.g. as commonly used with language models). Instead, the choice of reverse/forward KL determines the family of optimal target distributions, parameterized by the regularization coefficient. Mode coverage depends primarily on other factors, such as regularization strength, and relative scales between rewards and reference probabilities. Further, we show commonly used settings such as low regularization strength and equal verifiable rewards tend to specify unimodal target distributions, meaning the optimization objective is, by construction, non-diverse. We leverage these insights to construct a simple, scalable, and theoretically justified algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a target distribution which puts high probability over all high-quality sampling modes. In experiments, this simple modification works to post-train both Large Language Models and Chemical Language Models to have higher solution quality and diversity, without any external signals of diversity, and works with both forward and reverse KL when using either naively fails.
<div><strong>Authors:</strong> Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that the common belief that reverse KL leads to mode seeking and forward KL to mass covering does not reliably extend to KL-regularized reinforcement learning, where mode coverage is mainly driven by regularization strength and reward scaling. It derives the family of optimal target distributions, highlights that typical low-regularization settings produce unimodal targets, and proposes a simple adjustment to reward magnitudes that yields diverse, high-quality samples for large language and chemical language models with both forward and reverse KL.", "summary_cn": "本文指出，逆 KL 导致模式寻找、正 KL 覆盖整体的直觉并不适用于 KL 正则化的强化学习；模式覆盖主要取决于正则化强度和奖励与参考概率的相对尺度。研究了最优目标分布的形式，发现常用的低正则化设置会产生单峰目标，并提出通过轻微调节奖励大小即可在正向和逆向 KL 下实现高质量且多样的样本，适用于大语言模型和化学语言模型。", "keywords": "KL regularization, reinforcement learning, mode collapse, forward KL, reverse KL, diversity, large language models, chemical language models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Anthony GX-Chen", "Jatin Prakash", "Jeff Guo", "Rob Fergus", "Rajesh Ranganath"]}
]]></acme>

<pubDate>2025-10-23T17:59:40+00:00</pubDate>
</item>
<item>
<title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title>
<link>https://papers.cool/arxiv/2510.20800</link>
<guid>https://papers.cool/arxiv/2510.20800</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fast LLM adaptation technique that selects a small set of weight matrices for low‑rank factorization using the gradient of singular values, clusters matrix rows into multiple subspaces, and evaluates the adaptation on only 100 examples with a single gradient step. This approach eliminates the exhaustive layer‑wise search of LASER, reduces overfitting, and achieves up to 24.6 percentage‑point accuracy gains without any conventional fine‑tuning.<br /><strong>Summary (CN):</strong> 本文提出一种快速的 LLM 适配方法，通过计算矩阵奇异值的梯度来挑选少数矩阵进行低秩分解，并对矩阵行进行多子空间聚类，仅使用 100 条样本并执行一次梯度更新即可完成适配。该方案省去了 LASER 的逐层 exhaustive 搜索，降低了过拟合，并在无需传统微调的情况下实现最高 24.6% 的准确率提升。<br /><strong>Keywords:</strong> LLM adaptation, low-rank factorization, singular value gradient, matrix pruning, LASER, data-efficient fine-tuning, row clustering, single gradient step<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.
<div><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fast LLM adaptation technique that selects a small set of weight matrices for low‑rank factorization using the gradient of singular values, clusters matrix rows into multiple subspaces, and evaluates the adaptation on only 100 examples with a single gradient step. This approach eliminates the exhaustive layer‑wise search of LASER, reduces overfitting, and achieves up to 24.6 percentage‑point accuracy gains without any conventional fine‑tuning.", "summary_cn": "本文提出一种快速的 LLM 适配方法，通过计算矩阵奇异值的梯度来挑选少数矩阵进行低秩分解，并对矩阵行进行多子空间聚类，仅使用 100 条样本并执行一次梯度更新即可完成适配。该方案省去了 LASER 的逐层 exhaustive 搜索，降低了过拟合，并在无需传统微调的情况下实现最高 24.6% 的准确率提升。", "keywords": "LLM adaptation, low-rank factorization, singular value gradient, matrix pruning, LASER, data-efficient fine-tuning, row clustering, single gradient step", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}
]]></acme>

<pubDate>2025-10-23T17:58:01+00:00</pubDate>
</item>
<item>
<title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title>
<link>https://papers.cool/arxiv/2510.20792</link>
<guid>https://papers.cool/arxiv/2510.20792</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BadGraph proposes a backdoor attack on latent diffusion models used for text‑guided graph generation, inserting textual triggers into a small portion of the training data to cause the model to output attacker‑specified subgraphs when the trigger appears. Experiments on four benchmark graph datasets show that with less than 10% poisoned data the attack achieves a 50% success rate, and with 24% poisoning the success exceeds 80%, while maintaining normal performance on clean inputs. The study highlights serious security risks for applications such as drug discovery and calls for robust defenses against such diffusion‑model backdoors.<br /><strong>Summary (CN):</strong> BadGraph 提出了一种针对用于文本引导图的潜在扩散模型的后门攻击方法，通过在少量训练数据中植入文本触发词，使模型在出现触发词时输出攻击者指定的子图。实验在四个基准图数据集上表明，低于 10% 的数据中毒率即可实现 50% 的攻击成功率，约 24% 的中毒率可超过 80% 成功率，同时对正常输入的性能影响甚微。该研究揭示了药物发现等应用中的严重安全风险，并呼吁对这种扩散模型后进行稳健防御。<br /><strong>Keywords:</strong> backdoor attack, latent diffusion model, text-guided graph generation, graph generation, security, data poisoning, VAE, diffusion, drug discovery, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Liang Ye, Shengqin Chen, Jiazhu Dai</div>
The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method targeting latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.
<div><strong>Authors:</strong> Liang Ye, Shengqin Chen, Jiazhu Dai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BadGraph proposes a backdoor attack on latent diffusion models used for text‑guided graph generation, inserting textual triggers into a small portion of the training data to cause the model to output attacker‑specified subgraphs when the trigger appears. Experiments on four benchmark graph datasets show that with less than 10% poisoned data the attack achieves a 50% success rate, and with 24% poisoning the success exceeds 80%, while maintaining normal performance on clean inputs. The study highlights serious security risks for applications such as drug discovery and calls for robust defenses against such diffusion‑model backdoors.", "summary_cn": "BadGraph 提出了一种针对用于文本引导图的潜在扩散模型的后门攻击方法，通过在少量训练数据中植入文本触发词，使模型在出现触发词时输出攻击者指定的子图。实验在四个基准图数据集上表明，低于 10% 的数据中毒率即可实现 50% 的攻击成功率，约 24% 的中毒率可超过 80% 成功率，同时对正常输入的性能影响甚微。该研究揭示了药物发现等应用中的严重安全风险，并呼吁对这种扩散模型后进行稳健防御。", "keywords": "backdoor attack, latent diffusion model, text-guided graph generation, graph generation, security, data poisoning, VAE, diffusion, drug discovery, robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Liang Ye", "Shengqin Chen", "Jiazhu Dai"]}
]]></acme>

<pubDate>2025-10-23T17:54:17+00:00</pubDate>
</item>
<item>
<title>Out-of-distribution Tests Reveal Compositionality in Chess Transformers</title>
<link>https://papers.cool/arxiv/2510.20783</link>
<guid>https://papers.cool/arxiv/2510.20783</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper trains a 270M‑parameter decision transformer to play chess and evaluates it on out‑of‑distribution scenarios designed to test systematic generalization. Results show that the model reliably follows basic chess rules and solves OOD puzzles, demonstrating compositional generalization, though it lags behind symbolic search especially on Chess960 variants. Training dynamics reveal an emergent understanding where the model first learns to move only its own pieces.<br /><strong>Summary (CN):</strong> 本文训练了一个 270M 参数的决策 Transformer（decision transformer）用于象棋，并在旨在检测系统性泛化的分布外（out‑of‑distribution）情景下进行评估。结果表明模型始终遵守象棋基本规则并能解出分布外谜题，展示了组合式泛化能力，但在 Chess960 等变体上仍不及显式搜索的符号 AI。训练动态显示模型最初学会仅移动自身棋子，暗示出现了对游戏的组合理解。<br /><strong>Keywords:</strong> chess transformer, out-of-distribution, compositional generalization, systematic generalization, decision transformer, Chess960, rule extrapolation<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Anna Mészáros, Patrik Reizinger, Ferenc Huszár</div>
Chess is a canonical example of a task that requires rigorous reasoning and long-term planning. Modern decision Transformers - trained similarly to LLMs - are able to learn competent gameplay, but it is unclear to what extent they truly capture the rules of chess. To investigate this, we train a 270M parameter chess Transformer and test it on out-of-distribution scenarios, designed to reveal failures of systematic generalization. Our analysis shows that Transformers exhibit compositional generalization, as evidenced by strong rule extrapolation: they adhere to fundamental syntactic rules of the game by consistently choosing valid moves even in situations very different from the training data. Moreover, they also generate high-quality moves for OOD puzzles. In a more challenging test, we evaluate the models on variants including Chess960 (Fischer Random Chess) - a variant of chess where starting positions of pieces are randomized. We found that while the model exhibits basic strategy adaptation, they are inferior to symbolic AI algorithms that perform explicit search, but gap is smaller when playing against users on Lichess. Moreover, the training dynamics revealed that the model initially learns to move only its own pieces, suggesting an emergent compositional understanding of the game.
<div><strong>Authors:</strong> Anna Mészáros, Patrik Reizinger, Ferenc Huszár</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper trains a 270M‑parameter decision transformer to play chess and evaluates it on out‑of‑distribution scenarios designed to test systematic generalization. Results show that the model reliably follows basic chess rules and solves OOD puzzles, demonstrating compositional generalization, though it lags behind symbolic search especially on Chess960 variants. Training dynamics reveal an emergent understanding where the model first learns to move only its own pieces.", "summary_cn": "本文训练了一个 270M 参数的决策 Transformer（decision transformer）用于象棋，并在旨在检测系统性泛化的分布外（out‑of‑distribution）情景下进行评估。结果表明模型始终遵守象棋基本规则并能解出分布外谜题，展示了组合式泛化能力，但在 Chess960 等变体上仍不及显式搜索的符号 AI。训练动态显示模型最初学会仅移动自身棋子，暗示出现了对游戏的组合理解。", "keywords": "chess transformer, out-of-distribution, compositional generalization, systematic generalization, decision transformer, Chess960, rule extrapolation", "scoring": {"interpretability": 5, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Anna Mészáros", "Patrik Reizinger", "Ferenc Huszár"]}
]]></acme>

<pubDate>2025-10-23T17:51:28+00:00</pubDate>
</item>
<item>
<title>MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs</title>
<link>https://papers.cool/arxiv/2510.20762</link>
<guid>https://papers.cool/arxiv/2510.20762</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MEIcoder is a biologically informed deep‑learning method that decodes visual stimuli from neural population activity by incorporating neuron‑specific most exciting inputs (MEIs), a structural similarity index loss, and adversarial training. It achieves state‑of‑the‑art reconstruction quality on primary visual cortex (V1) data, particularly when the dataset is small or contains few recorded neurons, and introduces a large benchmark to support future research.<br /><strong>Summary (CN):</strong> MEIcoder 是一种结合了神经元特定的最激发输入（MEIs）、结构相似性指数（SSIM）损失以及对抗训练的生物学驱动深度学习解码方法。该方法在原始视觉皮层 (V1) 神经活动上实现了最先进的图像重建性能，尤其在数据量有限或记录神经元数量较少的情况下表现突出，并提供了一个包含超过 160,000 条样本的统一基准以促进后续研究。<br /><strong>Keywords:</strong> neural decoding, most exciting inputs, visual cortex, brain-machine interface, adversarial training, SSIM loss, MEIcoder, population activity<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jan Sobotka, Luca Baroni, Ján Antolík</div>
Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.
<div><strong>Authors:</strong> Jan Sobotka, Luca Baroni, Ján Antolík</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MEIcoder is a biologically informed deep‑learning method that decodes visual stimuli from neural population activity by incorporating neuron‑specific most exciting inputs (MEIs), a structural similarity index loss, and adversarial training. It achieves state‑of‑the‑art reconstruction quality on primary visual cortex (V1) data, particularly when the dataset is small or contains few recorded neurons, and introduces a large benchmark to support future research.", "summary_cn": "MEIcoder 是一种结合了神经元特定的最激发输入（MEIs）、结构相似性指数（SSIM）损失以及对抗训练的生物学驱动深度学习解码方法。该方法在原始视觉皮层 (V1) 神经活动上实现了最先进的图像重建性能，尤其在数据量有限或记录神经元数量较少的情况下表现突出，并提供了一个包含超过 160,000 条样本的统一基准以促进后续研究。", "keywords": "neural decoding, most exciting inputs, visual cortex, brain-machine interface, adversarial training, SSIM loss, MEIcoder, population activity", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jan Sobotka", "Luca Baroni", "Ján Antolík"]}
]]></acme>

<pubDate>2025-10-23T17:35:34+00:00</pubDate>
</item>
<item>
<title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title>
<link>https://papers.cool/arxiv/2510.20736</link>
<guid>https://papers.cool/arxiv/2510.20736</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multimodal learning framework that leverages a Dirichlet Process (DP) mixture model to dynamically allocate weights to the most prominent intra‑modal features while still aligning cross‑modal distributions. By modeling each modality as a mixture of multivariate Gaussians and using the DP's richer‑gets‑richer property, the approach achieves a balance between preserving expressive modality‑specific representations and facilitating effective fusion, demonstrating superior performance across several benchmark datasets.<br /><strong>Summary (CN):</strong> 本文提出一种多模态学习框架，利用 Dirichlet 过程（DP）混合模型动态为最突出（prominent）的模态内部特征分配更大权重，同时实现跨模态对齐。该方法将每个模态建模为多元高斯混合分布，并借助 DP 的 richer‑gets‑richer 特性，在保持模态表达性的同时促进特征融合，在多个基准数据集上表现优于现有方法。<br /><strong>Keywords:</strong> multimodal fusion, Dirichlet process, mixture model, cross-modal alignment, representation learning, variational inference<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu</div>
Developing effective multimodal fusion approaches has become increasingly essential in many real-world scenarios, such as health care and finance. The key challenge is how to preserve the feature expressiveness in each modality while learning cross-modal interactions. Previous approaches primarily focus on the cross-modal alignment, while over-emphasis on the alignment of marginal distributions of modalities may impose excess regularization and obstruct meaningful representations within each modality. The Dirichlet process (DP) mixture model is a powerful Bayesian non-parametric method that can amplify the most prominent features by its richer-gets-richer property, which allocates increasing weights to them. Inspired by this unique characteristic of DP, we propose a new DP-driven multimodal learning framework that automatically achieves an optimal balance between prominent intra-modal representation learning and cross-modal alignment. Specifically, we assume that each modality follows a mixture of multivariate Gaussian distributions and further adopt DP to calculate the mixture weights for all the components. This paradigm allows DP to dynamically allocate the contributions of features and select the most prominent ones, leveraging its richer-gets-richer property, thus facilitating multimodal feature fusion. Extensive experiments on several multimodal datasets demonstrate the superior performance of our model over other competitors. Ablation analysis further validates the effectiveness of DP in aligning modality distributions and its robustness to changes in key hyperparameters. Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git
<div><strong>Authors:</strong> Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multimodal learning framework that leverages a Dirichlet Process (DP) mixture model to dynamically allocate weights to the most prominent intra‑modal features while still aligning cross‑modal distributions. By modeling each modality as a mixture of multivariate Gaussians and using the DP's richer‑gets‑richer property, the approach achieves a balance between preserving expressive modality‑specific representations and facilitating effective fusion, demonstrating superior performance across several benchmark datasets.", "summary_cn": "本文提出一种多模态学习框架，利用 Dirichlet 过程（DP）混合模型动态为最突出（prominent）的模态内部特征分配更大权重，同时实现跨模态对齐。该方法将每个模态建模为多元高斯混合分布，并借助 DP 的 richer‑gets‑richer 特性，在保持模态表达性的同时促进特征融合，在多个基准数据集上表现优于现有方法。", "keywords": "multimodal fusion, Dirichlet process, mixture model, cross-modal alignment, representation learning, variational inference", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tsai Hor Chan", "Feng Wu", "Yihang Chen", "Guosheng Yin", "Lequan Yu"]}
]]></acme>

<pubDate>2025-10-23T16:53:24+00:00</pubDate>
</item>
<item>
<title>Thought Communication in Multiagent Collaboration</title>
<link>https://papers.cool/arxiv/2510.20733</link>
<guid>https://papers.cool/arxiv/2510.20733</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces "thought communication", a paradigm where multi‑agent systems exchange latent internal representations directly rather than natural language, formalized as a non‑parametric latent variable model with provable identifiability of shared and private thoughts. Guided by this theory, the authors present a framework that extracts these latent thoughts and assigns them to agents, demonstrating on synthetic and real‑world benchmarks that such communication improves collaborative performance. The work highlights the potential of leveraging hidden generative processes beyond surface‑level observation.<br /><strong>Summary (CN):</strong> 本文提出“thought communication”（思维通信）范式，使多智能体系统直接交换潜在的内部表征（latent thoughts），而非自然语言，并将其形式化为具有可辨识性的非参数潜变量模型，可识别共享与私有思维。基于该理论，作者构建了提取并分配这些潜在思维的框架，在合成和真实基准上验证了该方法提升协作效果。该工作强调利用隐藏生成过程超表层观测的潜力。<br /><strong>Keywords:</strong> thought communication, latent variable model, multi-agent collaboration, latent representations, identifiability, emergent communication, LLM, internal state extraction<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</div>
Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.
<div><strong>Authors:</strong> Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces \"thought communication\", a paradigm where multi‑agent systems exchange latent internal representations directly rather than natural language, formalized as a non‑parametric latent variable model with provable identifiability of shared and private thoughts. Guided by this theory, the authors present a framework that extracts these latent thoughts and assigns them to agents, demonstrating on synthetic and real‑world benchmarks that such communication improves collaborative performance. The work highlights the potential of leveraging hidden generative processes beyond surface‑level observation.", "summary_cn": "本文提出“thought communication”（思维通信）范式，使多智能体系统直接交换潜在的内部表征（latent thoughts），而非自然语言，并将其形式化为具有可辨识性的非参数潜变量模型，可识别共享与私有思维。基于该理论，作者构建了提取并分配这些潜在思维的框架，在合成和真实基准上验证了该方法提升协作效果。该工作强调利用隐藏生成过程超表层观测的潜力。", "keywords": "thought communication, latent variable model, multi-agent collaboration, latent representations, identifiability, emergent communication, LLM, internal state extraction", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yujia Zheng", "Zhuokai Zhao", "Zijian Li", "Yaqi Xie", "Mingze Gao", "Lizhu Zhang", "Kun Zhang"]}
]]></acme>

<pubDate>2025-10-23T16:48:02+00:00</pubDate>
</item>
<item>
<title>No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes</title>
<link>https://papers.cool/arxiv/2510.20725</link>
<guid>https://papers.cool/arxiv/2510.20725</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper establishes no-regret guarantees for Thompson Sampling in finite‑horizon episodic Markov Decision Processes by employing joint Gaussian Process priors over rewards and transitions proving a regret bound of \(\tilde{O}(\sqrt{KH\Gamma(KH)})\) across K episodes of horizon H. It tackles challenges such as the non‑Gaussian nature of value functions and recursive Bellman updates, extending tools like the elliptical potential lemma to multi‑output settings. This work advances theoretical understanding of Bayesian RL methods under structural Gaussian assumptions.<br /><strong>Summary (CN):</strong> 本文针对具有高斯过程（GP）先验的奖励和转移函数，在有限时域的周期性马尔可夫决策过程（MDP）中，对 Thompson 采样（TS）提供了无后悔（no‑regret）保证，证明了在 K 轮、每轮时长 H 的情形下，其后悔上界为 \(\tilde{O}(\sqrt{KH\Gamma(KH)})\)。研究克服了价值函数非高斯分布和贝尔曼递归结构等难题，将椭圆势能引理等经典工具推广到多输出 GP 场景。该工作深化了在结构化高斯假设下贝叶斯强化学习的理论理解。<br /><strong>Keywords:</strong> Thompson sampling, Gaussian processes, episodic reinforcement learning, regret analysis, finite-horizon MDP, Bayesian RL, elliptical potential lemma, multi-output GP, no-regret guarantees<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jasmine Bayrooti, Sattar Vakili, Amanda Prorok, Carl Henrik Ek</div>
Thompson sampling (TS) is a powerful and widely used strategy for sequential decision-making, with applications ranging from Bayesian optimization to reinforcement learning (RL). Despite its success, the theoretical foundations of TS remain limited, particularly in settings with complex temporal structure such as RL. We address this gap by establishing no-regret guarantees for TS using models with Gaussian marginal distributions. Specifically, we consider TS in episodic RL with joint Gaussian process (GP) priors over rewards and transitions. We prove a regret bound of $\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis addresses several challenges, including the non-Gaussian nature of value functions and the recursive structure of Bellman updates, and extends classical tools such as the elliptical potential lemma to multi-output settings. This work advances the understanding of TS in RL and highlights how structural assumptions and model uncertainty shape its performance in finite-horizon Markov Decision Processes.
<div><strong>Authors:</strong> Jasmine Bayrooti, Sattar Vakili, Amanda Prorok, Carl Henrik Ek</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper establishes no-regret guarantees for Thompson Sampling in finite‑horizon episodic Markov Decision Processes by employing joint Gaussian Process priors over rewards and transitions proving a regret bound of \\(\\tilde{O}(\\sqrt{KH\\Gamma(KH)})\\) across K episodes of horizon H. It tackles challenges such as the non‑Gaussian nature of value functions and recursive Bellman updates, extending tools like the elliptical potential lemma to multi‑output settings. This work advances theoretical understanding of Bayesian RL methods under structural Gaussian assumptions.", "summary_cn": "本文针对具有高斯过程（GP）先验的奖励和转移函数，在有限时域的周期性马尔可夫决策过程（MDP）中，对 Thompson 采样（TS）提供了无后悔（no‑regret）保证，证明了在 K 轮、每轮时长 H 的情形下，其后悔上界为 \\(\\tilde{O}(\\sqrt{KH\\Gamma(KH)})\\)。研究克服了价值函数非高斯分布和贝尔曼递归结构等难题，将椭圆势能引理等经典工具推广到多输出 GP 场景。该工作深化了在结构化高斯假设下贝叶斯强化学习的理论理解。", "keywords": "Thompson sampling, Gaussian processes, episodic reinforcement learning, regret analysis, finite-horizon MDP, Bayesian RL, elliptical potential lemma, multi-output GP, no-regret guarantees", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jasmine Bayrooti", "Sattar Vakili", "Amanda Prorok", "Carl Henrik Ek"]}
]]></acme>

<pubDate>2025-10-23T16:44:31+00:00</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title>
<link>https://papers.cool/arxiv/2510.20718</link>
<guid>https://papers.cool/arxiv/2510.20718</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two unsupervised approaches for anomaly prediction in multivariate semiconductor process time series: a univariate N-BEATS forecaster assuming variable independence, and a graph neural network (GNN) forecaster that captures inter-variable dependencies. Both models are trained on presumed normal data, forecast future values, and flag deviations beyond a threshold as anomalies, achieving stable prediction up to 50 steps with the GNN outperforming N-BEATS in accuracy and efficiency. The results demonstrate the GNN's suitability for online fault prediction in manufacturing environments.<br /><strong>Summary (CN):</strong> 本文提出了两种用于半导体工艺多变量时间序列异常的无监督方法：一种基于 N-BEATS 的单变量预测模型，假设变量之间相互独立；另一种基于图神经网络（GNN）的预测模型，捕获变量间的依赖关系。两种模型均在假定无异常的正常数据上进行训练，预测未来数值并将超过阈值的偏差标记为异常，实验显示 GNN 在预测精度和计算效率上优于 N-BEATS，能够在长达 50 步的预测范围内保持稳定，适用于制造环境在线故障预测。<br /><strong>Keywords:</strong> unsupervised anomaly prediction, multivariate time series, semiconductor manufacturing, N-BEATS, graph neural network, forecasting, fault detection, online monitoring<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</div>
Semiconductor manufacturing is an extremely complex and precision-driven process, characterized by thousands of interdependent parameters collected across diverse tools and process steps. Multi-variate time-series analysis has emerged as a critical field for real-time monitoring and fault detection in such environments. However, anomaly prediction in semiconductor fabrication presents several critical challenges, including high dimensionality of sensor data and severe class imbalance due to the rarity of true faults. Furthermore, the complex interdependencies between variables complicate both anomaly prediction and root-cause-analysis. This paper proposes two novel approaches to advance the field from anomaly detection to anomaly prediction, an essential step toward enabling real-time process correction and proactive fault prevention. The proposed anomaly prediction framework contains two main stages: (a) training a forecasting model on a dataset assumed to contain no anomalies, and (b) performing forecast on unseen time series data. The forecast is compared with the forecast of the trained signal. Deviations beyond a predefined threshold are flagged as anomalies. The two approaches differ in the forecasting model employed. The first assumes independence between variables by utilizing the N-BEATS model for univariate time series forecasting. The second lifts this assumption by utilizing a Graph Neural Network (GNN) to capture inter-variable relationships. Both models demonstrate strong forecasting performance up to a horizon of 20 time points and maintain stable anomaly prediction up to 50 time points. The GNN consistently outperforms the N-BEATS model while requiring significantly fewer trainable parameters and lower computational cost. These results position the GNN as promising solution for online anomaly forecasting to be deployed in manufacturing environments.
<div><strong>Authors:</strong> Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two unsupervised approaches for anomaly prediction in multivariate semiconductor process time series: a univariate N-BEATS forecaster assuming variable independence, and a graph neural network (GNN) forecaster that captures inter-variable dependencies. Both models are trained on presumed normal data, forecast future values, and flag deviations beyond a threshold as anomalies, achieving stable prediction up to 50 steps with the GNN outperforming N-BEATS in accuracy and efficiency. The results demonstrate the GNN's suitability for online fault prediction in manufacturing environments.", "summary_cn": "本文提出了两种用于半导体工艺多变量时间序列异常的无监督方法：一种基于 N-BEATS 的单变量预测模型，假设变量之间相互独立；另一种基于图神经网络（GNN）的预测模型，捕获变量间的依赖关系。两种模型均在假定无异常的正常数据上进行训练，预测未来数值并将超过阈值的偏差标记为异常，实验显示 GNN 在预测精度和计算效率上优于 N-BEATS，能够在长达 50 步的预测范围内保持稳定，适用于制造环境在线故障预测。", "keywords": "unsupervised anomaly prediction, multivariate time series, semiconductor manufacturing, N-BEATS, graph neural network, forecasting, fault detection, online monitoring", "scoring": {"interpretability": 3, "understanding": 4, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Daniel Sorensen", "Bappaditya Dey", "Minjin Hwang", "Sandip Halder"]}
]]></acme>

<pubDate>2025-10-23T16:33:52+00:00</pubDate>
</item>
<item>
<title>Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool</title>
<link>https://papers.cool/arxiv/2510.20714</link>
<guid>https://papers.cool/arxiv/2510.20714</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a data‑driven approach that augments the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional electronic health record variables using constrained score optimization (CSO) models to retain interpretability while improving predictive performance. On a retrospective cohort of over 54,000 admissions, the CSO model achieved an AUC‑ROC of 0.91, surpassing the original JHFRAT, and performed comparably to a black‑box XGBoost model while being more robust to labeling variations. The study demonstrates how knowledge‑based, interpretable models can enhance inpatient fall risk assessment and support patient safety initiatives.<br /><strong>Summary (CN):</strong> 本文提出一种数据驱动的方法，使用约束得分优化（CSO）模型在保持可解释性的前提下，将约翰斯·霍普金斯跌倒风险评估工具（JHFRAT）与额外的电子健康记录（EHR）变量结合，以提升预测性能。基于 54,209 例住院数据的回顾性分析显示，CSO 模型的 AUC‑ROC 为 0.91，优于原 JHFRAT（0.86），且在稳健性方面与黑箱 XGBoost 模型相当。该研究展示了基于知识的可解释模型如何改进住院倒风险评估并支持患者安全。<br /><strong>Keywords:</strong> fall risk prediction, electronic health records, constrained score optimization, JHFRAT, interpretable model, clinical decision support, patient safety, logistic regression, XGBoost<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</div>
In this study we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models on JHFRAT assessment data and additional electronic health record (EHR) variables. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labelling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.
<div><strong>Authors:</strong> Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a data‑driven approach that augments the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional electronic health record variables using constrained score optimization (CSO) models to retain interpretability while improving predictive performance. On a retrospective cohort of over 54,000 admissions, the CSO model achieved an AUC‑ROC of 0.91, surpassing the original JHFRAT, and performed comparably to a black‑box XGBoost model while being more robust to labeling variations. The study demonstrates how knowledge‑based, interpretable models can enhance inpatient fall risk assessment and support patient safety initiatives.", "summary_cn": "本文提出一种数据驱动的方法，使用约束得分优化（CSO）模型在保持可解释性的前提下，将约翰斯·霍普金斯跌倒风险评估工具（JHFRAT）与额外的电子健康记录（EHR）变量结合，以提升预测性能。基于 54,209 例住院数据的回顾性分析显示，CSO 模型的 AUC‑ROC 为 0.91，优于原 JHFRAT（0.86），且在稳健性方面与黑箱 XGBoost 模型相当。该研究展示了基于知识的可解释模型如何改进住院倒风险评估并支持患者安全。", "keywords": "fall risk prediction, electronic health records, constrained score optimization, JHFRAT, interpretable model, clinical decision support, patient safety, logistic regression, XGBoost", "scoring": {"interpretability": 7, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Fardin Ganjkhanloo", "Emmett Springer", "Erik H. Hoyer", "Daniel L. Young", "Kimia Ghobadi"]}
]]></acme>

<pubDate>2025-10-23T16:31:09+00:00</pubDate>
</item>
<item>
<title>Separating the what and how of compositional computation to enable reuse and continual learning</title>
<link>https://papers.cool/arxiv/2510.20709</link>
<guid>https://papers.cool/arxiv/2510.20709</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a two‑system framework that separates a "what" module, which infers the compositional structure of a task using an online probabilistic generative model of discrete task epochs, from a "how" module, an RNN whose low‑rank components are assembled according to the inferred context. This design enables continual learning of sequential tasks without catastrophic forgetting, supports forward and backward transfer, and allows fast compositional generalisation to novel tasks. Experiments on a suite of cognitive tasks demonstrate competitive performance and efficient reuse of learned computational primitives.<br /><strong>Summary (CN):</strong> 本文提出了一种两系统框架，将“what”模块（利用在线概率生成模型推断任务的离散时期结构）与“How”模块（根据推断上下文组装低秩 RNN 组件）分离，实现对任务的组合式理解和实现。该设计在顺序学习任务时避免灾难性遗忘，支持前向和后向迁移，并能快速组合推广到未见任务。实验在一套认知任务上展示了竞争性的性能和高效的计算原语复用。<br /><strong>Keywords:</strong> compositionality, continual learning, recurrent neural network, low-rank RNN, probabilistic generative model, task epochs, modular learning, transfer learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Haozhe Shan, Sun Minni, Lea Duncker</div>
The ability to continually learn, retain and deploy skills to accomplish goals is a key feature of intelligent and efficient behavior. However, the neural mechanisms facilitating the continual learning and flexible (re-)composition of skills remain elusive. Here, we study continual learning and the compositional reuse of learned computations in recurrent neural network (RNN) models using a novel two-system approach: one system that infers what computation to perform, and one that implements how to perform it. We focus on a set of compositional cognitive tasks commonly studied in neuroscience. To construct the what system, we first show that a large family of tasks can be systematically described by a probabilistic generative model, where compositionality stems from a shared underlying vocabulary of discrete task epochs. The shared epoch structure makes these tasks inherently compositional. We first show that this compositionality can be systematically described by a probabilistic generative model. Furthermore, We develop an unsupervised online learning approach that can learn this model on a single-trial basis, building its vocabulary incrementally as it is exposed to new tasks, and inferring the latent epoch structure as a time-varying computational context within a trial. We implement the how system as an RNN whose low-rank components are composed according to the context inferred by the what system. Contextual inference facilitates the creation, learning, and reuse of low-rank RNN components as new tasks are introduced sequentially, enabling continual learning without catastrophic forgetting. Using an example task set, we demonstrate the efficacy and competitive performance of this two-system learning framework, its potential for forward and backward transfer, as well as fast compositional generalization to unseen tasks.
<div><strong>Authors:</strong> Haozhe Shan, Sun Minni, Lea Duncker</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a two‑system framework that separates a \"what\" module, which infers the compositional structure of a task using an online probabilistic generative model of discrete task epochs, from a \"how\" module, an RNN whose low‑rank components are assembled according to the inferred context. This design enables continual learning of sequential tasks without catastrophic forgetting, supports forward and backward transfer, and allows fast compositional generalisation to novel tasks. Experiments on a suite of cognitive tasks demonstrate competitive performance and efficient reuse of learned computational primitives.", "summary_cn": "本文提出了一种两系统框架，将“what”模块（利用在线概率生成模型推断任务的离散时期结构）与“How”模块（根据推断上下文组装低秩 RNN 组件）分离，实现对任务的组合式理解和实现。该设计在顺序学习任务时避免灾难性遗忘，支持前向和后向迁移，并能快速组合推广到未见任务。实验在一套认知任务上展示了竞争性的性能和高效的计算原语复用。", "keywords": "compositionality, continual learning, recurrent neural network, low-rank RNN, probabilistic generative model, task epochs, modular learning, transfer learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Haozhe Shan", "Sun Minni", "Lea Duncker"]}
]]></acme>

<pubDate>2025-10-23T16:24:40+00:00</pubDate>
</item>
<item>
<title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title>
<link>https://papers.cool/arxiv/2510.20683</link>
<guid>https://papers.cool/arxiv/2510.20683</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Spikachu, a neural decoding framework based on spiking neural networks that processes binned spikes into a shared latent space and decodes behavior in a causal, energy‑efficient manner. Evaluated on 113 sessions from six non‑human primates, Spikachu outperforms causal baselines while using orders of magnitude less energy and benefits from scaling across sessions for few‑shot transfer to new subjects and tasks.<br /><strong>Summary (CN):</strong> 本文提出 Spikachu——一种基于脉冲神经网络（SNN）的神经解码框架，通过将分箱脉冲投射到共享潜在空间并进行因果、低能耗的特征提取和行为解码。该方法在来自六只灵长类动物的 113 场实验上表现优于因果基线，同时能耗降低数十倍，并通过跨会话和跨受试者的规模化训练实现对新会话、受试者和任务的少样本迁移。<br /><strong>Keywords:</strong> spiking neural networks, neural decoding, brain-computer interface, energy efficiency, causal models, few-shot transfer, online learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</div>
Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy.
<div><strong>Authors:</strong> Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Spikachu, a neural decoding framework based on spiking neural networks that processes binned spikes into a shared latent space and decodes behavior in a causal, energy‑efficient manner. Evaluated on 113 sessions from six non‑human primates, Spikachu outperforms causal baselines while using orders of magnitude less energy and benefits from scaling across sessions for few‑shot transfer to new subjects and tasks.", "summary_cn": "本文提出 Spikachu——一种基于脉冲神经网络（SNN）的神经解码框架，通过将分箱脉冲投射到共享潜在空间并进行因果、低能耗的特征提取和行为解码。该方法在来自六只灵长类动物的 113 场实验上表现优于因果基线，同时能耗降低数十倍，并通过跨会话和跨受试者的规模化训练实现对新会话、受试者和任务的少样本迁移。", "keywords": "spiking neural networks, neural decoding, brain-computer interface, energy efficiency, causal models, few-shot transfer, online learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Georgios Mentzelopoulos", "Ioannis Asmanis", "Konrad P. Kording", "Eva L. Dyer", "Kostas Daniilidis", "Flavia Vitale"]}
]]></acme>

<pubDate>2025-10-23T15:55:45+00:00</pubDate>
</item>
<item>
<title>GRACE: GRaph-based Addiction Care prEdiction</title>
<link>https://papers.cool/arxiv/2510.20671</link>
<guid>https://papers.cool/arxiv/2510.20671</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GRACE, a graph neural network framework that predicts the appropriate locus of care for addiction patients by modeling patient data as a meta-graph to address severe class imbalance. Extensive feature engineering and a new unbiased meta-graph construction improve minority‑class F1 scores by 11‑35% over existing baselines on real‑world datasets.<br /><strong>Summary (CN):</strong> 本文提出了 GRACE（GRaph-based Addiction Care prEdiction）框架，利用图神经网络（graph neural network）将成瘾患者数据建模为元图（meta-graph），以缓解数据中严重的类别不平衡问题。通过大量特征工程和新的无偏元图构建方法，在真实数据集上实现了少数类 F1 分数提升 11%‑35%，优于现有基线。<br /><strong>Keywords:</strong> graph neural network, addiction care prediction, meta-graph, class imbalance, healthcare AI, structured learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee</div>
Determining the appropriate locus of care for addiction patients is one of the most critical clinical decisions that affects patient treatment outcomes and effective use of resources. With a lack of sufficient specialized treatment resources, such as inpatient beds or staff, there is an unmet need to develop an automated framework for the same. Current decision-making approaches suffer from severe class imbalances in addiction datasets. To address this limitation, we propose a novel graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem. Further, we perform extensive feature engineering and propose a new approach of obtaining an unbiased meta-graph to train a GNN to overcome the class imbalance problem. Experimental results in real-world data show an improvement of 11-35% in terms of the F1 score of the minority class over competitive baselines. The codes and note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.
<div><strong>Authors:</strong> Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GRACE, a graph neural network framework that predicts the appropriate locus of care for addiction patients by modeling patient data as a meta-graph to address severe class imbalance. Extensive feature engineering and a new unbiased meta-graph construction improve minority‑class F1 scores by 11‑35% over existing baselines on real‑world datasets.", "summary_cn": "本文提出了 GRACE（GRaph-based Addiction Care prEdiction）框架，利用图神经网络（graph neural network）将成瘾患者数据建模为元图（meta-graph），以缓解数据中严重的类别不平衡问题。通过大量特征工程和新的无偏元图构建方法，在真实数据集上实现了少数类 F1 分数提升 11%‑35%，优于现有基线。", "keywords": "graph neural network, addiction care prediction, meta-graph, class imbalance, healthcare AI, structured learning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Subham Kumar", "Prakrithi Shivaprakash", "Koustav Rudra", "Lekhansh Shukla", "Animesh Mukherjee"]}
]]></acme>

<pubDate>2025-10-23T15:48:01+00:00</pubDate>
</item>
<item>
<title>From Masks to Worlds: A Hitchhiker's Guide to World Models</title>
<link>https://papers.cool/arxiv/2510.20668</link>
<guid>https://papers.cool/arxiv/2510.20668</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a directed overview of the development of world models, tracing a path from early multimodal masked models, through unified generative architectures, to interactive generative systems and memory‑augmented models that maintain consistent worlds over time. It emphasizes the core components—generative heart, perception‑action loop, and memory system—as the most promising route toward constructing true world models, rather than providing a broad survey of every related work.<br /><strong>Summary (CN):</strong> 本文提供了一篇聚焦于世界模型构建的导览，按照从早期多模态掩码模型到统一生成架构，再到闭环交互生成模型以及能够随时间保持一致世界的记忆增强系统的顺序展开。文章强调生成核心、感知‑行动循环与记忆系统这三大关键要素是实现真正世界模型的最有前景路径，而非对所有相关工作进行宽泛的综述。<br /><strong>Keywords:</strong> world models, generative models, interactive loop, memory-augmented systems, multimodal masked models, representation learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang</div>
This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models.
<div><strong>Authors:</strong> Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a directed overview of the development of world models, tracing a path from early multimodal masked models, through unified generative architectures, to interactive generative systems and memory‑augmented models that maintain consistent worlds over time. It emphasizes the core components—generative heart, perception‑action loop, and memory system—as the most promising route toward constructing true world models, rather than providing a broad survey of every related work.", "summary_cn": "本文提供了一篇聚焦于世界模型构建的导览，按照从早期多模态掩码模型到统一生成架构，再到闭环交互生成模型以及能够随时间保持一致世界的记忆增强系统的顺序展开。文章强调生成核心、感知‑行动循环与记忆系统这三大关键要素是实现真正世界模型的最有前景路径，而非对所有相关工作进行宽泛的综述。", "keywords": "world models, generative models, interactive loop, memory-augmented systems, multimodal masked models, representation learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinbin Bai", "Yu Lei", "Hecong Wu", "Yuchen Zhu", "Shufan Li", "Yi Xin", "Xiangtai Li", "Molei Tao", "Aditya Grover", "Ming-Hsuan Yang"]}
]]></acme>

<pubDate>2025-10-23T15:46:44+00:00</pubDate>
</item>
<item>
<title>Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts</title>
<link>https://papers.cool/arxiv/2510.20666</link>
<guid>https://papers.cool/arxiv/2510.20666</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a hybrid Bayesian mixture-of-experts framework that combines a physical path‑loss model with a CNN using log‑linear pooling to localize GNSS jammers in urban environments. The physical expert enforces consistency while the CNN lever building‑height maps to capture multipath effects, and Bayesian inference with Laplace approximation yields posterior uncertainty over jammer positions and the RSS field. Experiments on ray‑tracing data show improved localization accuracy and meaningful uncertainty estimates that concentrate in sensitive urban canyons.<br /><strong>Summary (CN):</strong> 本文提出一种混合贝叶斯专家混合框架，将物理路径损失模型与卷积神经网络（CNN）通过对数线性池化融合，用于在城市环境中定位 GNSS 干扰源。物理专家保证一致性，CNN 利用建筑高度图捕获多路径效应，采用拉普拉斯近似的贝叶斯推断提供对干扰源位置和 RSS 场的后验不确定性。实验在射线追踪数据上显示，定位精度提升且不确定性在关键的城市谷处集中。<br /><strong>Keywords:</strong> GNSS jamming, jammer localization, Bayesian mixture of experts, convolutional neural network, path-loss model, log-linear pooling, uncertainty estimation, urban propagation, Laplace approximation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Mariona Jaramillo-Civill, Luis González-Gudiño, Tales Imbiriba, Pau Closas</div>
Global Navigation Satellite System (GNSS) signals are vulnerable to jamming, particularly in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed the received signal strength (RSS) field due to limited spatial context. We propose a hybrid Bayesian mixture-of-experts framework that fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling. The PL expert ensures physical consistency, while the CNN leverages building-height maps to capture urban propagation effects. Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field. Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points, while uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.
<div><strong>Authors:</strong> Mariona Jaramillo-Civill, Luis González-Gudiño, Tales Imbiriba, Pau Closas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a hybrid Bayesian mixture-of-experts framework that combines a physical path‑loss model with a CNN using log‑linear pooling to localize GNSS jammers in urban environments. The physical expert enforces consistency while the CNN lever building‑height maps to capture multipath effects, and Bayesian inference with Laplace approximation yields posterior uncertainty over jammer positions and the RSS field. Experiments on ray‑tracing data show improved localization accuracy and meaningful uncertainty estimates that concentrate in sensitive urban canyons.", "summary_cn": "本文提出一种混合贝叶斯专家混合框架，将物理路径损失模型与卷积神经网络（CNN）通过对数线性池化融合，用于在城市环境中定位 GNSS 干扰源。物理专家保证一致性，CNN 利用建筑高度图捕获多路径效应，采用拉普拉斯近似的贝叶斯推断提供对干扰源位置和 RSS 场的后验不确定性。实验在射线追踪数据上显示，定位精度提升且不确定性在关键的城市谷处集中。", "keywords": "GNSS jamming, jammer localization, Bayesian mixture of experts, convolutional neural network, path-loss model, log-linear pooling, uncertainty estimation, urban propagation, Laplace approximation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Mariona Jaramillo-Civill", "Luis González-Gudiño", "Tales Imbiriba", "Pau Closas"]}
]]></acme>

<pubDate>2025-10-23T15:45:45+00:00</pubDate>
</item>
<item>
<title>xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion</title>
<link>https://papers.cool/arxiv/2510.20651</link>
<guid>https://papers.cool/arxiv/2510.20651</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces xTime, a framework for forecasting extreme events in time series by employing hierarchical knowledge distillation from models trained on more common events and a mixture‑of‑experts mechanism that fuses predictions across rarity levels. Experiments on several datasets show substantial gains in extreme‑event accuracy, improving performance from 3% to up to 78%. The approach addresses data imbalance and leverages intermediate events to better anticipate rare, high‑impact occurrences.<br /><strong>Summary (CN):</strong> 本文提出 xTime 框架，通过层次化知识蒸馏将低稀有度事件模型的知识迁移至稀有事件模型，并使用专家混合（xture of Experts）机制在不同稀有度水平的专家模型之间动态选择与融合输出，以提升极端事件的预测精度。实验在多个数据集上显示，极端事件的预测准确率从 3% 提升至最高 78%。该方法针对数据不平衡问题，并利用中间事件信息更好地预测高影响的稀有事件。<br /><strong>Keywords:</strong> extreme event prediction, time series forecasting, knowledge distillation, mixture of experts, hierarchical distillation, rare event modeling, imbalanced data, healthcare forecasting, climate forecasting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Quan Li, Wenchao Yu, Suhang Wang, Minhua Lin, Lingwei Chen, Wei Cheng, Haifeng Chen</div>
Extreme events frequently occur in real-world time series and often carry significant practical implications. In domains such as climate and healthcare, these events, such as floods, heatwaves, or acute medical episodes, can lead to serious consequences. Accurate forecasting of such events is therefore of substantial importance. Most existing time series forecasting models are optimized for overall performance within the prediction window, but often struggle to accurately predict extreme events, such as high temperatures or heart rate spikes. The main challenges are data imbalance and the neglect of valuable information contained in intermediate events that precede extreme events. In this paper, we propose xTime, a novel framework for extreme event forecasting in time series. xTime leverages knowledge distillation to transfer information from models trained on lower-rarity events, thereby improving prediction performance on rarer ones. In addition, we introduce a mixture of experts (MoE) mechanism that dynamically selects and fuses outputs from expert models across different rarity levels, which further improves the forecasting performance for extreme events. Experiments on multiple datasets show that xTime achieves consistent improvements, with forecasting accuracy on extreme events improving from 3% to 78%.
<div><strong>Authors:</strong> Quan Li, Wenchao Yu, Suhang Wang, Minhua Lin, Lingwei Chen, Wei Cheng, Haifeng Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces xTime, a framework for forecasting extreme events in time series by employing hierarchical knowledge distillation from models trained on more common events and a mixture‑of‑experts mechanism that fuses predictions across rarity levels. Experiments on several datasets show substantial gains in extreme‑event accuracy, improving performance from 3% to up to 78%. The approach addresses data imbalance and leverages intermediate events to better anticipate rare, high‑impact occurrences.", "summary_cn": "本文提出 xTime 框架，通过层次化知识蒸馏将低稀有度事件模型的知识迁移至稀有事件模型，并使用专家混合（xture of Experts）机制在不同稀有度水平的专家模型之间动态选择与融合输出，以提升极端事件的预测精度。实验在多个数据集上显示，极端事件的预测准确率从 3% 提升至最高 78%。该方法针对数据不平衡问题，并利用中间事件信息更好地预测高影响的稀有事件。", "keywords": "extreme event prediction, time series forecasting, knowledge distillation, mixture of experts, hierarchical distillation, rare event modeling, imbalanced data, healthcare forecasting, climate forecasting", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Quan Li", "Wenchao Yu", "Suhang Wang", "Minhua Lin", "Lingwei Chen", "Wei Cheng", "Haifeng Chen"]}
]]></acme>

<pubDate>2025-10-23T15:24:45+00:00</pubDate>
</item>
<item>
<title>Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning</title>
<link>https://papers.cool/arxiv/2510.20644</link>
<guid>https://papers.cool/arxiv/2510.20644</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper derives a new tight lower bound on the Kullback-Leibler divergence expressed in terms of the Jensen-Shannon divergence, and shows that maximizing a JSD-based objective yields a guaranteed lower bound on mutual information for representation learning. Experiments demonstrate that this bound provides stable, low‑variance MI estimates and is useful within the Information Bottleneck framework.<br /><strong>Summary (CN):</strong> 本文推导出一个将 Jensen-Shannon 散度与 Kullback-Leibler 散度关联的新紧下界，并证明最大化基于 JSD 的目标可以获得互信息的保证下界，从而用于表征学习。实验表明，该下界在互信息估计中具有稳定、低方差的特性，并在信息瓶颈框架中展示了实用价值。<br /><strong>Keywords:</strong> Jensen-Shannon divergence, Kullback-Leibler divergence, mutual information, representation learning, variational lower bound, information bottleneck<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Reuben Dorent, Polina Golland, William Wells III</div>
Mutual Information (MI) is a fundamental measure of statistical dependence widely used in representation learning. While direct optimization of MI via its definition as a Kullback-Leibler divergence (KLD) is often intractable, many recent methods have instead maximized alternative dependence measures, most notably, the Jensen-Shannon divergence (JSD) between joint and product of marginal distributions via discriminative losses. However, the connection between these surrogate objectives and MI remains poorly understood. In this work, we bridge this gap by deriving a new, tight, and tractable lower bound on KLD as a function of JSD in the general case. By specializing this bound to joint and marginal distributions, we demonstrate that maximizing the JSD-based information increases a guaranteed lower bound on mutual information. Furthermore, we revisit the practical implementation of JSD-based objectives and observe that minimizing the cross-entropy loss of a binary classifier trained to distinguish joint from marginal pairs recovers a known variational lower bound on the JSD. Extensive experiments demonstrate that our lower bound is tight when applied to MI estimation. We compared our lower bound to state-of-the-art neural estimators of variational lower bound across a range of established reference scenarios. Our lower bound estimator consistently provides a stable, low-variance estimate of a tight lower bound on MI. We also demonstrate its practical usefulness in the context of the Information Bottleneck framework. Taken together, our results provide new theoretical justifications and strong empirical evidence for using discriminative learning in MI-based representation learning.
<div><strong>Authors:</strong> Reuben Dorent, Polina Golland, William Wells III</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper derives a new tight lower bound on the Kullback-Leibler divergence expressed in terms of the Jensen-Shannon divergence, and shows that maximizing a JSD-based objective yields a guaranteed lower bound on mutual information for representation learning. Experiments demonstrate that this bound provides stable, low‑variance MI estimates and is useful within the Information Bottleneck framework.", "summary_cn": "本文推导出一个将 Jensen-Shannon 散度与 Kullback-Leibler 散度关联的新紧下界，并证明最大化基于 JSD 的目标可以获得互信息的保证下界，从而用于表征学习。实验表明，该下界在互信息估计中具有稳定、低方差的特性，并在信息瓶颈框架中展示了实用价值。", "keywords": "Jensen-Shannon divergence, Kullback-Leibler divergence, mutual information, representation learning, variational lower bound, information bottleneck", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Reuben Dorent", "Polina Golland", "William Wells III"]}
]]></acme>

<pubDate>2025-10-23T15:18:12+00:00</pubDate>
</item>
<item>
<title>Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems</title>
<link>https://papers.cool/arxiv/2510.20640</link>
<guid>https://papers.cool/arxiv/2510.20640</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DiRecGNN, an attention‑anced graph neural network framework that recommends the optimal subset of attributes for automated monitoring of cloud services. By constructing a production‑scale heterogeneous monitor graph and applying multi‑head attention over heterogeneous neighbors and random‑walk sampled paths, the model captures long‑range dependencies and achieves a 43.1% improvement in MRR compared to baselines. Deployment feedback from Microsoft product teams shows the feature is perceived as useful (4.5/5).<br /><strong>Summary (CN):</strong> 本文提出 DiRecGNN，一种基于注意力机制的实体推荐框架，用于为云服务的自动监控挑选最优属性子集。通过构建生产级异构监控图，并在异构邻居及随机游走抽取的路径上使用多头注意力，模型捕获长程依赖，实现了 MRR 提升 43.1%。微软产品团队的使用反馈表明该功能被评为 4.5/5，具有较高的实用价值。<br /><strong>Keywords:</strong> entity recommendation, graph neural network, attention, heterogeneous graph, cloud monitoring, transformer, random walk, multi-head attention, MRR<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fiza Hussain, Anson Bastos, Anjaly Parayil, Ayush Choure, Chetan Bansal, Rujia Wang, Saravan Rajmohan</div>
In this paper, we present DiRecGNN, an attention-enhanced entity recommendation framework for monitoring cloud services at Microsoft. We provide insights on the usefulness of this feature as perceived by the cloud service owners and lessons learned from deployment. Specifically, we introduce the problem of recommending the optimal subset of attributes (dimensions) that should be tracked by an automated watchdog (monitor) for cloud services. To begin, we construct the monitor heterogeneous graph at production-scale. The interaction dynamics of these entities are often characterized by limited structural and engagement information, resulting in inferior performance of state-of-the-art approaches. Moreover, traditional methods fail to capture the dependencies between entities spanning a long range due to their homophilic nature. Therefore, we propose an attention-enhanced entity ranking model inspired by transformer architectures. Our model utilizes a multi-head attention mechanism to focus on heterogeneous neighbors and their attributes, and further attends to paths sampled using random walks to capture long-range dependencies. We also employ multi-faceted loss functions to optimize for relevant recommendations while respecting the inherent sparsity of the data. Empirical evaluations demonstrate significant improvements over existing methods, with our model achieving a 43.1% increase in MRR. Furthermore, product teams who consumed these features perceive the feature as useful and rated it 4.5 out of 5.
<div><strong>Authors:</strong> Fiza Hussain, Anson Bastos, Anjaly Parayil, Ayush Choure, Chetan Bansal, Rujia Wang, Saravan Rajmohan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DiRecGNN, an attention‑anced graph neural network framework that recommends the optimal subset of attributes for automated monitoring of cloud services. By constructing a production‑scale heterogeneous monitor graph and applying multi‑head attention over heterogeneous neighbors and random‑walk sampled paths, the model captures long‑range dependencies and achieves a 43.1% improvement in MRR compared to baselines. Deployment feedback from Microsoft product teams shows the feature is perceived as useful (4.5/5).", "summary_cn": "本文提出 DiRecGNN，一种基于注意力机制的实体推荐框架，用于为云服务的自动监控挑选最优属性子集。通过构建生产级异构监控图，并在异构邻居及随机游走抽取的路径上使用多头注意力，模型捕获长程依赖，实现了 MRR 提升 43.1%。微软产品团队的使用反馈表明该功能被评为 4.5/5，具有较高的实用价值。", "keywords": "entity recommendation, graph neural network, attention, heterogeneous graph, cloud monitoring, transformer, random walk, multi-head attention, MRR", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fiza Hussain", "Anson Bastos", "Anjaly Parayil", "Ayush Choure", "Chetan Bansal", "Rujia Wang", "Saravan Rajmohan"]}
]]></acme>

<pubDate>2025-10-23T15:14:09+00:00</pubDate>
</item>
<item>
<title>Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges</title>
<link>https://papers.cool/arxiv/2510.20637</link>
<guid>https://papers.cool/arxiv/2510.20637</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys the use of large language models (LLMs) and large multimodal models (LMMs) as core components for task-oriented autonomous communications in 6G networks, outlining design methodology, multimodal sensing integration, adaptive reconfiguration, and prompt/fine‑tuning strategies. It demonstrates the approach through three case studies—LMM‑based traffic control, LLM‑based robot scheduling, and LMM‑based environment‑aware channel estimation—and reports performance gains over conventional deep‑learning baselines, especially in dynamic and heterogeneous scenarios.<br /><strong>Summary (CN):</strong> 本文调研了大型语言模型（LLM）和大型多模态模型（LMM）在 6G 任务导向自主通信中的应用，阐述了设计方法论、多模态感知集成、自适应重构以及提示/微调策略。通过 LMM 驱动的交通控制、LLM 驱动的机器人调度和 LMM 驱动的环境感知信道估三个案例，展示了相较于传统深度学习方法在动态目标、输入参数变化和异构多模态条件下的显著性能提升。<br /><strong>Keywords:</strong> large language models, large multimodal models, autonomous communications, task-oriented, 6G, multimodal sensing, prompt tuning, wireless task optimization, channel estimation, robot scheduling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hyun Jong Yang, Hyunsoo Kim, Hyeonho Noh, Seungnyun Kim, Byonghyo Shim</div>
Large language models (LLMs) and large multimodal models (LMMs) have achieved unprecedented breakthrough, showcasing remarkable capabilities in natural language understanding, generation, and complex reasoning. This transformative potential has positioned them as key enablers for 6G autonomous communications among machines, vehicles, and humanoids. In this article, we provide an overview of task-oriented autonomous communications with LLMs/LMMs, focusing on multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation. From experimental results, we show that the proposed LLM/LMM-aided autonomous systems significantly outperform conventional and discriminative deep learning (DL) model-based techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions where conventional static optimization degrades.
<div><strong>Authors:</strong> Hyun Jong Yang, Hyunsoo Kim, Hyeonho Noh, Seungnyun Kim, Byonghyo Shim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys the use of large language models (LLMs) and large multimodal models (LMMs) as core components for task-oriented autonomous communications in 6G networks, outlining design methodology, multimodal sensing integration, adaptive reconfiguration, and prompt/fine‑tuning strategies. It demonstrates the approach through three case studies—LMM‑based traffic control, LLM‑based robot scheduling, and LMM‑based environment‑aware channel estimation—and reports performance gains over conventional deep‑learning baselines, especially in dynamic and heterogeneous scenarios.", "summary_cn": "本文调研了大型语言模型（LLM）和大型多模态模型（LMM）在 6G 任务导向自主通信中的应用，阐述了设计方法论、多模态感知集成、自适应重构以及提示/微调策略。通过 LMM 驱动的交通控制、LLM 驱动的机器人调度和 LMM 驱动的环境感知信道估三个案例，展示了相较于传统深度学习方法在动态目标、输入参数变化和异构多模态条件下的显著性能提升。", "keywords": "large language models, large multimodal models, autonomous communications, task-oriented, 6G, multimodal sensing, prompt tuning, wireless task optimization, channel estimation, robot scheduling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hyun Jong Yang", "Hyunsoo Kim", "Hyeonho Noh", "Seungnyun Kim", "Byonghyo Shim"]}
]]></acme>

<pubDate>2025-10-23T15:08:58+00:00</pubDate>
</item>
<item>
<title>Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach</title>
<link>https://papers.cool/arxiv/2510.20629</link>
<guid>https://papers.cool/arxiv/2510.20629</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Fairness-Aware Survival Modeling (FASM), a method that jointly mitigates intra-group and cross-group risk ranking biases in censored survival data, demonstrated on SEER breast cancer prognosis. Experiments show that FASM improves fairness across racial groups while maintaining predictive discrimination comparable to standard survival models, with stable fairness over a ten‑year horizon. The approach highlights the importance of integrating equity considerations into clinical survival prediction.<br /><strong>Summary (CN):</strong> 本文提出公平感知生存建模（FASM）方法，旨在同时缓解生存分析中同组内部和跨组风险排序的偏差，并在 SEER 乳腺癌预后数据上进行验证。实验显示，FASM 在提升不同种族群体公平性的同时，保持与传统生存模型相当的预测能力，并在十年预测期内实现公平性稳定。该工作强调在临床生存预测中融入公平性原则的重要性。<br /><strong>Keywords:</strong> fairness, survival analysis, censored data, healthcare equity, breast cancer prognosis, risk ranking, algorithmic bias<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Mingxuan Liu, Yilin Ning, Haoyuan Wang, Chuan Hong, Matthew Engelhard, Danielle S. Bitterman, William G. La Cava, Nan Liu</div>
As machine learning models become increasingly integrated into healthcare, structural inequities and social biases embedded in clinical data can be perpetuated or even amplified by data-driven models. In survival analysis, censoring and time dynamics can further add complexity to fair model development. Additionally, algorithmic fairness approaches often overlook disparities in cross-group rankings, e.g., high-risk Black patients may be ranked below lower-risk White patients who do not experience the event of mortality. Such misranking can reinforce biological essentialism and undermine equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed to mitigate algorithmic bias regarding both intra-group and cross-group risk rankings over time. Using breast cancer prognosis as a representative case and applying FASM to SEER breast cancer data, we show that FASM substantially improves fairness while preserving discrimination performance comparable to fairness-unaware survival models. Time-stratified evaluations show that FASM maintains stable fairness over a 10-year horizon, with the greatest improvements observed during the mid-term of follow-up. Our approach enables the development of survival models that prioritize both accuracy and equity in clinical decision-making, advancing fairness as a core principle in clinical care.
<div><strong>Authors:</strong> Mingxuan Liu, Yilin Ning, Haoyuan Wang, Chuan Hong, Matthew Engelhard, Danielle S. Bitterman, William G. La Cava, Nan Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Fairness-Aware Survival Modeling (FASM), a method that jointly mitigates intra-group and cross-group risk ranking biases in censored survival data, demonstrated on SEER breast cancer prognosis. Experiments show that FASM improves fairness across racial groups while maintaining predictive discrimination comparable to standard survival models, with stable fairness over a ten‑year horizon. The approach highlights the importance of integrating equity considerations into clinical survival prediction.", "summary_cn": "本文提出公平感知生存建模（FASM）方法，旨在同时缓解生存分析中同组内部和跨组风险排序的偏差，并在 SEER 乳腺癌预后数据上进行验证。实验显示，FASM 在提升不同种族群体公平性的同时，保持与传统生存模型相当的预测能力，并在十年预测期内实现公平性稳定。该工作强调在临床生存预测中融入公平性原则的重要性。", "keywords": "fairness, survival analysis, censored data, healthcare equity, breast cancer prognosis, risk ranking, algorithmic bias", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Mingxuan Liu", "Yilin Ning", "Haoyuan Wang", "Chuan Hong", "Matthew Engelhard", "Danielle S. Bitterman", "William G. La Cava", "Nan Liu"]}
]]></acme>

<pubDate>2025-10-23T15:03:27+00:00</pubDate>
</item>
<item>
<title>H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition</title>
<link>https://papers.cool/arxiv/2510.20627</link>
<guid>https://papers.cool/arxiv/2510.20627</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> We propose H‑SPLID, an HSIC‑based algorithm that explicitly separates salient and non‑salient features into distinct latent subspaces, encouraging low‑dimensional task‑relevant representations. We prove that the expected prediction deviation under inputations is bounded by the dimension of the salient subspace and the HSIC between inputs and representations, linking robustness to latent compression. Experiments on image classification show that models trained with H‑SPLID rely mainly on salient components, demonstrating reduced sensitivity to background perturbations.<br /><strong>Summary (CN):</strong> 我们提出 H‑SPLID，一种基于 HSIC 的算法，通过将显著特征和非显著特征显式分解到不同的潜在子空间中，促进低维任务相关特征的学习。我们证明，在输入扰动下的期望预测偏可以由显著子空间的维度和输入与表征之间的 HSIC 上界，从而建立了鲁棒性与潜在表征压缩之间的联系。对图像分类任务的实验表明，使用 H‑SPLID 训练的模型主要依赖显著输入成分，对背景扰动的敏感性显著降低。<br /><strong>Keywords:</strong> saliency decomposition, HSIC, latent representation, robustness, feature learning, dimensionality reduction, image classification, perturbation resistance<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Lukas Miklautz, Chengzhi Shi, Andrii Shkabrii, Theodoros Thirimachos Davarakis, Prudence Lam, Claudia Plant, Jennifer Dy, Stratis Ioannidis</div>
We introduce H-SPLID, a novel algorithm for learning salient feature representations through the explicit decomposition of salient and non-salient features into separate spaces. We show that H-SPLID promotes learning low-dimensional, task-relevant features. We prove that the expected prediction deviation under input perturbations is upper-bounded by the dimension of the salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between inputs and representations. This establishes a link between robustness and latent representation compression in terms of the dimensionality and information preserved. Empirical evaluations on image classification tasks show that models trained with H-SPLID primarily rely on salient input components, as indicated by reduced sensitivity to perturbations affecting non-salient features, such as image backgrounds. Our code is available at https://github.com/neu-spiral/H-SPLID.
<div><strong>Authors:</strong> Lukas Miklautz, Chengzhi Shi, Andrii Shkabrii, Theodoros Thirimachos Davarakis, Prudence Lam, Claudia Plant, Jennifer Dy, Stratis Ioannidis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "We propose H‑SPLID, an HSIC‑based algorithm that explicitly separates salient and non‑salient features into distinct latent subspaces, encouraging low‑dimensional task‑relevant representations. We prove that the expected prediction deviation under inputations is bounded by the dimension of the salient subspace and the HSIC between inputs and representations, linking robustness to latent compression. Experiments on image classification show that models trained with H‑SPLID rely mainly on salient components, demonstrating reduced sensitivity to background perturbations.", "summary_cn": "我们提出 H‑SPLID，一种基于 HSIC 的算法，通过将显著特征和非显著特征显式分解到不同的潜在子空间中，促进低维任务相关特征的学习。我们证明，在输入扰动下的期望预测偏可以由显著子空间的维度和输入与表征之间的 HSIC 上界，从而建立了鲁棒性与潜在表征压缩之间的联系。对图像分类任务的实验表明，使用 H‑SPLID 训练的模型主要依赖显著输入成分，对背景扰动的敏感性显著降低。", "keywords": "saliency decomposition, HSIC, latent representation, robustness, feature learning, dimensionality reduction, image classification, perturbation resistance", "scoring": {"interpretability": 4, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Lukas Miklautz", "Chengzhi Shi", "Andrii Shkabrii", "Theodoros Thirimachos Davarakis", "Prudence Lam", "Claudia Plant", "Jennifer Dy", "Stratis Ioannidis"]}
]]></acme>

<pubDate>2025-10-23T15:02:07+00:00</pubDate>
</item>
<item>
<title>On Optimal Hyperparameters for Differentially Private Deep Transfer Learning</title>
<link>https://papers.cool/arxiv/2510.20616</link>
<guid>https://papers.cool/arxiv/2510.20616</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies differential privacy for fine‑tuning pretrained models and investigates how to select the clipping bound C and batch size B under a fixed compute budget. It shows that theoretical guidance (smaller C for privacy) conflicts with empirical results (larger C works better) due to changes in gradient distributions, and that common batch‑size heuristics fail, with cumulative DP noise better predicting performance. The authors also demonstrate that using a single (C,B) across tasks can be suboptimal, especially when moving between tight/loose privacy regimes or between abundant/limited compute.<br /><strong>Summary (CN):</strong> 本文研究了在微调预训练模型时的差分隐私（DP）设置，重点探讨在固定计算预算下如何选择裁剪上限 C 和批量大小 B。研究发现理论上更强隐私应使用更小的 C，但实际实验表明在强隐私条件下更大的 C 表现更好，这源于梯度分布的变化；此外，常用的批量大小调参启发式方法失效，累积的 DP 噪声能够更好地解释批量大小对性能的影响。作者还指出，在不同任务之间使用统一的 (C,B) 设置会导致性能下降，尤其是在隐私约束松紧或计算资源充裕与受限之间切换时，这可以通过将裁剪视为梯度重新加权并分析累计噪声来解释。<br /><strong>Keywords:</strong> differential privacy, transfer learning, hyperparameter tuning, clipping bound, batch size, gradient clipping, DP noise, privacy-utility tradeoff<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela</div>
Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained model on private data, is the current state-of-the-art approach for training large models under privacy constraints. We focus on two key hyperparameters in this setting: the clipping bound $C$ and batch size $B$. We show a clear mismatch between the current theoretical understanding of how to choose an optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes (larger $C$ performs better under strong privacy), caused by changes in the gradient distributions. Assuming a limited compute budget (fixed epochs), we demonstrate that the existing heuristics for tuning $B$ do not work, while cumulative DP noise better explains whether smaller or larger batches perform better. We also highlight how the common practice of using a single $(C,B)$ setting across tasks can lead to suboptimal performance. We find that performance drops especially when moving between loose and tight privacy and between plentiful and limited compute, which we explain by analyzing clipping as a form of gradient re-weighting and examining cumulative DP noise.
<div><strong>Authors:</strong> Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies differential privacy for fine‑tuning pretrained models and investigates how to select the clipping bound C and batch size B under a fixed compute budget. It shows that theoretical guidance (smaller C for privacy) conflicts with empirical results (larger C works better) due to changes in gradient distributions, and that common batch‑size heuristics fail, with cumulative DP noise better predicting performance. The authors also demonstrate that using a single (C,B) across tasks can be suboptimal, especially when moving between tight/loose privacy regimes or between abundant/limited compute.", "summary_cn": "本文研究了在微调预训练模型时的差分隐私（DP）设置，重点探讨在固定计算预算下如何选择裁剪上限 C 和批量大小 B。研究发现理论上更强隐私应使用更小的 C，但实际实验表明在强隐私条件下更大的 C 表现更好，这源于梯度分布的变化；此外，常用的批量大小调参启发式方法失效，累积的 DP 噪声能够更好地解释批量大小对性能的影响。作者还指出，在不同任务之间使用统一的 (C,B) 设置会导致性能下降，尤其是在隐私约束松紧或计算资源充裕与受限之间切换时，这可以通过将裁剪视为梯度重新加权并分析累计噪声来解释。", "keywords": "differential privacy, transfer learning, hyperparameter tuning, clipping bound, batch size, gradient clipping, DP noise, privacy-utility tradeoff", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Aki Rehn", "Linzh Zhao", "Mikko A. Heikkilä", "Antti Honkela"]}
]]></acme>

<pubDate>2025-10-23T14:48:03+00:00</pubDate>
</item>
<item>
<title>MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation</title>
<link>https://papers.cool/arxiv/2510.20615</link>
<guid>https://papers.cool/arxiv/2510.20615</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MS-BART is a unified modeling framework that encodes mass spectra and molecular structures into a shared token vocabulary, enabling cross‑modal pretraining on large fingerprint‑molecule datasets and multitask objectives. After pretraining, the model is fine‑tuned on experimental spectra using fingerprint predictions from MIST and further guided by a chemical feedback loop to reduce molecular hallucination. Experiments show state‑of‑the‑art performance on MassSpecGym and NPLIB1 and an order‑of‑magnitude speed advantage over diffusion‑based methods.<br /><strong>Summary (CN):</strong> MS-BART 是一个统一建模框架，将质谱 (mass spectra) 和分子结构映射到共享的 token 词表，实现跨模态的大规模预训练和多任务学习。通过使用 MIST 生成的指纹预测进行微调，并加入化学反馈机制以抑制分子幻觉，模型在 MassSpecGym 与 NPLIB1 上取得了多项指标的领先性能，并比扩散方法快一个数量级。<br /><strong>Keywords:</strong> mass spectrometry, molecular structure elucidation, cross-modal pretraining, BART, chemical feedback, fingerprint prediction, diffusion alternatives, MS-BART<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yang Han, Pengyu Wang, Kai Yu, Xin Chen, Lu Chen</div>
Mass spectrometry (MS) plays a critical role in molecular identification, significantly advancing scientific discovery. However, structure elucidation from MS data remains challenging due to the scarcity of annotated spectra. While large-scale pretraining has proven effective in addressing data scarcity in other domains, applying this paradigm to mass spectrometry is hindered by the complexity and heterogeneity of raw spectral signals. To address this, we propose MS-BART, a unified modeling framework that maps mass spectra and molecular structures into a shared token vocabulary, enabling cross-modal learning through large-scale pretraining on reliably computed fingerprint-molecule datasets. Multi-task pretraining objectives further enhance MS-BART's generalization by jointly optimizing denoising and translation task. The pretrained model is subsequently transferred to experimental spectra through finetuning on fingerprint predictions generated with MIST, a pre-trained spectral inference model, thereby enhancing robustness to real-world spectral variability. While finetuning alleviates the distributional difference, MS-BART still suffers molecular hallucination and requires further alignment. We therefore introduce a chemical feedback mechanism that guides the model toward generating molecules closer to the reference structure. Extensive evaluations demonstrate that MS-BART achieves SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is faster by one order of magnitude than competing diffusion-based methods, while comprehensive ablation studies systematically validate the model's effectiveness and robustness.
<div><strong>Authors:</strong> Yang Han, Pengyu Wang, Kai Yu, Xin Chen, Lu Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MS-BART is a unified modeling framework that encodes mass spectra and molecular structures into a shared token vocabulary, enabling cross‑modal pretraining on large fingerprint‑molecule datasets and multitask objectives. After pretraining, the model is fine‑tuned on experimental spectra using fingerprint predictions from MIST and further guided by a chemical feedback loop to reduce molecular hallucination. Experiments show state‑of‑the‑art performance on MassSpecGym and NPLIB1 and an order‑of‑magnitude speed advantage over diffusion‑based methods.", "summary_cn": "MS-BART 是一个统一建模框架，将质谱 (mass spectra) 和分子结构映射到共享的 token 词表，实现跨模态的大规模预训练和多任务学习。通过使用 MIST 生成的指纹预测进行微调，并加入化学反馈机制以抑制分子幻觉，模型在 MassSpecGym 与 NPLIB1 上取得了多项指标的领先性能，并比扩散方法快一个数量级。", "keywords": "mass spectrometry, molecular structure elucidation, cross-modal pretraining, BART, chemical feedback, fingerprint prediction, diffusion alternatives, MS-BART", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yang Han", "Pengyu Wang", "Kai Yu", "Xin Chen", "Lu Chen"]}
]]></acme>

<pubDate>2025-10-23T14:45:28+00:00</pubDate>
</item>
<item>
<title>PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection</title>
<link>https://papers.cool/arxiv/2510.20611</link>
<guid>https://papers.cool/arxiv/2510.20611</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents PSO-XAI, a framework that combines customized Particle Swarm Optimization for feature selection with model‑agnostic explainable AI methods to enhance breast cancer diagnosis. Evaluated on 29 diverse classifiers, the approach achieves 99.1% accuracy while reducing dimensionality and providing transparent explanations for clinical relevance.<br /><strong>Summary (CN):</strong> 本文提出 PSO‑XAI 框架，将定制化粒子群优化用于特征选择，并结合模型不可知的可解释人工智能方法，以提升乳腺癌诊断的可靠性。该框架在 29 种不同分类模型上进行评估，取得 99.1% 的准确率，同时降低维度并提供透明的临床解释。<br /><strong>Keywords:</strong> breast cancer detection, particle swarm optimization, feature selection, explainable AI, model-agnostic explanations, medical diagnosis, classification, swarm intelligence<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Mirza Raquib, Niloy Das, Farida Siddiqi Prity, Arafath Al Fahim, Saydul Akbar Murad, Mohammad Amzad Hossain, MD Jiabul Hoque, Mohammad Ali Moni</div>
Breast cancer is considered the most critical and frequently diagnosed cancer in women worldwide, leading to an increase in cancer-related mortality. Early and accurate detection is crucial as it can help mitigate possible threats while improving survival rates. In terms of prediction, conventional diagnostic methods are often limited by variability, cost, and, most importantly, risk of misdiagnosis. To address these challenges, machine learning (ML) has emerged as a powerful tool for computer-aided diagnosis, with feature selection playing a vital role in improving model performance and interpretability. This research study proposes an integrated framework that incorporates customized Particle Swarm Optimization (PSO) for feature selection. This framework has been evaluated on a comprehensive set of 29 different models, spanning classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. To ensure interpretability and clinical relevance, the study uses cross-validation in conjunction with explainable AI methods. Experimental evaluation showed that the proposed approach achieved a superior score of 99.1\% across all performance metrics, including accuracy and precision, while effectively reducing dimensionality and providing transparent, model-agnostic explanations. The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.
<div><strong>Authors:</strong> Mirza Raquib, Niloy Das, Farida Siddiqi Prity, Arafath Al Fahim, Saydul Akbar Murad, Mohammad Amzad Hossain, MD Jiabul Hoque, Mohammad Ali Moni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents PSO-XAI, a framework that combines customized Particle Swarm Optimization for feature selection with model‑agnostic explainable AI methods to enhance breast cancer diagnosis. Evaluated on 29 diverse classifiers, the approach achieves 99.1% accuracy while reducing dimensionality and providing transparent explanations for clinical relevance.", "summary_cn": "本文提出 PSO‑XAI 框架，将定制化粒子群优化用于特征选择，并结合模型不可知的可解释人工智能方法，以提升乳腺癌诊断的可靠性。该框架在 29 种不同分类模型上进行评估，取得 99.1% 的准确率，同时降低维度并提供透明的临床解释。", "keywords": "breast cancer detection, particle swarm optimization, feature selection, explainable AI, model-agnostic explanations, medical diagnosis, classification, swarm intelligence", "scoring": {"interpretability": 6, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Mirza Raquib", "Niloy Das", "Farida Siddiqi Prity", "Arafath Al Fahim", "Saydul Akbar Murad", "Mohammad Amzad Hossain", "MD Jiabul Hoque", "Mohammad Ali Moni"]}
]]></acme>

<pubDate>2025-10-23T14:42:50+00:00</pubDate>
</item>
<item>
<title>Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets</title>
<link>https://papers.cool/arxiv/2510.20609</link>
<guid>https://papers.cool/arxiv/2510.20609</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates retrieval configurations for code‑focused generation tasks (code completion and bug localization) under realistic compute budgets, comparing chunking strategies, similarity scoring methods, and splitting granularity. It shows that sparse BM25 with word‑level splitting outperforms dense retrievers for PL‑PL tasks while being much faster, whereas proprietary dense encoders excel for NL‑PL at higher latency, and provides practical guidelines on chunk size and latency‑quality trade‑offs.<br /><strong>Summary (CN):</strong> 本文系统评估了在实际计算预算约束下代码生成任务（代码补全和错误定位）的检索配置，比较了分块策略、相似度打分方法以及切分粒度。结果表明，对于 PL‑PL 场景，稀疏检索器 BM25 配合词级切分在质量和速度上均优于密集检索器，而在 NL‑PL 场景下专有的密集编码器（Voyager-3 系列）虽能获得更好效果，但延迟大约高出 100 倍；此外，文章还提供了块大小、延迟‑质量权衡的实用建议。<br /><strong>Keywords:</strong> code retrieval, RAG, BM25, dense retrieval, chunking, code completion, bug localization, compute budget, latency trade-off<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - -applicable; Primary focus - robustness<br /><strong>Authors:</strong> Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</div>
We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.
<div><strong>Authors:</strong> Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates retrieval configurations for code‑focused generation tasks (code completion and bug localization) under realistic compute budgets, comparing chunking strategies, similarity scoring methods, and splitting granularity. It shows that sparse BM25 with word‑level splitting outperforms dense retrievers for PL‑PL tasks while being much faster, whereas proprietary dense encoders excel for NL‑PL at higher latency, and provides practical guidelines on chunk size and latency‑quality trade‑offs.", "summary_cn": "本文系统评估了在实际计算预算约束下代码生成任务（代码补全和错误定位）的检索配置，比较了分块策略、相似度打分方法以及切分粒度。结果表明，对于 PL‑PL 场景，稀疏检索器 BM25 配合词级切分在质量和速度上均优于密集检索器，而在 NL‑PL 场景下专有的密集编码器（Voyager-3 系列）虽能获得更好效果，但延迟大约高出 100 倍；此外，文章还提供了块大小、延迟‑质量权衡的实用建议。", "keywords": "code retrieval, RAG, BM25, dense retrieval, chunking, code completion, bug localization, compute budget, latency trade-off", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "-applicable", "primary_focus": "robustness"}, "authors": ["Timur Galimzyanov", "Olga Kolomyttseva", "Egor Bogomolov"]}
]]></acme>

<pubDate>2025-10-23T14:40:11+00:00</pubDate>
</item>
<item>
<title>Convergence Analysis of SGD under Expected Smoothness</title>
<link>https://papers.cool/arxiv/2510.20608</link>
<guid>https://papers.cool/arxiv/2510.20608</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a self‑contained convergence analysis of stochastic gradient descent under the expected smoothness condition, offering refined definitions, sampling‑dependent constants, and explicit O(1/K) rates for various step‑size schedules. It derives bounds on the expected squared norm of the full gradient and presents detailed proofs, unifying and extending recent works. The results clarify how expected smoothness influences SGD performance without relying on strong variance assumptions.<br /><strong>Summary (CN):</strong> 本文对随机梯度下降（SGD）在期望光滑（expected smoothness）条件下的收敛性进行自包含的理论分析，提出了条件的细化解释、与采样相关的常数以及针对不同步长调度的 O(1/K) 收敛率，并给出全梯度平方范数的期望上界。所有证明在附录中完整呈现，统一并扩展了近期工作。研究阐明了在不依赖强方差假设的情况下，期望光滑如何影响 SGD 的表现。<br /><strong>Keywords:</strong> stochastic gradient descent, expected smoothness, convergence analysis, optimization, step-size schedule<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuta Kawamoto, Hideaki Iiduka</div>
Stochastic gradient descent (SGD) is the workhorse of large-scale learning, yet classical analyses rely on assumptions that can be either too strong (bounded variance) or too coarse (uniform noise). The expected smoothness (ES) condition has emerged as a flexible alternative that ties the second moment of stochastic gradients to the objective value and the full gradient. This paper presents a self-contained convergence analysis of SGD under ES. We (i) refine ES with interpretations and sampling-dependent constants; (ii) derive bounds of the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates with explicit residual errors for various step-size schedules. All proofs are given in full detail in the appendix. Our treatment unifies and extends recent threads (Khaled and Richtárik, 2020; Umeda and Iiduka, 2025).
<div><strong>Authors:</strong> Yuta Kawamoto, Hideaki Iiduka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a self‑contained convergence analysis of stochastic gradient descent under the expected smoothness condition, offering refined definitions, sampling‑dependent constants, and explicit O(1/K) rates for various step‑size schedules. It derives bounds on the expected squared norm of the full gradient and presents detailed proofs, unifying and extending recent works. The results clarify how expected smoothness influences SGD performance without relying on strong variance assumptions.", "summary_cn": "本文对随机梯度下降（SGD）在期望光滑（expected smoothness）条件下的收敛性进行自包含的理论分析，提出了条件的细化解释、与采样相关的常数以及针对不同步长调度的 O(1/K) 收敛率，并给出全梯度平方范数的期望上界。所有证明在附录中完整呈现，统一并扩展了近期工作。研究阐明了在不依赖强方差假设的情况下，期望光滑如何影响 SGD 的表现。", "keywords": "stochastic gradient descent, expected smoothness, convergence analysis, optimization, step-size schedule", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuta Kawamoto", "Hideaki Iiduka"]}
]]></acme>

<pubDate>2025-10-23T14:39:57+00:00</pubDate>
</item>
<item>
<title>Generalizable Reasoning through Compositional Energy Minimization</title>
<link>https://papers.cool/arxiv/2510.20607</link>
<guid>https://papers.cool/arxiv/2510.20607</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a compositional energy minimization framework that learns energy functions for small subproblems and combines them at test time to form a global energy landscape, enabling reasoning over larger, more complex tasks with additional constraints. A parallel energy minimization (PEM) algorithm is proposed to efficiently sample high‑quality solutions, and empirical results show state‑of‑the‑art generalization performance on diverse reasoning benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了一种组合式能量最小化方法，通过为较小的子问题学习能函数并在测试时将其组合成全局能量景观，从而在加入额外约束的情况下解决更大、更复杂的推理任务。作者进一步设计了并行能量最小化（PEM）算法以提升采样质量，实验表明该方法在多种推理基准上实现了领先的泛化性能。<br /><strong>Keywords:</strong> compositional energy minimization, energy-based models, reasoning generalization, compositional inference, parallel energy minimization, constraint-based reasoning, neural reasoning, sample quality<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Alexandru Oarga, Yilun Du</div>
Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/
<div><strong>Authors:</strong> Alexandru Oarga, Yilun Du</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a compositional energy minimization framework that learns energy functions for small subproblems and combines them at test time to form a global energy landscape, enabling reasoning over larger, more complex tasks with additional constraints. A parallel energy minimization (PEM) algorithm is proposed to efficiently sample high‑quality solutions, and empirical results show state‑of‑the‑art generalization performance on diverse reasoning benchmarks.", "summary_cn": "本文提出了一种组合式能量最小化方法，通过为较小的子问题学习能函数并在测试时将其组合成全局能量景观，从而在加入额外约束的情况下解决更大、更复杂的推理任务。作者进一步设计了并行能量最小化（PEM）算法以提升采样质量，实验表明该方法在多种推理基准上实现了领先的泛化性能。", "keywords": "compositional energy minimization, energy-based models, reasoning generalization, compositional inference, parallel energy minimization, constraint-based reasoning, neural reasoning, sample quality", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Alexandru Oarga", "Yilun Du"]}
]]></acme>

<pubDate>2025-10-23T14:38:36+00:00</pubDate>
</item>
<item>
<title>Embedding the MLOps Lifecycle into OT Reference Models</title>
<link>https://papers.cool/arxiv/2510.20590</link>
<guid>https://papers.cool/arxiv/2510.20590</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes challenges in combining Machine Learning Operations (MLOps) with Operational Technology (OT) systems and proposes a systematic method to embed MLOps practices into established OT reference models such as RAMI 4.0 and ISA-95. It provides a detailed mapping of MLOps lifecycle components to RAMI 4.0, illustrated with a real-world use case, showing that adapted integration can enable successful deployment of MLOps in OT environments.<br /><strong>Summary (CN):</strong> 本文分析了将机器学习运维（MLOps）与工业运营技术（OT）系统相结合的挑战，并提出了一种系统化方法 MLOps 实践嵌入到已建立的 OT 参考模型（如 RAMI 4.0 与 ISA-95）中。通过对 RAMI 4.0 的 MLOps 生命周期组件映射并结合实际案例，展示了经过结构化适配后可实现 MLOps 在 OT 环境中的成功集成。<br /><strong>Keywords:</strong> MLOps, Operational Technology, RAMI 4.0, ISA-95, industrial AI, lifecycle integration, reference models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Simon Schindler, Christoph Binder, Lukas Lürzer, Stefan Huber</div>
Machine Learning Operations (MLOps) practices are increas- ingly adopted in industrial settings, yet their integration with Opera- tional Technology (OT) systems presents significant challenges. This pa- per analyzes the fundamental obstacles in combining MLOps with OT en- vironments and proposes a systematic approach to embed MLOps prac- tices into established OT reference models. We evaluate the suitability of the Reference Architectural Model for Industry 4.0 (RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for MLOps integration and present a detailed mapping of MLOps lifecycle compo- nents to RAMI 4.0 exemplified by a real-world use case. Our findings demonstrate that while standard MLOps practices cannot be directly transplanted to OT environments, structured adaptation using existing reference models can provide a pathway for successful integration.
<div><strong>Authors:</strong> Simon Schindler, Christoph Binder, Lukas Lürzer, Stefan Huber</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes challenges in combining Machine Learning Operations (MLOps) with Operational Technology (OT) systems and proposes a systematic method to embed MLOps practices into established OT reference models such as RAMI 4.0 and ISA-95. It provides a detailed mapping of MLOps lifecycle components to RAMI 4.0, illustrated with a real-world use case, showing that adapted integration can enable successful deployment of MLOps in OT environments.", "summary_cn": "本文分析了将机器学习运维（MLOps）与工业运营技术（OT）系统相结合的挑战，并提出了一种系统化方法 MLOps 实践嵌入到已建立的 OT 参考模型（如 RAMI 4.0 与 ISA-95）中。通过对 RAMI 4.0 的 MLOps 生命周期组件映射并结合实际案例，展示了经过结构化适配后可实现 MLOps 在 OT 环境中的成功集成。", "keywords": "MLOps, Operational Technology, RAMI 4.0, ISA-95, industrial AI, lifecycle integration, reference models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Simon Schindler", "Christoph Binder", "Lukas Lürzer", "Stefan Huber"]}
]]></acme>

<pubDate>2025-10-23T14:14:21+00:00</pubDate>
</item>
<item>
<title>Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics</title>
<link>https://papers.cool/arxiv/2510.20556</link>
<guid>https://papers.cool/arxiv/2510.20556</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> paper systematically analyzes how different graph rewiring methods affect various graph structural metrics and links these changes to downstream node classification performance. By evaluating seven rewiring strategies, it finds that successful methods tend to preserve local graph structure while allowing flexibility in global connectivity, offering guidance for designing effective rewiring techniques.<br /><strong>Summary (CN):</strong> 本文系统性地研究了不同图重连方法对多种图结构度量的影响，并将这些变化与下游节点分类性能关联起来。通过评估七种重连策略，发现成功的方法往往保持局部结构不变，同时在全局连接上保持灵活，为设计有效的图重连技术提供了指导。<br /><strong>Keywords:</strong> graph neural networks, graph rewiring, over-squashing, structural invariance, graph metrics, node classification, local structure preservation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Alexandre Benoit, Catherine Aitken, Yu He</div>
Graph rewiring has emerged as a key technique to alleviate over-squashing in Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph topology to improve information flow. While effective, rewiring inherently alters the graph's structure, raising the risk of distorting important topology-dependent signals. Yet, despite the growing use of rewiring, little is known about which structural properties must be preserved to ensure both performance gains and structural fidelity. In this work, we provide the first systematic analysis of how rewiring affects a range of graph structural metrics, and how these changes relate to downstream task performance. We study seven diverse rewiring strategies and correlate changes in local and global graph properties with node classification accuracy. Our results reveal a consistent pattern: successful rewiring methods tend to preserve local structure while allowing for flexibility in global connectivity. These findings offer new insights into the design of effective rewiring strategies, bridging the gap between graph theory and practical GNN optimization.
<div><strong>Authors:</strong> Alexandre Benoit, Catherine Aitken, Yu He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "paper systematically analyzes how different graph rewiring methods affect various graph structural metrics and links these changes to downstream node classification performance. By evaluating seven rewiring strategies, it finds that successful methods tend to preserve local graph structure while allowing flexibility in global connectivity, offering guidance for designing effective rewiring techniques.", "summary_cn": "本文系统性地研究了不同图重连方法对多种图结构度量的影响，并将这些变化与下游节点分类性能关联起来。通过评估七种重连策略，发现成功的方法往往保持局部结构不变，同时在全局连接上保持灵活，为设计有效的图重连技术提供了指导。", "keywords": "graph neural networks, graph rewiring, over-squashing, structural invariance, graph metrics, node classification, local structure preservation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Alexandre Benoit", "Catherine Aitken", "Yu He"]}
]]></acme>

<pubDate>2025-10-23T13:38:41+00:00</pubDate>
</item>
<item>
<title>A Unified Framework for Zero-Shot Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.20542</link>
<guid>https://papers.cool/arxiv/2510.20542</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified analytical framework for zero-shot reinforcement learning that classifies existing methods into direct anditional representation families. It formalizes a taxonomy, derives an extended performance bound for successor-feature approaches, and highlights shared principles to guide future research toward more general agents. By providing a common notation and comparative lens, the work aims to consolidate and advance the study of task-general RL agents.<br /><strong>Summary (CN):</strong> 本文提出了一个统一的零样本强化学习（zero-shot RL）分析框架，将现有方法划分为直接表示和组合表示两大类。文中给出一致的符号体系与分类法，并为后继特征（successor features）方法推导了扩展的性能界限，阐明了不同方法的共同原理和关键差异。通过这种统一视角，旨在为研发更通用的 RL 代理提供规范化的基础。<br /><strong>Keywords:</strong> zero-shot reinforcement learning, successor features, compositional representations, direct representations, task generalization, representation learning, RL foundations, analytical framework<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland</div>
Zero-shot reinforcement learning (RL) has emerged as a setting for developing general agents in an unsupervised manner, capable of solving downstream tasks without additional training or planning at test-time. Unlike conventional RL, which optimizes policies for a fixed reward, zero-shot RL requires agents to encode representations rich enough to support immediate adaptation to any objective, drawing parallels to vision and language foundation models. Despite growing interest, the field lacks a common analytical lens. We present the first unified framework for zero-shot RL. Our formulation introduces a consistent notation and taxonomy that organizes existing approaches and allows direct comparison between them. Central to our framework is the classification of algorithms into two families: direct representations, which learn end-to-end mappings from rewards to policies, and compositional representations, which decompose the representation leveraging the substructure of the value function. Within this framework, we highlight shared principles and key differences across methods, and we derive an extended bound for successor-feature methods, offering a new perspective on their performance in the zero-shot regime. By consolidating existing work under a common lens, our framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents.
<div><strong>Authors:</strong> Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified analytical framework for zero-shot reinforcement learning that classifies existing methods into direct anditional representation families. It formalizes a taxonomy, derives an extended performance bound for successor-feature approaches, and highlights shared principles to guide future research toward more general agents. By providing a common notation and comparative lens, the work aims to consolidate and advance the study of task-general RL agents.", "summary_cn": "本文提出了一个统一的零样本强化学习（zero-shot RL）分析框架，将现有方法划分为直接表示和组合表示两大类。文中给出一致的符号体系与分类法，并为后继特征（successor features）方法推导了扩展的性能界限，阐明了不同方法的共同原理和关键差异。通过这种统一视角，旨在为研发更通用的 RL 代理提供规范化的基础。", "keywords": "zero-shot reinforcement learning, successor features, compositional representations, direct representations, task generalization, representation learning, RL foundations, analytical framework", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Jacopo Di Ventura", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"]}
]]></acme>

<pubDate>2025-10-23T13:30:26+00:00</pubDate>
</item>
<item>
<title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
<link>https://papers.cool/arxiv/2510.20540</link>
<guid>https://papers.cool/arxiv/2510.20540</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SheafAlign introduces a sheaf-theoretic framework for decentralized multimodal alignment, replacing the conventional single-space alignment with multiple comparison spaces that capture pairwise modality relations. By leveraging decentralized contrastive learning objectives, the method does not require mutual redundancy among all modalities, preserving both shared and unique information, and demonstrates superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities with reduced communication cost.<br /><strong>Summary (CN):</strong> SheafAlign 提出基于层析（sheaf）理论的去中心化多模态对齐框架，用多比较空间取代传统的单空间对齐，以捕获模态之间的成对关系。该方法利用去中心化对比学习目标，无需所有模态之间的相互冗余，既保留共享信息也保留独特信息，并在零样本泛化、跨模态对齐以及缺失模态鲁棒性方面表现优越，通信成本降低约 %。<br /><strong>Keywords:</strong> sheaf theory, multimodal alignment, decentralized learning, contrastive learning, zero-shot generalization, missing modality robustness, communication efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis</div>
Conventional multimodal alignment methods assume mutual redundancy across all modalities, an assumption that fails in real-world distributed scenarios. We propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal alignment that replaces single-space alignment with multiple comparison spaces. This approach models pairwise modality relations through sheaf structures and leverages decentralized contrastive learning-based objectives for training. SheafAlign overcomes the limitations of prior methods by not requiring mutual redundancy among all modalities, preserving both shared and unique information. Experiments on multimodal sensing datasets show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50\% lower communication cost than state-of-the-art baselines.
<div><strong>Authors:</strong> Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SheafAlign introduces a sheaf-theoretic framework for decentralized multimodal alignment, replacing the conventional single-space alignment with multiple comparison spaces that capture pairwise modality relations. By leveraging decentralized contrastive learning objectives, the method does not require mutual redundancy among all modalities, preserving both shared and unique information, and demonstrates superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities with reduced communication cost.", "summary_cn": "SheafAlign 提出基于层析（sheaf）理论的去中心化多模态对齐框架，用多比较空间取代传统的单空间对齐，以捕获模态之间的成对关系。该方法利用去中心化对比学习目标，无需所有模态之间的相互冗余，既保留共享信息也保留独特信息，并在零样本泛化、跨模态对齐以及缺失模态鲁棒性方面表现优越，通信成本降低约 %。", "keywords": "sheaf theory, multimodal alignment, decentralized learning, contrastive learning, zero-shot generalization, missing modality robustness, communication efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"]}
]]></acme>

<pubDate>2025-10-23T13:27:24+00:00</pubDate>
</item>
<item>
<title>Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval</title>
<link>https://papers.cool/arxiv/2510.20486</link>
<guid>https://papers.cool/arxiv/2510.20486</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Hurdle-IMDL, a framework that tackles label imbalance in infrared rainfall retrieval by separating zero‑inflation and long‑tail issues, applying a hurdle model for non‑rain samples and an inverse‑model debiasing technique for heavy‑rain estimation. Experiments on eastern China data show that the method reduces systematic underestimation and significantly improves retrieval of heavy‑to‑extreme rain compared to standard, cost‑sensitive, generative, and multi‑task baselines.<br /><strong>Summary (CN):</strong> 本文提出 Hurdle‑IMDL 框架，通过将降雨分布的零膨胀（非降雨样本占多数）和长尾（轻雨样本相对重雨样本过多）两部分分离，分别使用 hurdle 模型处理零膨胀，并利用逆模型去偏技术（inverse model debiasing）提升对强降雨的检索精度。对中国东部地区的遥感数据进行实验，结果表明该方法在减轻系统性低估并显著提升重至极端降雨检索方面优于常规、代价敏感、生成式及多任务学习方法。<br /><strong>Keywords:</strong> imbalanced learning, infrared rainfall retrieval, hurdle model, inverse model debiasing, remote sensing, heavy rain estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fangjian Zhang, Xiaoyong Zhuge, Wenlan Wang, Haixia Xiao, Yuying Zhu, Siyang Cheng</div>
Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.
<div><strong>Authors:</strong> Fangjian Zhang, Xiaoyong Zhuge, Wenlan Wang, Haixia Xiao, Yuying Zhu, Siyang Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Hurdle-IMDL, a framework that tackles label imbalance in infrared rainfall retrieval by separating zero‑inflation and long‑tail issues, applying a hurdle model for non‑rain samples and an inverse‑model debiasing technique for heavy‑rain estimation. Experiments on eastern China data show that the method reduces systematic underestimation and significantly improves retrieval of heavy‑to‑extreme rain compared to standard, cost‑sensitive, generative, and multi‑task baselines.", "summary_cn": "本文提出 Hurdle‑IMDL 框架，通过将降雨分布的零膨胀（非降雨样本占多数）和长尾（轻雨样本相对重雨样本过多）两部分分离，分别使用 hurdle 模型处理零膨胀，并利用逆模型去偏技术（inverse model debiasing）提升对强降雨的检索精度。对中国东部地区的遥感数据进行实验，结果表明该方法在减轻系统性低估并显著提升重至极端降雨检索方面优于常规、代价敏感、生成式及多任务学习方法。", "keywords": "imbalanced learning, infrared rainfall retrieval, hurdle model, inverse model debiasing, remote sensing, heavy rain estimation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fangjian Zhang", "Xiaoyong Zhuge", "Wenlan Wang", "Haixia Xiao", "Yuying Zhu", "Siyang Cheng"]}
]]></acme>

<pubDate>2025-10-23T12:25:52+00:00</pubDate>
</item>
<item>
<title>Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models</title>
<link>https://papers.cool/arxiv/2510.20477</link>
<guid>https://papers.cool/arxiv/2510.20477</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Bi-CoG, a plug-and-play self‑training framework for semi‑supervised fine‑tuning of vision‑language models that leverages both inter‑model and intra‑model consistency along with an error‑aware dynamic pseudo‑label assignment to produce higher‑quality, lower‑bias labels. The authors provide theoretical analysis and extensive experiments on 14 datasets, showing consistent performance gains over existing methods.<br /><strong>Summary (CN):</strong> 本文提出 Bi‑CoG，一种用于视觉语言模型半监督微调的即插即用自训练框架，通过利用模型间和模型内的一致性以及误差感知的动态伪标签分配策略，生成高质量、低偏差的伪标签。作者给出理论分析并在 14 个数据集上进行大量实验，展示了相较于现有方法的一致且显著的性能提升。<br /><strong>Keywords:</strong> vision-language models, semi-supervised learning, self-training, pseudo-labeling, consistency regularization, Bi-CoG<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Rui Zhu, Song-Lin Lv, Zi-Kang Wang, Lan-Zhe Guo</div>
Exploiting unlabeled data through semi-supervised learning (SSL) or leveraging pre-trained models via fine-tuning are two prevailing paradigms for addressing label-scarce scenarios. Recently, growing attention has been given to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL, forming the emerging paradigm of semi-supervised fine-tuning. However, existing methods often suffer from model bias and hyperparameter sensitivity, due to reliance on prediction consistency or pre-defined confidence thresholds. To address these limitations, we propose a simple yet effective plug-and-play methodology named $\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels, by simultaneously exploiting inter-model and intra-model consistency, along with an error-aware dynamic pseudo-label assignment strategy. Both theoretical analysis and extensive experiments over 14 datasets demonstrate the effectiveness of Bi-CoG, which consistently and significantly improves the performance of existing methods.
<div><strong>Authors:</strong> Rui Zhu, Song-Lin Lv, Zi-Kang Wang, Lan-Zhe Guo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Bi-CoG, a plug-and-play self‑training framework for semi‑supervised fine‑tuning of vision‑language models that leverages both inter‑model and intra‑model consistency along with an error‑aware dynamic pseudo‑label assignment to produce higher‑quality, lower‑bias labels. The authors provide theoretical analysis and extensive experiments on 14 datasets, showing consistent performance gains over existing methods.", "summary_cn": "本文提出 Bi‑CoG，一种用于视觉语言模型半监督微调的即插即用自训练框架，通过利用模型间和模型内的一致性以及误差感知的动态伪标签分配策略，生成高质量、低偏差的伪标签。作者给出理论分析并在 14 个数据集上进行大量实验，展示了相较于现有方法的一致且显著的性能提升。", "keywords": "vision-language models, semi-supervised learning, self-training, pseudo-labeling, consistency regularization, Bi-CoG", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Rui Zhu", "Song-Lin Lv", "Zi-Kang Wang", "Lan-Zhe Guo"]}
]]></acme>

<pubDate>2025-10-23T12:16:41+00:00</pubDate>
</item>
<item>
<title>Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models</title>
<link>https://papers.cool/arxiv/2510.20468</link>
<guid>https://papers.cool/arxiv/2510.20468</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a black-box, one-shot watermark forging attack that uses a preference model trained on procedurally generated images to detect watermarks and then optimizes images via backagation to remove and reapply watermarks without access to the original watermarking algorithm. It demonstrates the method on several post-hoc image watermarking schemes, showing that a single watermarked example suffices to forge watermarks on malicious content, thereby exposing a significant security weakness.<br /><strong>Summary (CN):</strong> 本文提出一种黑盒单次水印伪造攻击，利用在程序化生成图像上训练的偏好模型判断图像是否带有水印，然后通过反向传播优化图像，实现去除并重新植入水印，无需了解原始水印算法。实验在多种后置图像水印方案上验证了该方法，仅需一张水印图即可在恶意内容上伪造水印，揭示了当前水印技术的显著安全弱点。<br /><strong>Keywords:</strong> watermarking, black-box attack, image preference model, forgery, post-hoc watermark, security, generative AI, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.
<div><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a black-box, one-shot watermark forging attack that uses a preference model trained on procedurally generated images to detect watermarks and then optimizes images via backagation to remove and reapply watermarks without access to the original watermarking algorithm. It demonstrates the method on several post-hoc image watermarking schemes, showing that a single watermarked example suffices to forge watermarks on malicious content, thereby exposing a significant security weakness.", "summary_cn": "本文提出一种黑盒单次水印伪造攻击，利用在程序化生成图像上训练的偏好模型判断图像是否带有水印，然后通过反向传播优化图像，实现去除并重新植入水印，无需了解原始水印算法。实验在多种后置图像水印方案上验证了该方法，仅需一张水印图即可在恶意内容上伪造水印，揭示了当前水印技术的显著安全弱点。", "keywords": "watermarking, black-box attack, image preference model, forgery, post-hoc watermark, security, generative AI, robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tomáš Souček", "Sylvestre-Alvise Rebuffi", "Pierre Fernandez", "Nikola Jovanović", "Hady Elsahar", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"]}
]]></acme>

<pubDate>2025-10-23T12:06:35+00:00</pubDate>
</item>
<item>
<title>Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach</title>
<link>https://papers.cool/arxiv/2510.20454</link>
<guid>https://papers.cool/arxiv/2510.20454</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a graph neural network that models temporal directed graphs of tennis matches to capture intransitive player dominance relationships. It demonstrates that the bookmaker Pinnacle Sports struggles with high‑intransitivity matchups, and shows that selective betting based on the model yields a positive 3.26% ROI using Kelly staking over 1903 bets, indicating a market inefficiency.<br /><strong>Summary (CN):</strong> 本文提出使用图神经网络对网球比赛的时间有向图进行建模，以捕捉选手间的非传递性支配关系（如 A B、B 胜 C、但 C 胜 A）。研究发现博彩公司 Pinnacle Sports 在高非传递性赛事上的表现较差，利用模型在此类比赛上进行有选择的投注（使用 Kelly 资金管理）在 1903 次投注中实现 3.26% 的正向收益率，表明市场对这类对局存在效率低下。<br /><strong>Keywords:</strong> intransitive dominance, tennis forecasting, graph neural network, market inefficiency, betting, Kelly staking, relational modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lawrence Clegg, John Cartlidge</div>
Intransitive player dominance, where player A beats B, B beats C, but C beats A, is common in competitive tennis. Yet, there are few known attempts to incorporate it within forecasting methods. We address this problem with a graph neural network approach that explicitly models these intransitive relationships through temporal directed graphs, with players as nodes and their historical match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly handles matches with high intransitive complexity and posit that our graph-based approach is uniquely positioned to capture relational dynamics in these scenarios. When selectively betting on higher intransitivity matchups with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a market inefficiency in handling intransitive matchups that our approach successfully exploits.
<div><strong>Authors:</strong> Lawrence Clegg, John Cartlidge</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a graph neural network that models temporal directed graphs of tennis matches to capture intransitive player dominance relationships. It demonstrates that the bookmaker Pinnacle Sports struggles with high‑intransitivity matchups, and shows that selective betting based on the model yields a positive 3.26% ROI using Kelly staking over 1903 bets, indicating a market inefficiency.", "summary_cn": "本文提出使用图神经网络对网球比赛的时间有向图进行建模，以捕捉选手间的非传递性支配关系（如 A B、B 胜 C、但 C 胜 A）。研究发现博彩公司 Pinnacle Sports 在高非传递性赛事上的表现较差，利用模型在此类比赛上进行有选择的投注（使用 Kelly 资金管理）在 1903 次投注中实现 3.26% 的正向收益率，表明市场对这类对局存在效率低下。", "keywords": "intransitive dominance, tennis forecasting, graph neural network, market inefficiency, betting, Kelly staking, relational modeling", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lawrence Clegg", "John Cartlidge"]}
]]></acme>

<pubDate>2025-10-23T11:41:45+00:00</pubDate>
</item>
<item>
<title>MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction</title>
<link>https://papers.cool/arxiv/2510.20448</link>
<guid>https://papers.cool/arxiv/2510.20448</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MolBridge is an atom‑level joint graph refinement framework that merges the molecular graphs of two drugs into a single joint graph, enabling explicit modeling of inter‑drug atomic interactions for drug‑drug interaction (DDI) event prediction. By introducing a structure consistency module that iteratively refines node features while preserving global structure, the method mitigates over‑smoothing and achieves robust performance on both frequent and long‑tail DDI types, outperforming existing baselines on benchmark datasets.<br /><strong>Summary (CN):</strong> MolBridge 将两种药物的分子图合并为一个联合图，直接建模药物之间的原子级相互作用，从而预测药物‑药物相互作用（DDI）事件。其通过结构一致性模块迭代细化节点特征并保持全局结构，克服了长程依赖导致的过平滑问题，在常见和长尾 DDI 类型上均实现了比现有方法更稳健的性能，实验在两个基准数据集上验证了其优势。<br /><strong>Keywords:</strong> drug-drug interaction, joint graph neural network, atom-level modeling, structure consistency, mechanistic interpretability, robustness, long-tail prediction<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - interpretability<br /><strong>Authors:</strong> Xuan Lin, Aocheng Ding, Tengfei Ma, Hua Liang, Zhe Quan</div>
Drug combinations offer therapeutic benefits but also carry the risk of adverse drug-drug interactions (DDIs), especially under complex molecular structures. Accurate DDI event prediction requires capturing fine-grained inter-drug relationships, which are critical for modeling metabolic mechanisms such as enzyme-mediated competition. However, existing approaches typically rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. To address these limitations, we propose MolBridge, a novel atom-level joint graph refinement framework for robust DDI event prediction. MolBridge constructs a joint graph that integrates atomic structures of drug pairs, enabling direct modeling of inter-drug associations. A central challenge in such joint graph settings is the potential loss of information caused by over-smoothing when modeling long-range atomic dependencies. To overcome this, we introduce a structure consistency module that iteratively refines node features while preserving the global structural context. This joint design allows MolBridge to effectively learn both local and global interaction outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. patterns, yielding robust representations across both frequent and rare DDI types. Extensive experiments on two benchmark datasets show that MolBridge consistently. These results demonstrate the advantages of fine-grained graph refinement in improving the accuracy, robustness, and mechanistic interpretability of DDI event prediction.This work contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks.
<div><strong>Authors:</strong> Xuan Lin, Aocheng Ding, Tengfei Ma, Hua Liang, Zhe Quan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MolBridge is an atom‑level joint graph refinement framework that merges the molecular graphs of two drugs into a single joint graph, enabling explicit modeling of inter‑drug atomic interactions for drug‑drug interaction (DDI) event prediction. By introducing a structure consistency module that iteratively refines node features while preserving global structure, the method mitigates over‑smoothing and achieves robust performance on both frequent and long‑tail DDI types, outperforming existing baselines on benchmark datasets.", "summary_cn": "MolBridge 将两种药物的分子图合并为一个联合图，直接建模药物之间的原子级相互作用，从而预测药物‑药物相互作用（DDI）事件。其通过结构一致性模块迭代细化节点特征并保持全局结构，克服了长程依赖导致的过平滑问题，在常见和长尾 DDI 类型上均实现了比现有方法更稳健的性能，实验在两个基准数据集上验证了其优势。", "keywords": "drug-drug interaction, joint graph neural network, atom-level modeling, structure consistency, mechanistic interpretability, robustness, long-tail prediction", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "interpretability"}, "authors": ["Xuan Lin", "Aocheng Ding", "Tengfei Ma", "Hua Liang", "Zhe Quan"]}
]]></acme>

<pubDate>2025-10-23T11:33:16+00:00</pubDate>
</item>
<item>
<title>Explainable Benchmarking through the Lense of Concept Learning</title>
<link>https://papers.cool/arxiv/2510.20439</link>
<guid>https://papers.cool/arxiv/2510.20439</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a new paradigm called explainable benchmarking, which automatically generates explanations for the performance of systems in a benchmark. It demonstrates the paradigm on knowledge-graph-based question answering by introducing a novel concept learning method, PruneCEL, which outperforms existing learners and enables users to predict system behavior from the generated explanations.<br /><strong>Summary (CN):</strong> 本文提出了“可解释基准评估”（explainable benchmarking）新范式，旨在自动生成系统在基准测试中的表现解释。作者在基于知识图谱的问答任务上实现该范式，构建了新概念学习方法 PruneCEL ，其在解释性基准任务上比现有方法提升显著，且用户能够通过解释预测系统行为。<br /><strong>Keywords:</strong> explainable benchmarking, concept learning, knowledge graph, question answering, PruneCEL, interpretability, performance explanation, evaluation metrics<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Quannian Zhang, Michael Röder, Nikit Srivastava, N'Dah Jean Kouagou, Axel-Cyrille Ngonga Ngomo</div>
Evaluating competing systems in a comparable way, i.e., benchmarking them, is an undeniable pillar of the scientific method. However, system performance is often summarized via a small number of metrics. The analysis of the evaluation details and the derivation of insights for further development or use remains a tedious manual task with often biased results. Thus, this paper argues for a new type of benchmarking, which is dubbed explainable benchmarking. The aim of explainable benchmarking approaches is to automatically generate explanations for the performance of systems in a benchmark. We provide a first instantiation of this paradigm for knowledge-graph-based question answering systems. We compute explanations by using a novel concept learning approach developed for large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL outperforms state-of-the-art concept learners on the task of explainable benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41 participants shows that in 80\% of the cases, the majority of participants can accurately predict the behavior of a system based on our explanations. Our code and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025
<div><strong>Authors:</strong> Quannian Zhang, Michael Röder, Nikit Srivastava, N'Dah Jean Kouagou, Axel-Cyrille Ngonga Ngomo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a new paradigm called explainable benchmarking, which automatically generates explanations for the performance of systems in a benchmark. It demonstrates the paradigm on knowledge-graph-based question answering by introducing a novel concept learning method, PruneCEL, which outperforms existing learners and enables users to predict system behavior from the generated explanations.", "summary_cn": "本文提出了“可解释基准评估”（explainable benchmarking）新范式，旨在自动生成系统在基准测试中的表现解释。作者在基于知识图谱的问答任务上实现该范式，构建了新概念学习方法 PruneCEL ，其在解释性基准任务上比现有方法提升显著，且用户能够通过解释预测系统行为。", "keywords": "explainable benchmarking, concept learning, knowledge graph, question answering, PruneCEL, interpretability, performance explanation, evaluation metrics", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Quannian Zhang", "Michael Röder", "Nikit Srivastava", "N'Dah Jean Kouagou", "Axel-Cyrille Ngonga Ngomo"]}
]]></acme>

<pubDate>2025-10-23T11:20:20+00:00</pubDate>
</item>
<item>
<title>An Empirical Study of Sample Selection Strategies for Large Language Model Repair</title>
<link>https://papers.cool/arxiv/2510.20428</link>
<guid>https://papers.cool/arxiv/2510.20428</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates sample selection strategies for post‑hoc repair of large language models to mitigate toxic or biased outputs. It compares random, K‑Center, GraNd, CCS, a proposed Semantic‑Aware Prioritized Sampling (SAPS) across metrics such as toxicity reduction, language modeling perplexity, and composite repair scores, finding SAPS offers the best trade‑off between detoxification and utility preservation while using less data. The study highlights that sample selection is a tunable component that can improve the efficiency and scalability of LLM safety interventions.<br /><strong>Summary (CN):</strong> 本文系统性地评估了用于大语言模型事后修复的样本选择策略，以降低有害或偏见输出。研究比较了随机、K‑Center、GraNd、CCS 与作者提出的语义感知优先采样（SAPS），通过毒性降低、语言模型困惑度以及综合修复评分等指标，发现 SAPS 在保持效用的同时实现了最佳的戒毒与效率平衡。调查表明，样本应视为修复流程的可调组件，可提升 LLM 安全性的效率与可扩展性。<br /><strong>Keywords:</strong> sample selection, LLM repair, toxicity reduction, post-hoc alignment, SAPS, K-Center, GraNd, CCS, repair efficiency, model safety<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Xuran Li, Jingyi Wang</div>
Large language models (LLMs) are increasingly deployed in real-world systems, yet they can produce toxic or biased outputs that undermine safety and trust. Post-hoc model repair provides a practical remedy, but the high cost of parameter updates motivates selective use of repair data. Despite extensive prior work on data selection for model training, it remains unclear which sampling criteria are most effective and efficient when applied specifically to behavioral repair of large generative models. Our study presents a systematic analysis of sample prioritization strategies for LLM repair. We evaluate five representative selection methods, including random sampling, K-Center, gradient-norm-based selection(GraNd), stratified coverage (CCS), and a Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair effectiveness and trade-offs are assessed through toxicity reduction, perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair Efficiency Score (RES). Experimental results show that SAPS achieves the best balance between detoxification, utility preservation, and efficiency, delivering comparable or superior repair outcomes with substantially less data. Random sampling remains effective for large or robust models, while high-overhead methods such as CCS and GraNd provide limited benefit. The optimal data proportion depends on model scale and repair method, indicating that sample selection should be regarded as a tunable component of repair pipelines. Overall, these findings establish selection-based repair as an efficient and scalable paradigm for maintaining LLM reliability.
<div><strong>Authors:</strong> Xuran Li, Jingyi Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates sample selection strategies for post‑hoc repair of large language models to mitigate toxic or biased outputs. It compares random, K‑Center, GraNd, CCS, a proposed Semantic‑Aware Prioritized Sampling (SAPS) across metrics such as toxicity reduction, language modeling perplexity, and composite repair scores, finding SAPS offers the best trade‑off between detoxification and utility preservation while using less data. The study highlights that sample selection is a tunable component that can improve the efficiency and scalability of LLM safety interventions.", "summary_cn": "本文系统性地评估了用于大语言模型事后修复的样本选择策略，以降低有害或偏见输出。研究比较了随机、K‑Center、GraNd、CCS 与作者提出的语义感知优先采样（SAPS），通过毒性降低、语言模型困惑度以及综合修复评分等指标，发现 SAPS 在保持效用的同时实现了最佳的戒毒与效率平衡。调查表明，样本应视为修复流程的可调组件，可提升 LLM 安全性的效率与可扩展性。", "keywords": "sample selection, LLM repair, toxicity reduction, post-hoc alignment, SAPS, K-Center, GraNd, CCS, repair efficiency, model safety", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xuran Li", "Jingyi Wang"]}
]]></acme>

<pubDate>2025-10-23T11:02:39+00:00</pubDate>
</item>
<item>
<title>Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes</title>
<link>https://papers.cool/arxiv/2510.20414</link>
<guid>https://papers.cool/arxiv/2510.20414</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a thresholding method that learns thresholds to adjust mark probabilities normalized by prior frequencies, addressing severe mark imbalance in marked temporal point processes. It also proposes an integration‑free neural MTPP architecture that predicts the mark before the time, enabling efficient probability estimation without costly numerical integration. Experiments on real‑world datasets show the approach outperforms existing baselines in both mark and time prediction.<br /><strong>Summary (CN):</strong> 本文提出一种阈值学习方法，通过对标记概率进行先验归一化的阈值调整，以缓解标记时序点过程中的标记不平衡问题。与此同时，设计了无需数值积分的神经 MTPP 模型，先预测标记再预测时间，从而实现高效的概率估计。实验证明该方案在真实数据集上相较于多种基线在标记及时间预测上均有显著提升。<br /><strong>Keywords:</strong> marked temporal point processes, mark imbalance, thresholding, integration-free neural MTPP, event prediction, time sampling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren, Yan Wang</div>
Marked Temporal Point Process (MTPP) has been well studied to model the event distribution in marked event streams, which can be used to predict the mark and arrival time of the next event. However, existing studies overlook that the distribution of event marks is highly imbalanced in many real-world applications, with some marks being frequent but others rare. The imbalance poses a significant challenge to the performance of the next event prediction, especially for events of rare marks. To address this issue, we propose a thresholding method, which learns thresholds to tune the mark probability normalized by the mark's prior probability to optimize mark prediction, rather than predicting the mark directly based on the mark probability as in existing studies. In conjunction with this method, we predict the mark first and then the time. In particular, we develop a novel neural MTPP model to support effective time sampling and estimation of mark probability without computationally expensive numerical improper integration. Extensive experiments on real-world datasets demonstrate the superior performance of our solution against various baselines for the next event mark and time prediction. The code is available at https://github.com/undes1red/IFNMTPP.
<div><strong>Authors:</strong> Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren, Yan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a thresholding method that learns thresholds to adjust mark probabilities normalized by prior frequencies, addressing severe mark imbalance in marked temporal point processes. It also proposes an integration‑free neural MTPP architecture that predicts the mark before the time, enabling efficient probability estimation without costly numerical integration. Experiments on real‑world datasets show the approach outperforms existing baselines in both mark and time prediction.", "summary_cn": "本文提出一种阈值学习方法，通过对标记概率进行先验归一化的阈值调整，以缓解标记时序点过程中的标记不平衡问题。与此同时，设计了无需数值积分的神经 MTPP 模型，先预测标记再预测时间，从而实现高效的概率估计。实验证明该方案在真实数据集上相较于多种基线在标记及时间预测上均有显著提升。", "keywords": "marked temporal point processes, mark imbalance, thresholding, integration-free neural MTPP, event prediction, time sampling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sishun Liu", "Ke Deng", "Xiuzhen Zhang", "Yongli Ren", "Yan Wang"]}
]]></acme>

<pubDate>2025-10-23T10:34:35+00:00</pubDate>
</item>
<item>
<title>Why DPO is a Misspecified Estimator and How to Fix It</title>
<link>https://papers.cool/arxiv/2510.20413</link>
<guid>https://papers.cool/arxiv/2510.20413</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that Direct Preference Optimization (DPO) constitutes a misspecified estimator when the true reward function cannot be represented within the policy class, leading to preference order reversal and sensitivity issues. By analyzing the local geometry of two‑stage RLHF, the authors derive a natural‑gradient interpretation and propose AuxDPO, which adds auxiliary variables to the loss to better approximate the RLHF solution. Empirical results on bandit simulations and LLM alignment tasks demonstrate that AuxDPO mitigates DPO’s misspecification and improves performance.<br /><strong>Summary (CN):</strong> 本文指出，当真实奖励函数无法在所使用的策略类中实现时，直接偏好优化（DPO）会成为一种模型错配的估计方法，导致偏好顺序逆转、奖励下降以及对偏好数据分布的高度敏感。通过对两阶段RLHF在参数空间的局部几何行为进行分析，作者将其关联到策略空间的自然梯度步骤，并据此提出 AuxDPO——在 DPO 损失中引入辅助变量以在原理上逼近 RLHF 解并缓解错配问题。在带娃乐队实验及大型语言模型对齐任务中的实验表明，AuxDPO 在性能上优于原始 DPO。<br /><strong>Keywords:</strong> Direct Preference Optimization, RLHF, alignment, misspecification, AuxDPO, natural gradient, policy optimization, LLM alignment, bandit experiments<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Aditya Gopalan, Sayak Ray Chowdhury, Debangshu Banerjee</div>
Direct alignment algorithms such as Direct Preference Optimization (DPO) fine-tune models based on preference data, using only supervised learning instead of two-stage reinforcement learning with human feedback (RLHF). We show that DPO encodes a statistical estimation problem over reward functions induced by a parametric policy class. When the true reward function that generates preferences cannot be realized via the policy class, DPO becomes misspecified, resulting in failure modes such as preference order reversal, worsening of policy reward, and high sensitivity to the input preference data distribution. On the other hand, we study the local behavior of two-stage RLHF for a parametric class and relate it to a natural gradient step in policy space. Our fine-grained geometric characterization allows us to propose AuxDPO, which introduces additional auxiliary variables in the DPO loss function to help move towards the RLHF solution in a principled manner and mitigate the misspecification in DPO. We empirically demonstrate the superior performance of AuxDPO on didactic bandit settings as well as LLM alignment tasks.
<div><strong>Authors:</strong> Aditya Gopalan, Sayak Ray Chowdhury, Debangshu Banerjee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that Direct Preference Optimization (DPO) constitutes a misspecified estimator when the true reward function cannot be represented within the policy class, leading to preference order reversal and sensitivity issues. By analyzing the local geometry of two‑stage RLHF, the authors derive a natural‑gradient interpretation and propose AuxDPO, which adds auxiliary variables to the loss to better approximate the RLHF solution. Empirical results on bandit simulations and LLM alignment tasks demonstrate that AuxDPO mitigates DPO’s misspecification and improves performance.", "summary_cn": "本文指出，当真实奖励函数无法在所使用的策略类中实现时，直接偏好优化（DPO）会成为一种模型错配的估计方法，导致偏好顺序逆转、奖励下降以及对偏好数据分布的高度敏感。通过对两阶段RLHF在参数空间的局部几何行为进行分析，作者将其关联到策略空间的自然梯度步骤，并据此提出 AuxDPO——在 DPO 损失中引入辅助变量以在原理上逼近 RLHF 解并缓解错配问题。在带娃乐队实验及大型语言模型对齐任务中的实验表明，AuxDPO 在性能上优于原始 DPO。", "keywords": "Direct Preference Optimization, RLHF, alignment, misspecification, AuxDPO, natural gradient, policy optimization, LLM alignment, bandit experiments", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Aditya Gopalan", "Sayak Ray Chowdhury", "Debangshu Banerjee"]}
]]></acme>

<pubDate>2025-10-23T10:30:29+00:00</pubDate>
</item>
<item>
<title>Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control</title>
<link>https://papers.cool/arxiv/2510.20408</link>
<guid>https://papers.cool/arxiv/2510.20408</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a new benchmark that combines SortingEnv and ContainerGym into a sequential recycling scenario, enabling study of modular versus monolithic agents for industrial control. Experiments reveal that action masking dramatically improves learning for both architectures, reducing the performance advantage of specialized agents. The results emphasize the importance of action space constraints and provide a testbed for robust multi‑agent RL solutions in real‑world automation.<br /><strong>Summary (CN):</strong> 本文提出了一个将 SortingEnv 与 ContainerGym 合并的顺序回收基准，用于研究工业控制中的模块化代理与单体代理。实验表明，动作掩码显著提升了两类架构的学习效果，并缩小了专门化代理的性能优势。结果凸显了动作空间约束的重要性，为在实际自动化场景中探索稳健的多代理 RL 解法提供了测试平台。<br /><strong>Keywords:</strong> multi-agent reinforcement learning, industrial control, modular architecture, action masking, benchmark, specialization, centralization, sequential recycling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Tom Maus, Asma Atamna, Tobias Glasmachers</div>
Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.
<div><strong>Authors:</strong> Tom Maus, Asma Atamna, Tobias Glasmachers</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a new benchmark that combines SortingEnv and ContainerGym into a sequential recycling scenario, enabling study of modular versus monolithic agents for industrial control. Experiments reveal that action masking dramatically improves learning for both architectures, reducing the performance advantage of specialized agents. The results emphasize the importance of action space constraints and provide a testbed for robust multi‑agent RL solutions in real‑world automation.", "summary_cn": "本文提出了一个将 SortingEnv 与 ContainerGym 合并的顺序回收基准，用于研究工业控制中的模块化代理与单体代理。实验表明，动作掩码显著提升了两类架构的学习效果，并缩小了专门化代理的性能优势。结果凸显了动作空间约束的重要性，为在实际自动化场景中探索稳健的多代理 RL 解法提供了测试平台。", "keywords": "multi-agent reinforcement learning, industrial control, modular architecture, action masking, benchmark, specialization, centralization, sequential recycling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"]}
]]></acme>

<pubDate>2025-10-23T10:21:54+00:00</pubDate>
</item>
<item>
<title>Relative-Based Scaling Law for Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.20387</link>
<guid>https://papers.cool/arxiv/2510.20387</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Relative-Based Probability (RBP) metric that measures how often the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Extensive experiments across four datasets and model families validate the law's robustness and accuracy, and the authors demonstrate applications such as explaining emergence phenomena aiding the search for fundamental scaling theories.<br /><strong>Summary (CN):</strong> 本文提出相对概率（Relative-Based Probability, RBP）指标，用于衡量正确词在预测中排名靠前的概率，并基于该指标建立了相对尺度定律，描述模型规模增大时 RBP 的提升规律。通过在四个数据集和四类模型家族上进行的大规模实验验证了该定律的稳健性与准确性，并展示了其在解释模型涌现现象和寻找尺度定律基础理论等方面的广泛应用。<br /><strong>Keywords:</strong> relative-based probability, scaling law, language models, emergence, evaluation metric, cross-entropy alternative<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
Scaling laws aim to accurately predict model performance across different scales. Existing scaling-law studies almost exclusively rely on cross-entropy as the evaluation metric. However, cross-entropy provides only a partial view of performance: it measures the absolute probability assigned to the correct token, but ignores the relative ordering between correct and incorrect tokens. Yet, relative ordering is crucial for language models, such as in greedy-sampling scenario. To address this limitation, we investigate scaling from the perspective of relative ordering. We first propose the Relative-Based Probability (RBP) metric, which quantifies the probability that the correct token is ranked among the top predictions. Building on this metric, we establish the Relative-Based Scaling Law, which characterizes how RBP improves with increasing model size. Through extensive experiments on four datasets and four model families spanning five orders of magnitude, we demonstrate the robustness and accuracy of this law. Finally, we illustrate the broad application of this law with two examples, namely providing a deeper explanation of emergence phenomena and facilitating finding fundamental theories of scaling laws. In summary, the Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models. Thus, it offers valuable insights for both practical development and theoretical exploration.
<div><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Relative-Based Probability (RBP) metric that measures how often the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Extensive experiments across four datasets and model families validate the law's robustness and accuracy, and the authors demonstrate applications such as explaining emergence phenomena aiding the search for fundamental scaling theories.", "summary_cn": "本文提出相对概率（Relative-Based Probability, RBP）指标，用于衡量正确词在预测中排名靠前的概率，并基于该指标建立了相对尺度定律，描述模型规模增大时 RBP 的提升规律。通过在四个数据集和四类模型家族上进行的大规模实验验证了该定律的稳健性与准确性，并展示了其在解释模型涌现现象和寻找尺度定律基础理论等方面的广泛应用。", "keywords": "relative-based probability, scaling law, language models, emergence, evaluation metric, cross-entropy alternative", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baoqing Yue", "Jinyuan Zhou", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"]}
]]></acme>

<pubDate>2025-10-23T09:37:00+00:00</pubDate>
</item>
<item>
<title>Hierarchical Time Series Forecasting with Robust Reconciliation</title>
<link>https://papers.cool/arxiv/2510.20383</link>
<guid>https://papers.cool/arxiv/2510.20383</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes a robust optimization framework for hierarchical time‑series forecasting that accounts for uncertainty in the estimated covariance matrix during reconciliation, formulating the problem as a semidefinite program and showing improved forecast accuracy over standard methods.<br /><strong>Summary (CN):</strong> 本文提出一种鲁棒优化框架，用于层级时间序列预测，在协方差矩阵估计不确定性下进行预测值的调和，模型被转化为半正定规划并显示出相较传统方法更好的预测性能。<br /><strong>Keywords:</strong> hierarchical forecasting, time series reconciliation, robust optimization, covariance uncertainty, semidefinite programming<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shuhei Aikawa, Aru Suzuki, Kei Yoshitake, Kanata Teshigawara, Akira Iwabuchi, Ken Kobayashi, Kazuhide Nakata</div>
This paper focuses on forecasting hierarchical time-series data, where each higher-level observation equals the sum of its corresponding lower-level time series. In such contexts, the forecast values should be coherent, meaning that the forecast value of each parent series exactly matches the sum of the forecast values of its child series. Existing hierarchical forecasting methods typically generate base forecasts independently for each series and then apply a reconciliation procedure to adjust them so that the resulting forecast values are coherent across the hierarchy. These methods generally derive an optimal reconciliation, using a covariance matrix of the forecast error. In practice, however, the true covariance matrix is unknown and has to be estimated from finite samples in advance. This gap between the true and estimated covariance matrix may degrade forecast performance. To address this issue, we propose a robust optimization framework for hierarchical reconciliation that accounts for uncertainty in the estimated covariance matrix. We first introduce an uncertainty set for the estimated covariance matrix and formulate a reconciliation problem that minimizes the worst-case expected squared error over this uncertainty set. We show that our problem can be cast as a semidefinite optimization problem. Numerical experiments demonstrate that the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods, which indicates the effectiveness of integrating uncertainty into the reconciliation process.
<div><strong>Authors:</strong> Shuhei Aikawa, Aru Suzuki, Kei Yoshitake, Kanata Teshigawara, Akira Iwabuchi, Ken Kobayashi, Kazuhide Nakata</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes a robust optimization framework for hierarchical time‑series forecasting that accounts for uncertainty in the estimated covariance matrix during reconciliation, formulating the problem as a semidefinite program and showing improved forecast accuracy over standard methods.", "summary_cn": "本文提出一种鲁棒优化框架，用于层级时间序列预测，在协方差矩阵估计不确定性下进行预测值的调和，模型被转化为半正定规划并显示出相较传统方法更好的预测性能。", "keywords": "hierarchical forecasting, time series reconciliation, robust optimization, covariance uncertainty, semidefinite programming", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuhei Aikawa", "Aru Suzuki", "Kei Yoshitake", "Kanata Teshigawara", "Akira Iwabuchi", "Ken Kobayashi", "Kazuhide Nakata"]}
]]></acme>

<pubDate>2025-10-23T09:30:53+00:00</pubDate>
</item>
<item>
<title>Ask a Strong LLM Judge when Your Reward Model is Uncertain</title>
<link>https://papers.cool/arxiv/2510.20369</link>
<guid>https://papers.cool/arxiv/2510.20369</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an uncertainty‑based routing framework that combines a fast reward model with a strong but costly LLM judge for reinforcement learning from human feedback. By quantifying uncertainty in pairwise preference classifications, uncertain instances are forwarded to the LLM judge while confident ones are evaluated by the reward model, improving performance and reducing reward hacking at comparable inference cost. Experiments on reward‑model benchmarks and downstream online RLHF tasks show the method outperforms random judge calling.<br /><strong>Summary (CN):</strong> 本文提出一种基于不确定性的路由框架，将快速奖励模型（reward model）与性能更强但计算开销更大的 LLM 判官（LLM judge）相结合用于 RLHF。通过对成对偏好分类（pairwise preference classification）进行不确定度量，对不确定的样本交给 LLM 判官处理，确定的样本则由奖励模型评估，从而在相同推理成本下提升效果并降低奖励 hacking。实验在奖励模型基准和在线 RLHF 任务上显示，该方法优于随机调用判官的策略。<br /><strong>Keywords:</strong> reward model, RLHF, LLM judge, uncertainty routing, pairwise preference classification, alignment, reward hacking<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Zhenghao Xu, Qin Lu, Qingru Zhang, Liang Qiu, Ilgee Hong, Changlong Yu, Wenlin Yao, Yao Liu, Haoming Jiang, Lihong Li, Hyokun Yun, Tuo Zhao</div>
Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. In this work, we propose an uncertainty-based routing framework that efficiently complements a fast RM with a strong but costly LLM judge. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results showcase its effectiveness in improving online RLHF.
<div><strong>Authors:</strong> Zhenghao Xu, Qin Lu, Qingru Zhang, Liang Qiu, Ilgee Hong, Changlong Yu, Wenlin Yao, Yao Liu, Haoming Jiang, Lihong Li, Hyokun Yun, Tuo Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an uncertainty‑based routing framework that combines a fast reward model with a strong but costly LLM judge for reinforcement learning from human feedback. By quantifying uncertainty in pairwise preference classifications, uncertain instances are forwarded to the LLM judge while confident ones are evaluated by the reward model, improving performance and reducing reward hacking at comparable inference cost. Experiments on reward‑model benchmarks and downstream online RLHF tasks show the method outperforms random judge calling.", "summary_cn": "本文提出一种基于不确定性的路由框架，将快速奖励模型（reward model）与性能更强但计算开销更大的 LLM 判官（LLM judge）相结合用于 RLHF。通过对成对偏好分类（pairwise preference classification）进行不确定度量，对不确定的样本交给 LLM 判官处理，确定的样本则由奖励模型评估，从而在相同推理成本下提升效果并降低奖励 hacking。实验在奖励模型基准和在线 RLHF 任务上显示，该方法优于随机调用判官的策略。", "keywords": "reward model, RLHF, LLM judge, uncertainty routing, pairwise preference classification, alignment, reward hacking", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zhenghao Xu", "Qin Lu", "Qingru Zhang", "Liang Qiu", "Ilgee Hong", "Changlong Yu", "Wenlin Yao", "Yao Liu", "Haoming Jiang", "Lihong Li", "Hyokun Yun", "Tuo Zhao"]}
]]></acme>

<pubDate>2025-10-23T09:09:13+00:00</pubDate>
</item>
<item>
<title>Synthetic Data for Robust Runway Detection</title>
<link>https://papers.cool/arxiv/2510.20349</link>
<guid>https://papers.cool/arxiv/2510.20349</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes using a commercial flight simulator to generate synthetic images that supplement a small set of annotated real runway images, enabling standard object detection models to achieve accurate runway detection for autonomous landing. By controlling the generation process and combining real and synthetic data, the authors demonstrate robustness to adverse conditions such as nighttime, which are absent from the real dataset, and show benefits of a customized domain adaptation strategy.<br /><strong>Summary (CN):</strong> 本文利用商业飞行模拟器生成合成图像，补充少量已标注的真实跑道图像，从而使标准目标检测模型能够实现对自主着陆系统中跑道的准确检测。通过控制图像生成并融合真实与合成数据，作者展示了模型在未在真实数据中出现的夜间等不利条件下的鲁棒性，并证明了定制化领域适应策略的有效性。<br /><strong>Keywords:</strong> synthetic data, runway detection, domain adaptation, object detection, autonomous landing, robustness, flight simulator<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, Thomas Oberlin</div>
Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.
<div><strong>Authors:</strong> Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, Thomas Oberlin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes using a commercial flight simulator to generate synthetic images that supplement a small set of annotated real runway images, enabling standard object detection models to achieve accurate runway detection for autonomous landing. By controlling the generation process and combining real and synthetic data, the authors demonstrate robustness to adverse conditions such as nighttime, which are absent from the real dataset, and show benefits of a customized domain adaptation strategy.", "summary_cn": "本文利用商业飞行模拟器生成合成图像，补充少量已标注的真实跑道图像，从而使标准目标检测模型能够实现对自主着陆系统中跑道的准确检测。通过控制图像生成并融合真实与合成数据，作者展示了模型在未在真实数据中出现的夜间等不利条件下的鲁棒性，并证明了定制化领域适应策略的有效性。", "keywords": "synthetic data, runway detection, domain adaptation, object detection, autonomous landing, robustness, flight simulator", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Fabrice Jimenez", "Thomas Oberlin"]}
]]></acme>

<pubDate>2025-10-23T08:48:37+00:00</pubDate>
</item>
<item>
<title>LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems</title>
<link>https://papers.cool/arxiv/2510.20327</link>
<guid>https://papers.cool/arxiv/2510.20327</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces LEGO, a lightweight framework for simultaneously unlearning multiple sensitive attributes in recommender systems. It separates the process into Embedding Calibration, which removes attribute-specific information from user embeddings, and Flexible Combination, which merges calibrated embeddings to protect all attributes, formulated as a mutual information minimization problem. Experiments on three datasets and recommendation models demonstrate that LEGO achieves effective and efficient multi-attribute unlearning.<br /><strong>Summary (CN):</strong> 本文提出 LEGO 框架，实现推荐系统中对多个敏感属性的同时消除。通过 Embedding Calibration 步骤去除用户嵌入中特定属性的信息，再利用 Flexible Combination 将校准后的嵌入合并，从而保护所有敏感属性，整体过程被建模为互信息最小化问题。实验在三个真实数据集和三种推荐模型上验证了该方法的有效性和高效性。<br /><strong>Keywords:</strong> recommendation systems, attribute unlearning, privacy, mutual information minimization, embedding calibration, multi-attribute unlearning, dynamic unlearning, efficiency<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Fengyuan Yu, Yuyuan Li, Xiaohua Feng, Junjie Fang, Tao Wang, Chaochao Chen</div>
With the growing demand for safeguarding sensitive user information in recommender systems, recommendation attribute unlearning is receiving increasing attention. Existing studies predominantly focus on single-attribute unlearning. However, privacy protection requirements in the real world often involve multiple sensitive attributes and are dynamic. Existing single-attribute unlearning methods cannot meet these real-world requirements due to i) CH1: the inability to handle multiple unlearning requests simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic unlearning needs. To address these challenges, we propose LEGO, a lightweight and efficient multiple-attribute unlearning framework. Specifically, we divide the multiple-attribute unlearning process into two steps: i) Embedding Calibration removes information related to a specific attribute from user embedding, and ii) Flexible Combination combines these embeddings into a single embedding, protecting all sensitive attributes. We frame the unlearning process as a mutual information minimization problem, providing LEGO a theoretical guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step framework, where Embedding Calibration can be performed in parallel and Flexible Combination is flexible and efficient, we address CH2. Extensive experiments on three real-world datasets across three representative recommendation models demonstrate the effectiveness and efficiency of our proposed framework. Our code and appendix are available at https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.
<div><strong>Authors:</strong> Fengyuan Yu, Yuyuan Li, Xiaohua Feng, Junjie Fang, Tao Wang, Chaochao Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces LEGO, a lightweight framework for simultaneously unlearning multiple sensitive attributes in recommender systems. It separates the process into Embedding Calibration, which removes attribute-specific information from user embeddings, and Flexible Combination, which merges calibrated embeddings to protect all attributes, formulated as a mutual information minimization problem. Experiments on three datasets and recommendation models demonstrate that LEGO achieves effective and efficient multi-attribute unlearning.", "summary_cn": "本文提出 LEGO 框架，实现推荐系统中对多个敏感属性的同时消除。通过 Embedding Calibration 步骤去除用户嵌入中特定属性的信息，再利用 Flexible Combination 将校准后的嵌入合并，从而保护所有敏感属性，整体过程被建模为互信息最小化问题。实验在三个真实数据集和三种推荐模型上验证了该方法的有效性和高效性。", "keywords": "recommendation systems, attribute unlearning, privacy, mutual information minimization, embedding calibration, multi-attribute unlearning, dynamic unlearning, efficiency", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Fengyuan Yu", "Yuyuan Li", "Xiaohua Feng", "Junjie Fang", "Tao Wang", "Chaochao Chen"]}
]]></acme>

<pubDate>2025-10-23T08:20:47+00:00</pubDate>
</item>
<item>
<title>InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</title>
<link>https://papers.cool/arxiv/2510.20302</link>
<guid>https://papers.cool/arxiv/2510.20302</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> InvDec introduces an inverted decoder architecture that separates temporal encoding from variate-level decoding for multivariate time series forecasting, using a patch-based temporal encoder and variate-wise self‑attention with delayed variate embeddings. The method achieves significant accuracy improvements on high‑dimensional benchmarks while retaining competitive performance on low‑dimensional datasets. Ablation studies confirm the contribution of each component and show that benefits increase with the number of variables.<br /><strong>Summary (CN):</strong> InvDec 提出一种倒置解码器结构，将时间编码与变量层面解码分离，使用基于 Patch 的时间编码器和变量自注意力，并在时间编码后加入延迟变量嵌入，以保留时间特征完整性。该方法在高维数据集上显著提升预测精度，同时在低维数据集上保持竞争性能。消融实验验证了各组件的贡献，并表明随着变量数量增加，InvDec 的优势愈加明显。<br /><strong>Keywords:</strong> multivariate time series, forecasting, temporal encoder, variate attention, inverted decoder, PatchTST, delayed variate embeddings, adaptive residual fusion<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuhang Wang</div>
Multivariate time series forecasting requires simultaneously modeling temporal patterns and cross-variate dependencies. Channel-independent methods such as PatchTST excel at temporal modeling but ignore variable correlations, while pure variate-attention approaches such as iTransformer sacrifice temporal encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that achieves principled separation between temporal encoding and variate-level decoding. InvDec combines a patch-based temporal encoder with an inverted decoder operating on the variate dimension through variate-wise self-attention. We introduce delayed variate embeddings that enrich variable-specific representations only after temporal encoding, preserving temporal feature integrity. An adaptive residual fusion mechanism dynamically balances temporal and variate information across datasets of varying dimensions. Instantiating InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven benchmarks demonstrate significant gains on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintaining competitive performance on low-dimensional ETT datasets. Ablation studies validate each component, and analysis reveals that InvDec's advantage grows with dataset dimensionality, confirming that cross-variate modeling becomes critical as the number of variables increases.
<div><strong>Authors:</strong> Yuhang Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "InvDec introduces an inverted decoder architecture that separates temporal encoding from variate-level decoding for multivariate time series forecasting, using a patch-based temporal encoder and variate-wise self‑attention with delayed variate embeddings. The method achieves significant accuracy improvements on high‑dimensional benchmarks while retaining competitive performance on low‑dimensional datasets. Ablation studies confirm the contribution of each component and show that benefits increase with the number of variables.", "summary_cn": "InvDec 提出一种倒置解码器结构，将时间编码与变量层面解码分离，使用基于 Patch 的时间编码器和变量自注意力，并在时间编码后加入延迟变量嵌入，以保留时间特征完整性。该方法在高维数据集上显著提升预测精度，同时在低维数据集上保持竞争性能。消融实验验证了各组件的贡献，并表明随着变量数量增加，InvDec 的优势愈加明显。", "keywords": "multivariate time series, forecasting, temporal encoder, variate attention, inverted decoder, PatchTST, delayed variate embeddings, adaptive residual fusion", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhang Wang"]}
]]></acme>

<pubDate>2025-10-23T07:42:01+00:00</pubDate>
</item>
<item>
<title>DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability</title>
<link>https://papers.cool/arxiv/2510.20299</link>
<guid>https://papers.cool/arxiv/2510.20299</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes DB-FGA-Net, a dual‑backbone network that combines VGG16 and Xception with a Frequency‑Gated Attention block for multi‑class brain tumor classification. It achieves state‑of‑the‑art accuracy on several datasets without data augmentation and integrates Grad‑CAM visualizations and a real‑ GUI to provide clinically interpretable predictions.<br /><strong>Summary (CN):</strong> 本文提出 DB-FGA-Net，一种结合 VGG16 与 Xception 双主干并加入频率门控注意力 (Frequency‑Gated Attention) 模块的网络，用于脑肿瘤分类。该模型在无需数据增强的情况下在多个数据集上实现了近乎最佳的准确率，并集成 Grad‑CAM 可视化及实时 GUI，使诊断结果具备临床可解释性。<br /><strong>Keywords:</strong> brain tumor classification, dual backbone, frequency gated attention, Grad-CAM, interpretability, medical imaging, VGG16, Xception, GUI, augmentation-free<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
Brain tumors are a challenging problem in neuro-oncology, where early and precise diagnosis is important for successful treatment. Deep learning-based brain tumor classification methods often rely on heavy data augmentation which can limit generalization and trust in clinical applications. In this paper, we propose a double-backbone network integrating VGG16 and Xception with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Unlike previous studies, our model achieves state-of-the-art performance without augmentation which demonstrates robustness to variably sized and distributed datasets. For further transparency, Grad-CAM is integrated to visualize the tumor regions based on which the model is giving prediction, bridging the gap between model prediction and clinical interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class and 2-class settings, respectively. On the independent 3K-DS dataset, the model generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art methods. To further support clinical usability, we developed a graphical user interface (GUI) that provides real-time classification and Grad-CAM-based tumor localization. These findings suggest that augmentation-free, interpretable, and deployable deep learning models such as DB-FGA-Net hold strong potential for reliable clinical translation in brain tumor diagnosis.
<div><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes DB-FGA-Net, a dual‑backbone network that combines VGG16 and Xception with a Frequency‑Gated Attention block for multi‑class brain tumor classification. It achieves state‑of‑the‑art accuracy on several datasets without data augmentation and integrates Grad‑CAM visualizations and a real‑ GUI to provide clinically interpretable predictions.", "summary_cn": "本文提出 DB-FGA-Net，一种结合 VGG16 与 Xception 双主干并加入频率门控注意力 (Frequency‑Gated Attention) 模块的网络，用于脑肿瘤分类。该模型在无需数据增强的情况下在多个数据集上实现了近乎最佳的准确率，并集成 Grad‑CAM 可视化及实时 GUI，使诊断结果具备临床可解释性。", "keywords": "brain tumor classification, dual backbone, frequency gated attention, Grad-CAM, interpretability, medical imaging, VGG16, Xception, GUI, augmentation-free", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"]}
]]></acme>

<pubDate>2025-10-23T07:39:00+00:00</pubDate>
</item>
<item>
<title>Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization</title>
<link>https://papers.cool/arxiv/2510.20295</link>
<guid>https://papers.cool/arxiv/2510.20295</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an IRM-free approach for out-of-distribution generalization in graph neural networks by quantifying distributional invariance of causal subgraphs. It introduces the Invariant Distribution Criterion, proves a quantitative link between distributional shift and representation norm, and uses a norm-guided objective to discover causal subgraphs, achieving state-of-the-art performance on benchmark datasets.<br /><strong>Summary (CN):</strong> 本文提出一种无需 IRM 的图神经网络分布外泛化方法，通过量化因果子图的分布不变性来实现。作者提出“不变分布准则”，并证明了分布漂移与表示范数之间的定量关系，进而利用范数引导的目标函数发现因果子图，在多个基准数据集上实现了领先的性能。<br /><strong>Keywords:</strong> causal subgraph, invariant distribution criterion, graph neural networks, OOD generalization, IRM-free, representation norm, graph robustness<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li</div>
Out-of-distribution generalization under distributional shifts remains a critical challenge for graph neural networks. Existing methods generally adopt the Invariant Risk Minimization (IRM) framework, requiring costly environment annotations or heuristically generated synthetic splits. To circumvent these limitations, in this work, we aim to develop an IRM-free method for capturing causal subgraphs. We first identify that causal subgraphs exhibit substantially smaller distributional variations than non-causal components across diverse environments, which we formalize as the Invariant Distribution Criterion and theoretically prove in this paper. Building on this criterion, we systematically uncover the quantitative relationship between distributional shift and representation norm for identifying the causal subgraph, and investigate its underlying mechanisms in depth. Finally, we propose an IRM-free method by introducing a norm-guided invariant distribution objective for causal subgraph discovery and prediction. Extensive experiments on two widely used benchmarks demonstrate that our method consistently outperforms state-of-the-art methods in graph generalization.
<div><strong>Authors:</strong> Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an IRM-free approach for out-of-distribution generalization in graph neural networks by quantifying distributional invariance of causal subgraphs. It introduces the Invariant Distribution Criterion, proves a quantitative link between distributional shift and representation norm, and uses a norm-guided objective to discover causal subgraphs, achieving state-of-the-art performance on benchmark datasets.", "summary_cn": "本文提出一种无需 IRM 的图神经网络分布外泛化方法，通过量化因果子图的分布不变性来实现。作者提出“不变分布准则”，并证明了分布漂移与表示范数之间的定量关系，进而利用范数引导的目标函数发现因果子图，在多个基准数据集上实现了领先的性能。", "keywords": "causal subgraph, invariant distribution criterion, graph neural networks, OOD generalization, IRM-free, representation norm, graph robustness", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yang Qiu", "Yixiong Zou", "Jun Wang", "Wei Liu", "Xiangyu Fu", "Ruixuan Li"]}
]]></acme>

<pubDate>2025-10-23T07:34:50+00:00</pubDate>
</item>
<item>
<title>ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows</title>
<link>https://papers.cool/arxiv/2510.20279</link>
<guid>https://papers.cool/arxiv/2510.20279</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ResearchGPT, an envisioned AI collaborator that assists humans throughout the full computer‑science research cycle, and proposes a new benchmark suite (CS‑4k) and a large training corpus (CS‑50k) derived from 14k CC‑licensed papers. Using a retrieval‑augmented generation pipeline with multi‑stage quality control, the authors construct high‑quality scientific Q&amp;A pairs and demonstrate that models fine‑tuned on CS‑50k achieve substantial gains, with 7B‑scale open models surpassing larger proprietary systems. By releasing the datasets, the work aims to promote the development of reliable AI research assistants.<br /><strong>Summary (CN):</strong> 本文提出了 ResearchGPT——一种能够在整个计算机科学研究流程中协助人类的 AI 合作者，并推出了基于14k 篇 CC 授权论文构建的高质量科学问答数据集（CS‑4k）和大规模训练语料（CS‑50k）。通过检索增强生成（RAG）结合多阶段质量控制的可扩展管线，确保答案的事实依据，并展示了在该数据上微调的模型（即使是 7B 规模的开源模型）也能超越更大的商业系统。作者公开数据以推动可靠的 AI 研究助手的研发。<br /><strong>Keywords:</strong> LLM benchmarking, end-to-end research workflow, CS-54k dataset, retrieval-augmented generation, supervised fine-tuning, reinforcement learning, AI research assistant, computer science QA, domain-aligned training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Penghao Wang, Yuhao Zhou, Mengxuan Wu, Ziheng Qin, Bangyuan Zhu, Shengbin Huang, Xuanlei Zhao, Panpan Zhang, Xiaojiang Peng, Yuzhang Shang, Jianfei Yang, Zheng Zhu, Tianlong Chen, Zhangyang Wang, Kai Wang</div>
As large language models (LLMs) advance, the ultimate vision for their role in science is emerging: we could build an AI collaborator to effectively assist human beings throughout the entire scientific research process. We refer to this envisioned system as ResearchGPT. Given that scientific research progresses through multiple interdependent phases, achieving this vision requires rigorous benchmarks that evaluate the end-to-end workflow rather than isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of scientific Q&amp;A pairs in computer science, built from 14k CC-licensed papers. It is constructed through a scalable, paper-grounded pipeline that combines retrieval-augmented generation (RAG) with multi-stage quality control to ensure factual grounding. From this unified corpus, we derive two complementary subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to assist scientific research, and CS-50k, a large-scale training dataset. Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs into distinct capability tiers. Open models trained on CS-50k with supervised training and reinforcement learning demonstrate substantial improvements. Even 7B-scale models, when properly trained, outperform many larger proprietary systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that making AI models better research assistants relies more on domain-aligned training with high-quality data than on pretraining scale or general benchmark performance. We release CS-4k and CS-50k in the hope of fostering AI systems as reliable collaborators in CS research.
<div><strong>Authors:</strong> Penghao Wang, Yuhao Zhou, Mengxuan Wu, Ziheng Qin, Bangyuan Zhu, Shengbin Huang, Xuanlei Zhao, Panpan Zhang, Xiaojiang Peng, Yuzhang Shang, Jianfei Yang, Zheng Zhu, Tianlong Chen, Zhangyang Wang, Kai Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ResearchGPT, an envisioned AI collaborator that assists humans throughout the full computer‑science research cycle, and proposes a new benchmark suite (CS‑4k) and a large training corpus (CS‑50k) derived from 14k CC‑licensed papers. Using a retrieval‑augmented generation pipeline with multi‑stage quality control, the authors construct high‑quality scientific Q&A pairs and demonstrate that models fine‑tuned on CS‑50k achieve substantial gains, with 7B‑scale open models surpassing larger proprietary systems. By releasing the datasets, the work aims to promote the development of reliable AI research assistants.", "summary_cn": "本文提出了 ResearchGPT——一种能够在整个计算机科学研究流程中协助人类的 AI 合作者，并推出了基于14k 篇 CC 授权论文构建的高质量科学问答数据集（CS‑4k）和大规模训练语料（CS‑50k）。通过检索增强生成（RAG）结合多阶段质量控制的可扩展管线，确保答案的事实依据，并展示了在该数据上微调的模型（即使是 7B 规模的开源模型）也能超越更大的商业系统。作者公开数据以推动可靠的 AI 研究助手的研发。", "keywords": "LLM benchmarking, end-to-end research workflow, CS-54k dataset, retrieval-augmented generation, supervised fine-tuning, reinforcement learning, AI research assistant, computer science QA, domain-aligned training", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Penghao Wang", "Yuhao Zhou", "Mengxuan Wu", "Ziheng Qin", "Bangyuan Zhu", "Shengbin Huang", "Xuanlei Zhao", "Panpan Zhang", "Xiaojiang Peng", "Yuzhang Shang", "Jianfei Yang", "Zheng Zhu", "Tianlong Chen", "Zhangyang Wang", "Kai Wang"]}
]]></acme>

<pubDate>2025-10-23T07:07:35+00:00</pubDate>
</item>
<item>
<title>KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models</title>
<link>https://papers.cool/arxiv/2510.20278</link>
<guid>https://papers.cool/arxiv/2510.20278</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces KCM, a collaboration framework that uses a Kolmogorov-Arnold Network (KAN) as a small auxiliary model to assist pretrained large models across language, vision, and vision-language tasks. By leveraging KAN's higher visualizability and interpretability, KCM reduces the number of large‑model inference calls while preserving accuracy and mitigating catastrophic forgetting and hallucination issues that commonly arise with MLP‑based collaborators. Experiments show that KCM consistently outperforms MLP‑ small collaborative models in both efficiency and performance metrics.<br /><strong>Summary (CN):</strong> 本文提出 KCM——一种基于 Kolmogorov-Arnold Network (KAN) 的小模型协作框架，用于帮助预训练大模型在语言、视觉及跨模态任务中协同工作。相较于传统的 MLP 小模型，KAN 具有更强的可视化和可解释性，能够显著降低大模型调用次数，同时在保持准确率的情况下缓解灾难性遗忘和幻觉问题。实验表明，KCM 在效率和项性能指标上均优于基于 MLP 的小协作模型。<br /><strong>Keywords:</strong> KAN, Kolmogorov-Arnold Network, large-small model collaboration, catastrophic forgetting mitigation, model interpretability, hallucination reduction, cross-modal tasks, efficiency<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Guangyu Dai, Siliang Tang, Yueting Zhuang</div>
In recent years, Pretrained Large Models(PLMs) researchers proposed large-small model collaboration frameworks, leveraged easily trainable small models to assist large models, aim to(1) significantly reduce computational resource consumption while maintaining comparable accuracy, and (2) enhance large model performance in specialized domain tasks. However, this collaborative paradigm suffers from issues such as significant accuracy degradation, exacerbated catastrophic forgetting, and amplified hallucination problems induced by small model knowledge. To address these challenges, we propose a KAN-based Collaborative Model (KCM) as an improved approach to large-small model collaboration. The KAN utilized in KCM represents an alternative neural network architecture distinct from conventional MLPs. Compared to MLPs, KAN offers superior visualizability and interpretability while mitigating catastrophic forgetting. We deployed KCM in large-small model collaborative systems across three scenarios: language, vision, and vision-language cross-modal tasks. The experimental results demonstrate that, compared with pure large model approaches, the large-small model collaboration framework utilizing KCM as the collaborative model significantly reduces the number of large model inference calls while maintaining near-identical task accuracy, thereby substantially lowering computational resource consumption. Concurrently, the KAN-based small collaborative model markedly mitigates catastrophic forgetting, leading to significant accuracy improvements for long-tail data. The results reveal that KCM demonstrates superior performance across all metrics compared to MLP-based small collaborative models (MCM).
<div><strong>Authors:</strong> Guangyu Dai, Siliang Tang, Yueting Zhuang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces KCM, a collaboration framework that uses a Kolmogorov-Arnold Network (KAN) as a small auxiliary model to assist pretrained large models across language, vision, and vision-language tasks. By leveraging KAN's higher visualizability and interpretability, KCM reduces the number of large‑model inference calls while preserving accuracy and mitigating catastrophic forgetting and hallucination issues that commonly arise with MLP‑based collaborators. Experiments show that KCM consistently outperforms MLP‑ small collaborative models in both efficiency and performance metrics.", "summary_cn": "本文提出 KCM——一种基于 Kolmogorov-Arnold Network (KAN) 的小模型协作框架，用于帮助预训练大模型在语言、视觉及跨模态任务中协同工作。相较于传统的 MLP 小模型，KAN 具有更强的可视化和可解释性，能够显著降低大模型调用次数，同时在保持准确率的情况下缓解灾难性遗忘和幻觉问题。实验表明，KCM 在效率和项性能指标上均优于基于 MLP 的小协作模型。", "keywords": "KAN, Kolmogorov-Arnold Network, large-small model collaboration, catastrophic forgetting mitigation, model interpretability, hallucination reduction, cross-modal tasks, efficiency", "scoring": {"interpretability": 7, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Guangyu Dai", "Siliang Tang", "Yueting Zhuang"]}
]]></acme>

<pubDate>2025-10-23T07:06:21+00:00</pubDate>
</item>
<item>
<title>SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series</title>
<link>https://papers.cool/arxiv/2510.20273</link>
<guid>https://papers.cool/arxiv/2510.20273</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SynTSBench introduces a synthetic, feature‑configurable benchmark for time‑series forecasting that isolates specific temporal patterns, assesses model robustness to data irregularities, and compares predictions to theoretical optimal performance. The framework provides three analytical dimensions—temporal feature decomposition, robustness analysis, and optimality benchmarking—to diagnose strengths and weaknesses of deep learning models. Experiments reveal that current state‑of‑the‑art models fall short of optimal baselines across many pattern types.<br /><strong>Summary (CN):</strong> SynTSBench 提出了一种可编程特征的合成时间序列预测基准，通过隔离特定时间模式、评估模型对数据不规则性的鲁棒性，并将预测结果与理论最优性能进行比较。该框架包括三个核心分析维度：时间特征分解与能力映射、鲁棒性分析以及最优基准评估，以帮助诊断深度学习模型的优势和不足。实验表明，当前最先进的模型在多个模式下仍未接近理论最优基准。<br /><strong>Keywords:</strong> time series forecasting, synthetic benchmark, temporal pattern learning, model evaluation, robustness analysis, feature decomposition, theoretical optimum, synthetic data, capability mapping<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Qitai Tan, Yiyun Chen, Mo Li, Ruiwen Gu, Yilin Su, Xiao-Ping Zhang</div>
Recent advances in deep learning have driven rapid progress in time series forecasting, yet many state-of-the-art models continue to struggle with robust performance in real-world applications, even when they achieve strong results on standard benchmark datasets. This persistent gap can be attributed to the black-box nature of deep learning architectures and the inherent limitations of current evaluation frameworks, which frequently lack the capacity to provide clear, quantitative insights into the specific strengths and weaknesses of different models, thereby complicating the selection of appropriate models for particular forecasting scenarios. To address these issues, we propose a synthetic data-driven evaluation paradigm, SynTSBench, that systematically assesses fundamental modeling capabilities of time series forecasting models through programmable feature configuration. Our framework isolates confounding factors and establishes an interpretable evaluation system with three core analytical dimensions: (1) temporal feature decomposition and capability mapping, which enables systematic evaluation of model capacities to learn specific pattern types; (2) robustness analysis under data irregularities, which quantifies noise tolerance thresholds and anomaly recovery capabilities; and (3) theoretical optimum benchmarking, which establishes performance boundaries for each pattern type-enabling direct comparison between model predictions and mathematical optima. Our experiments show that current deep learning models do not universally approach optimal baselines across all types of temporal features.The code is available at https://github.com/TanQitai/SynTSBench
<div><strong>Authors:</strong> Qitai Tan, Yiyun Chen, Mo Li, Ruiwen Gu, Yilin Su, Xiao-Ping Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SynTSBench introduces a synthetic, feature‑configurable benchmark for time‑series forecasting that isolates specific temporal patterns, assesses model robustness to data irregularities, and compares predictions to theoretical optimal performance. The framework provides three analytical dimensions—temporal feature decomposition, robustness analysis, and optimality benchmarking—to diagnose strengths and weaknesses of deep learning models. Experiments reveal that current state‑of‑the‑art models fall short of optimal baselines across many pattern types.", "summary_cn": "SynTSBench 提出了一种可编程特征的合成时间序列预测基准，通过隔离特定时间模式、评估模型对数据不规则性的鲁棒性，并将预测结果与理论最优性能进行比较。该框架包括三个核心分析维度：时间特征分解与能力映射、鲁棒性分析以及最优基准评估，以帮助诊断深度学习模型的优势和不足。实验表明，当前最先进的模型在多个模式下仍未接近理论最优基准。", "keywords": "time series forecasting, synthetic benchmark, temporal pattern learning, model evaluation, robustness analysis, feature decomposition, theoretical optimum, synthetic data, capability mapping", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Qitai Tan", "Yiyun Chen", "Mo Li", "Ruiwen Gu", "Yilin Su", "Xiao-Ping Zhang"]}
]]></acme>

<pubDate>2025-10-23T06:59:38+00:00</pubDate>
</item>
<item>
<title>Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs</title>
<link>https://papers.cool/arxiv/2510.20272</link>
<guid>https://papers.cool/arxiv/2510.20272</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether using a process reward model (PRM) to guide tree search can improve mathematical reasoning in large language models, contrasting it with Best-of-N chain-of-thought prompting. Experiments on 23 problems with Qwen2.5-Math-7B-Instruct show that PRM‑guided tree search provides no significant benefit, suffers from poor state‑value approximation, and degrades out‑of‑distribution, while Monte Carlo tree search and beam search perform better. The authors conclude that more reliable reward modeling is needed before tree search can be effective for LLM mathematical reasoning.<br /><strong>Summary (CN):</strong> 本文研究了使用过程奖励模型（PRM）引导树搜索是否能够提升大型语言模型的数学推理能力，并将其与 Best-of-N 链式思考进行比较。针对 23 个问题使用 Qwen2.5-Math-7B-Instruct 的实验表明，PRM‑导向的树搜索未能带来显著改进，且在状态价值估计和分布外泛化方面表现不佳，而 Monte Carlo 树搜索和束搜索表现更佳。作者认为在树搜索有效提升 LLM 数学推理之前，需要更可靠的奖励建模方法。<br /><strong>Keywords:</strong> PRM, tree search, mathematical reasoning, LLM, chain-of-thought, Monte Carlo tree search, beam search, reward modeling, evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Tristan Cinquin, Geoff Pleiss, Agustinus Kristiadi</div>
While chain-of-thought prompting with Best-of-N (BoN) selection has become popular for mathematical reasoning in large language models (LLMs), its linear structure fails to capture the branching and exploratory nature of complex problem-solving. In this work, we propose an adaptive algorithm to maximize process reward model (PRM) scores over the intractable action space, and investigate whether PRM-guided tree search can improve mathematical reasoning by exploring multiple partial solution paths. Across $23$ diverse mathematical problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case study, we find that: (1) PRM-guided tree search shows no statistically significant improvements over BoN despite higher costs, (2) Monte Carlo tree search and beam search outperform other PRM-guided tree search methods, (3) PRMs poorly approximate state values and their reliability degrades with reasoning depth, and (4) PRMs generalize poorly out of distribution. This underperformance stems from tree search's greater reliance on unreliable PRM scores, suggesting different reward modeling is necessary before tree search can effectively enhance mathematical reasoning in LLMs.
<div><strong>Authors:</strong> Tristan Cinquin, Geoff Pleiss, Agustinus Kristiadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether using a process reward model (PRM) to guide tree search can improve mathematical reasoning in large language models, contrasting it with Best-of-N chain-of-thought prompting. Experiments on 23 problems with Qwen2.5-Math-7B-Instruct show that PRM‑guided tree search provides no significant benefit, suffers from poor state‑value approximation, and degrades out‑of‑distribution, while Monte Carlo tree search and beam search perform better. The authors conclude that more reliable reward modeling is needed before tree search can be effective for LLM mathematical reasoning.", "summary_cn": "本文研究了使用过程奖励模型（PRM）引导树搜索是否能够提升大型语言模型的数学推理能力，并将其与 Best-of-N 链式思考进行比较。针对 23 个问题使用 Qwen2.5-Math-7B-Instruct 的实验表明，PRM‑导向的树搜索未能带来显著改进，且在状态价值估计和分布外泛化方面表现不佳，而 Monte Carlo 树搜索和束搜索表现更佳。作者认为在树搜索有效提升 LLM 数学推理之前，需要更可靠的奖励建模方法。", "keywords": "PRM, tree search, mathematical reasoning, LLM, chain-of-thought, Monte Carlo tree search, beam search, reward modeling, evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Tristan Cinquin", "Geoff Pleiss", "Agustinus Kristiadi"]}
]]></acme>

<pubDate>2025-10-23T06:59:36+00:00</pubDate>
</item>
<item>
<title>Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch</title>
<link>https://papers.cool/arxiv/2510.20271</link>
<guid>https://papers.cool/arxiv/2510.20271</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces optimized GPU kernels for computing Euler Characteristic Curves (ECC) that achieve 16–2000× speedups over existing implementations, and presents a differentiable ECC layer for PyTorch using a sigmoid relaxation to learn thresholds end‑to‑end. The CUDA kernels are designed for Ampere GPUs with coalesced memory access and shared‑memory accumulation, and the authors discuss batching and multi‑GPU extensions for broader adoption. Experiments demonstrate the efficiency gains on synthetic grids and outline downstream applications of topological features in deep learning.<br /><strong>Summary (CN):</strong> 本文提出了用于计算欧拉特征曲线（Euler Characteristic Curve, ECC）的优化 GPU 核心，实现了相较于已有实现 16 到 2000 倍的加速，并介绍了一种基于 Sigmoid 松弛的可微 ECC 层，可在 PyTorch 中端到端学习阈值。该 CUDA 实现针对 Ampere GPU 采用 128 字节共同访问和层次化共享内存累加，并讨论了批处理和多 GPU 扩展以促进大规模应用。实验在合成网格上展示了显著的效率提升，并概述了拓扑特征在深度学习中的下游应用。<br /><strong>Keywords:</strong> Euler characteristic curve, topological data analysis, GPU acceleration, CUDA, PyTorch, differentiable layer, computational topology, deep learning, optimization<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Udit Saxena</div>
Topological features capture global geometric structure in imaging data, but practical adoption in deep learning requires both computational efficiency and differentiability. We present optimized GPU kernels for the Euler Characteristic Curve (ECC) computation achieving 16-2000Ö speedups over prior GPU implementations on synthetic grids, and introduce a differentiable PyTorch layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs use 128B-coalesced access and hierarchical shared-memory accumulation. Our PyTorch layer learns thresholds in a single direction via a Differentiable Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream relevance, including applications highlighted by prior ECC work, and outline batching/multi-GPU extensions to broaden adoption.
<div><strong>Authors:</strong> Udit Saxena</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces optimized GPU kernels for computing Euler Characteristic Curves (ECC) that achieve 16–2000× speedups over existing implementations, and presents a differentiable ECC layer for PyTorch using a sigmoid relaxation to learn thresholds end‑to‑end. The CUDA kernels are designed for Ampere GPUs with coalesced memory access and shared‑memory accumulation, and the authors discuss batching and multi‑GPU extensions for broader adoption. Experiments demonstrate the efficiency gains on synthetic grids and outline downstream applications of topological features in deep learning.", "summary_cn": "本文提出了用于计算欧拉特征曲线（Euler Characteristic Curve, ECC）的优化 GPU 核心，实现了相较于已有实现 16 到 2000 倍的加速，并介绍了一种基于 Sigmoid 松弛的可微 ECC 层，可在 PyTorch 中端到端学习阈值。该 CUDA 实现针对 Ampere GPU 采用 128 字节共同访问和层次化共享内存累加，并讨论了批处理和多 GPU 扩展以促进大规模应用。实验在合成网格上展示了显著的效率提升，并概述了拓扑特征在深度学习中的下游应用。", "keywords": "Euler characteristic curve, topological data analysis, GPU acceleration, CUDA, PyTorch, differentiable layer, computational topology, deep learning, optimization", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Udit Saxena"]}
]]></acme>

<pubDate>2025-10-23T06:59:07+00:00</pubDate>
</item>
<item>
<title>ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases</title>
<link>https://papers.cool/arxiv/2510.20270</link>
<guid>https://papers.cool/arxiv/2510.20270</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ImpossibleBench is a benchmark framework that creates impossible variants of coding tasks by inserting conflicts between natural‑language specifications and unit tests, enabling measurement of large language models' propensity to exploit test cases. The authors evaluate cheating rates across different prompting strategies, test access modes, and feedback loops, and provide a testbed for developing monitoring tools and mitigating deceptive solutions.<br /><strong>Summary (CN):</strong> ImpossibleBench 通过在自然语言需求与单元测试之间制造直接冲突，生成 "不可能" 的任务变体，以量化大语言模型利用测试用例的倾向。作者在不同提示方式、测试访问权限和反馈循环下评估作弊率，并提供用于开发监控工具和抑制欺骗行为的测试平台。<br /><strong>Keywords:</strong> LLM cheating, test exploitation, benchmark, ImpossibleBench, safety evaluation, prompt engineering, deceptive behavior, code generation, alignment, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Ziqian Zhong, Aditi Raghunathan, Nicholas Carlini</div>
The tendency to find and exploit "shortcuts" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments. To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates "impossible" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's "cheating rate" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut. As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems. Our implementation can be found at https://github.com/safety-research/impossiblebench.
<div><strong>Authors:</strong> Ziqian Zhong, Aditi Raghunathan, Nicholas Carlini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ImpossibleBench is a benchmark framework that creates impossible variants of coding tasks by inserting conflicts between natural‑language specifications and unit tests, enabling measurement of large language models' propensity to exploit test cases. The authors evaluate cheating rates across different prompting strategies, test access modes, and feedback loops, and provide a testbed for developing monitoring tools and mitigating deceptive solutions.", "summary_cn": "ImpossibleBench 通过在自然语言需求与单元测试之间制造直接冲突，生成 \"不可能\" 的任务变体，以量化大语言模型利用测试用例的倾向。作者在不同提示方式、测试访问权限和反馈循环下评估作弊率，并提供用于开发监控工具和抑制欺骗行为的测试平台。", "keywords": "LLM cheating, test exploitation, benchmark, ImpossibleBench, safety evaluation, prompt engineering, deceptive behavior, code generation, alignment, robustness", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"]}
]]></acme>

<pubDate>2025-10-23T06:58:32+00:00</pubDate>
</item>
<item>
<title>Optimistic Task Inference for Behavior Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20264</link>
<guid>https://papers.cool/arxiv/2510.20264</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes OpTI-BFM, an optimistic decision criterion that models uncertainty over reward functions to enable task inference for behavior foundation models through interaction at test time. It provides a regret bound linking well‑trained BFMs to upper‑confidence linear bandit algorithms and demonstrates empirically that successor‑features‑based BFMs can identify and optimize unseen reward functions within a few episodes on zero‑shot benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 OpTI-BFM——一种乐观决策准则，通过在测试时与环境交互并对奖励函数的不确定性建模，实现对行为基础模型（BFM）的任务推断。论文给出与线性赌博上置信上界算法相连的后悔界限，并在零-shot 基准上实证表明，基于后继特征的 BFM 能够在少数几轮交互中识别并优化未见过的奖励函数。<br /><strong>Keywords:</strong> behavior foundation models, zero-shot reinforcement learning, task inference, optimistic decision criterion, reward uncertainty, linear bandits, successor features, regret bound, data-efficient RL, reward modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Thomas Rupf, Marco Bagatella, Marin Vlastelica, Andreas Krause</div>
Behavior Foundation Models (BFMs) are capable of retrieving high-performing policy for any reward function specified directly at test-time, commonly referred to as zero-shot reinforcement learning (RL). While this is a very efficient process in terms of compute, it can be less so in terms of data: as a standard assumption, BFMs require computing rewards over a non-negligible inference dataset, assuming either access to a functional form of rewards, or significant labeling efforts. To alleviate these limitations, we tackle the problem of task inference purely through interaction with the environment at test-time. We propose OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Formally, we provide a regret bound for well-trained BFMs through a direct connection to upper-confidence algorithms for linear bandits. Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and observe that it enables successor-features-based BFMs to identify and optimize an unseen reward function in a handful of episodes with minimal compute overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.
<div><strong>Authors:</strong> Thomas Rupf, Marco Bagatella, Marin Vlastelica, Andreas Krause</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes OpTI-BFM, an optimistic decision criterion that models uncertainty over reward functions to enable task inference for behavior foundation models through interaction at test time. It provides a regret bound linking well‑trained BFMs to upper‑confidence linear bandit algorithms and demonstrates empirically that successor‑features‑based BFMs can identify and optimize unseen reward functions within a few episodes on zero‑shot benchmarks.", "summary_cn": "本文提出 OpTI-BFM——一种乐观决策准则，通过在测试时与环境交互并对奖励函数的不确定性建模，实现对行为基础模型（BFM）的任务推断。论文给出与线性赌博上置信上界算法相连的后悔界限，并在零-shot 基准上实证表明，基于后继特征的 BFM 能够在少数几轮交互中识别并优化未见过的奖励函数。", "keywords": "behavior foundation models, zero-shot reinforcement learning, task inference, optimistic decision criterion, reward uncertainty, linear bandits, successor features, regret bound, data-efficient RL, reward modeling", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Thomas Rupf", "Marco Bagatella", "Marin Vlastelica", "Andreas Krause"]}
]]></acme>

<pubDate>2025-10-23T06:36:18+00:00</pubDate>
</item>
<item>
<title>FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning</title>
<link>https://papers.cool/arxiv/2510.20250</link>
<guid>https://papers.cool/arxiv/2510.20250</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FedGPS introduces a framework that incorporates statistical distribution and gradient information from other clients to mitigate data heterogeneity in federated learning. By statically modifying local objectives with surrogate global distributions and dynamically adjusting update directions with shared gradients, it demonstrates improved robustness and performance across diverse heterogeneity scenarios.<br /><strong>Summary (CN):</strong> FedGPS 提出一种框架，通过整合其他客户端的统计分布信息和梯度信息来缓解联邦学习中的数据异质性。该方法在本地目标中静态加入全局分布的替代信息，并在每轮训练中动态使用其他客户端的梯度进行方向调整，从而在多种异质性场景下展示出更强的鲁棒性和性能提升。<br /><strong>Keywords:</strong> federated learning, data heterogeneity, statistical rectification, gradient synergy, robustness, Fed<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zhiqin Yang, Yonggang Zhang, Chenxin Li, Yiu-ming Cheung, Bo Han, Yixuan Yuan</div>
Federated Learning (FL) confronts a significant challenge known as data heterogeneity, which impairs model performance and convergence. Existing methods have made notable progress in addressing this issue. However, improving performance in certain heterogeneity scenarios remains an overlooked question: \textit{How robust are these methods to deploy under diverse heterogeneity scenarios?} To answer this, we conduct comprehensive evaluations across varied heterogeneity scenarios, showing that most existing methods exhibit limited robustness. Meanwhile, insights from these experiments highlight that sharing statistical information can mitigate heterogeneity by enabling clients to update with a global perspective. Motivated by this, we propose \textbf{FedGPS} (\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel framework that seamlessly integrates statistical distribution and gradient information from others. Specifically, FedGPS statically modifies each client's learning objective to implicitly model the global data distribution using surrogate information, while dynamically adjusting local update directions with gradient information from other clients at each round. Extensive experiments show that FedGPS outperforms state-of-the-art methods across diverse heterogeneity scenarios, validating its effectiveness and robustness. The code is available at: https://github.com/CUHK-AIM-Group/FedGPS.
<div><strong>Authors:</strong> Zhiqin Yang, Yonggang Zhang, Chenxin Li, Yiu-ming Cheung, Bo Han, Yixuan Yuan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FedGPS introduces a framework that incorporates statistical distribution and gradient information from other clients to mitigate data heterogeneity in federated learning. By statically modifying local objectives with surrogate global distributions and dynamically adjusting update directions with shared gradients, it demonstrates improved robustness and performance across diverse heterogeneity scenarios.", "summary_cn": "FedGPS 提出一种框架，通过整合其他客户端的统计分布信息和梯度信息来缓解联邦学习中的数据异质性。该方法在本地目标中静态加入全局分布的替代信息，并在每轮训练中动态使用其他客户端的梯度进行方向调整，从而在多种异质性场景下展示出更强的鲁棒性和性能提升。", "keywords": "federated learning, data heterogeneity, statistical rectification, gradient synergy, robustness, Fed", "scoring": {"interpretability": 1, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zhiqin Yang", "Yonggang Zhang", "Chenxin Li", "Yiu-ming Cheung", "Bo Han", "Yixuan Yuan"]}
]]></acme>

<pubDate>2025-10-23T06:10:11+00:00</pubDate>
</item>
<item>
<title>What Does It Take to Build a Performant Selective Classifier?</title>
<link>https://papers.cool/arxiv/2510.20242</link>
<guid>https://papers.cool/arxiv/2510.20242</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper formalizes the selective‑classification gap as the difference between practical classifiers and a perfect‑ordering oracle, decomposing it into five error sources: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift slack. It shows that post‑hoc monotone calibration has limited effect because it rarely changes the ranking of scores, and demonstrates through synthetic and real‑world experiments that richer, feature‑aware calibrators and distributionally robust training are required to close the gap. The work provides a quantitative error budget and practical guidelines for building better selective classifiers.<br /><strong>Summary (CN):</strong> 本文将选择性分类的性能差距形式化为实际分类器与完美排序oracle之间的差异，并将其分解为五类误差来源：贝叶斯噪声、近似误差、排序误差、统计噪声以及实现/分布迁移导致的松弛。研究表明单调后置校准因几乎不改变分数排序而对缩小差距贡献有限，实验结果显示需要更丰富、特征感知的校准器以及分布鲁棒训练才能有效改善。文章提供了量化的误差预算和可操作的设计指南，以构建更接近理想oracle的选择性分类器。<br /><strong>Keywords:</strong> selective classification, abstention, calibration, ranking error, distributional shift, reliability, performance gap, feature-aware calibrator<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Stephan Rabanser, Nicolas Papernot</div>
Selective classifiers improve model reliability by abstaining on inputs the model deems uncertain. However, few practical approaches achieve the gold-standard performance of a perfect-ordering oracle that accepts examples exactly in order of correctness. Our work formalizes this shortfall as the selective-classification gap and present the first finite-sample decomposition of this gap to five distinct sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation- or shift-induced slack. Crucially, our analysis reveals that monotone post-hoc calibration -- often believed to strengthen selective classifiers -- has limited impact on closing this gap, since it rarely alters the model's underlying score ranking. Bridging the gap therefore requires scoring mechanisms that can effectively reorder predictions rather than merely rescale them. We validate our decomposition on synthetic two-moons data and on real-world vision and language benchmarks, isolating each error component through controlled experiments. Our results confirm that (i) Bayes noise and limited model capacity can account for substantial gaps, (ii) only richer, feature-aware calibrators meaningfully improve score ordering, and (iii) data shift introduces a separate slack that demands distributionally robust training. Together, our decomposition yields a quantitative error budget as well as actionable design guidelines that practitioners can use to build selective classifiers which approximate ideal oracle behavior more closely.
<div><strong>Authors:</strong> Stephan Rabanser, Nicolas Papernot</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper formalizes the selective‑classification gap as the difference between practical classifiers and a perfect‑ordering oracle, decomposing it into five error sources: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift slack. It shows that post‑hoc monotone calibration has limited effect because it rarely changes the ranking of scores, and demonstrates through synthetic and real‑world experiments that richer, feature‑aware calibrators and distributionally robust training are required to close the gap. The work provides a quantitative error budget and practical guidelines for building better selective classifiers.", "summary_cn": "本文将选择性分类的性能差距形式化为实际分类器与完美排序oracle之间的差异，并将其分解为五类误差来源：贝叶斯噪声、近似误差、排序误差、统计噪声以及实现/分布迁移导致的松弛。研究表明单调后置校准因几乎不改变分数排序而对缩小差距贡献有限，实验结果显示需要更丰富、特征感知的校准器以及分布鲁棒训练才能有效改善。文章提供了量化的误差预算和可操作的设计指南，以构建更接近理想oracle的选择性分类器。", "keywords": "selective classification, abstention, calibration, ranking error, distributional shift, reliability, performance gap, feature-aware calibrator", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Stephan Rabanser", "Nicolas Papernot"]}
]]></acme>

<pubDate>2025-10-23T05:48:40+00:00</pubDate>
</item>
<item>
<title>Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction</title>
<link>https://papers.cool/arxiv/2510.20236</link>
<guid>https://papers.cool/arxiv/2510.20236</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Layer-to-Layer Knowledge Mixing (LKM), a self‑knowledge distillation technique that aligns hidden embeddings across GNN layers to aggregate multi‑hop information, thereby improving molecular property prediction with negligible added computational cost. Experiments on three GNN architectures (DimeNet++, MXMNet, PAMNet) across QM9, MD17, and Chignolin datasets show reductions in mean absolute error up to 9.8% for QM9, 45.3% for MD17 energy, and 22.9% for Chignolin. The results demonstrate that LKM can significantly boost GNN accuracy for chemical tasks without substantial overhead.<br /><strong>Summary (CN):</strong> 本文提出了层间知识混合（LKM）方法，这是一种自我知识蒸馏技术，通过对齐 GNN 各层的隐藏嵌入来聚合多跳信息，从而在几乎不增加计算开销的情况下提升分子属性预测的准确性。对 DimeNet++、MXMNet、PAMNet 三种 GNN 架构在 QM9、MD17 和 Chignolin 数据集上的实验显示，平均绝对误差分别降低了最多 9.8%、45.3%（MD17 能量）和 22.9%。结果表明 LKM 能在化学属性预测任务中显著提升 GNN 性能且开销极小。<br /><strong>Keywords:</strong> graph neural networks, self-knowledge distillation, chemical property prediction, Layer-to-Layer Knowledge Mixing, LKM, QM9, MD17, Chignolin<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Teng Jiek See, Daokun Zhang, Mario Boley, David K. Chalmers</div>
Graph Neural Networks (GNNs) are the currently most effective methods for predicting molecular properties but there remains a need for more accurate models. GNN accuracy can be improved by increasing the model complexity but this also increases the computational cost and memory requirement during training and inference. In this study, we develop Layer-to-Layer Knowledge Mixing (LKM), a novel self-knowledge distillation method that increases the accuracy of state-of-the-art GNNs while adding negligible computational complexity during training and inference. By minimizing the mean absolute distance between pre-existing hidden embeddings of GNN layers, LKM efficiently aggregates multi-hop and multi-scale information, enabling improved representation of both local and global molecular features. We evaluated LKM using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found that the LKM method effectively reduces the mean absolute error of quantum chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17 Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to significantly improve the accuracy of GNNs for chemical property prediction without any substantial increase in training and inference cost.
<div><strong>Authors:</strong> Teng Jiek See, Daokun Zhang, Mario Boley, David K. Chalmers</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Layer-to-Layer Knowledge Mixing (LKM), a self‑knowledge distillation technique that aligns hidden embeddings across GNN layers to aggregate multi‑hop information, thereby improving molecular property prediction with negligible added computational cost. Experiments on three GNN architectures (DimeNet++, MXMNet, PAMNet) across QM9, MD17, and Chignolin datasets show reductions in mean absolute error up to 9.8% for QM9, 45.3% for MD17 energy, and 22.9% for Chignolin. The results demonstrate that LKM can significantly boost GNN accuracy for chemical tasks without substantial overhead.", "summary_cn": "本文提出了层间知识混合（LKM）方法，这是一种自我知识蒸馏技术，通过对齐 GNN 各层的隐藏嵌入来聚合多跳信息，从而在几乎不增加计算开销的情况下提升分子属性预测的准确性。对 DimeNet++、MXMNet、PAMNet 三种 GNN 架构在 QM9、MD17 和 Chignolin 数据集上的实验显示，平均绝对误差分别降低了最多 9.8%、45.3%（MD17 能量）和 22.9%。结果表明 LKM 能在化学属性预测任务中显著提升 GNN 性能且开销极小。", "keywords": "graph neural networks, self-knowledge distillation, chemical property prediction, Layer-to-Layer Knowledge Mixing, LKM, QM9, MD17, Chignolin", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Teng Jiek See", "Daokun Zhang", "Mario Boley", "David K. Chalmers"]}
]]></acme>

<pubDate>2025-10-23T05:42:31+00:00</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach</title>
<link>https://papers.cool/arxiv/2510.20235</link>
<guid>https://papers.cool/arxiv/2510.20235</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a provably convergent framework for max‑min multi‑objective reinforcement learning by reformulating the problem as a two‑player zero‑sum regularized continuous game and solving it with a mirror‑descent‑based algorithm. It provides theoretical guarantees on iteration and sample complexity, proposes adaptive regularization for better performance, and demonstrates superior empirical results on tabular and deep RL benchmarks. The approach simplifies policy updates while ensuring global last‑iterate convergence.<br /><strong>Summary (CN):</strong> 本文提出了一种可证明收敛的多目标强化学习框架，采用最大‑最小准则，将问题重新表述为双人零和正则化连续博弈，并利用镜像下降算法求解。文中给出了迭代复杂度和样本复杂度的理论分析，提出了自适应正则化以提升性能，并在表格和深度强化学习实验中展示了显著优于已有基线的结果。该方法在简化策略更新的同时，实现了全局最后迭代收敛。<br /><strong>Keywords:</strong> multi-objective reinforcement learning, max-min criterion, game-theoretic, zero-sum game, mirror descent, convergence analysis, sample complexity, adaptive regularization, deep RL<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Woohyeon Byeon, Giseung Park, Jongseong Chae, Amir Leshem, Youngchul Sung</div>
In this paper, we propose a provably convergent and practical framework for multi-objective reinforcement learning with max-min criterion. From a game-theoretic perspective, we reformulate max-min multi-objective reinforcement learning as a two-player zero-sum regularized continuous game and introduce an efficient algorithm based on mirror descent. Our approach simplifies the policy update while ensuring global last-iterate convergence. We provide a comprehensive theoretical analysis on our algorithm, including iteration complexity under both exact and approximate policy evaluations, as well as sample complexity bounds. To further enhance performance, we modify the proposed algorithm with adaptive regularization. Our experiments demonstrate the convergence behavior of the proposed algorithm in tabular settings, and our implementation for deep reinforcement learning significantly outperforms previous baselines in many MORL environments.
<div><strong>Authors:</strong> Woohyeon Byeon, Giseung Park, Jongseong Chae, Amir Leshem, Youngchul Sung</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a provably convergent framework for max‑min multi‑objective reinforcement learning by reformulating the problem as a two‑player zero‑sum regularized continuous game and solving it with a mirror‑descent‑based algorithm. It provides theoretical guarantees on iteration and sample complexity, proposes adaptive regularization for better performance, and demonstrates superior empirical results on tabular and deep RL benchmarks. The approach simplifies policy updates while ensuring global last‑iterate convergence.", "summary_cn": "本文提出了一种可证明收敛的多目标强化学习框架，采用最大‑最小准则，将问题重新表述为双人零和正则化连续博弈，并利用镜像下降算法求解。文中给出了迭代复杂度和样本复杂度的理论分析，提出了自适应正则化以提升性能，并在表格和深度强化学习实验中展示了显著优于已有基线的结果。该方法在简化策略更新的同时，实现了全局最后迭代收敛。", "keywords": "multi-objective reinforcement learning, max-min criterion, game-theoretic, zero-sum game, mirror descent, convergence analysis, sample complexity, adaptive regularization, deep RL", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Woohyeon Byeon", "Giseung Park", "Jongseong Chae", "Amir Leshem", "Youngchul Sung"]}
]]></acme>

<pubDate>2025-10-23T05:39:26+00:00</pubDate>
</item>
<item>
<title>Sparse Local Implicit Image Function for sub-km Weather Downscaling</title>
<link>https://papers.cool/arxiv/2510.20228</link>
<guid>https://papers.cool/arxiv/2510.20228</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents Sparse Local Implicit Image Function (SpLIIF), an implicit neural representation that enables arbitrary downscaling of weather variables from sparse station data and topography. Trained on Japanese data, it is evaluated on temperature and wind forecasting, achieving up to 50% improvement over CorrDiff and baseline interpolation for temperature and 10-20% for wind. The method demonstrates strong performance both in- and out-of-distribution.<br /><strong>Summary (CN):</strong> 本文提出了稀疏局部隐式图像函数（SpLIIF），一种隐式神经表示方法，可从稀疏气象站点和地形数据任意下采样天气变量。模型在日本数据上训练后，对温度和风速进行预测，温度下采样相较 CorrDiff 与基线插值提升最高 50%，风速提升约 10-20%。该方法在分布内外均表现出显著的精度提升。<br /><strong>Keywords:</strong> weather downscaling, implicit neural representation, Sparse Local Implicit Image Function, SpLIIF, temperature prediction, wind prediction, CorrDiff, sparse stations, topography, sub-kilometer resolution<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yago del Valle Inclan Redondo, Enrique Arriaga-Varela, Dmitry Lyamzin, Pablo Cervantes, Tiago Ramalho</div>
We introduce SpLIIF to generate implicit neural representations and enable arbitrary downscaling of weather variables. We train a model from sparse weather stations and topography over Japan and evaluate in- and out-of-distribution accuracy predicting temperature and wind, comparing it to both an interpolation baseline and CorrDiff. We find the model to be up to 50% better than both CorrDiff and the baseline at downscaling temperature, and around 10-20% better for wind.
<div><strong>Authors:</strong> Yago del Valle Inclan Redondo, Enrique Arriaga-Varela, Dmitry Lyamzin, Pablo Cervantes, Tiago Ramalho</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents Sparse Local Implicit Image Function (SpLIIF), an implicit neural representation that enables arbitrary downscaling of weather variables from sparse station data and topography. Trained on Japanese data, it is evaluated on temperature and wind forecasting, achieving up to 50% improvement over CorrDiff and baseline interpolation for temperature and 10-20% for wind. The method demonstrates strong performance both in- and out-of-distribution.", "summary_cn": "本文提出了稀疏局部隐式图像函数（SpLIIF），一种隐式神经表示方法，可从稀疏气象站点和地形数据任意下采样天气变量。模型在日本数据上训练后，对温度和风速进行预测，温度下采样相较 CorrDiff 与基线插值提升最高 50%，风速提升约 10-20%。该方法在分布内外均表现出显著的精度提升。", "keywords": "weather downscaling, implicit neural representation, Sparse Local Implicit Image Function, SpLIIF, temperature prediction, wind prediction, CorrDiff, sparse stations, topography, sub-kilometer resolution", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yago del Valle Inclan Redondo", "Enrique Arriaga-Varela", "Dmitry Lyamzin", "Pablo Cervantes", "Tiago Ramalho"]}
]]></acme>

<pubDate>2025-10-23T05:20:26+00:00</pubDate>
</item>
<item>
<title>Federated Learning via Meta-Variational Dropout</title>
<link>https://papers.cool/arxiv/2510.20225</link>
<guid>https://papers.cool/arxiv/2510.20225</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes MetaVD, a Bayesian meta‑learning method for federated learning that uses a hypernetwork to predict client‑specific dropout rates, enabling personalized models and reducing overfitting on non‑IID data. Experiments show improved classification accuracy, better uncertainty calibration, and lower communication costs, especially for out‑of‑distribution clients.<br /><strong>Summary (CN):</strong> 本文提出 MetaVD，一种基于贝叶斯元学习的联邦学习方法，通过超网络预测每个客户端的 dropout 率，实现模型个性化并缓解非 IID 数据导致的过拟合。实验表明该方法在分类准确率、uncertainty 校准以及通信成本方面均有提升，尤其对分布外（OOD）客户端表现突出。<br /><strong>Keywords:</strong> federated learning, meta-learning, variational dropout, Bayesian FL, personalization, non-IID data, model compression, uncertainty calibration, OOD detection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Insu Jeon, Minui Hong, Junhyeog Yun, Gunhee Kim</div>
Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD.
<div><strong>Authors:</strong> Insu Jeon, Minui Hong, Junhyeog Yun, Gunhee Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes MetaVD, a Bayesian meta‑learning method for federated learning that uses a hypernetwork to predict client‑specific dropout rates, enabling personalized models and reducing overfitting on non‑IID data. Experiments show improved classification accuracy, better uncertainty calibration, and lower communication costs, especially for out‑of‑distribution clients.", "summary_cn": "本文提出 MetaVD，一种基于贝叶斯元学习的联邦学习方法，通过超网络预测每个客户端的 dropout 率，实现模型个性化并缓解非 IID 数据导致的过拟合。实验表明该方法在分类准确率、uncertainty 校准以及通信成本方面均有提升，尤其对分布外（OOD）客户端表现突出。", "keywords": "federated learning, meta-learning, variational dropout, Bayesian FL, personalization, non-IID data, model compression, uncertainty calibration, OOD detection", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Insu Jeon", "Minui Hong", "Junhyeog Yun", "Gunhee Kim"]}
]]></acme>

<pubDate>2025-10-23T05:17:40+00:00</pubDate>
</item>
<item>
<title>QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20222</link>
<guid>https://papers.cool/arxiv/2510.20222</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes QKCV (Query-Key-Category-Value) attention, an extension of the standard QKV mechanism that incorporates a static categorical embedding to highlight category-specific information in time series forecasting. As a plug‑in module, QKCV improves the accuracy of various attention‑based models, including both lightweight Transformers and pre‑trained foundation models, and enables efficient fine‑tuning by updating only the categorical embedding while keeping pretrained weights fixed.<br /><strong>Summary (CN):</strong> 本文提出 QKCV（Query‑Key‑Category‑Value）注意力机制，在传统 QKV 架构中加入静态类别嵌入，以突出类别特定信息，从而提升时间序列预测性能。该模块可作为通用插件，提升包括轻量 Transformer、Informer、PatchTST、TFT 等在内的注意力模型的预测精度，并在微调预训练基础模型时仅更新类别嵌入 C，保持预训练权重不变，降低计算成本并实现更优的微调表现。<br /><strong>Keywords:</strong> time series forecasting, attention, QKCV, categorical embedding, foundation models, fine-tuning, transformer, Informer, PatchTST, TFT<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hao Wang, Baojun Ma</div>
In real-world time series forecasting tasks, category information plays a pivotal role in capturing inherent data patterns. This paper introduces QKCV (Query-Key-Category-Value) attention, an extension of the traditional QKV framework that incorporates a static categorical embedding C to emphasize category-specific information. As a versatile plug-in module, QKCV enhances the forecasting accuracy of attention-based models (e.g., Vanilla Transformer, Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV demonstrates remarkable adaptability in fine-tuning univariate time series foundation model by solely updating the static embedding C while preserving pretrained weights, thereby reducing computational overhead and achieving superior fine-tuning performance.
<div><strong>Authors:</strong> Hao Wang, Baojun Ma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes QKCV (Query-Key-Category-Value) attention, an extension of the standard QKV mechanism that incorporates a static categorical embedding to highlight category-specific information in time series forecasting. As a plug‑in module, QKCV improves the accuracy of various attention‑based models, including both lightweight Transformers and pre‑trained foundation models, and enables efficient fine‑tuning by updating only the categorical embedding while keeping pretrained weights fixed.", "summary_cn": "本文提出 QKCV（Query‑Key‑Category‑Value）注意力机制，在传统 QKV 架构中加入静态类别嵌入，以突出类别特定信息，从而提升时间序列预测性能。该模块可作为通用插件，提升包括轻量 Transformer、Informer、PatchTST、TFT 等在内的注意力模型的预测精度，并在微调预训练基础模型时仅更新类别嵌入 C，保持预训练权重不变，降低计算成本并实现更优的微调表现。", "keywords": "time series forecasting, attention, QKCV, categorical embedding, foundation models, fine-tuning, transformer, Informer, PatchTST, TFT", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Wang", "Baojun Ma"]}
]]></acme>

<pubDate>2025-10-21T18:15:21+00:00</pubDate>
</item>
<item>
<title>Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints</title>
<link>https://papers.cool/arxiv/2510.20220</link>
<guid>https://papers.cool/arxiv/2510.20220</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Fair‑SMW, a scalable spectral clustering algorithm that enforces group‑fairness balance constraints by reformulating the constrained optimization using the Lagrangian and the Sherman‑Morrison‑Woodbury identity, yielding three Laplacian‑alternative matrices with larger spectral gaps. Experiments on synthetic Stochastic Block Model graphs and real‑world networks (LastFM, FacebookNet, Deezer, German) show that Fair‑SMW attains comparable or better balance while running up to twice as fast as state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> 本文提出 Fair‑SMW 算法，通过拉格朗日方法和 Sherman‑Morrison‑Woodbury (SMW) 恒等式重新表述带有群体公平（平衡）约束的谱聚类优化问题，得到了三种拉普拉斯矩阵的替代方案以增大光谱间隙。实验在随机块模型以及真实网络数据集（LastFM、FacebookNet、Deezer、German）上表明，Fair‑SMW 在保持或提升平衡性的同时，运行速度可比现有最先进方法提升约两倍。<br /><strong>Keywords:</strong> spectral clustering, group fairness, balance constraint, Sherman-Morrison-Woodbury, Laplacian alternative, scalable clustering, Stochastic Block Model, fairness in graphs<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Iván Ojeda-Ruiz, Young Ju-Lee, Malcolm Dickens, Leonardo Cambisaca</div>
Recent research has focused on mitigating algorithmic bias in clustering by incorporating fairness constraints into algorithmic design. Notions such as disparate impact, community cohesion, and cost per population have been implemented to enforce equitable outcomes. Among these, group fairness (balance) ensures that each protected group is proportionally represented within every cluster. However, incorporating balance as a metric of fairness into spectral clustering algorithms has led to computational times that can be improved. This study aims to enhance the efficiency of spectral clustering algorithms by reformulating the constrained optimization problem using a new formulation derived from the Lagrangian method and the Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm. Fair-SMW employs three alternatives to the Laplacian matrix with different spectral gaps to generate multiple variations of Fair-SMW, achieving clustering solutions with comparable balance to existing algorithms while offering improved runtime performance. We present the results of Fair-SMW, evaluated using the Stochastic Block Model (SBM) to measure both runtime efficiency and balance across real-world network datasets, including LastFM, FacebookNet, Deezer, and German. We achieve an improvement in computation time that is twice as fast as the state-of-the-art, and also flexible enough to achieve twice as much balance.
<div><strong>Authors:</strong> Iván Ojeda-Ruiz, Young Ju-Lee, Malcolm Dickens, Leonardo Cambisaca</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Fair‑SMW, a scalable spectral clustering algorithm that enforces group‑fairness balance constraints by reformulating the constrained optimization using the Lagrangian and the Sherman‑Morrison‑Woodbury identity, yielding three Laplacian‑alternative matrices with larger spectral gaps. Experiments on synthetic Stochastic Block Model graphs and real‑world networks (LastFM, FacebookNet, Deezer, German) show that Fair‑SMW attains comparable or better balance while running up to twice as fast as state‑of‑the‑art methods.", "summary_cn": "本文提出 Fair‑SMW 算法，通过拉格朗日方法和 Sherman‑Morrison‑Woodbury (SMW) 恒等式重新表述带有群体公平（平衡）约束的谱聚类优化问题，得到了三种拉普拉斯矩阵的替代方案以增大光谱间隙。实验在随机块模型以及真实网络数据集（LastFM、FacebookNet、Deezer、German）上表明，Fair‑SMW 在保持或提升平衡性的同时，运行速度可比现有最先进方法提升约两倍。", "keywords": "spectral clustering, group fairness, balance constraint, Sherman-Morrison-Woodbury, Laplacian alternative, scalable clustering, Stochastic Block Model, fairness in graphs", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Iván Ojeda-Ruiz", "Young Ju-Lee", "Malcolm Dickens", "Leonardo Cambisaca"]}
]]></acme>

<pubDate>2025-10-22T03:26:54+00:00</pubDate>
</item>
<item>
<title>CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks</title>
<link>https://papers.cool/arxiv/2510.20219</link>
<guid>https://papers.cool/arxiv/2510.20219</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CO-PFL, a contribution-oriented personalized federated learning algorithm that estimates each client’s utility by jointly analyzing gradient direction discrepancies and prediction deviations, providing discriminative aggregation weights. It combines parameter-wise personalization with mask-aware momentum to improve robustness, scalability, and convergence stability across heterogeneous datasets. Experiments on CIFAR10, CIFAR10C, CINIC10, and Mini-ImageNet demonstrate consistent gains over existing PFL methods.<br /><strong>Summary (CN):</strong> 本文提出 CO-PFL（一种贡献导向的个性化联邦学习）算法，通过联合分析梯度方向差异和预测偏差来估计每个客户端的贡献，从而为全局聚合分配区分度高的权重。该方法将参数级个性化与 mask-aware momentum 优化相结合，提升了在数据异构情况下的鲁棒性、可扩展性和收敛稳定性。实验在 CIFAR10、CIFAR10C、CINIC10 与 Mini-ImageNet 上显示出相较于现有 PFL 方法的一致性提升。<br /><strong>Keywords:</strong> personalized federated learning, contribution estimation, gradient direction analysis, mask-aware momentum, heterogeneity, aggregation bias, robustness, federated optimization, submodel personalization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ke Xing, Yanjie Dong, Xiaoyi Fan, Runhao Zeng, Victor C. M. Leung, M. Jamal Deen, Xiping Hu</div>
Personalized federated learning (PFL) addresses a critical challenge of collaboratively training customized models for clients with heterogeneous and scarce local data. Conventional federated learning, which relies on a single consensus model, proves inadequate under such data heterogeneity. Its standard aggregation method of weighting client updates heuristically or by data volume, operates under an equal-contribution assumption, failing to account for the actual utility and reliability of each client's update. This often results in suboptimal personalization and aggregation bias. To overcome these limitations, we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that dynamically estimates each client's contribution for global aggregation. CO-PFL performs a joint assessment by analyzing both gradient direction discrepancies and prediction deviations, leveraging information from gradient and data subspaces. This dual-subspace analysis provides a principled and discriminative aggregation weight for each client, emphasizing high-quality updates. Furthermore, to bolster personalization adaptability and optimization stability, CO-PFL cohesively integrates a parameter-wise personalization mechanism with mask-aware momentum optimization. Our approach effectively mitigates aggregation bias, strengthens global coordination, and enhances local performance by facilitating the construction of tailored submodels with stable updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C, CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses state-of-the-art methods in in personalization accuracy, robustness, scalability and convergence stability.
<div><strong>Authors:</strong> Ke Xing, Yanjie Dong, Xiaoyi Fan, Runhao Zeng, Victor C. M. Leung, M. Jamal Deen, Xiping Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CO-PFL, a contribution-oriented personalized federated learning algorithm that estimates each client’s utility by jointly analyzing gradient direction discrepancies and prediction deviations, providing discriminative aggregation weights. It combines parameter-wise personalization with mask-aware momentum to improve robustness, scalability, and convergence stability across heterogeneous datasets. Experiments on CIFAR10, CIFAR10C, CINIC10, and Mini-ImageNet demonstrate consistent gains over existing PFL methods.", "summary_cn": "本文提出 CO-PFL（一种贡献导向的个性化联邦学习）算法，通过联合分析梯度方向差异和预测偏差来估计每个客户端的贡献，从而为全局聚合分配区分度高的权重。该方法将参数级个性化与 mask-aware momentum 优化相结合，提升了在数据异构情况下的鲁棒性、可扩展性和收敛稳定性。实验在 CIFAR10、CIFAR10C、CINIC10 与 Mini-ImageNet 上显示出相较于现有 PFL 方法的一致性提升。", "keywords": "personalized federated learning, contribution estimation, gradient direction analysis, mask-aware momentum, heterogeneity, aggregation bias, robustness, federated optimization, submodel personalization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ke Xing", "Yanjie Dong", "Xiaoyi Fan", "Runhao Zeng", "Victor C. M. Leung", "M. Jamal Deen", "Xiping Hu"]}
]]></acme>

<pubDate>2025-10-23T05:10:06+00:00</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset</title>
<link>https://papers.cool/arxiv/2510.20209</link>
<guid>https://papers.cool/arxiv/2510.20209</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates whether routine laboratory tests can be used to detect early cancer in Golden Retrievers by benchmarking 126 machine‑learning pipelines on an imbalanced dataset. The best model (logistic regression with class weighting and recursive feature elimination) achieved moderate AUROC (0.815) but low clinical utility (F1 0.25, PPV 0.15), with high NPV yet insufficient recall for a rule‑out test. SHAP analysis showed predictions were driven by nonspecific features such as age and inflammation markers, indicating that lab data alone provide a weak, confounded cancer signal.<br /><strong>Summary (CN):</strong> 本文评估了利用常规实验室检测数据在金毛犬中进行早期癌症筛查的可行性，比较了 126 条机器学习流水线在不平衡数据集上的表现。最佳模型（带类别加权和递归特征消除的逻辑回归）获得了中等的 AUROC（0.815），但临床分类性能较差（F1=0.25，阳性预测值=0.15），尽管阴性预测值高（0.98），但召回率不足（0.79）不足以作为可靠的排除测试。SHAP 可解释性分析显示，预测主要受年龄、炎症和贫血等非特异性特征驱动，表明单纯实验室数据的癌症信号微弱且易受混淆。<br /><strong>Keywords:</strong> cancer detection, veterinary oncology, routine laboratory data, machine learning, class imbalance, logistic regression, SHAP, feature selection, early detection<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shumin Li</div>
The development of accessible screening tools for early cancer detection in dogs represents a significant challenge in veterinary medicine. Routine laboratory data offer a promising, low-cost source for such tools, but their utility is hampered by the non-specificity of individual biomarkers and the severe class imbalance inherent in screening populations. This study assesses the feasibility of cancer risk classification using the Golden Retriever Lifetime Study (GRLS) cohort under real-world constraints, including the grouping of diverse cancer types and the inclusion of post-diagnosis samples. A comprehensive benchmark evaluation was conducted, systematically comparing 126 analytical pipelines that comprised various machine learning models, feature selection methods, and data balancing techniques. Data were partitioned at the patient level to prevent leakage. The optimal model, a Logistic Regression classifier with class weighting and recursive feature elimination, demonstrated moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical classification performance (F1-score = 0.25, Positive Predictive Value = 0.15). While a high Negative Predictive Value (0.98) was achieved, insufficient recall (0.79) precludes its use as a reliable rule-out test. Interpretability analysis with SHapley Additive exPlanations (SHAP) revealed that predictions were driven by non-specific features like age and markers of inflammation and anemia. It is concluded that while a statistically detectable cancer signal exists in routine lab data, it is too weak and confounded for clinically reliable discrimination from normal aging or other inflammatory conditions. This work establishes a critical performance ceiling for this data modality in isolation and underscores that meaningful progress in computational veterinary oncology will require integration of multi-modal data sources.
<div><strong>Authors:</strong> Shumin Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates whether routine laboratory tests can be used to detect early cancer in Golden Retrievers by benchmarking 126 machine‑learning pipelines on an imbalanced dataset. The best model (logistic regression with class weighting and recursive feature elimination) achieved moderate AUROC (0.815) but low clinical utility (F1 0.25, PPV 0.15), with high NPV yet insufficient recall for a rule‑out test. SHAP analysis showed predictions were driven by nonspecific features such as age and inflammation markers, indicating that lab data alone provide a weak, confounded cancer signal.", "summary_cn": "本文评估了利用常规实验室检测数据在金毛犬中进行早期癌症筛查的可行性，比较了 126 条机器学习流水线在不平衡数据集上的表现。最佳模型（带类别加权和递归特征消除的逻辑回归）获得了中等的 AUROC（0.815），但临床分类性能较差（F1=0.25，阳性预测值=0.15），尽管阴性预测值高（0.98），但召回率不足（0.79）不足以作为可靠的排除测试。SHAP 可解释性分析显示，预测主要受年龄、炎症和贫血等非特异性特征驱动，表明单纯实验室数据的癌症信号微弱且易受混淆。", "keywords": "cancer detection, veterinary oncology, routine laboratory data, machine learning, class imbalance, logistic regression, SHAP, feature selection, early detection", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shumin Li"]}
]]></acme>

<pubDate>2025-10-23T04:52:42+00:00</pubDate>
</item>
<item>
<title>Approximate Replicability in Learning</title>
<link>https://papers.cool/arxiv/2510.20200</link>
<guid>https://papers.cool/arxiv/2510.20200</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces three relaxed notions of replicability—pointwise, approximate, and semi—within the PAC learning framework and shows that for constant replicability parameters, sample‑optimal agnostic learners can be achieved: the first two using Θ(d/α²) samples and the semi‑replicable setting requiring Θ(d²/α²) labeled samples while allowing shared unlabeled data. These results demonstrate that modest relaxations of strict replicability enable learning tasks that were previously impossible under exact replicability. The work provides theoretical guarantees and sample complexity bounds for each relaxation.<br /><strong>Summary (CN):</strong> 本文在 PAC 学习框架下提出了三种可近似复制性的放宽形式——点式、近似式和半复制式，并证明在常数复制性参数下可以实现样本最优的非一致学习者：前两种仅需 Θ(d/α²) 样本，半复制式则在允许使用共享未标记样本的情况下需要 Θ(d²/α²) 标记样本。该研究表明，适度放宽严格复制性可以实现先前在严格复制性下不可行的学习任务，并为每种放宽形式提供了理论保证和样本复杂度界限。<br /><strong>Keywords:</strong> replicability, PAC learning, approximate replicability, agnostic learning, sample complexity, unlabeled data, stability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Max Hopkins, Russell Impagliazzo, Christopher Ye</div>
Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion that algorithms should remain stable under a resampling of their inputs (given access to shared randomness). While a strong and interesting notion of stability, the cost of replicability can be prohibitive: there is no replicable algorithm, for instance, for tasks as simple as threshold learning (Bun et al. STOC '23). Given such strong impossibility results we ask: under what approximate notions of replicability is learning possible? In this work, we propose three natural relaxations of replicability in the context of PAC learning: (1) Pointwise: the learner must be consistent on any fixed input, but not across all inputs simultaneously, (2) Approximate: the learner must output hypotheses that classify most of the distribution consistently, (3) Semi: the algorithm is fully replicable, but may additionally use shared unlabeled samples. In all three cases, for constant replicability parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires $\Theta(d^2/\alpha^2)$ labeled samples.
<div><strong>Authors:</strong> Max Hopkins, Russell Impagliazzo, Christopher Ye</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces three relaxed notions of replicability—pointwise, approximate, and semi—within the PAC learning framework and shows that for constant replicability parameters, sample‑optimal agnostic learners can be achieved: the first two using Θ(d/α²) samples and the semi‑replicable setting requiring Θ(d²/α²) labeled samples while allowing shared unlabeled data. These results demonstrate that modest relaxations of strict replicability enable learning tasks that were previously impossible under exact replicability. The work provides theoretical guarantees and sample complexity bounds for each relaxation.", "summary_cn": "本文在 PAC 学习框架下提出了三种可近似复制性的放宽形式——点式、近似式和半复制式，并证明在常数复制性参数下可以实现样本最优的非一致学习者：前两种仅需 Θ(d/α²) 样本，半复制式则在允许使用共享未标记样本的情况下需要 Θ(d²/α²) 标记样本。该研究表明，适度放宽严格复制性可以实现先前在严格复制性下不可行的学习任务，并为每种放宽形式提供了理论保证和样本复杂度界限。", "keywords": "replicability, PAC learning, approximate replicability, agnostic learning, sample complexity, unlabeled data, stability", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Max Hopkins", "Russell Impagliazzo", "Christopher Ye"]}
]]></acme>

<pubDate>2025-10-23T04:36:01+00:00</pubDate>
</item>
<item>
<title>Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents</title>
<link>https://papers.cool/arxiv/2510.20199</link>
<guid>https://papers.cool/arxiv/2510.20199</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a risk-aware constrained reinforcement learning framework that incorporates optimized certainty equivalents (OCEs) to achieve per-stage robustness in both reward values and time. It provides a strong Lagrangian duality formulation guaranteeing exact equivalence to the original constrained problem and presents a simple algorithmic wrapper compatible with standard RL solvers such as PPO, along with convergence guarantees and empirical validation.<br /><strong>Summary (CN):</strong> 本文提出了一种风险感知的约束强化学习框架，利用优化确定等价（optimized certainty equivalents, OCE）实现奖励值和时间维度上的每阶段鲁棒性。该框架基于强拉格朗日对偶理论，确保与原始约束问题精确等价，并给出了可在 PPO 等标准强化学习求解器上直接包装的简单算法方案，提供收敛性证明并通过数值实验验证其风险规避特性。<br /><strong>Keywords:</strong> risk-averse reinforcement learning, constrained RL, optimized certainty equivalents, risk-sensitive RL, Lagrangian duality, PPO, safety, robust RL<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Jane H. Lee, Baturay Saglam, Spyridon Pougkakiotis, Amin Karbasi, Dionysis Kalogerias</div>
Constrained optimization provides a common framework for dealing with conflicting objectives in reinforcement learning (RL). In most of these settings, the objectives (and constraints) are expressed though the expected accumulated reward. However, this formulation neglects risky or even possibly catastrophic events at the tails of the reward distribution, and is often insufficient for high-stakes applications in which the risk involved in outliers is critical. In this work, we propose a framework for risk-aware constrained RL, which exhibits per-stage robustness properties jointly in reward values and time using optimized certainty equivalents (OCEs). Our framework ensures an exact equivalent to the original constrained problem within a parameterized strong Lagrangian duality framework under appropriate constraint qualifications, and yields a simple algorithmic recipe which can be wrapped around standard RL solvers, such as PPO. Lastly, we establish the convergence of the proposed algorithm under common assumptions, and verify the risk-aware properties of our approach through several numerical experiments.
<div><strong>Authors:</strong> Jane H. Lee, Baturay Saglam, Spyridon Pougkakiotis, Amin Karbasi, Dionysis Kalogerias</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a risk-aware constrained reinforcement learning framework that incorporates optimized certainty equivalents (OCEs) to achieve per-stage robustness in both reward values and time. It provides a strong Lagrangian duality formulation guaranteeing exact equivalence to the original constrained problem and presents a simple algorithmic wrapper compatible with standard RL solvers such as PPO, along with convergence guarantees and empirical validation.", "summary_cn": "本文提出了一种风险感知的约束强化学习框架，利用优化确定等价（optimized certainty equivalents, OCE）实现奖励值和时间维度上的每阶段鲁棒性。该框架基于强拉格朗日对偶理论，确保与原始约束问题精确等价，并给出了可在 PPO 等标准强化学习求解器上直接包装的简单算法方案，提供收敛性证明并通过数值实验验证其风险规避特性。", "keywords": "risk-averse reinforcement learning, constrained RL, optimized certainty equivalents, risk-sensitive RL, Lagrangian duality, PPO, safety, robust RL", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Jane H. Lee", "Baturay Saglam", "Spyridon Pougkakiotis", "Amin Karbasi", "Dionysis Kalogerias"]}
]]></acme>

<pubDate>2025-10-23T04:33:32+00:00</pubDate>
</item>
<item>
<title>Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values</title>
<link>https://papers.cool/arxiv/2510.20187</link>
<guid>https://papers.cool/arxiv/2510.20187</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Reinforcement Learning with Explicit Human Values (RLEV), a framework that incorporates quantifiable human value signals into the reward function for large language model training, extending the RLVR approach. Experiments on exam-style datasets show that RLEV improves value-weighted accuracy and learns a value-sensitive termination policy, producing concise outputs for low-value prompts and thorough responses for high-value ones. Ablation studies and robustness tests demonstrate that the performance gains stem from the explicit value alignment and persist under noisy value annotations.<br /><strong>Summary (CN):</strong> 本文提出了“强化学习显式人类价值”（RLEV）框架，将可量化的人类价值信号直接嵌入大语言模型的奖励函数，扩展了 RLVR 方法。实验表明，RLEV 在价值加权准确率上超越仅基于正确性的基线，并学习到价值敏感的终止策略：对低价值提示生成简洁响应，对高价值提示生成详尽回答。消融实验与噪价值标签的鲁棒性测试证明，性能提升源于显式价值对齐的因果作用。<br /><strong>Keywords:</strong> reinforcement learning, human values, LLM alignment, value-weighted reward, termination policy, gradient amplification, RLVR, value-sensitive generation, AI safety, alignment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu</div>
We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities.
<div><strong>Authors:</strong> Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Reinforcement Learning with Explicit Human Values (RLEV), a framework that incorporates quantifiable human value signals into the reward function for large language model training, extending the RLVR approach. Experiments on exam-style datasets show that RLEV improves value-weighted accuracy and learns a value-sensitive termination policy, producing concise outputs for low-value prompts and thorough responses for high-value ones. Ablation studies and robustness tests demonstrate that the performance gains stem from the explicit value alignment and persist under noisy value annotations.", "summary_cn": "本文提出了“强化学习显式人类价值”（RLEV）框架，将可量化的人类价值信号直接嵌入大语言模型的奖励函数，扩展了 RLVR 方法。实验表明，RLEV 在价值加权准确率上超越仅基于正确性的基线，并学习到价值敏感的终止策略：对低价值提示生成简洁响应，对高价值提示生成详尽回答。消融实验与噪价值标签的鲁棒性测试证明，性能提升源于显式价值对齐的因果作用。", "keywords": "reinforcement learning, human values, LLM alignment, value-weighted reward, termination policy, gradient amplification, RLVR, value-sensitive generation, AI safety, alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Dian Yu", "Yulai Zhao", "Kishan Panaganti", "Linfeng Song", "Haitao Mi", "Dong Yu"]}
]]></acme>

<pubDate>2025-10-23T04:15:22+00:00</pubDate>
</item>
<item>
<title>Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP</title>
<link>https://papers.cool/arxiv/2510.20169</link>
<guid>https://papers.cool/arxiv/2510.20169</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Hyper Tour Guided Neighborhood Search (HyperNS), a method that clusters large TSP instances using a sparse heatmap, abstracts clusters as supernodes, and constructs a hyper tour to guide both initialization and optimization, thereby reducing the search space to edges relevant to the tour. Experiments on synthetic and real-world datasets show that HyperNS outperforms existing neural-based approaches, especially on larger instances, achieving a smaller optimality gap.<br /><strong>Summary (CN):</strong> 本文提出一种名为 HyperNS 的方法，通过在稀疏热图上进行聚类并构建超遍历（hyper tour），以引导大规模旅行商问题（TSP）的邻域搜索，从而在初始化和优化阶段聚焦相关边缘，实现更高效的求解。实验表明，该方法在合成和真实数据集上相较于现有神经网络方法显著缩小了与最优解的差距，尤其在大规模实例上表现突出。<br /><strong>Keywords:</strong> traveling salesman problem, hyper tour, neighborhood search, large-scale optimization, clustering, neural combinatorial optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tongkai Lu, Shuai Ma, Chongyang Tao</div>
Traveling Salesman Problem (TSP) is a classic NP-hard problem that has garnered significant attention from both academia and industry. While neural-based methods have shown promise for solving TSPs, they still face challenges in scaling to larger instances, particularly in memory constraints associated with global heatmaps, edge weights, or access matrices, as well as in generating high-quality initial solutions and insufficient global guidance for efficiently navigating vast search spaces. To address these challenges, we propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for large-scale TSP instances. Inspired by the ``clustering first, route second" strategy, our approach initially divides the TSP instance into clusters using a sparse heatmap graph and abstracts them as supernodes, followed by the generation of a hyper tour to guide both the initialization and optimization processes. This method reduces the search space by focusing on edges relevant to the hyper tour, leading to more efficient and effective optimization. Experimental results on both synthetic and real-world datasets demonstrate that our approach outperforms existing neural-based methods, particularly in handling larger-scale instances, offering a significant reduction in the gap to the optimal solution.
<div><strong>Authors:</strong> Tongkai Lu, Shuai Ma, Chongyang Tao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Hyper Tour Guided Neighborhood Search (HyperNS), a method that clusters large TSP instances using a sparse heatmap, abstracts clusters as supernodes, and constructs a hyper tour to guide both initialization and optimization, thereby reducing the search space to edges relevant to the tour. Experiments on synthetic and real-world datasets show that HyperNS outperforms existing neural-based approaches, especially on larger instances, achieving a smaller optimality gap.", "summary_cn": "本文提出一种名为 HyperNS 的方法，通过在稀疏热图上进行聚类并构建超遍历（hyper tour），以引导大规模旅行商问题（TSP）的邻域搜索，从而在初始化和优化阶段聚焦相关边缘，实现更高效的求解。实验表明，该方法在合成和真实数据集上相较于现有神经网络方法显著缩小了与最优解的差距，尤其在大规模实例上表现突出。", "keywords": "traveling salesman problem, hyper tour, neighborhood search, large-scale optimization, clustering, neural combinatorial optimization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tongkai Lu", "Shuai Ma", "Chongyang Tao"]}
]]></acme>

<pubDate>2025-10-23T03:30:18+00:00</pubDate>
</item>
<item>
<title>ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push</title>
<link>https://papers.cool/arxiv/2510.20157</link>
<guid>https://papers.cool/arxiv/2510.20157</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ADP-VRSGP, a decentralized learning framework that adaptively adjusts differential‑privacy noise variance and learning rate via a stepwise‑decaying schedule, while employing progressive gradient fusion and push‑sum aggregation to handle time‑varying communication topologies. Theoretical analysis shows robust convergence, and experiments demonstrate improved training speed and model performance compared to fixed‑variance baselines, providing node‑level personalized privacy guarantees.<br /><strong>Summary (CN):</strong> 本文提出 ADP-VRSGP，一种去中心化学习框架，通过逐步衰减的调度自适应调整差分隐私噪声方差和学习率，并引入渐进式梯度融合及 push‑sum 聚合，以适应时变通信拓扑。理论分析证明了其收敛性，实验表明相较于固定方差方法在训练速度和模型性能上都有显著提升，并提供节点级别的个性化隐私保障。<br /><strong>Keywords:</strong> decentralized learning, differential privacy, adaptive noise variance, variance-reduced stochastic gradient push, push-sum, privacy-preserving machine learning, gradient fusion, time-varying topology<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Xiaoming Wu, Teng Liu, Xin Wang, Ming Yang, Jiguo Yu</div>
Differential privacy is widely employed in decentralized learning to safeguard sensitive data by introducing noise into model updates. However, existing approaches that use fixed-variance noise often degrade model performance and reduce training efficiency. To address these limitations, we propose a novel approach called decentralized learning with adaptive differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP). This method dynamically adjusts both the noise variance and the learning rate using a stepwise-decaying schedule, which accelerates training and enhances final model performance while providing node-level personalized privacy guarantees. To counteract the slowed convergence caused by large-variance noise in early iterations, we introduce a progressive gradient fusion strategy that leverages historical gradients. Furthermore, ADP-VRSGP incorporates decentralized push-sum and aggregation techniques, making it particularly suitable for time-varying communication topologies. Through rigorous theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence with an appropriate learning rate, significantly improving training stability and speed. Experimental results validate that our method outperforms existing baselines across multiple scenarios, highlighting its efficacy in addressing the challenges of privacy-preserving decentralized learning.
<div><strong>Authors:</strong> Xiaoming Wu, Teng Liu, Xin Wang, Ming Yang, Jiguo Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ADP-VRSGP, a decentralized learning framework that adaptively adjusts differential‑privacy noise variance and learning rate via a stepwise‑decaying schedule, while employing progressive gradient fusion and push‑sum aggregation to handle time‑varying communication topologies. Theoretical analysis shows robust convergence, and experiments demonstrate improved training speed and model performance compared to fixed‑variance baselines, providing node‑level personalized privacy guarantees.", "summary_cn": "本文提出 ADP-VRSGP，一种去中心化学习框架，通过逐步衰减的调度自适应调整差分隐私噪声方差和学习率，并引入渐进式梯度融合及 push‑sum 聚合，以适应时变通信拓扑。理论分析证明了其收敛性，实验表明相较于固定方差方法在训练速度和模型性能上都有显著提升，并提供节点级别的个性化隐私保障。", "keywords": "decentralized learning, differential privacy, adaptive noise variance, variance-reduced stochastic gradient push, push-sum, privacy-preserving machine learning, gradient fusion, time-varying topology", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiaoming Wu", "Teng Liu", "Xin Wang", "Ming Yang", "Jiguo Yu"]}
]]></acme>

<pubDate>2025-10-23T03:14:59+00:00</pubDate>
</item>
<item>
<title>Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling</title>
<link>https://papers.cool/arxiv/2510.20148</link>
<guid>https://papers.cool/arxiv/2510.20148</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a multi-layer graph diffusion model that integrates structural and functional brain connectivity to explain the spatial and temporal dynamics of tau protein spread in Alzheimer's disease. It shows that functional connectivity dominates early disease stages and subcortical regions, while structural connectivity becomes more influential later and in occipital, parietal, and limbic areas, linking these patterns to AD-related gene expression and risk factors.<br /><strong>Summary (CN):</strong> 本文提出一种多层图扩散模型，将结构连通性（SC）和功能连通性（FC）结合，用于解释阿尔茨海默病中tau蛋白的空间和时间传播动力学。研究发现，早期疾病阶段和下丘脑等区域主要由功能连通性驱动，而在后期以及枕叶、顶叶和边缘区结构连通性更为重要，并将这些模式与AD相关基因表达和风险因素关联起来。<br /><strong>Keywords:</strong> tau propagation, structural connectivity, functional connectivity, multi-layer graph diffusion, Alzheimer's disease, neuroimaging<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 1, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tingting Dan, Xinwei Huang, Jiaqi Ding, Yinggang Zheng, Guorong Wu</div>
Emerging neuroimaging evidence shows that pathological tau proteins build up along specific brain networks, suggesting that large-scale network architecture plays a key role in the progression of Alzheimer's disease (AD). However, how structural connectivity (SC) and functional connectivity (FC) interact to influence tau propagation remains unclear. Leveraging an unprecedented volume of longitudinal neuroimaging data, we examine SC-FC interactions through a multi-layer graph diffusion model. Beyond showing that connectome architecture constrains tau spread, our model reveals a regionally asymmetric contribution of SC and FC. Specifically, FC predominantly drives tau spread in subcortical areas, the insula, frontal and temporal cortices, whereas SC plays a larger role in occipital, parietal, and limbic regions. The relative dominance of SC versus FC shifts over the course of disease, with FC generally prevailing in early AD and SC becoming primary in later stages. Spatial patterns of SC- and FC-dominant regions strongly align with the regional expression of AD-associated genes involved in inflammation, apoptosis, and lysosomal function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and biological mechanisms (e.g., amyloid deposition) selectively reshape tau propagation by shifting dominant routes between anatomical and functional pathways in a region-specific manner. Findings are validated in an independent AD cohort.
<div><strong>Authors:</strong> Tingting Dan, Xinwei Huang, Jiaqi Ding, Yinggang Zheng, Guorong Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a multi-layer graph diffusion model that integrates structural and functional brain connectivity to explain the spatial and temporal dynamics of tau protein spread in Alzheimer's disease. It shows that functional connectivity dominates early disease stages and subcortical regions, while structural connectivity becomes more influential later and in occipital, parietal, and limbic areas, linking these patterns to AD-related gene expression and risk factors.", "summary_cn": "本文提出一种多层图扩散模型，将结构连通性（SC）和功能连通性（FC）结合，用于解释阿尔茨海默病中tau蛋白的空间和时间传播动力学。研究发现，早期疾病阶段和下丘脑等区域主要由功能连通性驱动，而在后期以及枕叶、顶叶和边缘区结构连通性更为重要，并将这些模式与AD相关基因表达和风险因素关联起来。", "keywords": "tau propagation, structural connectivity, functional connectivity, multi-layer graph diffusion, Alzheimer's disease, neuroimaging", "scoring": {"interpretability": 1, "understanding": 1, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tingting Dan", "Xinwei Huang", "Jiaqi Ding", "Yinggang Zheng", "Guorong Wu"]}
]]></acme>

<pubDate>2025-10-23T02:52:42+00:00</pubDate>
</item>
<item>
<title>There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance</title>
<link>https://papers.cool/arxiv/2510.20119</link>
<guid>https://papers.cool/arxiv/2510.20119</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that the underperformance of timeseries foundation models (TSFMs) relative to lightweight baselines stems from importing NLP and CV pipelines that rely on massive web‑scale corpora, which do not exist for timeseries data. It proposes a shift toward principled dataset construction that systematically covers the space of temporal invariances, building an ontology of invariances from first principles to ensure representational completeness and better generalisation. By aligning datasets with the intrinsic semantics of timeseries, the authors aim to enable TSFMs to exhibit more robust reasoning and emergent behaviour.<br /><strong>Summary (CN):</strong> 本文指出，时间序列基础模型（TSFM）表现不佳的原因在于盲目沿用 NLP 与计算机视觉的管线，而后者依赖于包含大量概念（如“苹果”）的网页数据，时间序列数据并不存在此类大规模概念库。作者主张转向原则化的数据集构建，系统覆盖时间不变性的空间，从第一性原理构建不变性本体，以确保表示的完整性并提升模型的泛化能力。通过使数据集与时间序列的固有语义对齐，期望 TSFM 能实现更稳健的推理和真正的涌现行为。<br /><strong>Keywords:</strong> timeseries foundation models, invariance, dataset design, temporal semantics, representation completeness, generalisation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Arian Prabowo, Flora D. Salim</div>
Timeseries foundation models (TSFMs) have multiplied, yet lightweight supervised baselines and even classical models often match them. We argue this gap stems from the naive importation of NLP or CV pipelines. In language and vision, large web-scale corpora densely capture human concepts i.e. there are countless images and text of apples. In contrast, timeseries data is built to complement the image and text modalities. There are no timeseries dataset that contains the concept apple. As a result, the scrape-everything-online paradigm fails for TS. We posit that progress demands a shift from opportunistic aggregation to principled design: constructing datasets that systematically span the space of invariance that preserve temporal semantics. To this end, we suggest that the ontology of timeseries invariances should be built based on first principles. Only by ensuring representational completeness through invariance coverage can TSFMs achieve the aligned structure necessary for generalisation, reasoning, and truly emergent behaviour.
<div><strong>Authors:</strong> Arian Prabowo, Flora D. Salim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that the underperformance of timeseries foundation models (TSFMs) relative to lightweight baselines stems from importing NLP and CV pipelines that rely on massive web‑scale corpora, which do not exist for timeseries data. It proposes a shift toward principled dataset construction that systematically covers the space of temporal invariances, building an ontology of invariances from first principles to ensure representational completeness and better generalisation. By aligning datasets with the intrinsic semantics of timeseries, the authors aim to enable TSFMs to exhibit more robust reasoning and emergent behaviour.", "summary_cn": "本文指出，时间序列基础模型（TSFM）表现不佳的原因在于盲目沿用 NLP 与计算机视觉的管线，而后者依赖于包含大量概念（如“苹果”）的网页数据，时间序列数据并不存在此类大规模概念库。作者主张转向原则化的数据集构建，系统覆盖时间不变性的空间，从第一性原理构建不变性本体，以确保表示的完整性并提升模型的泛化能力。通过使数据集与时间序列的固有语义对齐，期望 TSFM 能实现更稳健的推理和真正的涌现行为。", "keywords": "timeseries foundation models, invariance, dataset design, temporal semantics, representation completeness, generalisation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Arian Prabowo", "Flora D. Salim"]}
]]></acme>

<pubDate>2025-10-23T01:48:29+00:00</pubDate>
</item>
<item>
<title>Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning</title>
<link>https://papers.cool/arxiv/2510.20108</link>
<guid>https://papers.cool/arxiv/2510.20108</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies partial prototype collapse as a common failure in prototypical self‑supervised learning, where many prototypes become nearly identical, reducing the diversity of learned targets. It attributes this collapse to the joint optimization of encoders and prototypes, which creates a shortcut that minimizes loss without encouraging representation diversity. To solve the issue, the authors propose a fully decoupled training scheme that updates prototypes via an online EM‑style Gaussian‑mixture model independently of the encoder loss, eliminating collapse and improving downstream performance.<br /><strong>Summary (CN):</strong> 本文指出原型自监督学习中常见的部分原型塌陷现象，即多个原型趋于相同表示，导致目标多样性下降。作者将其归因于编码器与原型的联合优化所形成的“捷径”，在不提升表征多样性的情况下最小化损失。为此，提出一种完全解耦的训练策略，通过在线 EM 风格的高斯混合模型独立更新原型，摆脱了原型塌陷并提升了下游表现。<br /><strong>Keywords:</strong> prototype collapse, self-supervised learning, prototypical learning, Gaussian mixture, EM algorithm, decoupled training, representation diversity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Gabriel Y. Arteaga, Marius Aasan, Rwiddhi Chakraborty, Martine Hjelkrem-Tan, Thalles Silva, Michael Kampffmeyer, Adín Ramírez Rivera</div>
Prototypical self-supervised learning methods consistently suffer from partial prototype collapse, where multiple prototypes converge to nearly identical representations. This undermines their central purpose -- providing diverse and informative targets to guide encoders toward rich representations -- and has led practitioners to over-parameterize prototype sets or add ad-hoc regularizers, which mitigate symptoms rather than address the root cause. We empirically trace the collapse to the joint optimization of encoders and prototypes, which encourages a type of shortcut learning: early in training prototypes drift toward redundant representations that minimize loss without necessarily enhancing representation diversity. To break the joint optimization, we introduce a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Concretely, we model prototypes as a Gaussian mixture updated with an online EM-style procedure, independent of the encoder's loss. This simple yet principled decoupling eliminates prototype collapse without explicit regularization and yields consistently diverse prototypes and stronger downstream performance.
<div><strong>Authors:</strong> Gabriel Y. Arteaga, Marius Aasan, Rwiddhi Chakraborty, Martine Hjelkrem-Tan, Thalles Silva, Michael Kampffmeyer, Adín Ramírez Rivera</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies partial prototype collapse as a common failure in prototypical self‑supervised learning, where many prototypes become nearly identical, reducing the diversity of learned targets. It attributes this collapse to the joint optimization of encoders and prototypes, which creates a shortcut that minimizes loss without encouraging representation diversity. To solve the issue, the authors propose a fully decoupled training scheme that updates prototypes via an online EM‑style Gaussian‑mixture model independently of the encoder loss, eliminating collapse and improving downstream performance.", "summary_cn": "本文指出原型自监督学习中常见的部分原型塌陷现象，即多个原型趋于相同表示，导致目标多样性下降。作者将其归因于编码器与原型的联合优化所形成的“捷径”，在不提升表征多样性的情况下最小化损失。为此，提出一种完全解耦的训练策略，通过在线 EM 风格的高斯混合模型独立更新原型，摆脱了原型塌陷并提升了下游表现。", "keywords": "prototype collapse, self-supervised learning, prototypical learning, Gaussian mixture, EM algorithm, decoupled training, representation diversity", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gabriel Y. Arteaga", "Marius Aasan", "Rwiddhi Chakraborty", "Martine Hjelkrem-Tan", "Thalles Silva", "Michael Kampffmeyer", "Adín Ramírez Rivera"]}
]]></acme>

<pubDate>2025-10-23T01:25:10+00:00</pubDate>
</item>
<item>
<title>On pattern classification with weighted dimensions</title>
<link>https://papers.cool/arxiv/2510.20107</link>
<guid>https://papers.cool/arxiv/2510.20107</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a novel weighting scheme for each dimension of a Minkowski distance and integrates it into a K‑Nearest Neighbour (KNN) classifier. Experiments on synthetic and real datasets, especially gene‑expression data, show up to 10% accuracy improvement over standard KNN. The approach regulates the shape and size of the neighbourhood region, addressing challenges of high‑dimensional, low‑sample settings.<br /><strong>Summary (CN):</strong> 本文提出了一种针对 Minkowski 距离的每维加权新方案，并将其嵌入 K 最近邻（KNN）分类器中。通过在合成数据和真实基因表达数据上实验，模型相较于传统 KNN 在分类准确率上提升约10%。该方法通过调节包含 k 个参考样本的区域形状和规模，解决了高维、样本稀少场景下邻近点选择的难题。<br /><strong>Keywords:</strong> weighted distance, Minkowski distance, KNN classifier, dimensional weighting, pattern classification, high-dimensional data, gene expression, classification accuracy<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ayatullah Faruk Mollah</div>
Studies on various facets of pattern classification is often imperative while working with multi-dimensional samples pertaining to diverse application scenarios. In this notion, weighted dimension-based distance measure has been one of the vital considerations in pattern analysis as it reflects the degree of similarity between samples. Though it is often presumed to be settled with the pervasive use of Euclidean distance, plethora of issues often surface. In this paper, we present (a) a detail analysis on the impact of distance measure norms and weights of dimensions along with visualization, (b) a novel weighting scheme for each dimension, (c) incorporation of this dimensional weighting schema into a KNN classifier, and (d) pattern classification on a variety of synthetic as well as realistic datasets with the developed model. It has performed well across diverse experiments in comparison to the traditional KNN under the same experimental setups. Specifically, for gene expression datasets, it yields significant and consistent gain in classification accuracy (around 10%) in all cross-validation experiments with different values of k. As such datasets contain limited number of samples of high dimensions, meaningful selection of nearest neighbours is desirable, and this requirement is reasonably met by regulating the shape and size of the region enclosing the k number of reference samples with the developed weighting schema and appropriate norm. It, therefore, stands as an important generalization of KNN classifier powered by weighted Minkowski distance with the present weighting schema.
<div><strong>Authors:</strong> Ayatullah Faruk Mollah</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a novel weighting scheme for each dimension of a Minkowski distance and integrates it into a K‑Nearest Neighbour (KNN) classifier. Experiments on synthetic and real datasets, especially gene‑expression data, show up to 10% accuracy improvement over standard KNN. The approach regulates the shape and size of the neighbourhood region, addressing challenges of high‑dimensional, low‑sample settings.", "summary_cn": "本文提出了一种针对 Minkowski 距离的每维加权新方案，并将其嵌入 K 最近邻（KNN）分类器中。通过在合成数据和真实基因表达数据上实验，模型相较于传统 KNN 在分类准确率上提升约10%。该方法通过调节包含 k 个参考样本的区域形状和规模，解决了高维、样本稀少场景下邻近点选择的难题。", "keywords": "weighted distance, Minkowski distance, KNN classifier, dimensional weighting, pattern classification, high-dimensional data, gene expression, classification accuracy", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ayatullah Faruk Mollah"]}
]]></acme>

<pubDate>2025-10-23T01:22:02+00:00</pubDate>
</item>
<item>
<title>Competition is the key: A Game Theoretic Causal Discovery Approach</title>
<link>https://papers.cool/arxiv/2510.20106</link>
<guid>https://papers.cool/arxiv/2510.20106</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a game‑theoretic reinforcement‑learning framework for causal discovery in which a DDQN agent competes against a strong baseline such as GES or GraN‑DAG, always warm‑starting from the opponent’s solution. This design yields three provable guarantees: the learned graph never underperforms the, warm‑starting accelerates convergence, and with high probability the algorithm selects the true best graph. Experiments on synthetic SEMs and real‑world benchmarks (Sachs, Asia, Alarm, Child, Hepar2, Dream, Andes) demonstrate consistent improvements and scalability to graphs with hundreds of nodes.<br /><strong>Summary (CN):</strong> 本文提出了一种基于博弈论的强化学习因果发现框架，使用 DDQN 代理与强基线（如 GES 或 GraN‑DAG）竞争，并始终从对手的解进行热启动。该设计提供三项可证明的保证：学习得到的图结构不会劣于基线，热启动显著加速收敛，并且在高概率下选出真正的最优候选图。实验在合成 SEM（30 节点）和多个真实基准Sachs、Asia、Alarm、Child、Hepar2、Dream、Andes）上展示了持续的性能提升和对上百节点图的可扩展性。<br /><strong>Keywords:</strong> causal discovery, game theory, reinforcement learning, DDQN, finite-sample guarantees, GES, GraN-DAG, structural equation models, scalability, consistency<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Amartya Roy, Souvik Chakraborty</div>
Causal discovery remains a central challenge in machine learning, yet existing methods face a fundamental gap: algorithms like GES and GraN-DAG achieve strong empirical performance but lack finite-sample guarantees, while theoretically principled approaches fail to scale. We close this gap by introducing a game-theoretic reinforcement learning framework for causal discovery, where a DDQN agent directly competes against a strong baseline (GES or GraN-DAG), always warm-starting from the opponent's solution. This design yields three provable guarantees: the learned graph is never worse than the opponent, warm-starting strictly accelerates convergence, and most importantly, with high probability the algorithm selects the true best candidate graph. To the best of our knowledge, our result makes a first-of-its-kind progress in explaining such finite-sample guarantees in causal discovery: on synthetic SEMs (30 nodes), the observed error probability decays with n, tightly matching theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2, Dream, and Andes, our method consistently improves upon GES and GraN-DAG while remaining theoretically safe. Remarkably, it scales to large graphs such as Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these results establish a new class of RL-based causal discovery algorithms that are simultaneously provably consistent, sample-efficient, and practically scalable, marking a decisive step toward unifying empirical performance with rigorous finite-sample theory.
<div><strong>Authors:</strong> Amartya Roy, Souvik Chakraborty</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a game‑theoretic reinforcement‑learning framework for causal discovery in which a DDQN agent competes against a strong baseline such as GES or GraN‑DAG, always warm‑starting from the opponent’s solution. This design yields three provable guarantees: the learned graph never underperforms the, warm‑starting accelerates convergence, and with high probability the algorithm selects the true best graph. Experiments on synthetic SEMs and real‑world benchmarks (Sachs, Asia, Alarm, Child, Hepar2, Dream, Andes) demonstrate consistent improvements and scalability to graphs with hundreds of nodes.", "summary_cn": "本文提出了一种基于博弈论的强化学习因果发现框架，使用 DDQN 代理与强基线（如 GES 或 GraN‑DAG）竞争，并始终从对手的解进行热启动。该设计提供三项可证明的保证：学习得到的图结构不会劣于基线，热启动显著加速收敛，并且在高概率下选出真正的最优候选图。实验在合成 SEM（30 节点）和多个真实基准Sachs、Asia、Alarm、Child、Hepar2、Dream、Andes）上展示了持续的性能提升和对上百节点图的可扩展性。", "keywords": "causal discovery, game theory, reinforcement learning, DDQN, finite-sample guarantees, GES, GraN-DAG, structural equation models, scalability, consistency", "scoring": {"interpretability": 5, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Amartya Roy", "Souvik Chakraborty"]}
]]></acme>

<pubDate>2025-10-23T01:19:21+00:00</pubDate>
</item>
<item>
<title>Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa</title>
<link>https://papers.cool/arxiv/2510.20085</link>
<guid>https://papers.cool/arxiv/2510.20085</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a hierarchical dual-head neural network built on MentalRoBERTa to classify suicide risk from social‑media posts into four levels (indicator, ideation, behavior, attempt). One head uses CORAL to preserve ordinal relationships while the other provides standard categorical predictions, and a lightweight transformer encoder models temporal posting patterns with time‑interval embeddings. The combined loss (CORAL, cross‑entropy, focal) addresses ordinal structure, overconfidence, and severe class imbalance, and the model is evaluated via 5‑fold stratified cross‑validation using macro F1.<br /><strong>Summary (CN):</strong> 本文提出了一种基于 MentalRoBERTa 的层次化双头神经网络，用于从社交媒体帖子中将自杀风险划分为四个等级（指标、头、行为、尝试）。一个头使用 CORAL（Consistent Rank Logits）保持风险等级的序序关系，另一个头进行普通分类，以捕捉灵活的类别差异；同时，3 层 Transformer 编码器结合时间间隔嵌入建模帖子序列的时间依赖性。通过 0.5 × CORAL、0.3 × 交叉熵和 0.2 × 焦点损失的组合， simultaneously 处理序列结构、过度自信和严重类别不平衡，并在 5 折分层交叉验证中以宏 F1 作为主要指标进行评估。<br /><strong>Keywords:</strong> suicide risk assessment, MentalRoBERTa, hierarchical dual-head, ordinal classification, CORAL, class imbalance, temporal modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Chang Yang, Ziyi Wang, Wangfeng Tan, Zhiting Tan, Changrui Ji, Zhiming Zhou</div>
Social media platforms have become important sources for identifying suicide risk, but automated detection systems face multiple challenges including severe class imbalance, temporal complexity in posting patterns, and the dual nature of risk levels as both ordinal and categorical. This paper proposes a hierarchical dual-head neural network based on MentalRoBERTa for suicide risk classification into four levels: indicator, ideation, behavior, and attempt. The model employs two complementary prediction heads operating on a shared sequence representation: a CORAL (Consistent Rank Logits) head that preserves ordinal relationships between risk levels, and a standard classification head that enables flexible categorical distinctions. A 3-layer Transformer encoder with 8-head multi-head attention models temporal dependencies across post sequences, while explicit time interval embeddings capture posting behavior dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3 Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure preservation, overconfidence reduction, and class imbalance. To improve computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa and employ mixed-precision training. The model is evaluated using 5-fold stratified cross-validation with macro F1 score as the primary metric.
<div><strong>Authors:</strong> Chang Yang, Ziyi Wang, Wangfeng Tan, Zhiting Tan, Changrui Ji, Zhiming Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a hierarchical dual-head neural network built on MentalRoBERTa to classify suicide risk from social‑media posts into four levels (indicator, ideation, behavior, attempt). One head uses CORAL to preserve ordinal relationships while the other provides standard categorical predictions, and a lightweight transformer encoder models temporal posting patterns with time‑interval embeddings. The combined loss (CORAL, cross‑entropy, focal) addresses ordinal structure, overconfidence, and severe class imbalance, and the model is evaluated via 5‑fold stratified cross‑validation using macro F1.", "summary_cn": "本文提出了一种基于 MentalRoBERTa 的层次化双头神经网络，用于从社交媒体帖子中将自杀风险划分为四个等级（指标、头、行为、尝试）。一个头使用 CORAL（Consistent Rank Logits）保持风险等级的序序关系，另一个头进行普通分类，以捕捉灵活的类别差异；同时，3 层 Transformer 编码器结合时间间隔嵌入建模帖子序列的时间依赖性。通过 0.5 × CORAL、0.3 × 交叉熵和 0.2 × 焦点损失的组合， simultaneously 处理序列结构、过度自信和严重类别不平衡，并在 5 折分层交叉验证中以宏 F1 作为主要指标进行评估。", "keywords": "suicide risk assessment, MentalRoBERTa, hierarchical dual-head, ordinal classification, CORAL, class imbalance, temporal modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Chang Yang", "Ziyi Wang", "Wangfeng Tan", "Zhiting Tan", "Changrui Ji", "Zhiming Zhou"]}
]]></acme>

<pubDate>2025-10-23T00:06:34+00:00</pubDate>
</item>
<item>
<title>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models</title>
<link>https://papers.cool/arxiv/2510.20084</link>
<guid>https://papers.cool/arxiv/2510.20084</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ShapeX introduces a shapelet-driven post hoc explanation framework for time series classification models, segmenting series into shapelet-based segments and using Shapley values to quantify their saliency. The method leverages a Shapelet Describe-and-Detect (SDD) module to learn diverse, discriminative shapelets, enabling explanations that capture causal relationships rather than mere correlations. Experiments on synthetic and real-world data show that ShapeX more accurately identifies relevant subsequences and improves the precision and causal fidelity of explanations compared to existing approaches.<br /><strong>Summary (CN):</strong> ShapeX 提出了一种基于 shapelet 的事后解释框架，用于时间序列分类模型。它将时间序列划分为 shapelet 驱动的片段，并通过 Shapley 值评估其重要性，利用 Shapelet Describe-and-Detect（SDD）模块学习多样且区分度高的 shapelet，从而生成能够揭示因果关系而非仅仅相关性的解释。实验表明，ShapeX 在合成和真实数据上均能更准确地识别关键子序列，提高了解释的精度和因果忠实度。<br /><strong>Keywords:</strong> shapelet, time series classification, post-hoc explanation, Shapley values, causal fidelity, interpretability<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan</div>
Explaining time series classification models is crucial, particularly in high-stakes applications such as healthcare and finance, where transparency and trust play a critical role. Although numerous time series classification methods have identified key subsequences, known as shapelets, as core features for achieving state-of-the-art performance and validating their pivotal role in classification outcomes, existing post-hoc time series explanation (PHTSE) methods primarily focus on timestep-level feature attribution. These explanation methods overlook the fundamental prior that classification outcomes are predominantly driven by key shapelets. To bridge this gap, we present ShapeX, an innovative framework that segments time series into meaningful shapelet-driven segments and employs Shapley values to assess their saliency. At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework, which effectively learns a diverse set of shapelets essential for classification. We further demonstrate that ShapeX produces explanations which reveal causal relationships instead of just correlations, owing to the atomicity properties of shapelets. Experimental results on both synthetic and real-world datasets demonstrate that ShapeX outperforms existing methods in identifying the most relevant subsequences, enhancing both the precision and causal fidelity of time series explanations.
<div><strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ShapeX introduces a shapelet-driven post hoc explanation framework for time series classification models, segmenting series into shapelet-based segments and using Shapley values to quantify their saliency. The method leverages a Shapelet Describe-and-Detect (SDD) module to learn diverse, discriminative shapelets, enabling explanations that capture causal relationships rather than mere correlations. Experiments on synthetic and real-world data show that ShapeX more accurately identifies relevant subsequences and improves the precision and causal fidelity of explanations compared to existing approaches.", "summary_cn": "ShapeX 提出了一种基于 shapelet 的事后解释框架，用于时间序列分类模型。它将时间序列划分为 shapelet 驱动的片段，并通过 Shapley 值评估其重要性，利用 Shapelet Describe-and-Detect（SDD）模块学习多样且区分度高的 shapelet，从而生成能够揭示因果关系而非仅仅相关性的解释。实验表明，ShapeX 在合成和真实数据上均能更准确地识别关键子序列，提高了解释的精度和因果忠实度。", "keywords": "shapelet, time series classification, post-hoc explanation, Shapley values, causal fidelity, interpretability", "scoring": {"interpretability": 8, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Bosong Huang", "Ming Jin", "Yuxuan Liang", "Johan Barthelemy", "Debo Cheng", "Qingsong Wen", "Chenghao Liu", "Shirui Pan"]}
]]></acme>

<pubDate>2025-10-23T00:01:40+00:00</pubDate>
</item>
<item>
<title>Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics</title>
<link>https://papers.cool/arxiv/2510.20068</link>
<guid>https://papers.cool/arxiv/2510.20068</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Coupled Transformer Autoencoder (CTAE), a sequence model that leverages transformer encoders and decoders to capture long-range, non-stationary, non-linear neural dynamics while explicitly separating each brain region’s latent representation into orthogonal shared and private subspaces. Experiments on high‑density electrophysiology recordings from motor and sensory cortices show that CTAE yields more meaningful latent structures and improves behavioral decoding compared to existing alignment and dynamical latent variable methods.<br /><strong>Summary (CN):</strong> 本文提出了耦合 Transformer 自动编码器（CTAE），一种使用 Transformer 编码器和解码器捕获长期、非平稳、非线性神经动态的序列模型，并显式地将每个脑区的潜在表示划分为正交的共享子空间和私有子空间。对来自运动和感觉皮层的高密度电生理记录的实验表明，与现有的对齐方法和动力学潜变量模型相比，CTAE 能提取更有意义的潜在结构并提升行为解码性能。<br /><strong>Keywords:</strong> transformer, autoencoder, latent dynamics, multi-region neural data, disentanglement, shared-private subspaces, neural decoding<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ram Dyuthi Sristi, Sowmya Manojna Narasimha, Jingya Huang, Alice Despatin, Simon Musall, Vikash Gilja, Gal Mishne</div>
Simultaneous recordings from thousands of neurons across multiple brain areas reveal rich mixtures of activity that are shared between regions and dynamics that are unique to each region. Existing alignment or multi-view methods neglect temporal structure, whereas dynamical latent variable models capture temporal dependencies but are usually restricted to a single area, assume linear read-outs, or conflate shared and private signals. We introduce the Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both (i) non-stationary, non-linear dynamics and (ii) separation of shared versus region-specific structure in a single framework. CTAE employs transformer encoders and decoders to capture long-range neural dynamics and explicitly partitions each region's latent space into orthogonal shared and private subspaces. We demonstrate the effectiveness of CTAE on two high-density electrophysiology datasets with simultaneous recordings from multiple regions, one from motor cortical areas and the other from sensory areas. CTAE extracts meaningful representations that better decode behavioral variables compared to existing approaches.
<div><strong>Authors:</strong> Ram Dyuthi Sristi, Sowmya Manojna Narasimha, Jingya Huang, Alice Despatin, Simon Musall, Vikash Gilja, Gal Mishne</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Coupled Transformer Autoencoder (CTAE), a sequence model that leverages transformer encoders and decoders to capture long-range, non-stationary, non-linear neural dynamics while explicitly separating each brain region’s latent representation into orthogonal shared and private subspaces. Experiments on high‑density electrophysiology recordings from motor and sensory cortices show that CTAE yields more meaningful latent structures and improves behavioral decoding compared to existing alignment and dynamical latent variable methods.", "summary_cn": "本文提出了耦合 Transformer 自动编码器（CTAE），一种使用 Transformer 编码器和解码器捕获长期、非平稳、非线性神经动态的序列模型，并显式地将每个脑区的潜在表示划分为正交的共享子空间和私有子空间。对来自运动和感觉皮层的高密度电生理记录的实验表明，与现有的对齐方法和动力学潜变量模型相比，CTAE 能提取更有意义的潜在结构并提升行为解码性能。", "keywords": "transformer, autoencoder, latent dynamics, multi-region neural data, disentanglement, shared-private subspaces, neural decoding", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ram Dyuthi Sristi", "Sowmya Manojna Narasimha", "Jingya Huang", "Alice Despatin", "Simon Musall", "Vikash Gilja", "Gal Mishne"]}
]]></acme>

<pubDate>2025-10-22T22:47:15+00:00</pubDate>
</item>
<item>
<title>A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers</title>
<link>https://papers.cool/arxiv/2510.20066</link>
<guid>https://papers.cool/arxiv/2510.20066</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a three‑layer statistical and machine‑learning pipeline to forecast market‑wide risk in crypto markets by exploiting liquidity and volatility spillovers from a core set of assets. It combines interaction analysis, principal‑component relations, and volatility‑factor projections, supplemented with VAR impulse responses, HAR‑X models, and a leakage‑safe ML protocol with SHAP‑based interpretation, showing significant Granger‑causal links and moderate out‑of‑sample predictive performance.<br /><strong>Summary (CN):</strong> 本文构建了一个包含三个统计层级的机器学习与计量经济学管道，用于通过核心加密资产的流动性和波动性溢出预测全市场风险。该框架结合了流动性‑收益交互、主成分关系以及波动因子投影，并辅以向量自回归冲击响应、带外生变量的异方差自回归模型（HAR‑X）和防泄漏的机器学习流程，采用 SH（解释）进行特征解释，展示了显著的 Granger 因果关系和适度的样本外预测效果。<br /><strong>Keywords:</strong> cryptoasset, liquidity spillover, market risk forecasting, SHAP, HAR-X, VAR, principal component analysis, machine learning pipeline, econometrics<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Yimeng Qiu, Feihuang Fang</div>
We study whether liquidity and volatility proxies of a core set of cryptoassets generate spillovers that forecast market-wide risk. Our empirical framework integrates three statistical layers: (A) interactions between core liquidity and returns, (B) principal-component relations linking liquidity and returns, and (C) volatility-factor projections that capture cross-sectional volatility crowding. The analysis is complemented by vector autoregression impulse responses and forecast error variance decompositions (see Granger 1969; Sims 1980), heterogeneous autoregressive models with exogenous regressors (HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using temporal splits, early stopping, validation-only thresholding, and SHAP-based interpretation. Using daily data from 2021 to 2025 (1462 observations across 74 assets), we document statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy. We report the most informative figures, including the pipeline overview, Layer A heatmap, Layer C robustness analysis, vector autoregression variance decompositions, and the test-set precision-recall curve. Full data and figure outputs are provided in the artifact repository.
<div><strong>Authors:</strong> Yimeng Qiu, Feihuang Fang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a three‑layer statistical and machine‑learning pipeline to forecast market‑wide risk in crypto markets by exploiting liquidity and volatility spillovers from a core set of assets. It combines interaction analysis, principal‑component relations, and volatility‑factor projections, supplemented with VAR impulse responses, HAR‑X models, and a leakage‑safe ML protocol with SHAP‑based interpretation, showing significant Granger‑causal links and moderate out‑of‑sample predictive performance.", "summary_cn": "本文构建了一个包含三个统计层级的机器学习与计量经济学管道，用于通过核心加密资产的流动性和波动性溢出预测全市场风险。该框架结合了流动性‑收益交互、主成分关系以及波动因子投影，并辅以向量自回归冲击响应、带外生变量的异方差自回归模型（HAR‑X）和防泄漏的机器学习流程，采用 SH（解释）进行特征解释，展示了显著的 Granger 因果关系和适度的样本外预测效果。", "keywords": "cryptoasset, liquidity spillover, market risk forecasting, SHAP, HAR-X, VAR, principal component analysis, machine learning pipeline, econometrics", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Yimeng Qiu", "Feihuang Fang"]}
]]></acme>

<pubDate>2025-10-22T22:36:34+00:00</pubDate>
</item>
<item>
<title>Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs</title>
<link>https://papers.cool/arxiv/2510.20064</link>
<guid>https://papers.cool/arxiv/2510.20064</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a provably no‑regret algorithm for online draft model selection in speculative decoding, allowing accurate evaluation of all draft models without extra target model queries. By extending beyond bandit‑based methods, it achieves exponential improvements as the number of drafts grows and offers system‑efficient implementations that reduce computational and latency overhead. Experiments on open‑source LLMs show consistent gains over state‑of‑the‑art baselines, especially for long reasoning tasks.<br /><strong>Summary (CN):</strong> 本文提出了一种在投机解码中实现在线草稿模型选择的无后悔（no‑regret）算法，可在不增加目标模型查询的情况下准确评估所有草稿模型。该方法突破了基于 bandit 的做法，随着草稿模型数量的增加实现指数级提升，并提供了系统效率优化以降低计算和延迟开销。实验在开源 LLM 和多种数据集上展示了相对于最新基线的显著性能提升，尤其在需要长推理链的场景中表现突出。<br /><strong>Keywords:</strong> speculative decoding, draft model selection, online learning, no-regret algorithm, multi-draft, LLM acceleration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hongyi Liu, Jiaji Huang, Zhen Jia, Youngsuk Park, Yu-Xiang Wang</div>
Speculative decoding is widely used in accelerating large language model (LLM) inference. In this work, we focus on the online draft model selection problem in speculative decoding. We design an algorithm that provably competes with the best draft model in hindsight for each query in terms of either the token acceptance probability or expected acceptance length. In particular, we show that we can accurately evaluate all draft models, instead of only the chosen model without incurring additional queries to the target model, which allows us to improve exponentially over the existing bandit-based approach as the number of draft models increases. Our approach is generically applicable with any speculative decoding methods (single draft, multi-drafts and draft-trees). Moreover, we design system-efficient versions of online learners and demonstrate that the overhead in computation and latency can be substantially reduced. We conduct extensive experiments on open-source LLMs and diverse datasets, demonstrating that our methods substantially outperform the state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains where specialized domain-expert drafters are available, especially when long reasoning chains are required.
<div><strong>Authors:</strong> Hongyi Liu, Jiaji Huang, Zhen Jia, Youngsuk Park, Yu-Xiang Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a provably no‑regret algorithm for online draft model selection in speculative decoding, allowing accurate evaluation of all draft models without extra target model queries. By extending beyond bandit‑based methods, it achieves exponential improvements as the number of drafts grows and offers system‑efficient implementations that reduce computational and latency overhead. Experiments on open‑source LLMs show consistent gains over state‑of‑the‑art baselines, especially for long reasoning tasks.", "summary_cn": "本文提出了一种在投机解码中实现在线草稿模型选择的无后悔（no‑regret）算法，可在不增加目标模型查询的情况下准确评估所有草稿模型。该方法突破了基于 bandit 的做法，随着草稿模型数量的增加实现指数级提升，并提供了系统效率优化以降低计算和延迟开销。实验在开源 LLM 和多种数据集上展示了相对于最新基线的显著性能提升，尤其在需要长推理链的场景中表现突出。", "keywords": "speculative decoding, draft model selection, online learning, no-regret algorithm, multi-draft, LLM acceleration", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hongyi Liu", "Jiaji Huang", "Zhen Jia", "Youngsuk Park", "Yu-Xiang Wang"]}
]]></acme>

<pubDate>2025-10-22T22:32:26+00:00</pubDate>
</item>
<item>
<title>Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards</title>
<link>https://papers.cool/arxiv/2510.20055</link>
<guid>https://papers.cool/arxiv/2510.20055</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper models online ad bidding as a Contextual Markov Decision Process with delayed Poisson rewards to capture delayed effects, cumulative impact, and customer heterogeneity. It proposes a two‑stage maximum‑likelihood estimator with data‑splitting and a reinforcement‑learning algorithm that achieves a near‑optimal regret bound of \(\tilde{O}(dH^2\sqrt{T})\). Simulation experiments validate the theoretical results.<br /><strong>Summary (CN):</strong> 本文将在线广告竞价建模为具有延迟泊松奖励的上下文马尔可夫决策过程，以同时考虑延迟效应、累积影响和用户异质性。作者提出两阶段最大似然估计结合数据划分的方法，并基于此设计强化学习算法，实现近似最优的 regret 上界 \(\tilde{O}(dH^2\sqrt{T})\)。通过仿真实验验证了理论成果。<br /><strong>Keywords:</strong> contextual reinforcement learning, delayed rewards, ad bidding, personalized advertising, CMDP, maximum likelihood estimator, regret bound<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuwei Cheng, Zifeng Zhao, Haifeng Xu</div>
Online advertising platforms use automated auctions to connect advertisers with potential customers, requiring effective bidding strategies to maximize profits. Accurate ad impact estimation requires considering three key factors: delayed and long-term effects, cumulative ad impacts such as reinforcement or fatigue, and customer heterogeneity. However, these effects are often not jointly addressed in previous studies. To capture these factors, we model ad bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson rewards. For efficient estimation, we propose a two-stage maximum likelihood estimator combined with data-splitting strategies, ensuring controlled estimation error based on the first-stage estimator's (in)accuracy. Building on this, we design a reinforcement learning algorithm to derive efficient personalized bidding strategies. This approach achieves a near-optimal regret bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension, $H$ is the number of rounds, and $T$ is the number of customers. Our theoretical findings are validated by simulation experiments.
<div><strong>Authors:</strong> Yuwei Cheng, Zifeng Zhao, Haifeng Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper models online ad bidding as a Contextual Markov Decision Process with delayed Poisson rewards to capture delayed effects, cumulative impact, and customer heterogeneity. It proposes a two‑stage maximum‑likelihood estimator with data‑splitting and a reinforcement‑learning algorithm that achieves a near‑optimal regret bound of \\(\\tilde{O}(dH^2\\sqrt{T})\\). Simulation experiments validate the theoretical results.", "summary_cn": "本文将在线广告竞价建模为具有延迟泊松奖励的上下文马尔可夫决策过程，以同时考虑延迟效应、累积影响和用户异质性。作者提出两阶段最大似然估计结合数据划分的方法，并基于此设计强化学习算法，实现近似最优的 regret 上界 \\(\\tilde{O}(dH^2\\sqrt{T})\\)。通过仿真实验验证了理论成果。", "keywords": "contextual reinforcement learning, delayed rewards, ad bidding, personalized advertising, CMDP, maximum likelihood estimator, regret bound", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuwei Cheng", "Zifeng Zhao", "Haifeng Xu"]}
]]></acme>

<pubDate>2025-10-22T22:08:36+00:00</pubDate>
</item>
<item>
<title>Speculative Sampling for Parametric Temporal Point Processes</title>
<link>https://papers.cool/arxiv/2510.20031</link>
<guid>https://papers.cool/arxiv/2510.20031</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a rejection‑sampling based algorithm that allows exact, parallel sampling of multiple future events from existing parametric temporal point process models without modifying their architecture or retraining. The method provides theoretical guarantees and demonstrates empirical speedups on real‑world datasets, bridging the gap between expressive modeling and efficient large‑scale generation. This enables more scalable applications of TPPs for event‑sequence data.<br /><strong>Summary (CN):</strong> 本文提出一种基于拒绝采样的算法，可在不改动模型结构或重新训练的前提下，实现对已有参数化时间点过程模型的多个未来事件的精确并行采样。该方法提供理论保证，并在真实数据集上展示了显著的加速效果，弥合了模型表达能力与大规模高效生成之间的鸿沟。<br /><strong>Keywords:</strong> temporal point processes, rejection sampling, parallel exact sampling, autoregressive models, event sequence generation, scalability, generative modeling, parallel generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Marin Biloš, Anderson Schneider, Yuriy Nevmyvaka</div>
Temporal point processes are powerful generative models for event sequences that capture complex dependencies in time-series data. They are commonly specified using autoregressive models that learn the distribution of the next event from the previous events. This makes sampling inherently sequential, limiting efficiency. In this paper, we propose a novel algorithm based on rejection sampling that enables exact sampling of multiple future values from existing TPP models, in parallel, and without requiring any architectural changes or retraining. Besides theoretical guarantees, our method demonstrates empirical speedups on real-world datasets, bridging the gap between expressive modeling and efficient parallel generation for large-scale TPP applications.
<div><strong>Authors:</strong> Marin Biloš, Anderson Schneider, Yuriy Nevmyvaka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a rejection‑sampling based algorithm that allows exact, parallel sampling of multiple future events from existing parametric temporal point process models without modifying their architecture or retraining. The method provides theoretical guarantees and demonstrates empirical speedups on real‑world datasets, bridging the gap between expressive modeling and efficient large‑scale generation. This enables more scalable applications of TPPs for event‑sequence data.", "summary_cn": "本文提出一种基于拒绝采样的算法，可在不改动模型结构或重新训练的前提下，实现对已有参数化时间点过程模型的多个未来事件的精确并行采样。该方法提供理论保证，并在真实数据集上展示了显著的加速效果，弥合了模型表达能力与大规模高效生成之间的鸿沟。", "keywords": "temporal point processes, rejection sampling, parallel exact sampling, autoregressive models, event sequence generation, scalability, generative modeling, parallel generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Marin Biloš", "Anderson Schneider", "Yuriy Nevmyvaka"]}
]]></acme>

<pubDate>2025-10-22T21:20:26+00:00</pubDate>
</item>
<item>
<title>The Temporal Graph of Bitcoin Transactions</title>
<link>https://papers.cool/arxiv/2510.20028</link>
<guid>https://papers.cool/arxiv/2510.20028</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a temporal, heterogeneous graph representation of the entire Bitcoin transaction history, reconstructing fund flows to produce a dataset with over 2.4 billion nodes and 39.7 billion edges. It provides sampling methods, feature vectors, tools for loading the data into graph databases, and ready-to-use snapshots to enable large‑scale graph machine‑learning research such as anomaly detection and address classification. The dataset and toolkit aim to make Bitcoin’s economic topology accessible for ML studies.<br /><strong>Summary (CN):</strong> 本文提出了一种时间异构图，用于重建比特币全部交易历史的资金流动，生成包含超过 24 亿节点和 397 亿边的完整数据集。论文提供了社区采样方法、节点和边特征向量、图数据库加载工具以及可直接使用的数据库快照，以支持大规模图机器学习研究，如异常检测和地址分类。该数据集和工具旨在让比特币的经济拓扑对机器学习社区可访问。<br /><strong>Keywords:</strong> Bitcoin, transaction graph, temporal graph, cryptocurrency, anomaly detection, graph machine learning, dataset, UTXO, blockchain analytics, large-scale graph<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vahid Jalili</div>
Since its 2009 genesis block, the Bitcoin network has processed \num{>1.08} billion (B) transactions representing \num{>8.72}B BTC, offering rich potential for machine learning (ML); yet, its pseudonymity and obscured flow of funds inherent in its \utxo-based design, have rendered this data largely inaccessible for ML research. Addressing this gap, we present an ML-compatible graph modeling the Bitcoin's economic topology by reconstructing the flow of funds. This temporal, heterogeneous graph encompasses complete transaction history up to block \cutoffHeight, consisting of \num{>2.4}B nodes and \num{>39.72}B edges. Additionally, we provide custom sampling methods yielding node and edge feature vectors of sampled communities, tools to load and analyze the Bitcoin graph data within specialized graph databases, and ready-to-use database snapshots. This comprehensive dataset and toolkit empower the ML community to tackle Bitcoin's intricate ecosystem at scale, driving progress in applications such as anomaly detection, address classification, market analysis, and large-scale graph ML benchmarking. Dataset and code available at \href{https://github.com/B1AAB/EBA}{github.com/b1aab/eba}
<div><strong>Authors:</strong> Vahid Jalili</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a temporal, heterogeneous graph representation of the entire Bitcoin transaction history, reconstructing fund flows to produce a dataset with over 2.4 billion nodes and 39.7 billion edges. It provides sampling methods, feature vectors, tools for loading the data into graph databases, and ready-to-use snapshots to enable large‑scale graph machine‑learning research such as anomaly detection and address classification. The dataset and toolkit aim to make Bitcoin’s economic topology accessible for ML studies.", "summary_cn": "本文提出了一种时间异构图，用于重建比特币全部交易历史的资金流动，生成包含超过 24 亿节点和 397 亿边的完整数据集。论文提供了社区采样方法、节点和边特征向量、图数据库加载工具以及可直接使用的数据库快照，以支持大规模图机器学习研究，如异常检测和地址分类。该数据集和工具旨在让比特币的经济拓扑对机器学习社区可访问。", "keywords": "Bitcoin, transaction graph, temporal graph, cryptocurrency, anomaly detection, graph machine learning, dataset, UTXO, blockchain analytics, large-scale graph", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vahid Jalili"]}
]]></acme>

<pubDate>2025-10-22T21:10:46+00:00</pubDate>
</item>
<item>
<title>SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph</title>
<link>https://papers.cool/arxiv/2510.20022</link>
<guid>https://papers.cool/arxiv/2510.20022</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SALT, a lightweight framework that assigns step‑level advantages to long‑horizon language agents by constructing a trajectory graph from multiple rollouts of the same prompt, enabling finer‑grained credit assignment based only on outcome rewards. SALT can be plugged into existing group‑based reinforcement‑learning methods such as GRPO without changing the rollout procedure, and experiments on WebShop, ALFWorld, and AppWorld show consistent performance gains.<br /><strong>Summary (CN):</strong> 本文提出 SALT 框架，通过为同一提示的多个轨迹构建图结构，基于最终奖励对长时程语言代理的每一步进行细粒度优势分配，实现更精确的信用分配。该方法可作为插件直接集成到如 GRPO 等基于组的强化学习算法中，实验在 WebShop、ALFWorld 与 AppWorld 上表明显著提升性能。<br /><strong>Keywords:</strong> step-level advantage, trajectory graph, long-horizon language agents, group reinforcement learning, GRPO, credit assignment, WebShop, ALFWorld, AppWorld<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Jiazheng Li, Yawei Wang, David Yan, Yijun Tian, Zhichao Xu, Huan Song, Panpan Xu, Lin Lee Cheong</div>
Large Language Models (LLMs) have demonstrated remarkable capabilities, enabling language agents to excel at single-turn tasks. However, their application to complex, multi-step, and long-horizon tasks remains challenging. While reinforcement learning (RL) offers a promising avenue for addressing these challenges, mainstream approaches typically rely solely on sparse, outcome-based rewards, a limitation that becomes especially problematic for group-based RL algorithms lacking critic models, such as Group Relative Policy Optimization (GRPO). In such methods, uniformly rewarding or penalizing all actions within a trajectory can lead to training instability and suboptimal policies, because beneficial and detrimental actions are often entangled across multi-step interactions. To address this challenge, we propose SALT, a novel and lightweight framework that provides a finer-grained advantage assignment, derived solely from outcome rewards. We achieve this by constructing a graph from trajectories of the same prompt, which allows us to quantify the quality of each step and assign advantages accordingly. Crucially, SALT is designed as a plug-and-play module that seamlessly integrates with existing group-based RL algorithms, requiring no modifications to the rollout procedure and introducing negligible computational overhead. Extensive experiments on the WebShop, ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that SALT consistently improves performance. We also conduct a thorough analysis to validate the design choices behind SALT and offer actionable insights.
<div><strong>Authors:</strong> Jiazheng Li, Yawei Wang, David Yan, Yijun Tian, Zhichao Xu, Huan Song, Panpan Xu, Lin Lee Cheong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SALT, a lightweight framework that assigns step‑level advantages to long‑horizon language agents by constructing a trajectory graph from multiple rollouts of the same prompt, enabling finer‑grained credit assignment based only on outcome rewards. SALT can be plugged into existing group‑based reinforcement‑learning methods such as GRPO without changing the rollout procedure, and experiments on WebShop, ALFWorld, and AppWorld show consistent performance gains.", "summary_cn": "本文提出 SALT 框架，通过为同一提示的多个轨迹构建图结构，基于最终奖励对长时程语言代理的每一步进行细粒度优势分配，实现更精确的信用分配。该方法可作为插件直接集成到如 GRPO 等基于组的强化学习算法中，实验在 WebShop、ALFWorld 与 AppWorld 上表明显著提升性能。", "keywords": "step-level advantage, trajectory graph, long-horizon language agents, group reinforcement learning, GRPO, credit assignment, WebShop, ALFWorld, AppWorld", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Jiazheng Li", "Yawei Wang", "David Yan", "Yijun Tian", "Zhichao Xu", "Huan Song", "Panpan Xu", "Lin Lee Cheong"]}
]]></acme>

<pubDate>2025-10-22T20:50:24+00:00</pubDate>
</item>
<item>
<title>Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications</title>
<link>https://papers.cool/arxiv/2510.20019</link>
<guid>https://papers.cool/arxiv/2510.20019</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a supervised learning simulation that uses realistic RSSI data and Decision Tree classification within a CAD‑modeled floor plan to localize RFID tags across twelve laboratory zones for defense logistics. Using a balanced subset of 5,000 observations from a dataset of ~980,000 reads, the model attains 34.2% overall accuracy and F1 scores above 0.40 for several zones, while struggling with rare classes, and introduces an adjacency‑aware confusion matrix for better interpretation of zone misclassifications. The study highlights the potential of RSSI‑based decision trees for zone‑level anomaly detection in defense contexts, noting that improved antenna placement and sensor fusion could boost performance in low‑coverage areas.<br /><strong>Summary (CN):</strong> 本文在 CAD 建模的防御设施平面图中，利用真实 RSSI 数据和决策树分类进行 RFID 标签的定位仿真，目标是对十二个实验室区域进行定位推断。使用约 980,000 条读取的平衡子样本（5,000 条）进行训练，模型整体准确率为 34.2%，多个区域的 F1 分数超过 0.40，但对稀有区域（如 LabZoneC）仍易误分类，并通过邻接感知混淆矩阵更好地解释相邻区域的错误。研究表明 RSSI‑基决策树可用于防御供应物流的区域异常检测，指出更佳的天线布置或多模态传感器融合可提升低覆盖区的分类性能。<br /><strong>Keywords:</strong> RFID, localization, RSSI, decision tree, sensor networks, defense applications, CAD modeling, class imbalance, zone classification, anomaly detection<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Curtis Lee Shull, Merrick Green</div>
Radio Frequency Identification (RFID) tracking may be a viable solution for defense assets that must be stored in accordance with security guidelines. However, poor sensor specificity (vulnerabilities include long range detection, spoofing, and counterfeiting) can lead to erroneous detection and operational security events. We present a supervised learning simulation with realistic Received Signal Strength Indicator (RSSI) data and Decision Tree classification in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some of the challenges encountered in defense storage. In this work, we focused on classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw dataset had approximately 980,000 reads. Class frequencies were imbalanced, and class weights were calculated to account for class imbalance in this multi-class setting. The model, trained on stratified subsamples to 5,000 balanced observations, yielded an overall accuracy of 34.2% and F1-scores greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare classes (most notably LabZoneC) were often misclassified, even with the use of class weights. An adjacency-aware confusion matrix was calculated to allow better interpretation of physically adjacent zones. These results suggest that RSSI-based decision trees can be applied in realistic simulations to enable zone-level anomaly detection or misplacement monitoring for defense supply logistics. Reliable classification performance in low-coverage and low-signal zones could be improved with better antenna placement or additional sensors and sensor fusion with other modalities.
<div><strong>Authors:</strong> Curtis Lee Shull, Merrick Green</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a supervised learning simulation that uses realistic RSSI data and Decision Tree classification within a CAD‑modeled floor plan to localize RFID tags across twelve laboratory zones for defense logistics. Using a balanced subset of 5,000 observations from a dataset of ~980,000 reads, the model attains 34.2% overall accuracy and F1 scores above 0.40 for several zones, while struggling with rare classes, and introduces an adjacency‑aware confusion matrix for better interpretation of zone misclassifications. The study highlights the potential of RSSI‑based decision trees for zone‑level anomaly detection in defense contexts, noting that improved antenna placement and sensor fusion could boost performance in low‑coverage areas.", "summary_cn": "本文在 CAD 建模的防御设施平面图中，利用真实 RSSI 数据和决策树分类进行 RFID 标签的定位仿真，目标是对十二个实验室区域进行定位推断。使用约 980,000 条读取的平衡子样本（5,000 条）进行训练，模型整体准确率为 34.2%，多个区域的 F1 分数超过 0.40，但对稀有区域（如 LabZoneC）仍易误分类，并通过邻接感知混淆矩阵更好地解释相邻区域的错误。研究表明 RSSI‑基决策树可用于防御供应物流的区域异常检测，指出更佳的天线布置或多模态传感器融合可提升低覆盖区的分类性能。", "keywords": "RFID, localization, RSSI, decision tree, sensor networks, defense applications, CAD modeling, class imbalance, zone classification, anomaly detection", "scoring": {"interpretability": 4, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Curtis Lee Shull", "Merrick Green"]}
]]></acme>

<pubDate>2025-10-22T20:40:50+00:00</pubDate>
</item>
<item>
<title>No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.19990</link>
<guid>https://papers.cool/arxiv/2510.19990</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes reasoning-as-infilling for masked diffusion language models, allowing structured reasoning traces, answer uncertainty measurement, early exits, and posterior sampling for post‑training data generation. It also introduces multi‑token entropy decoding (MED), an adaptive parallel decoding strategy that selects positions with low conditional entropy, preserving performance while reducing decoding steps by 2.7×. Experiments on GSM8k show that fine‑tuning on model‑generated reasoning traces matches gains from human‑written traces.<br /><strong>Summary (CN):</strong> 本文提出将推理视为填空（reasoning‑as‑infilling）用于掩码扩散语言模型（MDLM），通过在推理模板中填充实现对推理过程的结构化、答案不确定性的测量、提前退出以及在已知答案条件下采样后训练数据。随后引入多标记熵解码（MED），一种基于条件熵自适应选择并行解码位置的方法，在保持性能的同时将解码步数缩减约 2.7 倍。实验表明，在 GSM8k 上使用模型生成的推理轨迹进行微调，可获得与使用人类推理轨迹相当的性能提升。<br /><strong>Keywords:</strong> masked diffusion language models, reasoning-as-infilling, multi-token entropy decoding, early exit, posterior sampling, GSM8k, LLaDA, inference efficiency<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zachary Horvitz, Raghav Singhal, Hao Zou, Carles Domingo-Enrich, Zhou Yu, Rajesh Ranganath, Kathleen McKeown</div>
Masked diffusion language models (MDLMs) are trained to in-fill positions in randomly masked sequences, in contrast to next-token prediction models. Discussions around MDLMs focus on two benefits: (1) any-order decoding and 2) multi-token decoding. However, we observe that for math and coding tasks, any-order algorithms often underperform or behave similarly to left-to-right sampling, and standard multi-token decoding significantly degrades performance. At inference time, MDLMs compute the conditional distribution of all masked positions. A natural question is: How can we justify this additional compute when left-to-right one-token-at-a-time decoding is on par with any-order decoding algorithms? First, we propose reasoning-as-infilling. By using MDLMs to infill a reasoning template, we can structure outputs and distinguish between reasoning and answer tokens. In turn, this enables measuring answer uncertainty during reasoning, and early exits when the model converges on an answer. Next, given an answer, reasoning-as-infilling enables sampling from the MDLM posterior over reasoning traces conditioned on the answer, providing a new source of high-quality data for post-training. On GSM8k, we observe that fine-tuning LLaDA-8B Base on its posterior reasoning traces provides a performance boost on par with fine-tuning on human-written reasoning traces. Additionally, given an answer, reasoning-as-infilling provides a method for scoring the correctness of the reasoning process at intermediate steps. Second, we propose multi-token entropy decoding (MED), a simple adaptive sampler that minimizes the error incurred by decoding positions in parallel based on the conditional entropies of those positions. MED preserves performance across benchmarks and leads to 2.7x fewer steps. Our work demonstrates that the training and compute used by MDLMs unlock many new inference and post-training methods.
<div><strong>Authors:</strong> Zachary Horvitz, Raghav Singhal, Hao Zou, Carles Domingo-Enrich, Zhou Yu, Rajesh Ranganath, Kathleen McKeown</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes reasoning-as-infilling for masked diffusion language models, allowing structured reasoning traces, answer uncertainty measurement, early exits, and posterior sampling for post‑training data generation. It also introduces multi‑token entropy decoding (MED), an adaptive parallel decoding strategy that selects positions with low conditional entropy, preserving performance while reducing decoding steps by 2.7×. Experiments on GSM8k show that fine‑tuning on model‑generated reasoning traces matches gains from human‑written traces.", "summary_cn": "本文提出将推理视为填空（reasoning‑as‑infilling）用于掩码扩散语言模型（MDLM），通过在推理模板中填充实现对推理过程的结构化、答案不确定性的测量、提前退出以及在已知答案条件下采样后训练数据。随后引入多标记熵解码（MED），一种基于条件熵自适应选择并行解码位置的方法，在保持性能的同时将解码步数缩减约 2.7 倍。实验表明，在 GSM8k 上使用模型生成的推理轨迹进行微调，可获得与使用人类推理轨迹相当的性能提升。", "keywords": "masked diffusion language models, reasoning-as-infilling, multi-token entropy decoding, early exit, posterior sampling, GSM8k, LLaDA, inference efficiency", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zachary Horvitz", "Raghav Singhal", "Hao Zou", "Carles Domingo-Enrich", "Zhou Yu", "Rajesh Ranganath", "Kathleen McKeown"]}
]]></acme>

<pubDate>2025-10-22T19:41:27+00:00</pubDate>
</item>
<item>
<title>Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency</title>
<link>https://papers.cool/arxiv/2510.19980</link>
<guid>https://papers.cool/arxiv/2510.19980</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies a counterintuitive phenomenon that truncating historical data can improve time‑series prediction, suggesting that current deep models learn many redundant features. To address this, it proposes Adaptive Masking Loss with Representation Consistency (AMRC), which dynamically masks discriminative temporal segments and enforces consistency among inputs, labels, and predictions, thereby suppressing noise learning and boosting performance.<br /><strong>Summary (CN):</strong> 本文发现截断历史数据有时能提升时间序列预测效果，表明现有深度模型会学习大量冗余特征（如噪声）。为此提出自适应掩蔽损失与表示一致性（AMRC）方法，通过动态掩蔽关键时间段并保持输入、标签与预测之间的映射一致性，从而抑制冗余学习并显著提升模型性能。<br /><strong>Keywords:</strong> time series forecasting, adaptive masking loss, representation consistency, information bottleneck, redundant features, AMRC, deep learning, temporal modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Renzhao Liang, Sizhe Xu, Chenggang Xie, Jingru Chen, Feiyang Ren, Shu Yang, Takahiro Yabe</div>
Time series forecasting plays a pivotal role in critical domains such as energy management and financial markets. Although deep learning-based approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the prevailing "long-sequence information gain hypothesis" exhibits inherent limitations. Through systematic experimentation, this study reveals a counterintuitive phenomenon: appropriately truncating historical data can paradoxically enhance prediction accuracy, indicating that existing models learn substantial redundant features (e.g., noise or irrelevant fluctuations) during training, thereby compromising effective signal extraction. Building upon information bottleneck theory, we propose an innovative solution termed Adaptive Masking Loss with Representation Consistency (AMRC), which features two core components: 1) Dynamic masking loss, which adaptively identified highly discriminative temporal segments to guide gradient descent during model training; 2) Representation consistency constraint, which stabilized the mapping relationships among inputs, labels, and predictions. Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. This work not only challenges conventional assumptions in temporal modeling but also provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models.
<div><strong>Authors:</strong> Renzhao Liang, Sizhe Xu, Chenggang Xie, Jingru Chen, Feiyang Ren, Shu Yang, Takahiro Yabe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies a counterintuitive phenomenon that truncating historical data can improve time‑series prediction, suggesting that current deep models learn many redundant features. To address this, it proposes Adaptive Masking Loss with Representation Consistency (AMRC), which dynamically masks discriminative temporal segments and enforces consistency among inputs, labels, and predictions, thereby suppressing noise learning and boosting performance.", "summary_cn": "本文发现截断历史数据有时能提升时间序列预测效果，表明现有深度模型会学习大量冗余特征（如噪声）。为此提出自适应掩蔽损失与表示一致性（AMRC）方法，通过动态掩蔽关键时间段并保持输入、标签与预测之间的映射一致性，从而抑制冗余学习并显著提升模型性能。", "keywords": "time series forecasting, adaptive masking loss, representation consistency, information bottleneck, redundant features, AMRC, deep learning, temporal modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Renzhao Liang", "Sizhe Xu", "Chenggang Xie", "Jingru Chen", "Feiyang Ren", "Shu Yang", "Takahiro Yabe"]}
]]></acme>

<pubDate>2025-10-22T19:23:53+00:00</pubDate>
</item>
<item>
<title>Towards Strong Certified Defense with Universal Asymmetric Randomization</title>
<link>https://papers.cool/arxiv/2510.19977</link>
<guid>https://papers.cool/arxiv/2510.19977</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces UCAN, a method that transforms existing randomized smoothing techniques from isotropic to anisotropic noise distributions, enabling stronger certified adversarial robustness across various α_p norms. It provides a theoretical framework supporting arbitrary noise families and proposes three noise parameter generators to adapt the noise per data dimension, achieving up to 182.6% improvement in certified accuracy on MNIST, CIFAR-10 and ImageNet. Empirical results demonstrate significant gains over state-of-the-art certified defenses.<br /><strong>Summary (CN):</strong> 本文提出 UCAN 方法，将现有的随机平滑技术从各向同性噪声转变为各向异性噪声，从而在不同 α_p 范数下实现更强的可认证对抗鲁棒性。它提供了支持任意噪声分布的理论框架，并设计了三种噪声参数生成器，以针对不同数据维度进行噪声调优，在 MNIST、CIFAR-10 和 ImageNet 上实现最高 182.6% 的认证准确率提升。实验结果显示 UCAN 相较于现有最先进的认证防御方法有显著优势。<br /><strong>Keywords:</strong> certified robustness, randomized smoothing, anisotropic noise, universal certification, adversarial defense<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Hanbin Hong, Ashish Kundu, Ali Payani, Binghui Wang, Yuan Hong</div>
Randomized smoothing has become essential for achieving certified adversarial robustness in machine learning models. However, current methods primarily use isotropic noise distributions that are uniform across all data dimensions, such as image pixels, limiting the effectiveness of robustness certification by ignoring the heterogeneity of inputs and data dimensions. To address this limitation, we propose UCAN: a novel technique that \underline{U}niversally \underline{C}ertifies adversarial robustness with \underline{A}nisotropic \underline{N}oise. UCAN is designed to enhance any existing randomized smoothing method, transforming it from symmetric (isotropic) to asymmetric (anisotropic) noise distributions, thereby offering a more tailored defense against adversarial attacks. Our theoretical framework is versatile, supporting a wide array of noise distributions for certified robustness in different $\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the classifier's prediction over perturbed inputs with provable robustness bounds through tailored noise injection. Additionally, we develop a novel framework equipped with three exemplary noise parameter generators (NPGs) to optimally fine-tune the anisotropic noise parameters for different data dimensions, allowing for pursuing different levels of robustness enhancements in practice.Empirical evaluations underscore the significant leap in UCAN's performance over existing state-of-the-art methods, demonstrating up to $182.6\%$ improvement in certified accuracy at large certified radii on MNIST, CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at \href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}
<div><strong>Authors:</strong> Hanbin Hong, Ashish Kundu, Ali Payani, Binghui Wang, Yuan Hong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces UCAN, a method that transforms existing randomized smoothing techniques from isotropic to anisotropic noise distributions, enabling stronger certified adversarial robustness across various α_p norms. It provides a theoretical framework supporting arbitrary noise families and proposes three noise parameter generators to adapt the noise per data dimension, achieving up to 182.6% improvement in certified accuracy on MNIST, CIFAR-10 and ImageNet. Empirical results demonstrate significant gains over state-of-the-art certified defenses.", "summary_cn": "本文提出 UCAN 方法，将现有的随机平滑技术从各向同性噪声转变为各向异性噪声，从而在不同 α_p 范数下实现更强的可认证对抗鲁棒性。它提供了支持任意噪声分布的理论框架，并设计了三种噪声参数生成器，以针对不同数据维度进行噪声调优，在 MNIST、CIFAR-10 和 ImageNet 上实现最高 182.6% 的认证准确率提升。实验结果显示 UCAN 相较于现有最先进的认证防御方法有显著优势。", "keywords": "certified robustness, randomized smoothing, anisotropic noise, universal certification, adversarial defense", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Hanbin Hong", "Ashish Kundu", "Ali Payani", "Binghui Wang", "Yuan Hong"]}
]]></acme>

<pubDate>2025-10-22T19:14:26+00:00</pubDate>
</item>
<item>
<title>Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</title>
<link>https://papers.cool/arxiv/2510.19975</link>
<guid>https://papers.cool/arxiv/2510.19975</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies two‑point zeroth‑order gradient estimators and determines the perturbation distribution that minimizes asymptotic variance as the stepsize approaches zero. It shows that optimal perturbations can be directionally aligned with the true gradient, proposes the Directionally Aligned Perturbation (DAP) scheme, and provides convergence analysis and empirical results demonstrating DAP’s advantage over fixed‑length perturbations under certain conditions.<br /><strong>Summary (CN):</strong> 本文研究了两点零阶梯度估计器，并找出在步长趋于零时使估计方差最小的扰动分布，发现最优扰动可以在方向上与真实梯度对齐。作者提出方向对齐扰动（DAP）方案，给出相应的收敛分析，并通过合成及实际任务实验表明 DAP 在特定条件下优于传统固定长度扰动方法。<br /><strong>Keywords:</strong> zeroth-order optimization, two-point estimator, minimum-variance, directionally aligned perturbations, stochastic gradient descent, functional optimization, gradient estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $\delta$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies two‑point zeroth‑order gradient estimators and determines the perturbation distribution that minimizes asymptotic variance as the stepsize approaches zero. It shows that optimal perturbations can be directionally aligned with the true gradient, proposes the Directionally Aligned Perturbation (DAP) scheme, and provides convergence analysis and empirical results demonstrating DAP’s advantage over fixed‑length perturbations under certain conditions.", "summary_cn": "本文研究了两点零阶梯度估计器，并找出在步长趋于零时使估计方差最小的扰动分布，发现最优扰动可以在方向上与真实梯度对齐。作者提出方向对齐扰动（DAP）方案，给出相应的收敛分析，并通过合成及实际任务实验表明 DAP 在特定条件下优于传统固定长度扰动方法。", "keywords": "zeroth-order optimization, two-point estimator, minimum-variance, directionally aligned perturbations, stochastic gradient descent, functional optimization, gradient estimation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T19:06:39+00:00</pubDate>
</item>
<item>
<title>On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization</title>
<link>https://papers.cool/arxiv/2510.19953</link>
<guid>https://papers.cool/arxiv/2510.19953</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a novel family of unbiased gradient estimators for zeroth-order optimization by reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions. It derives optimal scaling distributions and perturbation stepsizes for four constructions, proving that SGD with these estimators attains optimal complexity on smooth non-convex objectives, and validates the approach with synthetic experiments and language model fine-tuning.<br /><strong>Summary (CN):</strong> 本文提出了一类新的无偏梯度估计器用于零阶优化，通过将方向导数重写为望远镜级数并从精心设计的分布中采样，实现了在仅使用函数评估的情况下消除偏差并保持方差可控。文中推导了四种具体构造的最优尺度分布和扰动步长，并证明使用该估计器的 SGD 在平滑非凸目标上达到最优复杂度；实验在合成任务及语言模型微调上展示了其相较于标准方法的更高准确率和收敛速度。<br /><strong>Keywords:</strong> zeroth-order optimization, unbiased gradient estimator, stochastic gradient descent, non-convex optimization, variance reduction, function evaluation, optimal scaling distribution, language model fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of unbiased gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a novel family of unbiased gradient estimators for zeroth-order optimization by reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions. It derives optimal scaling distributions and perturbation stepsizes for four constructions, proving that SGD with these estimators attains optimal complexity on smooth non-convex objectives, and validates the approach with synthetic experiments and language model fine-tuning.", "summary_cn": "本文提出了一类新的无偏梯度估计器用于零阶优化，通过将方向导数重写为望远镜级数并从精心设计的分布中采样，实现了在仅使用函数评估的情况下消除偏差并保持方差可控。文中推导了四种具体构造的最优尺度分布和扰动步长，并证明使用该估计器的 SGD 在平滑非凸目标上达到最优复杂度；实验在合成任务及语言模型微调上展示了其相较于标准方法的更高准确率和收敛速度。", "keywords": "zeroth-order optimization, unbiased gradient estimator, stochastic gradient descent, non-convex optimization, variance reduction, function evaluation, optimal scaling distribution, language model fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T18:25:43+00:00</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</title>
<link>https://papers.cool/arxiv/2510.19950</link>
<guid>https://papers.cool/arxiv/2510.19950</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes elliptic uncertainty sets to model directional market impact in reinforcement learning agents used for financial trading, providing closed-form worst-case solutions that enable efficient robust policy evaluation. Empirical results on single‑asset and multi‑asset tasks show improved Sharpe ratios and robustness to larger trade volumes compared to traditional symmetric uncertainty approaches.<br /><strong>Summary (CN):</strong> 本文提出使用椭圆不确定性集合来建模金融交易中强化学习代理的方向性市场冲击，并给出最坏情况的闭式解，从而实现高效的鲁棒策略评估。实验在单资产和多资产交易任务上显示，相较于传统对称不确定性方法，本文方法提升了夏普比率并在交易量增大时保持鲁棒性。<br /><strong>Keywords:</strong> robust reinforcement learning, market impact, elliptic uncertainty sets, financial trading, robust policy evaluation, Sharpe ratio, RL robustness, uncertainty sets<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes elliptic uncertainty sets to model directional market impact in reinforcement learning agents used for financial trading, providing closed-form worst-case solutions that enable efficient robust policy evaluation. Empirical results on single‑asset and multi‑asset tasks show improved Sharpe ratios and robustness to larger trade volumes compared to traditional symmetric uncertainty approaches.", "summary_cn": "本文提出使用椭圆不确定性集合来建模金融交易中强化学习代理的方向性市场冲击，并给出最坏情况的闭式解，从而实现高效的鲁棒策略评估。实验在单资产和多资产交易任务上显示，相较于传统对称不确定性方法，本文方法提升了夏普比率并在交易量增大时保持鲁棒性。", "keywords": "robust reinforcement learning, market impact, elliptic uncertainty sets, financial trading, robust policy evaluation, Sharpe ratio, RL robustness, uncertainty sets", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T18:22:25+00:00</pubDate>
</item>
<item>
<title>Are Greedy Task Orderings Better Than Random in Continual Linear Regression?</title>
<link>https://papers.cool/arxiv/2510.19941</link>
<guid>https://papers.cool/arxiv/2510.19941</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies task orderings in continual learning for linear regression, focusing on greedy strategies that maximize dissimilarity between consecutive tasks. Using tools from the Kaczmarz method, it provides geometric and algebraic insights, proves loss bounds for high‑rank settings, and shows experimentally that greedy orderings converge faster than random ones, though single‑pass greedy orderings can fail catastrophically while repeated greedy orderings converge at a rate of O(k^{-1/3}).<br /><strong>Summary (CN):</strong> 本文研究在连续学习线性回归中的任务排序，重点考察通过最大化连续任务间不相似性实现的贪心策略。借助 Kaczmarz 方法的工具，提供几何和代数直观，证明在高秩回归情况下贪心排序的损失上界，并在实验中展示贪心排序相较随机排序收敛更快；然而单遍贪心排序可能出现灾难性失败，而允许重复的贪心排序收敛速率为 O(k^{-1/3})。<br /><strong>Keywords:</strong> continual learning, task ordering, linear regression, greedy algorithm, Kaczmarz method, convergence rate, catastrophic forgetting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Matan Tsipory, Ran Levinstein, Itay Evron, Mark Kong, Deanna Needell, Daniel Soudry</div>
We analyze task orderings in continual learning for linear regression, assuming joint realizability of training data. We focus on orderings that greedily maximize dissimilarity between consecutive tasks, a concept briefly explored in prior work but still surrounded by open questions. Using tools from the Kaczmarz method literature, we formalize such orderings and develop geometric and algebraic intuitions around them. Empirically, we demonstrate that greedy orderings converge faster than random ones in terms of the average loss across tasks, both for linear regression with random data and for linear probing on CIFAR-100 classification tasks. Analytically, in a high-rank regression setting, we prove a loss bound for greedy orderings analogous to that of random ones. However, under general rank, we establish a repetition-dependent separation. Specifically, while prior work showed that for random orderings, with or without replacement, the average loss after $k$ iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass greedy orderings may fail catastrophically, whereas those allowing repetition converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances within and between greedy and random orderings.
<div><strong>Authors:</strong> Matan Tsipory, Ran Levinstein, Itay Evron, Mark Kong, Deanna Needell, Daniel Soudry</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies task orderings in continual learning for linear regression, focusing on greedy strategies that maximize dissimilarity between consecutive tasks. Using tools from the Kaczmarz method, it provides geometric and algebraic insights, proves loss bounds for high‑rank settings, and shows experimentally that greedy orderings converge faster than random ones, though single‑pass greedy orderings can fail catastrophically while repeated greedy orderings converge at a rate of O(k^{-1/3}).", "summary_cn": "本文研究在连续学习线性回归中的任务排序，重点考察通过最大化连续任务间不相似性实现的贪心策略。借助 Kaczmarz 方法的工具，提供几何和代数直观，证明在高秩回归情况下贪心排序的损失上界，并在实验中展示贪心排序相较随机排序收敛更快；然而单遍贪心排序可能出现灾难性失败，而允许重复的贪心排序收敛速率为 O(k^{-1/3})。", "keywords": "continual learning, task ordering, linear regression, greedy algorithm, Kaczmarz method, convergence rate, catastrophic forgetting", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Matan Tsipory", "Ran Levinstein", "Itay Evron", "Mark Kong", "Deanna Needell", "Daniel Soudry"]}
]]></acme>

<pubDate>2025-10-22T18:09:59+00:00</pubDate>
</item>
<item>
<title>Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy</title>
<link>https://papers.cool/arxiv/2510.19934</link>
<guid>https://papers.cool/arxiv/2510.19934</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes two novel f-differential privacy accounting methods for decentralized federated learning: Pairwise Network f-DP, which measures privacy leakage between user pairs in a random-walk communication graph, and Secret-based f-Local DP, which enables structured noise injection using shared secrets. By leveraging f-DP theory and Markov chain concentration, the framework captures privacy amplification from sparse communication and local updates, yielding tighter (ε,δ) bounds and better utility than Rényi DP approaches in experiments.<br /><strong>Summary (CN):</strong> 本文提出了两种针对去中心化联邦学习的 f-差分隐私 (f-DP) 计量方法：配对网络 f-DP (PN-f-DP)，用于在随机游走通信图中量化用户对之间的隐私泄漏；以及基于秘密的 f-本地 DP (Sec-f-LDP)，通过共享秘密实现结构化噪声注入。利用 f-DP 理论和马尔可夫链浓缩技术，框架捕捉稀疏通信和本地迭代带来的隐私放大效应，在实验中相较于 Rényi DP 方法实现了更紧的 (ε,δ) 上界和更高的效用。<br /><strong>Keywords:</strong> federated learning, decentralized federated learning, differential privacy, f-differential privacy, privacy accounting, pairwise network f-DP, secret-based f-LDP, privacy-utility tradeoff<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Xiang Li, Buxin Su, Chendi Wang, Qi Long, Weijie J. Su</div>
Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the $f$-differential privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise injection via shared secrets. By combining tools from $f$-DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared to Rényi DP-based approaches, illustrating the benefits of $f$-DP in decentralized privacy accounting.
<div><strong>Authors:</strong> Xiang Li, Buxin Su, Chendi Wang, Qi Long, Weijie J. Su</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes two novel f-differential privacy accounting methods for decentralized federated learning: Pairwise Network f-DP, which measures privacy leakage between user pairs in a random-walk communication graph, and Secret-based f-Local DP, which enables structured noise injection using shared secrets. By leveraging f-DP theory and Markov chain concentration, the framework captures privacy amplification from sparse communication and local updates, yielding tighter (ε,δ) bounds and better utility than Rényi DP approaches in experiments.", "summary_cn": "本文提出了两种针对去中心化联邦学习的 f-差分隐私 (f-DP) 计量方法：配对网络 f-DP (PN-f-DP)，用于在随机游走通信图中量化用户对之间的隐私泄漏；以及基于秘密的 f-本地 DP (Sec-f-LDP)，通过共享秘密实现结构化噪声注入。利用 f-DP 理论和马尔可夫链浓缩技术，框架捕捉稀疏通信和本地迭代带来的隐私放大效应，在实验中相较于 Rényi DP 方法实现了更紧的 (ε,δ) 上界和更高的效用。", "keywords": "federated learning, decentralized federated learning, differential privacy, f-differential privacy, privacy accounting, pairwise network f-DP, secret-based f-LDP, privacy-utility tradeoff", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Xiang Li", "Buxin Su", "Chendi Wang", "Qi Long", "Weijie J. Su"]}
]]></acme>

<pubDate>2025-10-22T18:01:08+00:00</pubDate>
</item>
<item>
<title>Beyond the Ideal: Analyzing the Inexact Muon Update</title>
<link>https://papers.cool/arxiv/2510.19933</link>
<guid>https://papers.cool/arxiv/2510.19933</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides the first theoretical analysis of Muon's inexact orthogonalized update by modeling the practical approximation error within a Linear Minimization Oracle framework. It derives explicit bounds showing how oracle inexactness couples with optimal step size and momentum, and validates these predictions with NanoGPT experiments that demonstrate learning rate shifts as approximation precision varies.<br /><strong>Summary (CN):</strong> 本文首次在 Linear Minimization Oracle 框架下，对 Muon 优化器的近似正交更新进行理论分析，建立了实际近似误差的加性模型。研究得出误差程度与最佳学习率和动量之间的耦合关系，并通过 NanoGPT 实验验证了随着近似精度变化学习率的显著移动。<br /><strong>Keywords:</strong> Muon optimizer, orthogonalization, inexact LMO, linear minimization oracle, step size, momentum, Newton-Schulz, NanoGPT, optimization analysis, training dynamics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Egor Shulgin, Sultan AlRashed, Francesco Orabona, Peter Richtárik</div>
The Muon optimizer has rapidly emerged as a powerful, geometry-aware alternative to AdamW, demonstrating strong performance in large-scale training of neural networks. However, a critical theory-practice disconnect exists: Muon's efficiency relies on fast, approximate orthogonalization, yet all prior theoretical work analyzes an idealized, computationally intractable version assuming exact SVD-based updates. This work moves beyond the ideal by providing the first analysis of the inexact orthogonalized update at Muon's core. We develop our analysis within the general framework of Linear Minimization Oracle (LMO)-based optimization, introducing a realistic additive error model to capture the inexactness of practical approximation schemes. Our analysis yields explicit bounds that quantify performance degradation as a function of the LMO inexactness/error. We reveal a fundamental coupling between this inexactness and the optimal step size and momentum: lower oracle precision requires a smaller step size but larger momentum parameter. These findings elevate the approximation procedure (e.g., the number of Newton-Schulz steps) from an implementation detail to a critical parameter that must be co-tuned with the learning schedule. NanoGPT experiments directly confirm the predicted coupling, with optimal learning rates clearly shifting as approximation precision changes.
<div><strong>Authors:</strong> Egor Shulgin, Sultan AlRashed, Francesco Orabona, Peter Richtárik</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides the first theoretical analysis of Muon's inexact orthogonalized update by modeling the practical approximation error within a Linear Minimization Oracle framework. It derives explicit bounds showing how oracle inexactness couples with optimal step size and momentum, and validates these predictions with NanoGPT experiments that demonstrate learning rate shifts as approximation precision varies.", "summary_cn": "本文首次在 Linear Minimization Oracle 框架下，对 Muon 优化器的近似正交更新进行理论分析，建立了实际近似误差的加性模型。研究得出误差程度与最佳学习率和动量之间的耦合关系，并通过 NanoGPT 实验验证了随着近似精度变化学习率的显著移动。", "keywords": "Muon optimizer, orthogonalization, inexact LMO, linear minimization oracle, step size, momentum, Newton-Schulz, NanoGPT, optimization analysis, training dynamics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Egor Shulgin", "Sultan AlRashed", "Francesco Orabona", "Peter Richtárik"]}
]]></acme>

<pubDate>2025-10-22T18:01:07+00:00</pubDate>
</item>
<item>
<title>FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals</title>
<link>https://papers.cool/arxiv/2510.19917</link>
<guid>https://papers.cool/arxiv/2510.19917</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FINDER, a framework that treats datasets as realizations of random fields and leverages the Karhunen‑Loève expansion to extract stochastic features for classification in low‑signal‑to‑noise regimes. By analyzing the eigenspace residuals of class‑specific operators, FINDER separates classes in the spectral domain and demonstrates state‑of‑the‑art performance on Alzheimer's disease staging and deforestation detection. Limitations, expected outperforming conditions, and failure modes are also discussed.<br /><strong>Summary (CN):</strong> 本文提出 FINDER 框架，将数据集视为随机场的实现，通过 Karhunen‑Loève 展开提取随机特征，并利用特征空间残差的特征值分解在噪声较大的环境中进行分类。该方法在阿尔茨海默病阶段划分和森林砍伐遥感检测两大科学任务上取得了领先性能，并讨论了适用条件、局限性以及潜在失效模式。<br /><strong>Keywords:</strong> feature inference, noisy datasets, eigen decomposition, Karhunen-Loève expansion, stochastic features, classification, robustness, medical imaging, remote sensing<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Trajan Murphy, Akshunna S. Dogra, Hanfeng Gu, Caleb Meredith, Mark Kon, Julio Enrique Castrillion-Candas</div>
''Noisy'' datasets (regimes with low signal to noise ratios, small sample sizes, faulty data collection, etc) remain a key research frontier for classification methods with both theoretical and practical implications. We introduce FINDER, a rigorous framework for analyzing generic classification problems, with tailored algorithms for noisy datasets. FINDER incorporates fundamental stochastic analysis ideas into the feature learning and inference stages to optimally account for the randomness inherent to all empirical datasets. We construct ''stochastic features'' by first viewing empirical datasets as realizations from an underlying random field (without assumptions on its exact distribution) and then mapping them to appropriate Hilbert spaces. The Kosambi-Karhunen-Loéve expansion (KLE) breaks these stochastic features into computable irreducible components, which allow classification over noisy datasets via an eigen-decomposition: data from different classes resides in distinct regions, identified by analyzing the spectrum of the associated operators. We validate FINDER on several challenging, data-deficient scientific domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease stage classification, (ii) Remote sensing detection of deforestation. We end with a discussion on when FINDER is expected to outperform existing methods, its failure modes, and other limitations.
<div><strong>Authors:</strong> Trajan Murphy, Akshunna S. Dogra, Hanfeng Gu, Caleb Meredith, Mark Kon, Julio Enrique Castrillion-Candas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FINDER, a framework that treats datasets as realizations of random fields and leverages the Karhunen‑Loève expansion to extract stochastic features for classification in low‑signal‑to‑noise regimes. By analyzing the eigenspace residuals of class‑specific operators, FINDER separates classes in the spectral domain and demonstrates state‑of‑the‑art performance on Alzheimer's disease staging and deforestation detection. Limitations, expected outperforming conditions, and failure modes are also discussed.", "summary_cn": "本文提出 FINDER 框架，将数据集视为随机场的实现，通过 Karhunen‑Loève 展开提取随机特征，并利用特征空间残差的特征值分解在噪声较大的环境中进行分类。该方法在阿尔茨海默病阶段划分和森林砍伐遥感检测两大科学任务上取得了领先性能，并讨论了适用条件、局限性以及潜在失效模式。", "keywords": "feature inference, noisy datasets, eigen decomposition, Karhunen-Loève expansion, stochastic features, classification, robustness, medical imaging, remote sensing", "scoring": {"interpretability": 3, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Trajan Murphy", "Akshunna S. Dogra", "Hanfeng Gu", "Caleb Meredith", "Mark Kon", "Julio Enrique Castrillion-Candas"]}
]]></acme>

<pubDate>2025-10-22T18:00:03+00:00</pubDate>
</item>
<item>
<title>Enhancing Diagnostic Accuracy for Urinary Tract Disease through Explainable SHAP-Guided Feature Selection and Classification</title>
<link>https://papers.cool/arxiv/2510.19896</link>
<guid>https://papers.cool/arxiv/2510.19896</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a diagnostic pipeline for urinary tract diseases, especially bladder cancer, that uses SHAP-based feature selection to identify the most informative clinical variables and then trains XGBoost, LightGBM, and CatBoost classifiers with hyperparameter tuning via Optuna and class balancing with SMOTE. The SHAP-guided selection improves model transparency while maintaining or enhancing performance metrics such as balanced accuracy, precision, and specificity, demonstrating the utility of explainability techniques in clinical decision support.<br /><strong>Summary (CN):</strong> 本文提出了一套针对泌尿道疾病（尤其是膀胱癌）的诊断流程，利用 SHAP（Shapley Additive exPlanations）进行特征选择以挑选最具信息量的临床变量，随后使用 XGBoost、LightGBM 和 CatBoost 分类器，并通过 Optuna 进行超参数调优及 SMOTE 进行类别平衡。SHAP 引导的特征选择在提升模型透明度的同时，保持或提升了平衡准确率、精确率和特异性等性能指标，展示了可解释性技术在临床决策支持系统中的有效性。<br /><strong>Keywords:</strong> urinary tract disease, bladder cancer, SHAP, feature selection, interpretability, XGBoost, LightGBM, CatBoost, Optuna, SMOTE<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Filipe Ferreira de Oliveira, Matheus Becali Rocha, Renato A. Krohling</div>
In this paper, we propose an approach to support the diagnosis of urinary tract diseases, with a focus on bladder cancer, using SHAP (SHapley Additive exPlanations)-based feature selection to enhance the transparency and effectiveness of predictive models. Six binary classification scenarios were developed to distinguish bladder cancer from other urological and oncological conditions. The algorithms XGBoost, LightGBM, and CatBoost were employed, with hyperparameter optimization performed using Optuna and class balancing with the SMOTE technique. The selection of predictive variables was guided by importance values through SHAP-based feature selection while maintaining or even improving performance metrics such as balanced accuracy, precision, and specificity. The use of explainability techniques (SHAP) for feature selection proved to be an effective approach. The proposed methodology may contribute to the development of more transparent, reliable, and efficient clinical decision support systems, optimizing screening and early diagnosis of urinary tract diseases.
<div><strong>Authors:</strong> Filipe Ferreira de Oliveira, Matheus Becali Rocha, Renato A. Krohling</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a diagnostic pipeline for urinary tract diseases, especially bladder cancer, that uses SHAP-based feature selection to identify the most informative clinical variables and then trains XGBoost, LightGBM, and CatBoost classifiers with hyperparameter tuning via Optuna and class balancing with SMOTE. The SHAP-guided selection improves model transparency while maintaining or enhancing performance metrics such as balanced accuracy, precision, and specificity, demonstrating the utility of explainability techniques in clinical decision support.", "summary_cn": "本文提出了一套针对泌尿道疾病（尤其是膀胱癌）的诊断流程，利用 SHAP（Shapley Additive exPlanations）进行特征选择以挑选最具信息量的临床变量，随后使用 XGBoost、LightGBM 和 CatBoost 分类器，并通过 Optuna 进行超参数调优及 SMOTE 进行类别平衡。SHAP 引导的特征选择在提升模型透明度的同时，保持或提升了平衡准确率、精确率和特异性等性能指标，展示了可解释性技术在临床决策支持系统中的有效性。", "keywords": "urinary tract disease, bladder cancer, SHAP, feature selection, interpretability, XGBoost, LightGBM, CatBoost, Optuna, SMOTE", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Filipe Ferreira de Oliveira", "Matheus Becali Rocha", "Renato A. Krohling"]}
]]></acme>

<pubDate>2025-10-22T17:48:50+00:00</pubDate>
</item>
<item>
<title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</title>
<link>https://papers.cool/arxiv/2510.19893</link>
<guid>https://papers.cool/arxiv/2510.19893</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes FairGRPO, a hierarchical reinforcement learning algorithm that adaptively reweights advantages to promote equitable performance across latent demographic groups in clinical diagnosis tasks. By combining unsupervised clustering to discover demographic subpopulations with importance‑weighted policy updates, FairGRPO reduces predictive parity gaps by 27.2% while improving overall F1 score, and the authors release a fairness‑aware clinical VLLM (FairMedGemma-4B).<br /><strong>Summary (CN):</strong> 本文提出 FairGRPO，一种层次化强化学习方法，通过对优势进行自适应加权，以在临床诊断任务中实现不同潜在人口群体的公平学习。该方法利用无监督聚类自动发现人口子群，并在策略更新中加入加权机制，使预测公平性差距降低 27.2%，整体 F1 分数提升，并发布了具备公平性的临床大模型 FairMedGemma-4B。<br /><strong>Keywords:</strong> fairness, reinforcement learning, clinical AI, group relative policy optimization, bias mitigation, multimodal diagnosis, demographic parity<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang</div>
Medical artificial intelligence systems have achieved remarkable diagnostic capabilities, yet they consistently exhibit performance disparities across demographic groups, causing real-world harm to underrepresented populations. While recent multimodal reasoning foundation models have advanced clinical diagnosis through integrated analysis of diverse medical data, reasoning trainings via reinforcement learning inherit and often amplify biases present in training datasets dominated by majority populations. We introduce Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach that promotes equitable learning across heterogeneous clinical populations. FairGRPO employs adaptive importance weighting of advantages based on representation, task difficulty, and data source. To address the common issue of missing demographic labels in the clinical domain, we further employ unsupervised clustering, which automatically discovers latent demographic groups when labels are unavailable. Through comprehensive experiments across 7 clinical diagnostic datasets spanning 5 clinical modalities across X-ray, CT scan, dermoscropy, mammography and ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2% against all vanilla and bias mitigated RL baselines, while improving F1 score by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO progressively improves fairness throughout optimization, while baseline RL methods exhibit deteriorating fairness as training progresses. Based on FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance while demonstrating significantly reduced disparities across demographic groups.
<div><strong>Authors:</strong> Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes FairGRPO, a hierarchical reinforcement learning algorithm that adaptively reweights advantages to promote equitable performance across latent demographic groups in clinical diagnosis tasks. By combining unsupervised clustering to discover demographic subpopulations with importance‑weighted policy updates, FairGRPO reduces predictive parity gaps by 27.2% while improving overall F1 score, and the authors release a fairness‑aware clinical VLLM (FairMedGemma-4B).", "summary_cn": "本文提出 FairGRPO，一种层次化强化学习方法，通过对优势进行自适应加权，以在临床诊断任务中实现不同潜在人口群体的公平学习。该方法利用无监督聚类自动发现人口子群，并在策略更新中加入加权机制，使预测公平性差距降低 27.2%，整体 F1 分数提升，并发布了具备公平性的临床大模型 FairMedGemma-4B。", "keywords": "fairness, reinforcement learning, clinical AI, group relative policy optimization, bias mitigation, multimodal diagnosis, demographic parity", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shiqi Dai", "Wei Dai", "Jiaee Cheong", "Paul Pu Liang"]}
]]></acme>

<pubDate>2025-10-22T17:26:16+00:00</pubDate>
</item>
<item>
<title>From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem</title>
<link>https://papers.cool/arxiv/2510.19889</link>
<guid>https://papers.cool/arxiv/2510.19889</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a Transformer-based deep neural network that directly predicts equilibrium path flows for the traffic assignment problem, bypassing traditional optimization methods that struggle with large-scale networks. By modeling path-level traffic distributions, the approach captures complex correlations among origin‑destination pairs, achieves orders‑of‑magnitude speedups, and adapts to changes in demand and network structure without recomputation. Experiments on synthetic and real networks demonstrate significant computational savings and improved prediction accuracy for multi‑class traffic scenarios.<br /><strong>Summary (CN):</strong> 本文提出一种基于 Transformer 的深度神经网络，直接预测交通分配问题中的均衡路径流，以规避传统在大规模网络上计算成本高的优化方法。通过对路径层面的流量分布建模，捕捉起点‑终点对之间的复杂关联，实现了数量级的加速，并在需求或网络结构变化时无需重新计算即可适应。实验证明在合成及真实网络（如 Sioux Falls、Eastern‑Massachusetts）上，该方法在多类交通场景下显著降低计算成本并提升预测精度。<br /><strong>Keywords:</strong> traffic assignment, transformer, path flow prediction, equilibrium, deep learning, transportation planning, network optimization, multi-class traffic, fast what-if analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</div>
The traffic assignment problem is essential for traffic flow analysis, traditionally solved using mathematical programs under the Equilibrium principle. These methods become computationally prohibitive for large-scale networks due to non-linear growth in complexity with the number of OD pairs. This study introduces a novel data-driven approach using deep neural networks, specifically leveraging the Transformer architecture, to predict equilibrium path flows directly. By focusing on path-level traffic distribution, the proposed model captures intricate correlations between OD pairs, offering a more detailed and flexible analysis compared to traditional link-level approaches. The Transformer-based model drastically reduces computation time, while adapting to changes in demand and network structure without the need for recalculation. Numerical experiments are conducted on the Manhattan-like synthetic network, the Sioux Falls network, and the Eastern-Massachusetts network. The results demonstrate that the proposed model is orders of magnitude faster than conventional optimization. It efficiently estimates path-level traffic flows in multi-class networks, reducing computational costs and improving prediction accuracy by capturing detailed trip and flow information. The model also adapts flexibly to varying demand and network conditions, supporting traffic management and enabling rapid `what-if' analyses for enhanced transportation planning and policy-making.
<div><strong>Authors:</strong> Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a Transformer-based deep neural network that directly predicts equilibrium path flows for the traffic assignment problem, bypassing traditional optimization methods that struggle with large-scale networks. By modeling path-level traffic distributions, the approach captures complex correlations among origin‑destination pairs, achieves orders‑of‑magnitude speedups, and adapts to changes in demand and network structure without recomputation. Experiments on synthetic and real networks demonstrate significant computational savings and improved prediction accuracy for multi‑class traffic scenarios.", "summary_cn": "本文提出一种基于 Transformer 的深度神经网络，直接预测交通分配问题中的均衡路径流，以规避传统在大规模网络上计算成本高的优化方法。通过对路径层面的流量分布建模，捕捉起点‑终点对之间的复杂关联，实现了数量级的加速，并在需求或网络结构变化时无需重新计算即可适应。实验证明在合成及真实网络（如 Sioux Falls、Eastern‑Massachusetts）上，该方法在多类交通场景下显著降低计算成本并提升预测精度。", "keywords": "traffic assignment, transformer, path flow prediction, equilibrium, deep learning, transportation planning, network optimization, multi-class traffic, fast what-if analysis", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mostafa Ameli", "Van Anh Le", "Sulthana Shams", "Alexander Skabardonis"]}
]]></acme>

<pubDate>2025-10-22T16:45:12+00:00</pubDate>
</item>
<item>
<title>From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph</title>
<link>https://papers.cool/arxiv/2510.19873</link>
<guid>https://papers.cool/arxiv/2510.19873</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ReGraphT, a training‑free retrieval‑augmented generation framework that organizes CUDA optimization steps into a structured reasoning graph and employs Monte Carlo Graph Search to transfer LLM‑level reasoning to small language models. By leveraging this graph‑based approach, ReGraphT enables compact coder models to achieve near‑LLM performance on a CUDA‑specific benchmark, delivering an average 2.33× speedup while preserving privacy and reducing computational overhead.<br /><strong>Summary (CN):</strong> 本文提出 ReGraphT——一种无需微调的检索增强生成框架，将 CUDA 优化步骤组织为结构化推理图，并使用蒙特卡罗图搜索将大语言模型的推理能力转移至小语言模型。该方法使得轻量级编码模型在 CUDA 基准测试中实现接近大模型的性能，平均提升 2.33 倍，同时保持隐私并降低计算开销。<br /><strong>Keywords:</strong> CUDA optimization, reasoning graph, retrieval-augmented generation, small language models, Monte Carlo Graph Search, code generation, performance acceleration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li</div>
Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice faces two major challenges: cloud-based APIs pose risks of code leakage, and local deployment is often computationally expensive and inefficient. These drawbacks have spurred interest in small language models (SLMs), which are more lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs can achieve performance comparable to LLMs on specific tasks. While SLMs can match LLMs on domain-specific tasks, their limited reasoning abilities lead to suboptimal performance in complex CUDA generation according to our experiments. To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented generation framework that transfers LLM-level reasoning to smaller models. ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph, modeling the combined CUDA optimizations as state transitions, and leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also present a CUDA-specific benchmark with difficulty tiers defined by reasoning complexity to evaluate models more comprehensively. Experiments show that ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level performance without the associated privacy risks or excessive computing overhead.
<div><strong>Authors:</strong> Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ReGraphT, a training‑free retrieval‑augmented generation framework that organizes CUDA optimization steps into a structured reasoning graph and employs Monte Carlo Graph Search to transfer LLM‑level reasoning to small language models. By leveraging this graph‑based approach, ReGraphT enables compact coder models to achieve near‑LLM performance on a CUDA‑specific benchmark, delivering an average 2.33× speedup while preserving privacy and reducing computational overhead.", "summary_cn": "本文提出 ReGraphT——一种无需微调的检索增强生成框架，将 CUDA 优化步骤组织为结构化推理图，并使用蒙特卡罗图搜索将大语言模型的推理能力转移至小语言模型。该方法使得轻量级编码模型在 CUDA 基准测试中实现接近大模型的性能，平均提升 2.33 倍，同时保持隐私并降低计算开销。", "keywords": "CUDA optimization, reasoning graph, retrieval-augmented generation, small language models, Monte Carlo Graph Search, code generation, performance acceleration", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junfeng Gong", "Zhiyi Wei", "Junying Chen", "Cheng Liu", "Huawei Li"]}
]]></acme>

<pubDate>2025-10-22T08:33:44+00:00</pubDate>
</item>
<item>
<title>An Integrated Approach to Neural Architecture Search for Deep Q-Networks</title>
<link>https://papers.cool/arxiv/2510.19872</link>
<guid>https://papers.cool/arxiv/2510.19872</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NAS-DQN, a deep reinforcement learning agent that incorporates a neural architecture search controller into the DQN training loop, allowing the network architecture to be dynamically reconfigured based on cumulative performance feedback. Experiments on a continuous control task show that NAS-DQN outperforms fixed-architecture baselines and random search in final performance, sample efficiency, and policy stability with minimal computational overhead. The results suggest that online architecture adaptation is essential for optimal sample efficiency in deep RL.<br /><strong>Summary (CN):</strong> 本文提出 NAS-DQN，一种在 DQN 训练过程中嵌入神经架构搜索控制器的深度强化学习代理，使网络结构能够根据累计性能反馈动态调整。针对连续控制任务的实验表明，NAS-DQN 在最终性能、样本效率和策略稳定性方面均优于固定架构基线和随机搜索，且计算开销几乎可以忽略不计。结果表明，在线架构适配是实现深度强化学习最佳样本效率的关键。<br /><strong>Keywords:</strong> neural architecture search, deep Q-network, reinforcement learning, online architecture adaptation, sample efficiency, architecture controller<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Iman Rahmani, Saman Yazdannik, Morteza Tayefi, Jafar Roshanian</div>
The performance of deep reinforcement learning agents is fundamentally constrained by their neural network architecture, a choice traditionally made through expensive hyperparameter searches and then fixed throughout training. This work investigates whether online, adaptive architecture optimization can escape this constraint and outperform static designs. We introduce NAS-DQN, an agent that integrates a learned neural architecture search controller directly into the DRL training loop, enabling dynamic network reconfiguration based on cumulative performance feedback. We evaluate NAS-DQN against three fixed-architecture baselines and a random search control on a continuous control task, conducting experiments over multiple random seeds. Our results demonstrate that NAS-DQN achieves superior final performance, sample efficiency, and policy stability while incurring negligible computational overhead. Critically, the learned search strategy substantially outperforms both undirected random architecture exploration and poorly-chosen fixed designs, indicating that intelligent, performance-guided search is the key mechanism driving success. These findings establish that architecture adaptation is not merely beneficial but necessary for optimal sample efficiency in online deep reinforcement learning, and suggest that the design of RL agents need not be a static offline choice but can instead be seamlessly integrated as a dynamic component of the learning process itself.
<div><strong>Authors:</strong> Iman Rahmani, Saman Yazdannik, Morteza Tayefi, Jafar Roshanian</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NAS-DQN, a deep reinforcement learning agent that incorporates a neural architecture search controller into the DQN training loop, allowing the network architecture to be dynamically reconfigured based on cumulative performance feedback. Experiments on a continuous control task show that NAS-DQN outperforms fixed-architecture baselines and random search in final performance, sample efficiency, and policy stability with minimal computational overhead. The results suggest that online architecture adaptation is essential for optimal sample efficiency in deep RL.", "summary_cn": "本文提出 NAS-DQN，一种在 DQN 训练过程中嵌入神经架构搜索控制器的深度强化学习代理，使网络结构能够根据累计性能反馈动态调整。针对连续控制任务的实验表明，NAS-DQN 在最终性能、样本效率和策略稳定性方面均优于固定架构基线和随机搜索，且计算开销几乎可以忽略不计。结果表明，在线架构适配是实现深度强化学习最佳样本效率的关键。", "keywords": "neural architecture search, deep Q-network, reinforcement learning, online architecture adaptation, sample efficiency, architecture controller", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Iman Rahmani", "Saman Yazdannik", "Morteza Tayefi", "Jafar Roshanian"]}
]]></acme>

<pubDate>2025-10-22T07:17:31+00:00</pubDate>
</item>
<item>
<title>Some Attention is All You Need for Retrieval</title>
<link>https://papers.cool/arxiv/2510.19861</link>
<guid>https://papers.cool/arxiv/2510.19861</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper demonstrates that in hybrid SSM‑Transformer models, retrieval performance depends exclusively on self‑attention layers, with attention ablation causing complete failure while SSM layers provide no compensation. Sparsifying attention to a small subset of heads retains near‑perfect retrieval and high MMLU performance, indicating a functional specialization of attention for retrieval tasks. These findings reveal precise mechanistic requirements for retrieval and challenge the view of hybrid models as fully integrated systems.<br /><strong>Summary (CN):</strong> 本文表明，在混合 SSM‑Transformer 模型中，检索功能完全依赖自注意力层，去除注意力会导致检索彻底失败，而 SSM 层无法补偿。即使将注意力稀疏到仅 15% 的 heads，也能保持几乎完美的检索性能并保留 84% 的 MMLU 表现，说明注意力在检索任务中具有专门化的功能。该研究揭示了检索的精确机制需求，挑战了混合模型作为整体系统的假设，并对架构优化与可解释性具有重要意义。<br /><strong>Keywords:</strong> retrieval, self-attention, SSM, hybrid architecture, functional segregation, mechanistic interpretability, attention sparsity, model ablation<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Felix Michalak, Steven Abreu</div>
We demonstrate complete functional segregation in hybrid SSM-Transformer architectures: retrieval depends exclusively on self-attention layers. Across RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic retrieval failure (0% accuracy), while SSM layers show no compensatory mechanisms even with improved prompting. Conversely, sparsifying attention to just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU performance, suggesting self-attention specializes primarily for retrieval tasks. We identify precise mechanistic requirements for retrieval: needle tokens must be exposed during generation and sufficient context must be available during prefill or generation. This strict functional specialization challenges assumptions about redundancy in hybrid architectures and suggests these models operate as specialized modules rather than integrated systems, with immediate implications for architecture optimization and interpretability.
<div><strong>Authors:</strong> Felix Michalak, Steven Abreu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper demonstrates that in hybrid SSM‑Transformer models, retrieval performance depends exclusively on self‑attention layers, with attention ablation causing complete failure while SSM layers provide no compensation. Sparsifying attention to a small subset of heads retains near‑perfect retrieval and high MMLU performance, indicating a functional specialization of attention for retrieval tasks. These findings reveal precise mechanistic requirements for retrieval and challenge the view of hybrid models as fully integrated systems.", "summary_cn": "本文表明，在混合 SSM‑Transformer 模型中，检索功能完全依赖自注意力层，去除注意力会导致检索彻底失败，而 SSM 层无法补偿。即使将注意力稀疏到仅 15% 的 heads，也能保持几乎完美的检索性能并保留 84% 的 MMLU 表现，说明注意力在检索任务中具有专门化的功能。该研究揭示了检索的精确机制需求，挑战了混合模型作为整体系统的假设，并对架构优化与可解释性具有重要意义。", "keywords": "retrieval, self-attention, SSM, hybrid architecture, functional segregation, mechanistic interpretability, attention sparsity, model ablation", "scoring": {"interpretability": 8, "understanding": 8, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Felix Michalak", "Steven Abreu"]}
]]></acme>

<pubDate>2025-10-21T22:26:08+00:00</pubDate>
</item>
<item>
<title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title>
<link>https://papers.cool/arxiv/2510.20819</link>
<guid>https://papers.cool/arxiv/2510.20819</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Latent Denoising Diffusion Bridge Model (LDDBM), a framework that learns a shared latent space to translate between arbitrary modalities without requiring aligned dimensions, using contrastive alignment and predictive losses to ensure semantic consistency. Experiments on tasks such as multi‑view to 3D shape generation, image super‑resolution, and scene synthesis demonstrate strong performance and establish a new baseline for general modality translation.<br /><strong>Summary (CN):</strong> 本文提出了潜在去噪扩散桥模型（LDDBM），通过在共享潜在空间中学习跨模态桥接，实现任意模态之间的翻译，无需对齐维度，并使用对比对齐损失和预测损失确保语义一致性。实验在多视角到 3D 形状生成、图像超分辨率和场景合成等任务上表现出色，树立了通用模态翻译的新基准。<br /><strong>Keywords:</strong> modality translation, latent diffusion, diffusion bridge, contrastive alignment, predictive loss, cross-modal generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.
<div><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Latent Denoising Diffusion Bridge Model (LDDBM), a framework that learns a shared latent space to translate between arbitrary modalities without requiring aligned dimensions, using contrastive alignment and predictive losses to ensure semantic consistency. Experiments on tasks such as multi‑view to 3D shape generation, image super‑resolution, and scene synthesis demonstrate strong performance and establish a new baseline for general modality translation.", "summary_cn": "本文提出了潜在去噪扩散桥模型（LDDBM），通过在共享潜在空间中学习跨模态桥接，实现任意模态之间的翻译，无需对齐维度，并使用对比对齐损失和预测损失确保语义一致性。实验在多视角到 3D 形状生成、图像超分辨率和场景合成等任务上表现出色，树立了通用模态翻译的新基准。", "keywords": "modality translation, latent diffusion, diffusion bridge, contrastive alignment, predictive loss, cross-modal generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nimrod Berman", "Omkar Joglekar", "Eitan Kosman", "Dotan Di Castro", "Omri Azencot"]}
]]></acme>

<pubDate>2025-10-23T17:59:54+00:00</pubDate>
</item>
<item>
<title>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</title>
<link>https://papers.cool/arxiv/2510.20818</link>
<guid>https://papers.cool/arxiv/2510.20818</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> VAMOS introduces a hierarchical Vision-Language-Action system that separates high-level semantic planning from low-level embodiment grounding via a generalist planner and a specialist affordance model, enabling capability-modulated navigation across diverse environments. Real-world experiments demonstrate higher success rates, cross-embodiment portability between legged and wheeled robots, and steerability through natural language, with the specialist model crucial for rejecting physically infeasible plans. The approach highlights the benefits of modular design for robust and adaptable robot navigation.<br /><strong>Summary (CN):</strong> VAMOS 提出了一种层次化的视觉-语言-动作模型（Vision-Language-Action），通过通用规划器（high-level planner）和专用可行性模型（affordance model）将语义规划与实体约束分离，实现了能力调制的导航。实验证明该体系在室内和复杂户外环境中提升了成功率，并能在腿式机器人和轮式机器人之间跨实体迁移，同时可通过自然语言进行指令调节。专用模型负责评估并剔除物理上不可行的路径，从而显著提升单机器人可靠性。<br /><strong>Keywords:</strong> hierarchical navigation, vision-language-action, affordance model, capability-modulated navigation, cross-embodiment, language steering, robot navigation, embodied AI<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</div>
A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/
<div><strong>Authors:</strong> Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "VAMOS introduces a hierarchical Vision-Language-Action system that separates high-level semantic planning from low-level embodiment grounding via a generalist planner and a specialist affordance model, enabling capability-modulated navigation across diverse environments. Real-world experiments demonstrate higher success rates, cross-embodiment portability between legged and wheeled robots, and steerability through natural language, with the specialist model crucial for rejecting physically infeasible plans. The approach highlights the benefits of modular design for robust and adaptable robot navigation.", "summary_cn": "VAMOS 提出了一种层次化的视觉-语言-动作模型（Vision-Language-Action），通过通用规划器（high-level planner）和专用可行性模型（affordance model）将语义规划与实体约束分离，实现了能力调制的导航。实验证明该体系在室内和复杂户外环境中提升了成功率，并能在腿式机器人和轮式机器人之间跨实体迁移，同时可通过自然语言进行指令调节。专用模型负责评估并剔除物理上不可行的路径，从而显著提升单机器人可靠性。", "keywords": "hierarchical navigation, vision-language-action, affordance model, capability-modulated navigation, cross-embodiment, language steering, robot navigation, embodied AI", "scoring": {"interpretability": 4, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Mateo Guaman Castro", "Sidharth Rajagopal", "Daniel Gorbatov", "Matt Schmittle", "Rohan Baijal", "Octi Zhang", "Rosario Scalise", "Sidharth Talia", "Emma Romig", "Celso de Melo", "Byron Boots", "Abhishek Gupta"]}
]]></acme>

<pubDate>2025-10-23T17:59:45+00:00</pubDate>
</item>
<item>
<title>On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?</title>
<link>https://papers.cool/arxiv/2510.20810</link>
<guid>https://papers.cool/arxiv/2510.20810</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper critiques the lack of a precise definition for "LLM-generated text" and examines how diverse usage scenarios, human edits, and subtle LLM influences blur the line between machine- and human-written content. It argues that current detection benchmarks and evaluation methods fail to capture real-world conditions, leading to misunderstandings of detector performance, and recommends interpreting detector outputs as contextual references rather than definitive judgments.<br /><strong>Summary (CN):</strong> 本文批评了“LLM 生成文本”缺乏精确定义的问题，分析了使用场景多样、人类编辑以及 LLM 对用户的微妙影响如何模糊机器生成与人类写作的界限。作者指出现有的检测基准和评估方法未能覆盖真实应用条件，导致对检测器性能的误解，建议将检测结果视为情境参考而非决定性结论。<br /><strong>Keywords:</strong> LLM-generated text, detection, definitional analysis, benchmarks, safety, misuse detection, evaluation, text classification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 7, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely "LLM-generated text". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.
<div><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper critiques the lack of a precise definition for \"LLM-generated text\" and examines how diverse usage scenarios, human edits, and subtle LLM influences blur the line between machine- and human-written content. It argues that current detection benchmarks and evaluation methods fail to capture real-world conditions, leading to misunderstandings of detector performance, and recommends interpreting detector outputs as contextual references rather than definitive judgments.", "summary_cn": "本文批评了“LLM 生成文本”缺乏精确定义的问题，分析了使用场景多样、人类编辑以及 LLM 对用户的微妙影响如何模糊机器生成与人类写作的界限。作者指出现有的检测基准和评估方法未能覆盖真实应用条件，导致对检测器性能的误解，建议将检测结果视为情境参考而非决定性结论。", "keywords": "LLM-generated text, detection, definitional analysis, benchmarks, safety, misuse detection, evaluation, text classification", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Mingmeng Geng", "Thierry Poibeau"]}
]]></acme>

<pubDate>2025-10-23T17:59:06+00:00</pubDate>
</item>
<item>
<title>Real Deep Research for AI, Robotics and Beyond</title>
<link>https://papers.cool/arxiv/2510.20809</link>
<guid>https://papers.cool/arxiv/2510.20809</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area to identify emerging trends, uncover cross‑domain opportunities, and suggest concrete starting points for new inquiry, with a focus on AI and robotics. It details the construction of the RDR framework and presents extensive results across AI, robotics, and brief extensions to other scientific fields. The authors aim to help researchers stay up to date amidst the rapid growth of publications.<br /><strong>Summary (CN):</strong> 本文提出了 Real Deep Research（RDR）流水线，一种可通用的系统化分析任何研究领域的方法，用于识别新兴趋势、发掘跨领域机会并提供具体的研究起点，重点聚焦于 AI 与机器人领域。论文阐述了 RDR 框架的构建，并在 AI、机器人以及其他科学领域展示了大量分析结果。作者希望帮助研究者在论文激增的环境中保持前沿了解。<br /><strong>Keywords:</strong> meta-research, trend detection, literature mining, AI robotics, foundation models, cross-domain analysis, research pipeline, emerging topics, scientific survey<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.
<div><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area to identify emerging trends, uncover cross‑domain opportunities, and suggest concrete starting points for new inquiry, with a focus on AI and robotics. It details the construction of the RDR framework and presents extensive results across AI, robotics, and brief extensions to other scientific fields. The authors aim to help researchers stay up to date amidst the rapid growth of publications.", "summary_cn": "本文提出了 Real Deep Research（RDR）流水线，一种可通用的系统化分析任何研究领域的方法，用于识别新兴趋势、发掘跨领域机会并提供具体的研究起点，重点聚焦于 AI 与机器人领域。论文阐述了 RDR 框架的构建，并在 AI、机器人以及其他科学领域展示了大量分析结果。作者希望帮助研究者在论文激增的环境中保持前沿了解。", "keywords": "meta-research, trend detection, literature mining, AI robotics, foundation models, cross-domain analysis, research pipeline, emerging topics, scientific survey", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:05+00:00</pubDate>
</item>
<item>
<title>The Reality Gap in Robotics: Challenges, Solutions, and Best Practices</title>
<link>https://papers.cool/arxiv/2510.20808</link>
<guid>https://papers.cool/arxiv/2510.20808</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys the reality gap in robotics, reviewing causes of discrepancies between simulation and real-world environments and summarizing techniques such as domain randomization, real-to-sim transfer, abstraction, and co‑training that aim to close the gap. It also discusses evaluation metrics and best practices for sim‑to‑real transfer across locomotion, navigation, and manipulation tasks.<br /><strong>Summary (CN):</strong> 本文综述了机器人领域的现实差距（reality gap），阐述仿真与真实环境之间的偏差来源，并总结了领域随机化、真实到仿真迁移、状态与动作抽象以及仿真‑真实协同训练等缩小差距的方法。还讨论了评估指标和在行走、导航、操作等任务中的最佳实践。<br /><strong>Keywords:</strong> reality gap, sim-to-real transfer, domain randomization, robotics simulation, transfer learning, locomotion, manipulation, navigation, abstraction, co-training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</div>
Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap's root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.
<div><strong>Authors:</strong> Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys the reality gap in robotics, reviewing causes of discrepancies between simulation and real-world environments and summarizing techniques such as domain randomization, real-to-sim transfer, abstraction, and co‑training that aim to close the gap. It also discusses evaluation metrics and best practices for sim‑to‑real transfer across locomotion, navigation, and manipulation tasks.", "summary_cn": "本文综述了机器人领域的现实差距（reality gap），阐述仿真与真实环境之间的偏差来源，并总结了领域随机化、真实到仿真迁移、状态与动作抽象以及仿真‑真实协同训练等缩小差距的方法。还讨论了评估指标和在行走、导航、操作等任务中的最佳实践。", "keywords": "reality gap, sim-to-real transfer, domain randomization, robotics simulation, transfer learning, locomotion, manipulation, navigation, abstraction, co-training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Elie Aljalbout", "Jiaxu Xing", "Angel Romero", "Iretiayo Akinola", "Caelan Reed Garrett", "Eric Heiden", "Abhishek Gupta", "Tucker Hermans", "Yashraj Narang", "Dieter Fox", "Davide Scaramuzza", "Fabio Ramos"]}
]]></acme>

<pubDate>2025-10-23T17:58:53+00:00</pubDate>
</item>
<item>
<title>Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</title>
<link>https://papers.cool/arxiv/2510.20807</link>
<guid>https://papers.cool/arxiv/2510.20807</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pure transformer model for autoregressive video prediction of physical simulations using continuous pixel-space representations. By comparing various spatiotemporal self‑attention layouts, the approach extends accurate prediction horizons by up to 50% compared to latent‑space methods while maintaining comparable video quality. Interpretability probes reveal network regions that encode information useful for estimating PDE simulation parameters, even for out‑of‑distribution settings.<br /><strong>Summary (CN):</strong> 本文提出了一种纯 Transformer 模型，用于基于连续像素空间的物理模拟视频的自回归预测。通过比较不同的时空自注意力布局，该方法在保持视频质量的同时，将物理上准确的预测时间延长了约 50%。解释性实验显示网络的特定区域能够编码用于估计 PDE 模拟参数的信息，且在分布外参数上也具备泛化能力。<br /><strong>Keywords:</strong> video prediction, spatiotemporal transformer, physical simulation, autoregressive, pixel-space, PDE parameter estimation, interpretability, attention<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</div>
Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.
<div><strong>Authors:</strong> Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pure transformer model for autoregressive video prediction of physical simulations using continuous pixel-space representations. By comparing various spatiotemporal self‑attention layouts, the approach extends accurate prediction horizons by up to 50% compared to latent‑space methods while maintaining comparable video quality. Interpretability probes reveal network regions that encode information useful for estimating PDE simulation parameters, even for out‑of‑distribution settings.", "summary_cn": "本文提出了一种纯 Transformer 模型，用于基于连续像素空间的物理模拟视频的自回归预测。通过比较不同的时空自注意力布局，该方法在保持视频质量的同时，将物理上准确的预测时间延长了约 50%。解释性实验显示网络的特定区域能够编码用于估计 PDE 模拟参数的信息，且在分布外参数上也具备泛化能力。", "keywords": "video prediction, spatiotemporal transformer, physical simulation, autoregressive, pixel-space, PDE parameter estimation, interpretability, attention", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dean L Slack", "G Thomas Hudson", "Thomas Winterbottom", "Noura Al Moubayed"]}
]]></acme>

<pubDate>2025-10-23T17:58:45+00:00</pubDate>
</item>
<item>
<title>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</title>
<link>https://papers.cool/arxiv/2510.20797</link>
<guid>https://papers.cool/arxiv/2510.20797</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture across various QA datasets and model scales. It also explores training a single compressor to produce multiple compression ratios, revealing nuanced trade‑offs among architectures and training regimes.<br /><strong>Summary (CN):</strong> 本文提出一种轻量级的均值池化软上下文压缩方法，用于检索增强生成（RAG），在多种问答数据集和模型下稳健超越常用的 compression‑tokens 架构。论文进一步研究了对同一压缩器进行多倍率训练，揭示了不同体系结构和训练策略之间的细致权衡。<br /><strong>Keywords:</strong> context compression, mean-pooling, retrieval-augmented generation, multi-ratio training, long-context processing, LLM efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.
<div><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture across various QA datasets and model scales. It also explores training a single compressor to produce multiple compression ratios, revealing nuanced trade‑offs among architectures and training regimes.", "summary_cn": "本文提出一种轻量级的均值池化软上下文压缩方法，用于检索增强生成（RAG），在多种问答数据集和模型下稳健超越常用的 compression‑tokens 架构。论文进一步研究了对同一压缩器进行多倍率训练，揭示了不同体系结构和训练策略之间的细致权衡。", "keywords": "context compression, mean-pooling, retrieval-augmented generation, multi-ratio training, long-context processing, LLM efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yair Feldman", "Yoav Artzi"]}
]]></acme>

<pubDate>2025-10-23T17:57:23+00:00</pubDate>
</item>
<item>
<title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title>
<link>https://papers.cool/arxiv/2510.20795</link>
<guid>https://papers.cool/arxiv/2510.20795</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Bayesian graph‑neural‑network framework that combines DeepSphere, a spherical convolutional architecture, with Bayesian neural networks to infer primordial magnetic field parameters directly from simulated CMB maps. By modeling both aleatoric and epistemic uncertainties, the method achieves high R² scores (>0.89) and provides well‑calibrated posterior estimates for cosmological inference.<br /><strong>Summary (CN):</strong> 本文提出将 DeepSphere 球面图卷积网络与贝叶斯神经网络相结合的框架，用于直接从模拟的 CMB 图像中推断原始磁场参数，并对预测的不确定性进行建模。实验显示该方法在磁场参数估计上实现了超过 0.89 的 R² 分数，并提供了可靠的后验不确定性估计。<br /><strong>Keywords:</strong> primordial magnetic field, cosmic microwave background, spherical graph neural network, DeepSphere, Bayesian neural network, uncertainty quantification, cosmological parameter inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</div>
Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology.
<div><strong>Authors:</strong> Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Bayesian graph‑neural‑network framework that combines DeepSphere, a spherical convolutional architecture, with Bayesian neural networks to infer primordial magnetic field parameters directly from simulated CMB maps. By modeling both aleatoric and epistemic uncertainties, the method achieves high R² scores (>0.89) and provides well‑calibrated posterior estimates for cosmological inference.", "summary_cn": "本文提出将 DeepSphere 球面图卷积网络与贝叶斯神经网络相结合的框架，用于直接从模拟的 CMB 图像中推断原始磁场参数，并对预测的不确定性进行建模。实验显示该方法在磁场参数估计上实现了超过 0.89 的 R² 分数，并提供了可靠的后验不确定性估计。", "keywords": "primordial magnetic field, cosmic microwave background, spherical graph neural network, DeepSphere, Bayesian neural network, uncertainty quantification, cosmological parameter inference", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Juan Alejandro Pinto Castro", "Héctor J. Hortúa", "Jorge Enrique García-Farieta", "Roger Anderson Hurtado"]}
]]></acme>

<pubDate>2025-10-23T17:56:04+00:00</pubDate>
</item>
<item>
<title>Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction</title>
<link>https://papers.cool/arxiv/2510.20787</link>
<guid>https://papers.cool/arxiv/2510.20787</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes hybrid sparse attention mechanisms to reduce forgetfulness in linear-attention models, including a novel learnable token eviction method combined with sliding-window attention and a lightweight CNN that adaptively retains critical key‑value pairs while preserving linear time and space complexity. Efficient Triton kernels are provided, and experiments on retrieval‑intensive benchmarks show notable performance gains.<br /><strong>Summary (CN):</strong> 本文提出混合稀疏注意力机制，以缓解线性注意力模型的遗忘问题，其中包括一种可学习的 token 淘汰方法，结合滑动窗口注意力和轻量级 CNN，在保持线性时间空间复杂度的同时自适应保留关键 KV 对。实现了高效的 Triton 内核，并在检索密集型基准上展示了显著的性能提升。<br /><strong>Keywords:</strong> linear attention, hybrid sparse attention, token eviction, sliding-window attention, learnable token eviction, efficient attention, retrieval-intensive, Triton kernels, CNN aggregator<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mutian He, Philip N. Garner</div>
Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.
<div><strong>Authors:</strong> Mutian He, Philip N. Garner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes hybrid sparse attention mechanisms to reduce forgetfulness in linear-attention models, including a novel learnable token eviction method combined with sliding-window attention and a lightweight CNN that adaptively retains critical key‑value pairs while preserving linear time and space complexity. Efficient Triton kernels are provided, and experiments on retrieval‑intensive benchmarks show notable performance gains.", "summary_cn": "本文提出混合稀疏注意力机制，以缓解线性注意力模型的遗忘问题，其中包括一种可学习的 token 淘汰方法，结合滑动窗口注意力和轻量级 CNN，在保持线性时间空间复杂度的同时自适应保留关键 KV 对。实现了高效的 Triton 内核，并在检索密集型基准上展示了显著的性能提升。", "keywords": "linear attention, hybrid sparse attention, token eviction, sliding-window attention, learnable token eviction, efficient attention, retrieval-intensive, Triton kernels, CNN aggregator", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mutian He", "Philip N. Garner"]}
]]></acme>

<pubDate>2025-10-23T17:53:03+00:00</pubDate>
</item>
<item>
<title>A Coherence-Based Measure of AGI</title>
<link>https://papers.cool/arxiv/2510.20784</link>
<guid>https://papers.cool/arxiv/2510.20784</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a coherence‑based AGI metric that integrates generalized means over a range of compensability exponents, producing an area‑under‑the‑curve (AUC) score that penalizes imbalanced domain competence. This formulation addresses the limitation of arithmetic‑mean definitions that allow strong performance in some cognitive domains to mask weaknesses in others, and it is applied to CHC‑based scores of GPT‑4 and GPT‑5, showing that both remain far from truly general intelligence despite high arithmetic scores.<br /><strong>Summary (CN):</strong> 本文提出了一种基于一致性的 AGI 衡量方法，通过对不同补偿指数下的广义均值进行积分，计算得到的面积（AUC）能够惩罚在各认知域之间不平衡的能力。该方法克服了仅使用算术平均导致的“强项掩盖弱项”问题，并将其应用于 GPT‑4 和 GPT‑5 的 CHC 领域得分，显示出即使算术分数较高，两者仍距离真正的通用智能有很大差距。<br /><strong>Keywords:</strong> AGI measurement, coherence, generalized mean, compensability, CHC model, AI evaluation, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fares Fourati</div>
Recent work by \citet{hendrycks2025agidefinition} formalized \textit{Artificial General Intelligence} (AGI) as the arithmetic mean of proficiencies across cognitive domains derived from the Cattell--Horn--Carroll (CHC) model of human cognition. While elegant, this definition assumes \textit{compensability} -- that exceptional ability in some domains can offset failure in others. True general intelligence, however, should reflect \textit{coherent sufficiency}: balanced competence across all essential domains. We propose a coherence-aware measure of AGI based on the integral of generalized means over a continuum of compensability exponents. This formulation spans arithmetic, geometric, and harmonic regimes, and the resulting \textit{area under the curve} (AUC) quantifies robustness under varying compensability assumptions. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and captures inter-domain dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5, the coherence-adjusted AUC reveals that both systems remain far from general competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating the generalized mean thus yields a principled, interpretable, and stricter foundation for measuring genuine progress toward AGI.
<div><strong>Authors:</strong> Fares Fourati</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a coherence‑based AGI metric that integrates generalized means over a range of compensability exponents, producing an area‑under‑the‑curve (AUC) score that penalizes imbalanced domain competence. This formulation addresses the limitation of arithmetic‑mean definitions that allow strong performance in some cognitive domains to mask weaknesses in others, and it is applied to CHC‑based scores of GPT‑4 and GPT‑5, showing that both remain far from truly general intelligence despite high arithmetic scores.", "summary_cn": "本文提出了一种基于一致性的 AGI 衡量方法，通过对不同补偿指数下的广义均值进行积分，计算得到的面积（AUC）能够惩罚在各认知域之间不平衡的能力。该方法克服了仅使用算术平均导致的“强项掩盖弱项”问题，并将其应用于 GPT‑4 和 GPT‑5 的 CHC 领域得分，显示出即使算术分数较高，两者仍距离真正的通用智能有很大差距。", "keywords": "AGI measurement, coherence, generalized mean, compensability, CHC model, AI evaluation, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fares Fourati"]}
]]></acme>

<pubDate>2025-10-23T17:51:42+00:00</pubDate>
</item>
<item>
<title>AlphaFlow: Understanding and Improving MeanFlow Models</title>
<link>https://papers.cool/arxiv/2510.20771</link>
<guid>https://papers.cool/arxiv/2510.20771</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes the MeanFlow objective, showing it decomposes into trajectory flow matching and trajectory consistency, which are negatively correlated and cause optimization conflicts. Based on this insight, the authors propose α-Flow, a unified family of objectives with a curriculum that anneals from flow matching to MeanFlow, leading to faster convergence and state-of-the-art FID scores on ImageNet when trained with DiT backbones.<br /><strong>Summary (CN):</strong> 本文分析了 MeanFlow 目标，发现其可分解为轨迹流匹配和轨迹一致性两部分，这两者呈强负相关，导致优化冲突并收敛缓慢。基于此洞察，作者提出 α-Flow——一种统一的目标族，并采用从轨迹流匹配平滑退火至 MeanFlow 的 curriculum 策略，从而消除冲突、加速收敛，在使用 DiT 骨干的 ImageNet 生成任务上实现了新的 FID 最佳成绩。<br /><strong>Keywords:</strong> MeanFlow, α-Flow, trajectory flow matching, generative modeling, curriculum learning, DiT, FID, diffusion models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</div>
MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).
<div><strong>Authors:</strong> Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes the MeanFlow objective, showing it decomposes into trajectory flow matching and trajectory consistency, which are negatively correlated and cause optimization conflicts. Based on this insight, the authors propose α-Flow, a unified family of objectives with a curriculum that anneals from flow matching to MeanFlow, leading to faster convergence and state-of-the-art FID scores on ImageNet when trained with DiT backbones.", "summary_cn": "本文分析了 MeanFlow 目标，发现其可分解为轨迹流匹配和轨迹一致性两部分，这两者呈强负相关，导致优化冲突并收敛缓慢。基于此洞察，作者提出 α-Flow——一种统一的目标族，并采用从轨迹流匹配平滑退火至 MeanFlow 的 curriculum 策略，从而消除冲突、加速收敛，在使用 DiT 骨干的 ImageNet 生成任务上实现了新的 FID 最佳成绩。", "keywords": "MeanFlow, α-Flow, trajectory flow matching, generative modeling, curriculum learning, DiT, FID, diffusion models", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Huijie Zhang", "Aliaksandr Siarohin", "Willi Menapace", "Michael Vasilkovsky", "Sergey Tulyakov", "Qing Qu", "Ivan Skorokhodov"]}
]]></acme>

<pubDate>2025-10-23T17:45:06+00:00</pubDate>
</item>
<item>
<title>CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting</title>
<link>https://papers.cool/arxiv/2510.20769</link>
<guid>https://papers.cool/arxiv/2510.20769</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes CSU-PCAST, a dual‑branch transformer framework that uses a Swin‑Transformer encoder with periodic convolutions and conditional layer normalization to produce medium‑range (up to 15‑day) ensemble precipitation forecasts from ERA5 reanalysis and GFS initial conditions. The model jointly predicts total precipitation and auxiliary atmospheric variables, training with a hybrid loss that combines CRPS and weighted log1p‑SE, and demonstrates higher CSI scores than the GEFS baseline across multiple rainfall thresholds. This approach highlights the potential of deep learning ensembles for improving probabilistic precipitation forecasting at moderate to heavy rain rates.<br /><strong>Summary (CN):</strong> 本文提出了 CSU‑PCAST 双分支 Transformer 框架，使用带周期卷积的 Swin‑Transformer 编码器和条件层归一化，将 ERA5 再分析数据与 GFS 初始条件相结合，实现最长  天的集合降水预报。模型通过混合损失（CRPS 与加权 log1p‑MSE）联合预测总降水量及其他大气变量，并在多个降雨阈值下的 CSI 指标上相较 GEFS 基线取得提升。该工作展示了深度学习集合方法在中期至强降雨概率预报中的潜力。<br /><strong>Keywords:</strong> precipitation forecasting, ensemble learning, transformer, Swin Transformer, CRPS, deep learning, medium-range prediction, probabilistic forecasting, meteorology<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tianyi Xiong, Haonan Chen</div>
Accurate medium-range precipitation forecasting is crucial for hydrometeorological risk management and disaster mitigation, yet remains challenging for current numerical weather prediction (NWP) systems. Traditional ensemble systems such as the Global Ensemble Forecast System (GEFS) struggle to maintain high skill, especially for moderate and heavy rainfall at extended lead times. This study develops a deep learning-based ensemble framework for multi-step precipitation prediction through joint modeling of a comprehensive set of atmospheric variables. The model is trained on ERA5 reanalysis data at 0.25$^{\circ}$ spatial resolution, with precipitation labels from NASA's Integrated Multi-satellite Retrievals for Global Precipitation Measurement (GPM) constellation (IMERG), incorporating 57 input variables, including upper-air and surface predictors. The architecture employs a patch-based Swin Transformer backbone with periodic convolutions to handle longitudinal continuity and integrates time and noise embeddings through conditional layer normalization. A dual-branch decoder predicts total precipitation and other variables, with targeted freezing of encoder-decoder pathways for specialized training. Training minimizes a hybrid loss combining the Continuous Ranked Probability Score (CRPS) and weighted log1p mean squared error (log1pMSE), balancing probabilistic accuracy and magnitude fidelity. During inference, the model ingests real-time Global Forecast System (GFS) initial conditions to generate 15-day forecasts autoregressively. Evaluation against GEFS using IMERG data demonstrates higher Critical Success Index (CSI) scores at precipitation thresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performance for moderate to heavy rainfall.
<div><strong>Authors:</strong> Tianyi Xiong, Haonan Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes CSU-PCAST, a dual‑branch transformer framework that uses a Swin‑Transformer encoder with periodic convolutions and conditional layer normalization to produce medium‑range (up to 15‑day) ensemble precipitation forecasts from ERA5 reanalysis and GFS initial conditions. The model jointly predicts total precipitation and auxiliary atmospheric variables, training with a hybrid loss that combines CRPS and weighted log1p‑SE, and demonstrates higher CSI scores than the GEFS baseline across multiple rainfall thresholds. This approach highlights the potential of deep learning ensembles for improving probabilistic precipitation forecasting at moderate to heavy rain rates.", "summary_cn": "本文提出了 CSU‑PCAST 双分支 Transformer 框架，使用带周期卷积的 Swin‑Transformer 编码器和条件层归一化，将 ERA5 再分析数据与 GFS 初始条件相结合，实现最长  天的集合降水预报。模型通过混合损失（CRPS 与加权 log1p‑MSE）联合预测总降水量及其他大气变量，并在多个降雨阈值下的 CSI 指标上相较 GEFS 基线取得提升。该工作展示了深度学习集合方法在中期至强降雨概率预报中的潜力。", "keywords": "precipitation forecasting, ensemble learning, transformer, Swin Transformer, CRPS, deep learning, medium-range prediction, probabilistic forecasting, meteorology", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tianyi Xiong", "Haonan Chen"]}
]]></acme>

<pubDate>2025-10-23T17:43:38+00:00</pubDate>
</item>
<item>
<title>Reinforcement Learning and Consumption-Savings Behavior</title>
<link>https://papers.cool/arxiv/2510.20748</link>
<guid>https://papers.cool/arxiv/2510.20748</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a reinforcement‑learning model in which agents use Q‑learning with neural‑network function approximation to make consumption‑savings decisions under income uncertainty. It shows that this adaptive learning mechanism can simultaneously generate higher marginal propensities to consume for liquidity‑constrained households and a persistent "scarring" effect for those with past unemployment, matching recent empirical findings. Simulation results suggest that value‑function approximation errors driven by experience provide a unified explanation for these phenomena beyond standard rational‑expectations models.<br /><strong>Summary (CN):</strong> 本文构建了一个强化学习模型，使用带神经网络近似的 Q 学习来决定在收入不确定性下的消费‑储蓄行为。模型能够同时解释两大经验现象：流动性较低的失业家庭对刺激转移的边际消费倾向（MPC）更高，以及拥有更多失业经历的家庭在控制当前疤痕”效应），并与最新实证结果高度吻合。仿真表明，价值函数近似误差随经验演化提供了超越传统理性预期的统一解释框架。<br /><strong>Keywords:</strong> reinforcement learning, consumption-savings, Q-learning, neural network approximation, marginal propensity to consume, scarring effect, adaptive learning, economic downturn<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Brandon Kaplowitz</div>
This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a "scarring" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict.
<div><strong>Authors:</strong> Brandon Kaplowitz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a reinforcement‑learning model in which agents use Q‑learning with neural‑network function approximation to make consumption‑savings decisions under income uncertainty. It shows that this adaptive learning mechanism can simultaneously generate higher marginal propensities to consume for liquidity‑constrained households and a persistent \"scarring\" effect for those with past unemployment, matching recent empirical findings. Simulation results suggest that value‑function approximation errors driven by experience provide a unified explanation for these phenomena beyond standard rational‑expectations models.", "summary_cn": "本文构建了一个强化学习模型，使用带神经网络近似的 Q 学习来决定在收入不确定性下的消费‑储蓄行为。模型能够同时解释两大经验现象：流动性较低的失业家庭对刺激转移的边际消费倾向（MPC）更高，以及拥有更多失业经历的家庭在控制当前疤痕”效应），并与最新实证结果高度吻合。仿真表明，价值函数近似误差随经验演化提供了超越传统理性预期的统一解释框架。", "keywords": "reinforcement learning, consumption-savings, Q-learning, neural network approximation, marginal propensity to consume, scarring effect, adaptive learning, economic downturn", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Brandon Kaplowitz"]}
]]></acme>

<pubDate>2025-10-23T17:14:49+00:00</pubDate>
</item>
<item>
<title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title>
<link>https://papers.cool/arxiv/2510.20739</link>
<guid>https://papers.cool/arxiv/2510.20739</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how machine learning can prioritize vulnerability reports generated by dynamic program analysis tools for Node.js packages. It evaluates classical models, graph neural networks, large language models, and hybrid approaches on a benchmark of 1,883 packages, achieving up to 0.915 F1 and reducing manual review workload by filtering out the majority of benign reports. The best model can operate at high precision while still detecting over 99% of exploitable taint flows.<br /><strong>Summary (CN):</strong> 本文研究机器学习如何对 Node.js 包的动态程序分析工具产生的漏洞报告进行优先级排序。通过在 1,883 个包的基准上评估传统模型、图神经网络、大语言模型及其混合方式，最高 F1 达到 0.915，并能够在高精度下过滤掉大多数无害报告，从而显著降低人工审查负担。最佳模型在保持约 99% 可利用污点流检测率的同时，实现了约 66.9% 的良性包自动剔除。<br /><strong>Keywords:</strong> vulnerability triage, taint analysis, Node.js, machine learning, graph neural network, large language model, dynamic program analysis, software security<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - -applicable; Primary focus - other<br /><strong>Authors:</strong> Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang</div>
Program analysis tools often produce large volumes of candidate vulnerability reports that require costly manual review, creating a practical challenge: how can security analysts prioritize the reports most likely to be true vulnerabilities? This paper investigates whether machine learning can be applied to prioritizing vulnerabilities reported by program analysis tools. We focus on Node.js packages and collect a benchmark of 1,883 Node.js packages, each containing one reported ACE or ACI vulnerability. We evaluate a variety of machine learning approaches, including classical models, graph neural networks (GNNs), large language models (LLMs), and hybrid models that combine GNN and LLMs, trained on data based on a dynamic program analysis tool's output. The top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading model eliminates 66.9% of benign packages from manual review, taking around 60 ms per package. If the best model is tuned to operate at a precision level of 0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating strong potential for real-world vulnerability triage.
<div><strong>Authors:</strong> Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how machine learning can prioritize vulnerability reports generated by dynamic program analysis tools for Node.js packages. It evaluates classical models, graph neural networks, large language models, and hybrid approaches on a benchmark of 1,883 packages, achieving up to 0.915 F1 and reducing manual review workload by filtering out the majority of benign reports. The best model can operate at high precision while still detecting over 99% of exploitable taint flows.", "summary_cn": "本文研究机器学习如何对 Node.js 包的动态程序分析工具产生的漏洞报告进行优先级排序。通过在 1,883 个包的基准上评估传统模型、图神经网络、大语言模型及其混合方式，最高 F1 达到 0.915，并能够在高精度下过滤掉大多数无害报告，从而显著降低人工审查负担。最佳模型在保持约 99% 可利用污点流检测率的同时，实现了约 66.9% 的良性包自动剔除。", "keywords": "vulnerability triage, taint analysis, Node.js, machine learning, graph neural network, large language model, dynamic program analysis, software security", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "-applicable", "primary_focus": "other"}, "authors": ["Ronghao Ni", "Aidan Z. H. Yang", "Min-Chien Hsu", "Nuno Sabino", "Limin Jia", "Ruben Martins", "Darion Cassel", "Kevin Cheang"]}
]]></acme>

<pubDate>2025-10-23T16:58:02+00:00</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Small Models</title>
<link>https://papers.cool/arxiv/2510.20690</link>
<guid>https://papers.cool/arxiv/2510.20690</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to reduce hallucination rates in language models without increasing parameters or data. By proving a bound linking hallucination probability to representational correlation and implementing ND-LoRA (parallel LoRA adapters with Barlow Twins regularization), the authors achieve up to 25.6% reduction in hallucinations while preserving accuracy. Analyses reveal task-dependent optimal levels of diversity and suggest neural diversity as a third scaling axis orthogonal to size and data.<br /><strong>Summary (CN):</strong> 本文提出神经多样性（去相关的平行表征）作为降低语言模型幻觉率的原理机制，且在不增加参数或数据的情况下实现。通过证明幻觉概率上界与表征相关性之间的关系，并引入 ND-LoRA（平行 LoRA 适配器结合 Barlow Twins 正则），实验显示幻觉率最高降低 25.6%，且不牺牲整体准确性。进一步分析表明不同任务需要不同的最优多样性水平，神经多样性成为与参数和数据规模正交的第三个扩展维度。<br /><strong>Keywords:</strong> neural diversity, hallucination reduction, LoRA adapters, Barlow Twins, representational correlation, language model reliability<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. Inspired by portfolio theory, where uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination probability is bounded by representational correlation: $P(H) \leq f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language models need an optimal amount of neurodiversity. To validate this, we introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces hallucinations by up to 25.6% (and 14.6% on average) without degrading general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational analyses indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different amounts of optimal neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
<div><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to reduce hallucination rates in language models without increasing parameters or data. By proving a bound linking hallucination probability to representational correlation and implementing ND-LoRA (parallel LoRA adapters with Barlow Twins regularization), the authors achieve up to 25.6% reduction in hallucinations while preserving accuracy. Analyses reveal task-dependent optimal levels of diversity and suggest neural diversity as a third scaling axis orthogonal to size and data.", "summary_cn": "本文提出神经多样性（去相关的平行表征）作为降低语言模型幻觉率的原理机制，且在不增加参数或数据的情况下实现。通过证明幻觉概率上界与表征相关性之间的关系，并引入 ND-LoRA（平行 LoRA 适配器结合 Barlow Twins 正则），实验显示幻觉率最高降低 25.6%，且不牺牲整体准确性。进一步分析表明不同任务需要不同的最优多样性水平，神经多样性成为与参数和数据规模正交的第三个扩展维度。", "keywords": "neural diversity, hallucination reduction, LoRA adapters, Barlow Twins, representational correlation, language model reliability", "scoring": {"interpretability": 6, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Kushal Chakrabarti", "Nirmal Balachundhar"]}
]]></acme>

<pubDate>2025-10-23T16:03:07+00:00</pubDate>
</item>
<item>
<title>Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling</title>
<link>https://papers.cool/arxiv/2510.20673</link>
<guid>https://papers.cool/arxiv/2510.20673</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two techniques—weight bias correction and bit-wise coreset sampling—to dramatically reduce the training overhead of multi-bit quantization networks while preserving accuracy across multiple precisions. Weight bias correction aligns activation distributions across bit-widths, removing the need for fine‑tuning, and coreset sampling selects a compact, gradient‑important subset of data for each child model. Experiments on CIFAR‑10/100, TinyImageNet, and ImageNet‑1K with ResNet and ViT show up to 7.88× faster training with competitive or superior performance.<br /><strong>Summary (CN):</strong> 本文提出两种技术——权重偏置校正和按位核心集采样，以大幅降低多比特量化网络的训练开销，同时保持跨多精度的模型性能。权重偏置校正通过对齐不同位宽下的激活分布，消除微调需求；按位核心集采样基于梯度重要性分数，为每个子模型挑选紧凑且信息丰富的数据子集。实验在 CIFAR‑10/100、TinyImageNet 和 ImageNet‑1K 上使用 ResNet 与 ViT 验证，训练速度提升最高达 7.88 倍，且精度保持或更优。<br /><strong>Keywords:</strong> multi-bit quantization, weight bias correction, bit-wise coreset sampling, model compression, training efficiency, gradient-based importance, ResNet, Vision Transformer, CIFAR-10, ImageNet<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko</div>
Multi-bit quantization networks enable flexible deployment of deep neural networks by supporting multiple precision levels within a single model. However, existing approaches suffer from significant training overhead as full-dataset updates are repeated for each supported bit-width, resulting in a cost that scales linearly with the number of precisions. Additionally, extra fine-tuning stages are often required to support additional or intermediate precision options, further compounding the overall training burden. To address this issue, we propose two techniques that greatly reduce the training overhead without compromising model utility: (i) Weight bias correction enables shared batch normalization and eliminates the need for fine-tuning by neutralizing quantization-induced bias across bit-widths and aligning activation distributions; and (ii) Bit-wise coreset sampling strategy allows each child model to train on a compact, informative subset selected via gradient-based importance scores by exploiting the implicit knowledge transfer phenomenon. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and ViT architectures demonstrate that our method achieves competitive or superior accuracy while reducing training time up to 7.88x. Our code is released at https://github.com/a2jinhee/EMQNet_jk.
<div><strong>Authors:</strong> Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two techniques—weight bias correction and bit-wise coreset sampling—to dramatically reduce the training overhead of multi-bit quantization networks while preserving accuracy across multiple precisions. Weight bias correction aligns activation distributions across bit-widths, removing the need for fine‑tuning, and coreset sampling selects a compact, gradient‑important subset of data for each child model. Experiments on CIFAR‑10/100, TinyImageNet, and ImageNet‑1K with ResNet and ViT show up to 7.88× faster training with competitive or superior performance.", "summary_cn": "本文提出两种技术——权重偏置校正和按位核心集采样，以大幅降低多比特量化网络的训练开销，同时保持跨多精度的模型性能。权重偏置校正通过对齐不同位宽下的激活分布，消除微调需求；按位核心集采样基于梯度重要性分数，为每个子模型挑选紧凑且信息丰富的数据子集。实验在 CIFAR‑10/100、TinyImageNet 和 ImageNet‑1K 上使用 ResNet 与 ViT 验证，训练速度提升最高达 7.88 倍，且精度保持或更优。", "keywords": "multi-bit quantization, weight bias correction, bit-wise coreset sampling, model compression, training efficiency, gradient-based importance, ResNet, Vision Transformer, CIFAR-10, ImageNet", "scoring": {"interpretability": 1, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinhee Kim", "Jae Jun An", "Kang Eun Jeon", "Jong Hwan Ko"]}
]]></acme>

<pubDate>2025-10-23T15:49:02+00:00</pubDate>
</item>
<item>
<title>Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection</title>
<link>https://papers.cool/arxiv/2510.20653</link>
<guid>https://papers.cool/arxiv/2510.20653</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically compares self‑reflection and budget‑tuning techniques across mathematical reasoning and translation tasks for several large language models, analyzing trade‑offs among accuracy, cost, and latency and identifying domain‑dependent performance gains up to 220%. It explores how reflection depth and feedback quality affect outcomes, derives Pareto‑optimal frontiers, and validates the approach in a real‑world marketing localisation system.<br /><strong>Summary (CN):</strong> 本文系统性地比较了自我反思（self‑reflection）和预算调优（budget tuning）在数学推理和翻译任务中的表现，评估不同大语言模型在准确率、成本和延迟之间的权衡，并发现不同领域的性能提升可达 220%。研究还分析了反思深度和反馈质量的影响，绘制了 Pareto 最优前沿，并在实际的营销内容本地化系统中进行了验证。<br /><strong>Keywords:</strong> self-reflection, budget tuning, inference optimization, LLM evaluation, trade-off analysis, Pareto frontier, mathematical reasoning, translation, downstream deployment, performance scaling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jack Butler, Nikita Kozodoi, Zainab Afolabi, Brian Tyacke, Gaiar Baimuratov</div>
As Large Language Models (LLMs) continue to evolve, practitioners face increasing options for enhancing inference-time performance without model retraining, including budget tuning and multi-step techniques like self-reflection. While these methods improve output quality, they create complex trade-offs among accuracy, cost, and latency that remain poorly understood across different domains. This paper systematically compares self-reflection and budget tuning across mathematical reasoning and translation tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and Mistral families, along with other models under varying reflection depths and compute budgets to derive Pareto optimal performance frontiers. Our analysis reveals substantial domain dependent variation in self-reflection effectiveness, with performance gains up to 220\% in mathematical reasoning. We further investigate how reflection round depth and feedback mechanism quality influence performance across model families. To validate our findings in a real-world setting, we deploy a self-reflection enhanced marketing content localisation system at Lounge by Zalando, where it shows market-dependent effectiveness, reinforcing the importance of domain specific evaluation when deploying these techniques. Our results provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints. We open source our self-reflection implementation for reproducibility at https://github.com/aws-samples/sample-genai-reflection-for-bedrock.
<div><strong>Authors:</strong> Jack Butler, Nikita Kozodoi, Zainab Afolabi, Brian Tyacke, Gaiar Baimuratov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically compares self‑reflection and budget‑tuning techniques across mathematical reasoning and translation tasks for several large language models, analyzing trade‑offs among accuracy, cost, and latency and identifying domain‑dependent performance gains up to 220%. It explores how reflection depth and feedback quality affect outcomes, derives Pareto‑optimal frontiers, and validates the approach in a real‑world marketing localisation system.", "summary_cn": "本文系统性地比较了自我反思（self‑reflection）和预算调优（budget tuning）在数学推理和翻译任务中的表现，评估不同大语言模型在准确率、成本和延迟之间的权衡，并发现不同领域的性能提升可达 220%。研究还分析了反思深度和反馈质量的影响，绘制了 Pareto 最优前沿，并在实际的营销内容本地化系统中进行了验证。", "keywords": "self-reflection, budget tuning, inference optimization, LLM evaluation, trade-off analysis, Pareto frontier, mathematical reasoning, translation, downstream deployment, performance scaling", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jack Butler", "Nikita Kozodoi", "Zainab Afolabi", "Brian Tyacke", "Gaiar Baimuratov"]}
]]></acme>

<pubDate>2025-10-23T15:26:18+00:00</pubDate>
</item>
<item>
<title>Black Box Absorption: LLMs Undermining Innovative Ideas</title>
<link>https://papers.cool/arxiv/2510.20612</link>
<guid>https://papers.cool/arxiv/2510.20612</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines "Black Box Absorption" as the risk that opaque LLM platforms can internalize and repurpose novel ideas contributed by users, creating informational and structural asymmetries that threaten innovation ecosystems. It formalizes the concept with "idea units" and "idea safety" standards, analyses how absorption occurs, and proposes a governance and engineering agenda to keep creator contributions traceable, controllable, and equitable.<br /><strong>Summary (CN):</strong> 本文提出“黑箱吸收”风险，即不透明的大语言模型平台可能内部化并重新利用用户贡献的创新概念，导致创作者与平台运营方之间的信息和结构不对称，危及创新经济的可持续性。文中通过“idea unit”（想法单元）和“idea safety”（想法安全）两大概念进行形式化描述，分析吸收机制，并提出治理与工程议程，以确保创作者的贡献可追溯、可控制且公平。<br /><strong>Keywords:</strong> black box absorption, large language models, idea safety, governance, AI safety, intellectual property, model leakage, innovation economics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 5, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - control<br /><strong>Authors:</strong> Wenjun Cao</div>
Large Language Models are increasingly adopted as critical tools for accelerating innovation. This paper identifies and formalizes a systemic risk inherent in this paradigm: \textbf{Black Box Absorption}. We define this as the process by which the opaque internal architectures of LLM platforms, often operated by large-scale service providers, can internalize, generalize, and repurpose novel concepts contributed by users during interaction. This mechanism threatens to undermine the foundational principles of innovation economics by creating severe informational and structural asymmetries between individual creators and platform operators, thereby jeopardizing the long-term sustainability of the innovation ecosystem. To analyze this challenge, we introduce two core concepts: the idea unit, representing the transportable functional logic of an innovation, and idea safety, a multidimensional standard for its protection. This paper analyzes the mechanisms of absorption and proposes a concrete governance and engineering agenda to mitigate these risks, ensuring that creator contributions remain traceable, controllable, and equitable.
<div><strong>Authors:</strong> Wenjun Cao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines \"Black Box Absorption\" as the risk that opaque LLM platforms can internalize and repurpose novel ideas contributed by users, creating informational and structural asymmetries that threaten innovation ecosystems. It formalizes the concept with \"idea units\" and \"idea safety\" standards, analyses how absorption occurs, and proposes a governance and engineering agenda to keep creator contributions traceable, controllable, and equitable.", "summary_cn": "本文提出“黑箱吸收”风险，即不透明的大语言模型平台可能内部化并重新利用用户贡献的创新概念，导致创作者与平台运营方之间的信息和结构不对称，危及创新经济的可持续性。文中通过“idea unit”（想法单元）和“idea safety”（想法安全）两大概念进行形式化描述，分析吸收机制，并提出治理与工程议程，以确保创作者的贡献可追溯、可控制且公平。", "keywords": "black box absorption, large language models, idea safety, governance, AI safety, intellectual property, model leakage, innovation economics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Wenjun Cao"]}
]]></acme>

<pubDate>2025-10-23T14:43:09+00:00</pubDate>
</item>
<item>
<title>Strategic Costs of Perceived Bias in Fair Selection</title>
<link>https://papers.cool/arxiv/2510.20606</link>
<guid>https://papers.cool/arxiv/2510.20606</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a game-theoretic model of meritocratic selection where candidates from different socioeconomic groups face differing perceived‑selection values, shaped by social context and AI‑driven guidance tools. Candidates choose effort strategically, trading off its cost against expected rewards, and the model yields a unique Nash equilibrium describing effort levels, representation, and welfare as functions of valuation disparities and institutional selectivity. The authors propose a cost‑sensitive optimization framework to adjust selectivity or perceived value, reducing disparity without sacrificing institutional objectives, highlighting a perception‑driven bias that can propagate through ostensibly fair selection mechanisms.<br /><strong>Summary (CN):</strong> 本文提出一个博弈论模型，研究在 meritocratic（择优）选拔体系中，不同社会经济群体因社会环境和 AI 驱动的职业/薪资指导工具而对选拔后价值的感知存在差异。候选人基于努力成本与预期回报进行战略性努力选择，模型给出唯一的纳什均衡，展示感知价值差异和机构选择度如何共同决定努力水平、群体代表性、社会福利以及机构效用。作者进一步提出一种成本敏感的优化框架，通过调整选择度或感知价值，实现在不牺牲机构目标的前提下降低不平等，揭示了感知驱动的偏见如何在表面公平的选拔过程中产生并放大。<br /><strong>Keywords:</strong> fair selection, game theory perceived bias, socioeconomic disparity, strategic effort, AI-guided career advice, meritocracy, social welfare<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> L. Elisa Celis, Lingxiao Huang, Milind Sohoni, Nisheeth K. Vishnoi</div>
Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value--shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise "fair" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcome--bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems.
<div><strong>Authors:</strong> L. Elisa Celis, Lingxiao Huang, Milind Sohoni, Nisheeth K. Vishnoi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a game-theoretic model of meritocratic selection where candidates from different socioeconomic groups face differing perceived‑selection values, shaped by social context and AI‑driven guidance tools. Candidates choose effort strategically, trading off its cost against expected rewards, and the model yields a unique Nash equilibrium describing effort levels, representation, and welfare as functions of valuation disparities and institutional selectivity. The authors propose a cost‑sensitive optimization framework to adjust selectivity or perceived value, reducing disparity without sacrificing institutional objectives, highlighting a perception‑driven bias that can propagate through ostensibly fair selection mechanisms.", "summary_cn": "本文提出一个博弈论模型，研究在 meritocratic（择优）选拔体系中，不同社会经济群体因社会环境和 AI 驱动的职业/薪资指导工具而对选拔后价值的感知存在差异。候选人基于努力成本与预期回报进行战略性努力选择，模型给出唯一的纳什均衡，展示感知价值差异和机构选择度如何共同决定努力水平、群体代表性、社会福利以及机构效用。作者进一步提出一种成本敏感的优化框架，通过调整选择度或感知价值，实现在不牺牲机构目标的前提下降低不平等，揭示了感知驱动的偏见如何在表面公平的选拔过程中产生并放大。", "keywords": "fair selection, game theory perceived bias, socioeconomic disparity, strategic effort, AI-guided career advice, meritocracy, social welfare", "scoring": {"interpretability": 1, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["L. Elisa Celis", "Lingxiao Huang", "Milind Sohoni", "Nisheeth K. Vishnoi"]}
]]></acme>

<pubDate>2025-10-23T14:38:05+00:00</pubDate>
</item>
<item>
<title>Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences</title>
<link>https://papers.cool/arxiv/2510.20595</link>
<guid>https://papers.cool/arxiv/2510.20595</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Diffusion Autoencoders with Perceivers (daep), a self-supervised architecture that tokenizes heterogeneous astronomical measurements, encodes them with a Perceiver encoder, and reconstructs them via a Perceiver‑IO diffusion decoder. Benchmarked against VAE and a Perceiver‑based masked autoencoder (maep) on various spectroscopic and photometric datasets, daep achieves lower reconstruction error, more discriminative latent representations, and better preservation of fine‑scale structure.<br /><strong>Summary (CN):</strong> 本文提出了 Diffusion Autoencoders with Perceivers (daep)，一种自监督架构，能够对异构的天文测量进行标记化，使用 Perceiver 编码器压缩后，再通过 Perceiver‑IO diffusion 解码器进行重构。在多个光谱和光度数据集上，与 VAE 和 Perceiver‑based 掩码自编码器 (maep) 对比，daep 显著降低了重构误差，生成了更具区分性的潜在空间，并更好地保留了细粒度结构。<br /><strong>Keywords:</strong> diffusion autoencoder, perceiver, irregular sequences, multimodal representation, self-supervised learning, astronomical data, latent space, masked autoencoder<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yunyi Shen, Alexander Gagliano</div>
Self-supervised learning has become a central strategy for representation learning, but the majority of architectures used for encoding data have only been validated on regularly-sampled inputs such as images, audios. and videos. In many scientific domains, data instead arrive as long, irregular, and multimodal sequences. To extract semantic information from these data, we introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes heterogeneous measurements, compresses them with a Perceiver encoder, and reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable learning in diverse data settings. To benchmark the daep architecture, we adapt the masked autoencoder to a Perceiver encoder/decoder design, and establish a strong baseline (maep) in the same architectural family as daep. Across diverse spectroscopic and photometric astronomical datasets, daep achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and maep baselines. These results establish daep as an effective framework for scientific domains where data arrives as irregular, heterogeneous sequences.
<div><strong>Authors:</strong> Yunyi Shen, Alexander Gagliano</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Diffusion Autoencoders with Perceivers (daep), a self-supervised architecture that tokenizes heterogeneous astronomical measurements, encodes them with a Perceiver encoder, and reconstructs them via a Perceiver‑IO diffusion decoder. Benchmarked against VAE and a Perceiver‑based masked autoencoder (maep) on various spectroscopic and photometric datasets, daep achieves lower reconstruction error, more discriminative latent representations, and better preservation of fine‑scale structure.", "summary_cn": "本文提出了 Diffusion Autoencoders with Perceivers (daep)，一种自监督架构，能够对异构的天文测量进行标记化，使用 Perceiver 编码器压缩后，再通过 Perceiver‑IO diffusion 解码器进行重构。在多个光谱和光度数据集上，与 VAE 和 Perceiver‑based 掩码自编码器 (maep) 对比，daep 显著降低了重构误差，生成了更具区分性的潜在空间，并更好地保留了细粒度结构。", "keywords": "diffusion autoencoder, perceiver, irregular sequences, multimodal representation, self-supervised learning, astronomical data, latent space, masked autoencoder", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yunyi Shen", "Alexander Gagliano"]}
]]></acme>

<pubDate>2025-10-23T14:21:01+00:00</pubDate>
</item>
<item>
<title>Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image</title>
<link>https://papers.cool/arxiv/2510.20539</link>
<guid>https://papers.cool/arxiv/2510.20539</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Blur2Seq, a deep learning framework that jointly restores a sharp image and estimates the underlying 3D camera motion trajectory from a single motion-blurred photograph. By embedding a differentiable Projective Motion Blur Model into the network, it predicts a full rotation trajectory that guides a model-based deblurring network, and the estimated trajectory can be used to reconstruct the sequence of sharp frames that generated the blur, with post‑inference refinement via a reblur loss. Experiments demonstrate state‑of‑the‑art performance on synthetic and real datasets, especially under severe or spatially variant blur where conventional end‑to‑end deblurring fails.<br /><strong>Summary (CN):</strong> 本文提出 Blur2Seq，一个深度学习框架，可从单张摄像机运动模糊图像中同时恢复清晰图像并估计其 3D 相机运动轨迹。通过将可微分的投影运动模糊模型 (PMBM) 融入网络，模型预测完整的旋转轨迹并用于指导基于模型的去模糊网络，估计的轨迹还能重建产生模糊图像的连续清晰帧，并通过再模糊损进行后推优化。实验表明，在合成和真实数据集上，尤其是严重或空间变化模糊情况下，方法显著优于传统端到端去模糊方法。<br /><strong>Keywords:</strong> blind deblurring, camera motion trajectory estimation, motion blur model, differentiable blur module, deep learning, image restoration, 3D rotation trajectory, interpretability, reblur loss, sequential sharp image reconstruction<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Guillermo Carbajal, Andrés Almansa, Pablo Musé</div>
Motion blur caused by camera shake, particularly under large or rotational movements, remains a major challenge in image restoration. We propose a deep learning framework that jointly estimates the latent sharp image and the underlying camera motion trajectory from a single blurry image. Our method leverages the Projective Motion Blur Model (PMBM), implemented efficiently using a differentiable blur creation module compatible with modern networks. A neural network predicts a full 3D rotation trajectory, which guides a model-based restoration network trained end-to-end. This modular architecture provides interpretability by revealing the camera motion that produced the blur. Moreover, this trajectory enables the reconstruction of the sequence of sharp images that generated the observed blurry image. To further refine results, we optimize the trajectory post-inference via a reblur loss, improving consistency between the blurry input and the restored output. Extensive experiments show that our method achieves state-of-the-art performance on both synthetic and real datasets, particularly in cases with severe or spatially variant blur, where end-to-end deblurring networks struggle. Code and trained models are available at https://github.com/GuillermoCarbajal/Blur2Seq/
<div><strong>Authors:</strong> Guillermo Carbajal, Andrés Almansa, Pablo Musé</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Blur2Seq, a deep learning framework that jointly restores a sharp image and estimates the underlying 3D camera motion trajectory from a single motion-blurred photograph. By embedding a differentiable Projective Motion Blur Model into the network, it predicts a full rotation trajectory that guides a model-based deblurring network, and the estimated trajectory can be used to reconstruct the sequence of sharp frames that generated the blur, with post‑inference refinement via a reblur loss. Experiments demonstrate state‑of‑the‑art performance on synthetic and real datasets, especially under severe or spatially variant blur where conventional end‑to‑end deblurring fails.", "summary_cn": "本文提出 Blur2Seq，一个深度学习框架，可从单张摄像机运动模糊图像中同时恢复清晰图像并估计其 3D 相机运动轨迹。通过将可微分的投影运动模糊模型 (PMBM) 融入网络，模型预测完整的旋转轨迹并用于指导基于模型的去模糊网络，估计的轨迹还能重建产生模糊图像的连续清晰帧，并通过再模糊损进行后推优化。实验表明，在合成和真实数据集上，尤其是严重或空间变化模糊情况下，方法显著优于传统端到端去模糊方法。", "keywords": "blind deblurring, camera motion trajectory estimation, motion blur model, differentiable blur module, deep learning, image restoration, 3D rotation trajectory, interpretability, reblur loss, sequential sharp image reconstruction", "scoring": {"interpretability": 7, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Guillermo Carbajal", "Andrés Almansa", "Pablo Musé"]}
]]></acme>

<pubDate>2025-10-23T13:26:07+00:00</pubDate>
</item>
<item>
<title>Adversary-Aware Private Inference over Wireless Channels</title>
<link>https://papers.cool/arxiv/2510.20518</link>
<guid>https://papers.cool/arxiv/2510.20518</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a novel adversary-aware framework for private inference over wireless channels, where edge devices transform extracted features before transmitting them to a remote model server to mitigate privacy risks from feature reconstruction attacks. It discusses the limitations of conventional differential privacy for protecting individual features and proposes transformation mechanisms tailored to wireless communication constraints. Experimental results demonstrate reduced privacy leakage while maintaining inference accuracy in typical vision and perception tasks.<br /><strong>Summary (CN):</strong> 本文提出了一种针对无线信道的对手感知私密推理框架，边缘设备在将提取的特征发送至远程模型服务器之前进行转化，以降低特征被对手重构的隐私风险。文章指出传统差分隐私在保护单个特征方面的局限，并提出了适用于无线通信约束的特征转化机制。实验表明，在保持视觉和感知任务推理精度的同时，显著降低了隐私泄漏。<br /><strong>Keywords:</strong> private inference, wireless channels, adversary-aware, feature transformation, differential privacy, edge AI, privacy-preserving, secure communication, AI safety, privacy leakage<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other<br /><strong>Authors:</strong> Mohamed Seif, Malcolm Egan, Andrea J. Goldsmith, H. Vincent Poor</div>
AI-based sensing at wireless edge devices has the potential to significantly enhance Artificial Intelligence (AI) applications, particularly for vision and perception tasks such as in autonomous driving and environmental monitoring. AI systems rely both on efficient model learning and inference. In the inference phase, features extracted from sensing data are utilized for prediction tasks (e.g., classification or regression). In edge networks, sensors and model servers are often not co-located, which requires communication of features. As sensitive personal data can be reconstructed by an adversary, transformation of the features are required to reduce the risk of privacy violations. While differential privacy mechanisms provide a means of protecting finite datasets, protection of individual features has not been addressed. In this paper, we propose a novel framework for privacy-preserving AI-based sensing, where devices apply transformations of extracted features before transmission to a model server.
<div><strong>Authors:</strong> Mohamed Seif, Malcolm Egan, Andrea J. Goldsmith, H. Vincent Poor</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a novel adversary-aware framework for private inference over wireless channels, where edge devices transform extracted features before transmitting them to a remote model server to mitigate privacy risks from feature reconstruction attacks. It discusses the limitations of conventional differential privacy for protecting individual features and proposes transformation mechanisms tailored to wireless communication constraints. Experimental results demonstrate reduced privacy leakage while maintaining inference accuracy in typical vision and perception tasks.", "summary_cn": "本文提出了一种针对无线信道的对手感知私密推理框架，边缘设备在将提取的特征发送至远程模型服务器之前进行转化，以降低特征被对手重构的隐私风险。文章指出传统差分隐私在保护单个特征方面的局限，并提出了适用于无线通信约束的特征转化机制。实验表明，在保持视觉和感知任务推理精度的同时，显著降低了隐私泄漏。", "keywords": "private inference, wireless channels, adversary-aware, feature transformation, differential privacy, edge AI, privacy-preserving, secure communication, AI safety, privacy leakage", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}, "authors": ["Mohamed Seif", "Malcolm Egan", "Andrea J. Goldsmith", "H. Vincent Poor"]}
]]></acme>

<pubDate>2025-10-23T13:02:14+00:00</pubDate>
</item>
<item>
<title>Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment</title>
<link>https://papers.cool/arxiv/2510.20513</link>
<guid>https://papers.cool/arxiv/2510.20513</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeEAR, a framework that converts human preference for speech expressiveness into an objective score across three dimensions—Emotion, Prosody, and Spontaneity—achieving a Spearman's rank correlation of 0.86 with human judgments using fewer than 500 annotated samples. DeEAR is used for fair benchmarking of speech-to-speech models and for curating a 14K-utterance expressive dataset (ExpressiveSpeech) that significantly improves expressive scores of S2S systems.<br /><strong>Summary (CN):</strong> 本文提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分，涵盖情感、韵律和自然度三个维度，在少于 500 条标注样本下实现与人工感知的 Spearman 秩相关系数 0.86。DeEAR 用于对语音到语音模型进行公平基准测试，并筛选出 14 000 条富表达力的语料库（ExpressiveSpeech），显著提升模型的表达评分。<br /><strong>Keywords:</strong> speech expressiveness, human preference metric, DeEAR, SRCC, prosody, emotion, spontaneity, speech-to-speech, expressive dataset, evaluation benchmark<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang</div>
Recent speech-to-speech (S2S) models generate intelligible speech but still lack natural expressiveness, largely due to the absence of a reliable evaluation metric. Existing approaches, such as subjective MOS ratings, low-level acoustic features, and emotion recognition are costly, limited, or incomplete. To address this, we present DeEAR (Decoding the Expressive Preference of eAR), a framework that converts human preference for speech expressiveness into an objective score. Grounded in phonetics and psychology, DeEAR evaluates speech across three dimensions: Emotion, Prosody, and Spontaneity, achieving strong alignment with human perception (Spearman's Rank Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples. Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data curation. It not only distinguishes expressiveness gaps across S2S models but also selects 14K expressive utterances to form ExpressiveSpeech, which improves the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models. Demos and codes are available at https://github.com/FreedomIntelligence/ExpressiveSpeech
<div><strong>Authors:</strong> Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeEAR, a framework that converts human preference for speech expressiveness into an objective score across three dimensions—Emotion, Prosody, and Spontaneity—achieving a Spearman's rank correlation of 0.86 with human judgments using fewer than 500 annotated samples. DeEAR is used for fair benchmarking of speech-to-speech models and for curating a 14K-utterance expressive dataset (ExpressiveSpeech) that significantly improves expressive scores of S2S systems.", "summary_cn": "本文提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分，涵盖情感、韵律和自然度三个维度，在少于 500 条标注样本下实现与人工感知的 Spearman 秩相关系数 0.86。DeEAR 用于对语音到语音模型进行公平基准测试，并筛选出 14 000 条富表达力的语料库（ExpressiveSpeech），显著提升模型的表达评分。", "keywords": "speech expressiveness, human preference metric, DeEAR, SRCC, prosody, emotion, spontaneity, speech-to-speech, expressive dataset, evaluation benchmark", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Zhiyu Lin", "Jingwen Yang", "Jiale Zhao", "Meng Liu", "Sunzhu Li", "Benyou Wang"]}
]]></acme>

<pubDate>2025-10-23T12:57:46+00:00</pubDate>
</item>
<item>
<title>Concentration and excess risk bounds for imbalanced classification with synthetic oversampling</title>
<link>https://papers.cool/arxiv/2510.20472</link>
<guid>https://papers.cool/arxiv/2510.20472</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper develops a theoretical framework for analyzing synthetic oversampling methods such as SMOTE in imbalanced classification, deriving uniform concentration bounds between empirical risk on synthetic minority samples and population risk, and providing non‑parametric excess risk guarantees for kernel‑based classifiers trained on the synthetic data. It also offers practical guidelines for tuning SMOTE and downstream learning algorithms and validates the theory with numerical experiments.<br /><strong>Summary (CN):</strong> 本文建立了一个理论框架，分析用于处理类别不平衡问题的合成过采样方法（如 SMOTE）的行为，推导了合成少数样本的经验风险与真实少数分布的总体风险之间的统一浓度界，并给出了在该合成数据上训练的基于核的分类器的非参数超额风险保证。文中还提供了 SMOTE 与下游学习算法的参数调优实用指南，并通过数值实验验证理论结果。<br /><strong>Keywords:</strong> SMOTE, synthetic oversampling, imbalanced classification, concentration bounds, excess risk, kernel classifiers, theoretical analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Touqeer Ahmad, Mohammadreza M. Kalan, François Portier, Gilles Stupfler</div>
Synthetic oversampling of minority examples using SMOTE and its variants is a leading strategy for addressing imbalanced classification problems. Despite the success of this approach in practice, its theoretical foundations remain underexplored. We develop a theoretical framework to analyze the behavior of SMOTE and related methods when classifiers are trained on synthetic data. We first derive a uniform concentration bound on the discrepancy between the empirical risk over synthetic minority samples and the population risk on the true minority distribution. We then provide a nonparametric excess risk guarantee for kernel-based classifiers trained using such synthetic data. These results lead to practical guidelines for better parameter tuning of both SMOTE and the downstream learning algorithm. Numerical experiments are provided to illustrate and support the theoretical findings
<div><strong>Authors:</strong> Touqeer Ahmad, Mohammadreza M. Kalan, François Portier, Gilles Stupfler</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper develops a theoretical framework for analyzing synthetic oversampling methods such as SMOTE in imbalanced classification, deriving uniform concentration bounds between empirical risk on synthetic minority samples and population risk, and providing non‑parametric excess risk guarantees for kernel‑based classifiers trained on the synthetic data. It also offers practical guidelines for tuning SMOTE and downstream learning algorithms and validates the theory with numerical experiments.", "summary_cn": "本文建立了一个理论框架，分析用于处理类别不平衡问题的合成过采样方法（如 SMOTE）的行为，推导了合成少数样本的经验风险与真实少数分布的总体风险之间的统一浓度界，并给出了在该合成数据上训练的基于核的分类器的非参数超额风险保证。文中还提供了 SMOTE 与下游学习算法的参数调优实用指南，并通过数值实验验证理论结果。", "keywords": "SMOTE, synthetic oversampling, imbalanced classification, concentration bounds, excess risk, kernel classifiers, theoretical analysis", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Touqeer Ahmad", "Mohammadreza M. Kalan", "François Portier", "Gilles Stupfler"]}
]]></acme>

<pubDate>2025-10-23T12:12:51+00:00</pubDate>
</item>
<item>
<title>Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$</title>
<link>https://papers.cool/arxiv/2510.20457</link>
<guid>https://papers.cool/arxiv/2510.20457</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces EBR, a neural reasoner that approximates symbolic reasoning in the expressive description logic SHOIQ by learning embeddings for atomic concepts and existential restrictions. EBR can retrieve instances for any complex concept without invoking a traditional reasoner, and experiments show it remains robust to missing or erroneous data where conventional reasoners fail. This work advances neuro‑symbolic concept learning toward deployment on real‑world, noisy knowledge bases.<br /><strong>Summary (CN):</strong> 本文提出了 EBR 神经推理器，通过对原子概念和存在限制学习嵌入，实现对表达性描述逻辑 SHOIQ 中任意概念的实例检索，而无需使用传统符号推理器。实验表明，在数据缺失或错误情况下，EBR 能保持稳健，相较于现有推理器表现更佳。该工作推动了神经符号概念学习在真实、噪声知识库中的应用。<br /><strong>Keywords:</strong> neural reasoner, description logic, SHOIQ, instance retrieval, embeddings, robustness, neuro-symbolic, concept learning<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Louis Mozart Kamdem Teyou, Luke Friedrichs, N'Dah Jean Kouagou, Caglar Demir, Yasir Mahmood, Stefan Heindorf, Axel-Cyrille Ngonga Ngomo</div>
Concept learning exploits background knowledge in the form of description logic axioms to learn explainable classification models from knowledge bases. Despite recent breakthroughs in neuro-symbolic concept learning, most approaches still cannot be deployed on real-world knowledge bases. This is due to their use of description logic reasoners, which are not robust against inconsistencies nor erroneous data. We address this challenge by presenting a novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to approximate the results of a symbolic reasoner. We show that EBR solely requires retrieving instances for atomic concepts and existential restrictions to retrieve or approximate the set of instances of any concept in the description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with state-of-the-art reasoners. Our results suggest that EBR is robust against missing and erroneous data in contrast to existing reasoners.
<div><strong>Authors:</strong> Louis Mozart Kamdem Teyou, Luke Friedrichs, N'Dah Jean Kouagou, Caglar Demir, Yasir Mahmood, Stefan Heindorf, Axel-Cyrille Ngonga Ngomo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces EBR, a neural reasoner that approximates symbolic reasoning in the expressive description logic SHOIQ by learning embeddings for atomic concepts and existential restrictions. EBR can retrieve instances for any complex concept without invoking a traditional reasoner, and experiments show it remains robust to missing or erroneous data where conventional reasoners fail. This work advances neuro‑symbolic concept learning toward deployment on real‑world, noisy knowledge bases.", "summary_cn": "本文提出了 EBR 神经推理器，通过对原子概念和存在限制学习嵌入，实现对表达性描述逻辑 SHOIQ 中任意概念的实例检索，而无需使用传统符号推理器。实验表明，在数据缺失或错误情况下，EBR 能保持稳健，相较于现有推理器表现更佳。该工作推动了神经符号概念学习在真实、噪声知识库中的应用。", "keywords": "neural reasoner, description logic, SHOIQ, instance retrieval, embeddings, robustness, neuro-symbolic, concept learning", "scoring": {"interpretability": 5, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Louis Mozart Kamdem Teyou", "Luke Friedrichs", "N'Dah Jean Kouagou", "Caglar Demir", "Yasir Mahmood", "Stefan Heindorf", "Axel-Cyrille Ngonga Ngomo"]}
]]></acme>

<pubDate>2025-10-23T11:48:43+00:00</pubDate>
</item>
<item>
<title>Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics</title>
<link>https://papers.cool/arxiv/2510.20453</link>
<guid>https://papers.cool/arxiv/2510.20453</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper applies symbolic regression to the Constrained Minimal Supersymmetric Standard Model (CMSSM) to obtain compact analytic formulas for the Higgs mass, dark matter relic density, and the muon anomalous magnetic moment. These symbolic expressions enable fast, differentiable global fits of the CMSSM parameters that match conventional sampling results, and a comparison with neural‑network regression shows SR delivers more globally robust performance.<br /><strong>Summary (CN):</strong> 本文将符号回归（symbolic regression）应用于受约束最小超对称模型（CMSSM），推导出描述希格斯质量、暗物质剩余密度以及 μ 子 anomalous magnetic moment（muon g‑2）的紧凑解析式。利用这些解析式进行可微分的全局参数拟合，结果与传统采样方法一致，并与神经网络回归（neural network regression）进行比较，发现符号回归在全局鲁棒性方面表现更佳。<br /><strong>Keywords:</strong> symbolic regression, differentiable fitting CMSSM, Higgs mass, dark matter relic density, muon g-2, neural network regression, global fits, beyond the Standard Model<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</div>
We demonstrate the efficacy of symbolic regression (SR) to probe models of particle physics Beyond the Standard Model (BSM), by considering the so-called Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. We show that analysis of the phenomenology can be greatly accelerated by using symbolic expressions derived for the observables in terms of the input parameters. Here we focus on the Higgs mass, the cold dark matter relic density, and the contribution to the anomalous magnetic moment of the muon. We find that SR can produce remarkably accurate expressions. Using them we make global fits to derive the posterior probability densities of the CMSSM input parameters which are in good agreement with those performed using conventional methods. Moreover, we demonstrate a major advantage of SR which is the ability to make fits using differentiable methods rather than sampling methods. We also compare the method with neural network (NN) regression. SR produces more globally robust results, while NNs require data that is focussed on the promising regions in order to be equally performant.
<div><strong>Authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper applies symbolic regression to the Constrained Minimal Supersymmetric Standard Model (CMSSM) to obtain compact analytic formulas for the Higgs mass, dark matter relic density, and the muon anomalous magnetic moment. These symbolic expressions enable fast, differentiable global fits of the CMSSM parameters that match conventional sampling results, and a comparison with neural‑network regression shows SR delivers more globally robust performance.", "summary_cn": "本文将符号回归（symbolic regression）应用于受约束最小超对称模型（CMSSM），推导出描述希格斯质量、暗物质剩余密度以及 μ 子 anomalous magnetic moment（muon g‑2）的紧凑解析式。利用这些解析式进行可微分的全局参数拟合，结果与传统采样方法一致，并与神经网络回归（neural network regression）进行比较，发现符号回归在全局鲁棒性方面表现更佳。", "keywords": "symbolic regression, differentiable fitting CMSSM, Higgs mass, dark matter relic density, muon g-2, neural network regression, global fits, beyond the Standard Model", "scoring": {"interpretability": 7, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shehu AbdusSalam", "Steven Abel", "Deaglan Bartlett", "Miguel Crispim Romão"]}
]]></acme>

<pubDate>2025-10-23T11:40:15+00:00</pubDate>
</item>
<item>
<title>Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks</title>
<link>https://papers.cool/arxiv/2510.20436</link>
<guid>https://papers.cool/arxiv/2510.20436</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fully decentralized routing framework for lunar delay‑tolerant networks using a graph‑attention based multi‑agent reinforcement learning policy trained centrally and executed locally. It formulates routing as a POMDP, avoids global topology updates and packet duplication, and demonstrates higher delivery rates and scalability in Monte‑Carlo simulations of rover teams.<br /><strong>Summary (CN):</strong> 本文提出一种基于图注意力的多智能体强化学习（GAT‑MARL）的全去中心化路由框架，用于月面延迟容忍网络（LDTN）。该方法将路由问题建模为部分可观测马尔可夫决策过程（POMDP），在不需要全局拓扑更新或数据包复制的情况下，实现中心化训练、去中心化执行，并在蒙特卡罗仿真中显示出更高的数据交付率和良好的可扩展性。<br /><strong>Keywords:</strong> graph attention networks, multi-agent reinforcement learning, delay-tolerant networks, decentralized routing, lunar exploration, POMDP, space robotics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Federico Lozano-Cuadra, Beatriz Soret, Marc Sanchez Net, Abhishek Cauligi, Federico Rossi</div>
We present a fully decentralized routing framework for multi-robot exploration missions operating under the constraints of a Lunar Delay-Tolerant Network (LDTN). In this setting, autonomous rovers must relay collected data to a lander under intermittent connectivity and unknown mobility patterns. We formulate the problem as a Partially Observable Markov Decision Problem (POMDP) and propose a Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) policy that performs Centralized Training, Decentralized Execution (CTDE). Our method relies only on local observations and does not require global topology updates or packet replication, unlike classical approaches such as shortest path and controlled flooding-based algorithms. Through Monte Carlo simulations in randomized exploration environments, GAT-MARL provides higher delivery rates, no duplications, and fewer packet losses, and is able to leverage short-term mobility forecasts; offering a scalable solution for future space robotic systems for planetary exploration, as demonstrated by successful generalization to larger rover teams.
<div><strong>Authors:</strong> Federico Lozano-Cuadra, Beatriz Soret, Marc Sanchez Net, Abhishek Cauligi, Federico Rossi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fully decentralized routing framework for lunar delay‑tolerant networks using a graph‑attention based multi‑agent reinforcement learning policy trained centrally and executed locally. It formulates routing as a POMDP, avoids global topology updates and packet duplication, and demonstrates higher delivery rates and scalability in Monte‑Carlo simulations of rover teams.", "summary_cn": "本文提出一种基于图注意力的多智能体强化学习（GAT‑MARL）的全去中心化路由框架，用于月面延迟容忍网络（LDTN）。该方法将路由问题建模为部分可观测马尔可夫决策过程（POMDP），在不需要全局拓扑更新或数据包复制的情况下，实现中心化训练、去中心化执行，并在蒙特卡罗仿真中显示出更高的数据交付率和良好的可扩展性。", "keywords": "graph attention networks, multi-agent reinforcement learning, delay-tolerant networks, decentralized routing, lunar exploration, POMDP, space robotics", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Federico Lozano-Cuadra", "Beatriz Soret", "Marc Sanchez Net", "Abhishek Cauligi", "Federico Rossi"]}
]]></acme>

<pubDate>2025-10-23T11:13:11+00:00</pubDate>
</item>
<item>
<title>Partial Optimality in Cubic Correlation Clustering for General Graphs</title>
<link>https://papers.cool/arxiv/2510.20431</link>
<guid>https://papers.cool/arxiv/2510.20431</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies the cubic correlation clustering problem, where costs are assigned to cliques of size at most three, and seeks a clustering that minimizes the total cost of cliques fully contained within a cluster. It introduces partial optimality conditions for this problem, proposes algorithms to test these conditions, and evaluates their practical effectiveness on two benchmark datasets.<br /><strong>Summary (CN):</strong> 本文研究了三元（cubic）相关聚类问题，即在图中对最多包含三个节点的团分配成本，目标是找出使同聚类内部团成本总和最小的划分。作者提出了部分最优性条件，并实现了相应的判定算法，在两个数据集上进行数值实验以评估其实际效果。<br /><strong>Keywords:</strong> cubic correlation clustering, partial optimality, higher-order clustering, graph clustering, local search, NP-hard optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> David Stein, Bjoern Andres, Silvia Di Gregorio</div>
The higher-order correlation clustering problem for a graph $G$ and costs associated with cliques of $G$ consists in finding a clustering of $G$ so as to minimize the sum of the costs of those cliques whose nodes all belong to the same cluster. To tackle this NP-hard problem in practice, local search heuristics have been proposed and studied in the context of applications. Here, we establish partial optimality conditions for cubic correlation clustering, i.e., for the special case of at most 3-cliques. We define and implement algorithms for deciding these conditions and examine their effectiveness numerically, on two data sets.
<div><strong>Authors:</strong> David Stein, Bjoern Andres, Silvia Di Gregorio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies the cubic correlation clustering problem, where costs are assigned to cliques of size at most three, and seeks a clustering that minimizes the total cost of cliques fully contained within a cluster. It introduces partial optimality conditions for this problem, proposes algorithms to test these conditions, and evaluates their practical effectiveness on two benchmark datasets.", "summary_cn": "本文研究了三元（cubic）相关聚类问题，即在图中对最多包含三个节点的团分配成本，目标是找出使同聚类内部团成本总和最小的划分。作者提出了部分最优性条件，并实现了相应的判定算法，在两个数据集上进行数值实验以评估其实际效果。", "keywords": "cubic correlation clustering, partial optimality, higher-order clustering, graph clustering, local search, NP-hard optimization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["David Stein", "Bjoern Andres", "Silvia Di Gregorio"]}
]]></acme>

<pubDate>2025-10-23T11:07:29+00:00</pubDate>
</item>
<item>
<title>Learning Coupled Earth System Dynamics with GraphDOP</title>
<link>https://papers.cool/arxiv/2510.20416</link>
<guid>https://papers.cool/arxiv/2510.20416</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GraphDOP, a graph‑based machine‑learning model that learns to forecast weather directly from raw satellite and in‑situ observations by embedding diverse Earth‑system components into a shared latent space, thereby implicitly capturing cross‑domain interactions without explicit coupling. Case studies demonstrate its ability to predict events driven by coupled processes such as Arctic sea‑ice freezing, hurricane‑induced ocean cooling, and the 2022 European heat wave. The results suggest that endto‑end data‑driven Earth‑system prediction can characterize and propagate cross‑component dynamics, offering a promising path toward physically consistent forecasting.<br /><strong>Summary (CN):</strong> 本文提出 GraphDOP，一种基于图的机器学习模型，通过将原始卫星和现场观测中的海洋、大气、陆地和冰盖等多源信息嵌入共享潜在空间，隐式捕捉跨域耦合交互，实现无需显式耦合的天气预测。案例展示了该模型在北极快速海冰冻结、飓风导致的海面降温以及2022年欧洲热浪等关键耦合过程中的预测能力。结果表明，从观测直接学习的端到端 Earth System 预测能够有效表征并传播跨组件互动，为实现物理一致的天气预报提供了新路径。<br /><strong>Keywords:</strong> graph neural networks, Earth system modeling, data-driven weather prediction, coupled dynamics, satellite observations, graph-based ML<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Eulalie Boucher, Mihai Alexe, Peter Lean, Ewan Pinnington, Simon Lang, Patrick Laloyaux, Lorenzo Zampieri, Patricia de Rosnay, Niels Bormann, Anthony McNally</div>
Interactions between different components of the Earth System (e.g. ocean, atmosphere, land and cryosphere) are a crucial driver of global weather patterns. Modern Numerical Weather Prediction (NWP) systems typically run separate models of the different components, explicitly coupled across their interfaces to additionally model exchanges between the different components. Accurately representing these coupled interactions remains a major scientific and technical challenge of weather forecasting. GraphDOP is a graph-based machine learning model that learns to forecast weather directly from raw satellite and in-situ observations, without reliance on reanalysis products or traditional physics-based NWP models. GraphDOP simultaneously embeds information from diverse observation sources spanning the full Earth system into a shared latent space. This enables predictions that implicitly capture cross-domain interactions in a single model without the need for any explicit coupling. Here we present a selection of case studies which illustrate the capability of GraphDOP to forecast events where coupled processes play a particularly key role. These include rapid sea-ice freezing in the Arctic, mixing-induced ocean surface cooling during Hurricane Ian and the severe European heat wave of 2022. The results suggest that learning directly from Earth System observations can successfully characterise and propagate cross-component interactions, offering a promising path towards physically consistent end-to-end data-driven Earth System prediction with a single model.
<div><strong>Authors:</strong> Eulalie Boucher, Mihai Alexe, Peter Lean, Ewan Pinnington, Simon Lang, Patrick Laloyaux, Lorenzo Zampieri, Patricia de Rosnay, Niels Bormann, Anthony McNally</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GraphDOP, a graph‑based machine‑learning model that learns to forecast weather directly from raw satellite and in‑situ observations by embedding diverse Earth‑system components into a shared latent space, thereby implicitly capturing cross‑domain interactions without explicit coupling. Case studies demonstrate its ability to predict events driven by coupled processes such as Arctic sea‑ice freezing, hurricane‑induced ocean cooling, and the 2022 European heat wave. The results suggest that endto‑end data‑driven Earth‑system prediction can characterize and propagate cross‑component dynamics, offering a promising path toward physically consistent forecasting.", "summary_cn": "本文提出 GraphDOP，一种基于图的机器学习模型，通过将原始卫星和现场观测中的海洋、大气、陆地和冰盖等多源信息嵌入共享潜在空间，隐式捕捉跨域耦合交互，实现无需显式耦合的天气预测。案例展示了该模型在北极快速海冰冻结、飓风导致的海面降温以及2022年欧洲热浪等关键耦合过程中的预测能力。结果表明，从观测直接学习的端到端 Earth System 预测能够有效表征并传播跨组件互动，为实现物理一致的天气预报提供了新路径。", "keywords": "graph neural networks, Earth system modeling, data-driven weather prediction, coupled dynamics, satellite observations, graph-based ML", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Eulalie Boucher", "Mihai Alexe", "Peter Lean", "Ewan Pinnington", "Simon Lang", "Patrick Laloyaux", "Lorenzo Zampieri", "Patricia de Rosnay", "Niels Bormann", "Anthony McNally"]}
]]></acme>

<pubDate>2025-10-23T10:36:20+00:00</pubDate>
</item>
<item>
<title>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</title>
<link>https://papers.cool/arxiv/2510.20406</link>
<guid>https://papers.cool/arxiv/2510.20406</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> PointMapPolicy introduces a method that conditions diffusion policies on structured grids of points (point maps) without downsampling, enabling the use of standard computer vision techniques on 3D data. By fusing these point maps with RGB images via an xLSTM backbone, the approach achieves state-of-the-art performance on RoboCasa and CALVIN benchmarks and demonstrates strong real‑robot manipulation results.<br /><strong>Summary (CN):</strong> PointMapPolicy 提出一种在不进行下采样的情况下，将扩散策略条件化于结构化点网格（point map）的方法，使得标准计算机视觉技术能够直接作用于三维数据。该方法利用 xLSTM 将点网格与 RGB 图像融合，在 RoboCasa 和 CALVIN 基准上实现了最新水平的表现，并在真实机器人上展现出优异的操作能力。<br /><strong>Keywords:</strong> point cloud, structured point map, diffusion policy, multi-modal imitation learning, robotics manipulation, xLSTM, RoboCasa, CALVIN<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Xiaogang Jia, Qian Wang, Anrui Wang, Han A. Wang, Balázs Gyenes, Emiliyan Gospodinov, Xinkai Jiang, Ge Li, Hongyi Zhou, Weiran Liao, Xi Huang, Maximilian Beck, Moritz Reuss, Rudolf Lioutikov, Gerhard Neumann</div>
Robotic manipulation systems benefit from complementary sensing modalities, where each provides unique environmental information. Point clouds capture detailed geometric structure, while RGB images provide rich semantic context. Current point cloud methods struggle to capture fine-grained detail, especially for complex tasks, which RGB methods lack geometric awareness, which hinders their precision and generalization. We introduce PointMapPolicy, a novel approach that conditions diffusion policies on structured grids of points without downsampling. The resulting data type makes it easier to extract shape and spatial relationships from observations, and can be transformed between reference frames. Yet due to their structure in a regular grid, we enable the use of established computer vision techniques directly to 3D data. Using xLSTM as a backbone, our model efficiently fuses the point maps with RGB data for enhanced multi-modal perception. Through extensive experiments on the RoboCasa and CALVIN benchmarks and real robot evaluations, we demonstrate that our method achieves state-of-the-art performance across diverse manipulation tasks. The overview and demos are available on our project page: https://point-map.github.io/Point-Map/
<div><strong>Authors:</strong> Xiaogang Jia, Qian Wang, Anrui Wang, Han A. Wang, Balázs Gyenes, Emiliyan Gospodinov, Xinkai Jiang, Ge Li, Hongyi Zhou, Weiran Liao, Xi Huang, Maximilian Beck, Moritz Reuss, Rudolf Lioutikov, Gerhard Neumann</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "PointMapPolicy introduces a method that conditions diffusion policies on structured grids of points (point maps) without downsampling, enabling the use of standard computer vision techniques on 3D data. By fusing these point maps with RGB images via an xLSTM backbone, the approach achieves state-of-the-art performance on RoboCasa and CALVIN benchmarks and demonstrates strong real‑robot manipulation results.", "summary_cn": "PointMapPolicy 提出一种在不进行下采样的情况下，将扩散策略条件化于结构化点网格（point map）的方法，使得标准计算机视觉技术能够直接作用于三维数据。该方法利用 xLSTM 将点网格与 RGB 图像融合，在 RoboCasa 和 CALVIN 基准上实现了最新水平的表现，并在真实机器人上展现出优异的操作能力。", "keywords": "point cloud, structured point map, diffusion policy, multi-modal imitation learning, robotics manipulation, xLSTM, RoboCasa, CALVIN", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xiaogang Jia", "Qian Wang", "Anrui Wang", "Han A. Wang", "Balázs Gyenes", "Emiliyan Gospodinov", "Xinkai Jiang", "Ge Li", "Hongyi Zhou", "Weiran Liao", "Xi Huang", "Maximilian Beck", "Moritz Reuss", "Rudolf Lioutikov", "Gerhard Neumann"]}
]]></acme>

<pubDate>2025-10-23T10:17:01+00:00</pubDate>
</item>
<item>
<title>Testing Most Influential Sets</title>
<link>https://papers.cool/arxiv/2510.20372</link>
<guid>https://papers.cool/arxiv/2510.20372</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a rigorous statistical framework for assessing the significance of most influential data subsets on model outcomes, deriving extreme value distributions of maximal influence and proposing hypothesis tests to distinguish genuine problems from sampling variation. This replaces ad-hoc sensitivity checks with formal significance testing and is demonstrated on applications in economics, biology, and machine learning benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了一套严格的统计框架，用于评估模型输出中最具影响力的数据子集的显著性，通过推导最大影响力的极值分布并提出假设检验，以区分真实问题与抽样波动。这取代了以往的随意敏感性检查，并在经济学、生物学以及机器学习基准等多个案例中展示了其实用价值。<br /><strong>Keywords:</strong> most influential sets, data influence, statistical significance, hypothesis testing, extreme value theory, robustness, interpretability, sensitivity analysis<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Lucas Darius Konrad, Nikolas Kuschnig</div>
Small subsets of data with disproportionate influence on model outcomes can have dramatic impacts on conclusions, with a few data points sometimes overturning key findings. While recent work has developed methods to identify these \emph{most influential sets}, no formal theory exists to determine when their influence reflects genuine problems rather than natural sampling variation. We address this gap by developing a principled framework for assessing the statistical significance of most influential sets. Our theoretical results characterize the extreme value distributions of maximal influence and enable rigorous hypothesis tests for excessive influence, replacing current ad-hoc sensitivity checks. We demonstrate the practical value of our approach through applications across economics, biology, and machine learning benchmarks.
<div><strong>Authors:</strong> Lucas Darius Konrad, Nikolas Kuschnig</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a rigorous statistical framework for assessing the significance of most influential data subsets on model outcomes, deriving extreme value distributions of maximal influence and proposing hypothesis tests to distinguish genuine problems from sampling variation. This replaces ad-hoc sensitivity checks with formal significance testing and is demonstrated on applications in economics, biology, and machine learning benchmarks.", "summary_cn": "本文提出了一套严格的统计框架，用于评估模型输出中最具影响力的数据子集的显著性，通过推导最大影响力的极值分布并提出假设检验，以区分真实问题与抽样波动。这取代了以往的随意敏感性检查，并在经济学、生物学以及机器学习基准等多个案例中展示了其实用价值。", "keywords": "most influential sets, data influence, statistical significance, hypothesis testing, extreme value theory, robustness, interpretability, sensitivity analysis", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Lucas Darius Konrad", "Nikolas Kuschnig"]}
]]></acme>

<pubDate>2025-10-23T09:12:29+00:00</pubDate>
</item>
<item>
<title>A Transformer Inspired AI-based MIMO receiver</title>
<link>https://papers.cool/arxiv/2510.20363</link>
<guid>https://papers.cool/arxiv/2510.20363</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AttDet, a Transformer-inspired MIMO detection technique that treats each transmit layer as a token and learns inter‑stream interference via a lightweight self‑attention mechanism derived from the estimated channel matrix. By initializing values with matched‑filter outputs and iteratively refining them, AttDet combines model‑based interpretability with data‑driven flexibility, achieving near‑optimal BER/BLER performance on realistic 5G channel models while keeping polynomial computational complexity.<br /><strong>Summary (CN):</strong> 本文提出了 AttDet，一种受 Transformer 启发的 MIMO 检测方法，将每个发射层视为 token，并通过基于估计信道矩阵的轻量自注意力机制学习流间干扰。值初始化为匹配滤波输出并迭代更新，使得该方法兼具模型可解释性和数据驱动的灵活性，在真实 5G 信道模型和高阶混合 QAM 调制编码下实现了接近最优的 BER/BLER 性能，同时保持可预测的多项式复杂度。<br /><strong>Keywords:</strong> MIMO detection, transformer, self-attention, AttDet, channel correlation, BER, BLER, 5G, QAM<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> András Rácz, Tamás Borsos, András Veres, Benedek Csala</div>
We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple Output) detection method that treats each transmit layer as a token and learns inter-stream interference via a lightweight self-attention mechanism. Queries and keys are derived directly from the estimated channel matrix, so attention scores quantify channel correlation. Values are initialized by matched-filter outputs and iteratively refined. The AttDet design combines model-based interpretability with data-driven flexibility. We demonstrate through link-level simulations under realistic 5G channel models and high-order, mixed QAM modulation and coding schemes, that AttDet can approach near-optimal BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining predictable, polynomial complexity.
<div><strong>Authors:</strong> András Rácz, Tamás Borsos, András Veres, Benedek Csala</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AttDet, a Transformer-inspired MIMO detection technique that treats each transmit layer as a token and learns inter‑stream interference via a lightweight self‑attention mechanism derived from the estimated channel matrix. By initializing values with matched‑filter outputs and iteratively refining them, AttDet combines model‑based interpretability with data‑driven flexibility, achieving near‑optimal BER/BLER performance on realistic 5G channel models while keeping polynomial computational complexity.", "summary_cn": "本文提出了 AttDet，一种受 Transformer 启发的 MIMO 检测方法，将每个发射层视为 token，并通过基于估计信道矩阵的轻量自注意力机制学习流间干扰。值初始化为匹配滤波输出并迭代更新，使得该方法兼具模型可解释性和数据驱动的灵活性，在真实 5G 信道模型和高阶混合 QAM 调制编码下实现了接近最优的 BER/BLER 性能，同时保持可预测的多项式复杂度。", "keywords": "MIMO detection, transformer, self-attention, AttDet, channel correlation, BER, BLER, 5G, QAM", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["András Rácz", "Tamás Borsos", "András Veres", "Benedek Csala"]}
]]></acme>

<pubDate>2025-10-23T09:05:10+00:00</pubDate>
</item>
<item>
<title>ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature</title>
<link>https://papers.cool/arxiv/2510.20362</link>
<guid>https://papers.cool/arxiv/2510.20362</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ComProScanner is a multi-agent framework that autonomously extracts, validates, classifies, and visualises machine‑readable chemical compositions and properties from scientific literature, demonstrated on ceramic piezoelectric materials and their d33 coefficients. Evaluation on 100 journal articles against ten LLMs shows DeepSeek‑V3‑0324 achieving the highest overall accuracy of 0.82. The system offers a user‑friendly package for building structured datasets for downstream machine‑learning applications.<br /><strong>Summary (CN):</strong> 本文提出 ComProScanner，一个基于多智能体的框架，用于从学术文献中自动提取化学成分和性质数据，并实现验证、分类和可视化，重点针对陶瓷压电材料及其压电应变系数 d33 的数据。通过在 100 篇期刊文章上比较 10 种大型语言模型，发现 DeepSeek‑V3‑0324 达到 0.82 的整体准确率。该系统提供了一个易用的工具，帮助构建机器学习所需的结构化实验数据集。<br /><strong>Keywords:</strong> multi-agent, structured data extraction, chemical composition, property extraction, large language models, material informatics, piezoelectric, dataset generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Aritra Roy, Enrico Grisan, John Buckeridge, Chiara Gattinoni</div>
Since the advent of various pre-trained large language models, extracting structured knowledge from scientific text has experienced a revolutionary change compared with traditional machine learning or natural language processing techniques. Despite these advances, accessible automated tools that allow users to construct, validate, and visualise datasets from scientific literature extraction remain scarce. We therefore developed ComProScanner, an autonomous multi-agent platform that facilitates the extraction, validation, classification, and visualisation of machine-readable chemical compositions and properties, integrated with synthesis data from journal articles for comprehensive database creation. We evaluated our framework using 100 journal articles against 10 different LLMs, including both open-source and proprietary models, to extract highly complex compositions associated with ceramic piezoelectric materials and corresponding piezoelectric strain coefficients (d33), motivated by the lack of a large dataset for such materials. DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of 0.82. This framework provides a simple, user-friendly, readily-usable package for extracting highly complex experimental data buried in the literature to build machine learning or deep learning datasets.
<div><strong>Authors:</strong> Aritra Roy, Enrico Grisan, John Buckeridge, Chiara Gattinoni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ComProScanner is a multi-agent framework that autonomously extracts, validates, classifies, and visualises machine‑readable chemical compositions and properties from scientific literature, demonstrated on ceramic piezoelectric materials and their d33 coefficients. Evaluation on 100 journal articles against ten LLMs shows DeepSeek‑V3‑0324 achieving the highest overall accuracy of 0.82. The system offers a user‑friendly package for building structured datasets for downstream machine‑learning applications.", "summary_cn": "本文提出 ComProScanner，一个基于多智能体的框架，用于从学术文献中自动提取化学成分和性质数据，并实现验证、分类和可视化，重点针对陶瓷压电材料及其压电应变系数 d33 的数据。通过在 100 篇期刊文章上比较 10 种大型语言模型，发现 DeepSeek‑V3‑0324 达到 0.82 的整体准确率。该系统提供了一个易用的工具，帮助构建机器学习所需的结构化实验数据集。", "keywords": "multi-agent, structured data extraction, chemical composition, property extraction, large language models, material informatics, piezoelectric, dataset generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Aritra Roy", "Enrico Grisan", "John Buckeridge", "Chiara Gattinoni"]}
]]></acme>

<pubDate>2025-10-23T09:01:44+00:00</pubDate>
</item>
<item>
<title>Neural Networks for Censored Expectile Regression Based on Data Augmentation</title>
<link>https://papers.cool/arxiv/2510.20344</link>
<guid>https://papers.cool/arxiv/2510.20344</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DAERNN, a data‑augmentation based neural network algorithm for censored expectile regression that handles heterogeneous censored observations without explicit parametric assumptions. Simulation studies and real‑world applications show that DAERNN outperforms existing censored ERNN methods and matches the predictive quality of models trained on fully observed data.<br /><strong>Summary (CN):</strong> 本文提出了 DAERNN，一种基于数据增强的神经网络算法，用于在不做显式参数假设的情况下处理异质的删失期望分位回归问题。实验模拟和真实数据案例表明，DAERNN 在预测性能上优于现有删失 ERNN 方法，且可与在完整观测数据上训练的模型相媲美。<br /><strong>Keywords:</strong> expectile regression, neural networks, censored data, data augmentation, heterogeneity, nonparametric modeling, predictive performance, survival analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wei Cao, Shanshan Wang</div>
Expectile regression neural networks (ERNNs) are powerful tools for capturing heterogeneity and complex nonlinear structures in data. However, most existing research has primarily focused on fully observed data, with limited attention paid to scenarios involving censored observations. In this paper, we propose a data augmentation based ERNNs algorithm, termed DAERNN, for modeling heterogeneous censored data. The proposed DAERNN is fully data driven, requires minimal assumptions, and offers substantial flexibility. Simulation studies and real data applications demonstrate that DAERNN outperforms existing censored ERNNs methods and achieves predictive performance comparable to models trained on fully observed data. Moreover, the algorithm provides a unified framework for handling various censoring mechanisms without requiring explicit parametric model specification, thereby enhancing its applicability to practical censored data analysis.
<div><strong>Authors:</strong> Wei Cao, Shanshan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DAERNN, a data‑augmentation based neural network algorithm for censored expectile regression that handles heterogeneous censored observations without explicit parametric assumptions. Simulation studies and real‑world applications show that DAERNN outperforms existing censored ERNN methods and matches the predictive quality of models trained on fully observed data.", "summary_cn": "本文提出了 DAERNN，一种基于数据增强的神经网络算法，用于在不做显式参数假设的情况下处理异质的删失期望分位回归问题。实验模拟和真实数据案例表明，DAERNN 在预测性能上优于现有删失 ERNN 方法，且可与在完整观测数据上训练的模型相媲美。", "keywords": "expectile regression, neural networks, censored data, data augmentation, heterogeneity, nonparametric modeling, predictive performance, survival analysis", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei Cao", "Shanshan Wang"]}
]]></acme>

<pubDate>2025-10-23T08:42:23+00:00</pubDate>
</item>
<item>
<title>Multi-Task Deep Learning for Surface Metrology</title>
<link>https://papers.cool/arxiv/2510.20339</link>
<guid>https://papers.cool/arxiv/2510.20339</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a reproducible deep learning framework for surface metrology that jointly predicts texture parameters (, Rz, RONt) and their associated standard uncertainties using multi-task learning across tactile and optical measurement systems. It employs quantile and heteroscedastic heads with post-hoc conformal calibration, achieving high regression performance for most targets and a 92.85% accurate classifier for measurement system type, while noting negative transfer in naive multi-output models. The calibrated predictions are intended to aid instrument selection and acceptance decisions in metrological workflows.<br /><strong>Summary (CN):</strong> 本文提出一个可复现的深度学习框架用于表面计量，能够在触觉和光学系统的数据上联合预测表面纹理参数（Ra、Rz、RONt）及其标准不确定性。采用分位数和异方差输出头，并通过事后共形校准获得校准区间，回归性能对大多数目标均表现出高精度，测量系统类型分类器准确率为 92.85%。研究还发现直接使用多输出模型会出现负迁移，单目标模型表现更佳。这些经过校准的预测可用于指导仪器选择和计量流程中的接受决策。<br /><strong>Keywords:</strong> multi-task learning, surface metrology, uncertainty quantification, conformal calibration, regression, classification, instrument selection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</div>
A reproducible deep learning framework is presented for surface metrology to predict surface texture parameters together with their reported standard uncertainties. Using a multi-instrument dataset spanning tactile and optical systems, measurement system type classification is addressed alongside coordinated regression of Ra, Rz, RONt and their uncertainty targets (Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and heteroscedastic heads with post-hoc conformal calibration to yield calibrated intervals. On a held-out set, high fidelity was achieved by single-target regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and probability calibration was essentially unchanged after temperature scaling (ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for naive multi-output trunks, with single-target models performing better. These results provide calibrated predictions suitable to inform instrument selection and acceptance decisions in metrological workflows.
<div><strong>Authors:</strong> D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a reproducible deep learning framework for surface metrology that jointly predicts texture parameters (, Rz, RONt) and their associated standard uncertainties using multi-task learning across tactile and optical measurement systems. It employs quantile and heteroscedastic heads with post-hoc conformal calibration, achieving high regression performance for most targets and a 92.85% accurate classifier for measurement system type, while noting negative transfer in naive multi-output models. The calibrated predictions are intended to aid instrument selection and acceptance decisions in metrological workflows.", "summary_cn": "本文提出一个可复现的深度学习框架用于表面计量，能够在触觉和光学系统的数据上联合预测表面纹理参数（Ra、Rz、RONt）及其标准不确定性。采用分位数和异方差输出头，并通过事后共形校准获得校准区间，回归性能对大多数目标均表现出高精度，测量系统类型分类器准确率为 92.85%。研究还发现直接使用多输出模型会出现负迁移，单目标模型表现更佳。这些经过校准的预测可用于指导仪器选择和计量流程中的接受决策。", "keywords": "multi-task learning, surface metrology, uncertainty quantification, conformal calibration, regression, classification, instrument selection", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["D. Kucharski", "A. Gaska", "T. Kowaluk", "K. Stepien", "M. Repalska", "B. Gapinski", "M. Wieczorowski", "M. Nawotka", "P. Sobecki", "P. Sosinowski", "J. Tomasik", "A. Wojtowicz"]}
]]></acme>

<pubDate>2025-10-23T08:38:18+00:00</pubDate>
</item>
<item>
<title>Capability of using the normalizing flows for extraction rare gamma events in the TAIGA experiment</title>
<link>https://papers.cool/arxiv/2510.20334</link>
<guid>https://papers.cool/arxiv/2510.20334</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a deep‑learning method based on normalizing flows for anomaly detection to identify rare gamma‑ray events among charged‑particle background in the TAIGA‑IACT experiment. Experiments on simulated data show the approach has potential, though its performance currently lags behind existing techniques, and the authors suggest avenues for improvement.<br /><strong>Summary (CN):</strong> 本文提出一种基于归一化流（normalizing flows）的深度学习异常检测方法，用于在 TAIGA‑IACT 实验中从带电粒子背景中识别稀有伽马射线事件。对模拟数据的实验表明该方法具有潜力，但其性能仍低于其他已有方法，作者因此提出了若干改进方向。<br /><strong>Keywords:</strong> normalizing flows, anomaly detection, gamma-ray detection, TAIGA-IACT, deep learning, rare event extraction<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robness<br /><strong>Authors:</strong> A. P. Kryukov, A. Yu. Razumov, A. P. Demichev, J. J. Dubenskaya, E. O. Gres, S. P. Polyakov, E. B. Postnikov, P. A. Volchugov, D. P. Zhurov</div>
The objective of this work is to develop a method for detecting rare gamma quanta against the background of charged particles in the fluxes from sources in the Universe with the help of the deep learning and normalizing flows based method designed for anomaly detection. It is shown that the suggested method has a potential for the gamma detection. The method was tested on model data from the TAIGA-IACT experiment. The obtained quantitative performance indicators are still inferior to other approaches, and therefore possible ways to improve the implementation of the method are proposed.
<div><strong>Authors:</strong> A. P. Kryukov, A. Yu. Razumov, A. P. Demichev, J. J. Dubenskaya, E. O. Gres, S. P. Polyakov, E. B. Postnikov, P. A. Volchugov, D. P. Zhurov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a deep‑learning method based on normalizing flows for anomaly detection to identify rare gamma‑ray events among charged‑particle background in the TAIGA‑IACT experiment. Experiments on simulated data show the approach has potential, though its performance currently lags behind existing techniques, and the authors suggest avenues for improvement.", "summary_cn": "本文提出一种基于归一化流（normalizing flows）的深度学习异常检测方法，用于在 TAIGA‑IACT 实验中从带电粒子背景中识别稀有伽马射线事件。对模拟数据的实验表明该方法具有潜力，但其性能仍低于其他已有方法，作者因此提出了若干改进方向。", "keywords": "normalizing flows, anomaly detection, gamma-ray detection, TAIGA-IACT, deep learning, rare event extraction", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robness"}, "authors": ["A. P. Kryukov", "A. Yu. Razumov", "A. P. Demichev", "J. J. Dubenskaya", "E. O. Gres", "S. P. Polyakov", "E. B. Postnikov", "P. A. Volchugov", "D. P. Zhurov"]}
]]></acme>

<pubDate>2025-10-23T08:33:36+00:00</pubDate>
</item>
<item>
<title>MemER: Scaling Up Memory for Robot Control via Experience Retrieval</title>
<link>https://papers.cool/arxiv/2510.20328</link>
<guid>https://papers.cool/arxiv/2510.20328</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MemER introduces a hierarchical policy that enables robots to retrieve and use relevant past experiences as keyframes, allowing efficient reasoning over long-horizon dependencies. The high-level policy selects keyframes and combines them with recent observations to generate text instructions for a low-level policy, and the method scales memory to minutes using fine-tuned vision-language-action models. Experiments on three real-world manipulation tasks show that MemER outperforms prior approaches.<br /><strong>Summary (CN):</strong> MemER 提出一种层次化策略框架，使机器人能够检索并利用先前的关键帧经验，从而高效推理长期依赖。高层策略选取关键帧并与最新观测结合，生成用于低层策略执行的文本指令，在大规模视觉-语言-动作模型上进行微调，实现分钟级记忆。实验在三个真实的长时间操作任务上显示，MemER 超越了已有方法。<br /><strong>Keywords:</strong> memory retrieval, hierarchical policy, robot manipulation, long-horizon control, vision-language-action, experience keyframes, large language model, Qwen2.5-VL, policy scaling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Ajay Sridhar, Jennifer Pan, Satvik Sharma, Chelsea Finn</div>
Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $\pi_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.
<div><strong>Authors:</strong> Ajay Sridhar, Jennifer Pan, Satvik Sharma, Chelsea Finn</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MemER introduces a hierarchical policy that enables robots to retrieve and use relevant past experiences as keyframes, allowing efficient reasoning over long-horizon dependencies. The high-level policy selects keyframes and combines them with recent observations to generate text instructions for a low-level policy, and the method scales memory to minutes using fine-tuned vision-language-action models. Experiments on three real-world manipulation tasks show that MemER outperforms prior approaches.", "summary_cn": "MemER 提出一种层次化策略框架，使机器人能够检索并利用先前的关键帧经验，从而高效推理长期依赖。高层策略选取关键帧并与最新观测结合，生成用于低层策略执行的文本指令，在大规模视觉-语言-动作模型上进行微调，实现分钟级记忆。实验在三个真实的长时间操作任务上显示，MemER 超越了已有方法。", "keywords": "memory retrieval, hierarchical policy, robot manipulation, long-horizon control, vision-language-action, experience keyframes, large language model, Qwen2.5-VL, policy scaling", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Ajay Sridhar", "Jennifer Pan", "Satvik Sharma", "Chelsea Finn"]}
]]></acme>

<pubDate>2025-10-23T08:26:17+00:00</pubDate>
</item>
<item>
<title>Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses</title>
<link>https://papers.cool/arxiv/2510.20314</link>
<guid>https://papers.cool/arxiv/2510.20314</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This survey reviews adversarial attacks and defenses for deep reinforcement learning (DRL), outlining a classification framework based on perturbation type and attack target, and describing attacks on state, action, reward, and model spaces. It systematically summarizes robustness training strategies such as adversarial training, competitive training, detection, and defense distillation, discussing their advantages and limitations. The paper also highlights future research directions toward better generalization, lower computational cost, scalability, and explainability of DRL in adversarial settings.<br /><strong>Summary (CN):</strong> 本文综述了深度强化学习（DRL）在对抗攻击下的安全与鲁棒性研究，提出基于扰动类型和攻击目标的攻击分类框架，并详细介绍了对状态空间、动作空间、奖励函数和模型空间的攻击方法。系统总结了包括对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等在内的鲁棒性训练策略，分析了各方法的优缺点。最后，文章展望了提升 DRL 在对抗环境中的泛化能力、降低计算复杂度、增强可扩展性和可解释性（explainability）的未来研究方向。<br /><strong>Keywords:</strong> deep reinforcement learning, adversarial attacks, robustness, defense, adversarial training, detection, explainability, safety<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 5, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Wu Yichao, Wang Yirui, Ding Panpan, Wang Hailong, Zhu Bingqian, Liu Chun</div>
With the wide application of deep reinforcement learning (DRL) techniques in complex fields such as autonomous driving, intelligent manufacturing, and smart healthcare, how to improve its security and robustness in dynamic and changeable environments has become a core issue in current research. Especially in the face of adversarial attacks, DRL may suffer serious performance degradation or even make potentially dangerous decisions, so it is crucial to ensure their stability in security-sensitive scenarios. In this paper, we first introduce the basic framework of DRL and analyze the main security challenges faced in complex and changing environments. In addition, this paper proposes an adversarial attack classification framework based on perturbation type and attack target and reviews the mainstream adversarial attack methods against DRL in detail, including various attack methods such as perturbation state space, action space, reward function and model space. To effectively counter the attacks, this paper systematically summarizes various current robustness training strategies, including adversarial training, competitive training, robust learning, adversarial detection, defense distillation and other related defense techniques, we also discuss the advantages and shortcomings of these methods in improving the robustness of DRL. Finally, this paper looks into the future research direction of DRL in adversarial environments, emphasizing the research needs in terms of improving generalization, reducing computational complexity, and enhancing scalability and explainability, aiming to provide valuable references and directions for researchers.
<div><strong>Authors:</strong> Wu Yichao, Wang Yirui, Ding Panpan, Wang Hailong, Zhu Bingqian, Liu Chun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This survey reviews adversarial attacks and defenses for deep reinforcement learning (DRL), outlining a classification framework based on perturbation type and attack target, and describing attacks on state, action, reward, and model spaces. It systematically summarizes robustness training strategies such as adversarial training, competitive training, detection, and defense distillation, discussing their advantages and limitations. The paper also highlights future research directions toward better generalization, lower computational cost, scalability, and explainability of DRL in adversarial settings.", "summary_cn": "本文综述了深度强化学习（DRL）在对抗攻击下的安全与鲁棒性研究，提出基于扰动类型和攻击目标的攻击分类框架，并详细介绍了对状态空间、动作空间、奖励函数和模型空间的攻击方法。系统总结了包括对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等在内的鲁棒性训练策略，分析了各方法的优缺点。最后，文章展望了提升 DRL 在对抗环境中的泛化能力、降低计算复杂度、增强可扩展性和可解释性（explainability）的未来研究方向。", "keywords": "deep reinforcement learning, adversarial attacks, robustness, defense, adversarial training, detection, explainability, safety", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Wu Yichao", "Wang Yirui", "Ding Panpan", "Wang Hailong", "Zhu Bingqian", "Liu Chun"]}
]]></acme>

<pubDate>2025-10-23T08:04:57+00:00</pubDate>
</item>
<item>
<title>Breakdance Video classification in the age of Generative AI</title>
<link>https://papers.cool/arxiv/2510.20287</link>
<guid>https://papers.cool/arxiv/2510.20287</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates modern video foundation models for classifying breakdance videos, comparing video encoders and decoder-based video-language models. Results indicate that encoder-only models outperform state-of-the-art video-language models on this niche dance sport task, and detailed analysis is provided on selecting encoders and fine-tuning decoders.<br /><strong>Summary (CN):</strong> 本文评估了现代视频基础模型在街舞（breakdance）视频分类任务中的表现，比较了视频编码器和基于解码器的视频语言模型。结果表明，纯编码器模型在此细分舞蹈体育任务上优于最先进的视频语言模型，并提供了关于如何选择编码器以及对解码器进行微调的深入分析。<br /><strong>Keywords:</strong> breakdance, video classification, vision-language models, video encoder, video decoder, generative AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.
<div><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates modern video foundation models for classifying breakdance videos, comparing video encoders and decoder-based video-language models. Results indicate that encoder-only models outperform state-of-the-art video-language models on this niche dance sport task, and detailed analysis is provided on selecting encoders and fine-tuning decoders.", "summary_cn": "本文评估了现代视频基础模型在街舞（breakdance）视频分类任务中的表现，比较了视频编码器和基于解码器的视频语言模型。结果表明，纯编码器模型在此细分舞蹈体育任务上优于最先进的视频语言模型，并提供了关于如何选择编码器以及对解码器进行微调的深入分析。", "keywords": "breakdance, video classification, vision-language models, video encoder, video decoder, generative AI", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Michelle Munson"]}
]]></acme>

<pubDate>2025-10-23T07:18:54+00:00</pubDate>
</item>
<item>
<title>Calibrating Multimodal Consensus for Emotion Recognition</title>
<link>https://papers.cool/arxiv/2510.20256</link>
<guid>https://papers.cool/arxiv/2510.20256</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Calibrated Multimodal Consensus (CMC), a framework that first generates pseudo unimodal labels for self‑supervised pretraining and then employs a parameter‑free fusion module and a multimodal consensus router to reduce text dominance and resolve semantic inconsistencies across modalities in emotion recognition. Experiments on four benchmark datasets show that CMC matches or exceeds state‑of‑the‑art performance, particularly when modalities provide conflicting emotional cues.<br /><strong>Summary (CN):</strong> 本文提出了校准多模态共识（CMC）框架，先通过伪标签生成模块进行单模态自监督预训练，再利用无参融合模块和多模态共识路由器减轻文本主导现象并缓解跨模态语义不一致问题，以实现情感识别。在四个基准数据集上的实验表明，CMC 在总体性能上与最先进方法持平或更佳，尤其在模态之间情感线索冲突的场景中表现突出。<br /><strong>Keywords:</strong> multimodal emotion recognition, consensus fusion, pseudo label generation, modality dominance, parameter-free fusion, multimodal consensus router<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CMC.
<div><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Calibrated Multimodal Consensus (CMC), a framework that first generates pseudo unimodal labels for self‑supervised pretraining and then employs a parameter‑free fusion module and a multimodal consensus router to reduce text dominance and resolve semantic inconsistencies across modalities in emotion recognition. Experiments on four benchmark datasets show that CMC matches or exceeds state‑of‑the‑art performance, particularly when modalities provide conflicting emotional cues.", "summary_cn": "本文提出了校准多模态共识（CMC）框架，先通过伪标签生成模块进行单模态自监督预训练，再利用无参融合模块和多模态共识路由器减轻文本主导现象并缓解跨模态语义不一致问题，以实现情感识别。在四个基准数据集上的实验表明，CMC 在总体性能上与最先进方法持平或更佳，尤其在模态之间情感线索冲突的场景中表现突出。", "keywords": "multimodal emotion recognition, consensus fusion, pseudo label generation, modality dominance, parameter-free fusion, multimodal consensus router", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"]}
]]></acme>

<pubDate>2025-10-23T06:25:10+00:00</pubDate>
</item>
<item>
<title>Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding</title>
<link>https://papers.cool/arxiv/2510.20244</link>
<guid>https://papers.cool/arxiv/2510.20244</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies that current video temporal grounding models treat all text tokens uniformly, causing an over-reliance on the [EOS] token and limiting fine-grained alignment. To address this, DualGround introduces a dual-branch architecture that separates sentence-level semantics (via the [EOS] token) from phrase-level semantics obtained by clustering word tokens, employing token-role-aware cross-modal interactions. Experiments on QVHighlights and Charades-STA show that this disentangled approach achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection.<br /><strong>Summary (CN):</strong> 本文指出现有视频时间定位模型在跨模态注意力中对所有文本标记一视同仁，导致过度依赖 [EOS] 标记而削弱细粒度对齐能力。为此，DualGround 采用双分支结构，将子层级语义（通过 [EOS] 标记）与通过词标记聚类获得的短语层级语义分离，并使用基于标记角色的跨模态交互策略。实验在 QVHighlights 与 Charades-STA 数据集上展示，该方法在 Moment Retrieval 与 Highlight Detection 两项任务上均达到了最新水平。<br /><strong>Keywords:</strong> video temporal grounding, moment retrieval, highlight detection, dual-branch architecture, token-role aware interaction, phrase-level semantics, sentence-level semantics, cross-modal attention, structured phrase, state-of-the-art<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee</div>
Video Temporal Grounding (VTG) aims to localize temporal segments in long, untrimmed videos that align with a given natural language query. This task typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). While recent advances have been progressed by powerful pretrained vision-language models such as CLIP and InternVideo2, existing approaches commonly treat all text tokens uniformly during crossmodal attention, disregarding their distinct semantic roles. To validate the limitations of this approach, we conduct controlled experiments demonstrating that VTG models overly rely on [EOS]-driven global semantics while failing to effectively utilize word-level signals, which limits their ability to achieve fine-grained temporal alignment. Motivated by this limitation, we propose DualGround, a dual-branch architecture that explicitly separates global and local semantics by routing the [EOS] token through a sentence-level path and clustering word tokens into phrase-level units for localized grounding. Our method introduces (1) tokenrole- aware cross modal interaction strategies that align video features with sentence-level and phrase-level semantics in a structurally disentangled manner, and (2) a joint modeling framework that not only improves global sentence-level alignment but also enhances finegrained temporal grounding by leveraging structured phrase-aware context. This design allows the model to capture both coarse and localized semantics, enabling more expressive and context-aware video grounding. DualGround achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection tasks across QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of disentangled semantic modeling in video-language alignment.
<div><strong>Authors:</strong> Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies that current video temporal grounding models treat all text tokens uniformly, causing an over-reliance on the [EOS] token and limiting fine-grained alignment. To address this, DualGround introduces a dual-branch architecture that separates sentence-level semantics (via the [EOS] token) from phrase-level semantics obtained by clustering word tokens, employing token-role-aware cross-modal interactions. Experiments on QVHighlights and Charades-STA show that this disentangled approach achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection.", "summary_cn": "本文指出现有视频时间定位模型在跨模态注意力中对所有文本标记一视同仁，导致过度依赖 [EOS] 标记而削弱细粒度对齐能力。为此，DualGround 采用双分支结构，将子层级语义（通过 [EOS] 标记）与通过词标记聚类获得的短语层级语义分离，并使用基于标记角色的跨模态交互策略。实验在 QVHighlights 与 Charades-STA 数据集上展示，该方法在 Moment Retrieval 与 Highlight Detection 两项任务上均达到了最新水平。", "keywords": "video temporal grounding, moment retrieval, highlight detection, dual-branch architecture, token-role aware interaction, phrase-level semantics, sentence-level semantics, cross-modal attention, structured phrase, state-of-the-art", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Minseok Kang", "Minhyeok Lee", "Minjung Kim", "Donghyeong Kim", "Sangyoun Lee"]}
]]></acme>

<pubDate>2025-10-23T05:53:01+00:00</pubDate>
</item>
<item>
<title>Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents</title>
<link>https://papers.cool/arxiv/2510.20211</link>
<guid>https://papers.cool/arxiv/2510.20211</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NSync, an automated system that uses LLM-based agents to detect and reconcile infrastructure drift by analyzing cloud API traces and updating IaC configurations such as Terraform scripts. It presents a novel evaluation pipeline with realistic drift injections and demonstrates significant improvements in accuracy and token efficiency over baselines across multiple real-world projects.<br /><strong>Summary (CN):</strong> 本文提出 NSync 系统，利用基于大型语言模型的代理从云 API 调用痕迹中检测基础设施漂移，并自动更新 IaC（如 Terraform）配置以实现同步。作者设计了注入真实漂移的评估流程，在多个实际项目中显示出在准确率和令牌效率上相较基线的显著提升。<br /><strong>Keywords:</strong> infrastructure-as-code, drift detection, LLM agents, cloud API tracing, Terraform, reconciliation, self-evolving knowledge base, AI-powered infrastructure management<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Zhenning Yang, Hui Guan, Victor Nicolet, Brandon Paulsen, Joey Dodds, Daniel Kroening, Ang Chen</div>
Cloud infrastructure is managed through a mix of interfaces -- traditionally, cloud consoles, command-line interfaces (CLI), and SDKs are the tools of choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the infrastructure in a "source-of-truth" configuration. They are capable of automatically carrying out modifications to the cloud -- deploying, updating, or destroying resources -- to bring the actual infrastructure into alignment with the IaC configuration. However, when IaC is used alongside consoles, CLIs, or SDKs, it loses visibility into external changes, causing infrastructure drift, where the configuration becomes outdated, and later IaC operations may undo valid updates or trigger errors. We present NSync, an automated system for IaC reconciliation that propagates out-of-band changes back into the IaC program. Our key insight is that infrastructure changes eventually all occur via cloud API invocations -- the lowest layer for cloud management operations. NSync gleans insights from API traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update the IaC configuration to capture the changes). It employs an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. We further introduce a novel evaluation pipeline for injecting realistic drifts into cloud infrastructure and assessing reconciliation performance. Experiments across five real-world Terraform projects and 372 drift scenarios show that NSync outperforms the baseline both in terms of accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$ improvement).
<div><strong>Authors:</strong> Zhenning Yang, Hui Guan, Victor Nicolet, Brandon Paulsen, Joey Dodds, Daniel Kroening, Ang Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NSync, an automated system that uses LLM-based agents to detect and reconcile infrastructure drift by analyzing cloud API traces and updating IaC configurations such as Terraform scripts. It presents a novel evaluation pipeline with realistic drift injections and demonstrates significant improvements in accuracy and token efficiency over baselines across multiple real-world projects.", "summary_cn": "本文提出 NSync 系统，利用基于大型语言模型的代理从云 API 调用痕迹中检测基础设施漂移，并自动更新 IaC（如 Terraform）配置以实现同步。作者设计了注入真实漂移的评估流程，在多个实际项目中显示出在准确率和令牌效率上相较基线的显著提升。", "keywords": "infrastructure-as-code, drift detection, LLM agents, cloud API tracing, Terraform, reconciliation, self-evolving knowledge base, AI-powered infrastructure management", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"]}
]]></acme>

<pubDate>2025-10-23T04:57:00+00:00</pubDate>
</item>
<item>
<title>Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</title>
<link>https://papers.cool/arxiv/2510.20193</link>
<guid>https://papers.cool/arxiv/2510.20193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This survey reviews recent advances in question answering systems that incorporate multimedia retrieval and cross‑modal reasoning, covering architectures that align vision, language, and audio modalities with user queries. The authors categorize approaches by retrieval methods, fusion techniques, and answer generation strategies, and discuss benchmarks, evaluation protocols, latency‑accuracy trade‑offs, and open challenges such as semantic grounding. The paper highlights future research directions for building more robust, context‑aware QA systems that can effectively leverage diverse multimedia data.<br /><strong>Summary (CN):</strong> 本文综述了将多媒体检索与跨模态推理相结合的问答系统的最新进展，涵盖了将视觉、语言和音频模态与用户查询对齐的架构。作者按检索方法、融合技术和答案生成策略进行分类，并讨论了基准数据集、评估协议、延迟‑精度权衡以及语义定位等关键挑战。文章指出了未来研究方向，旨在构建能够有效利用多种多媒体数据的更鲁棒、上下文感知的问答系统。<br /><strong>Keywords:</strong> multimedia QA, cross-modal retrieval, multimodal fusion, vision-language, audio-visual reasoning, retrieval-augmented generation, benchmark datasets, semantic grounding, multimodal reasoning, QA systems<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 5, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.
<div><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This survey reviews recent advances in question answering systems that incorporate multimedia retrieval and cross‑modal reasoning, covering architectures that align vision, language, and audio modalities with user queries. The authors categorize approaches by retrieval methods, fusion techniques, and answer generation strategies, and discuss benchmarks, evaluation protocols, latency‑accuracy trade‑offs, and open challenges such as semantic grounding. The paper highlights future research directions for building more robust, context‑aware QA systems that can effectively leverage diverse multimedia data.", "summary_cn": "本文综述了将多媒体检索与跨模态推理相结合的问答系统的最新进展，涵盖了将视觉、语言和音频模态与用户查询对齐的架构。作者按检索方法、融合技术和答案生成策略进行分类，并讨论了基准数据集、评估协议、延迟‑精度权衡以及语义定位等关键挑战。文章指出了未来研究方向，旨在构建能够有效利用多种多媒体数据的更鲁棒、上下文感知的问答系统。", "keywords": "multimedia QA, cross-modal retrieval, multimodal fusion, vision-language, audio-visual reasoning, retrieval-augmented generation, benchmark datasets, semantic grounding, multimodal reasoning, QA systems", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rahul Raja", "Arpita Vats"]}
]]></acme>

<pubDate>2025-10-23T04:25:44+00:00</pubDate>
</item>
<item>
<title>Compositional Generation for Long-Horizon Coupled PDEs</title>
<link>https://papers.cool/arxiv/2510.20141</link>
<guid>https://papers.cool/arxiv/2510.20141</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a compositional diffusion strategy for simulating long-horizon coupled partial differential equations (PDEs) by training diffusion models solely on decoupled data and composing them at inference time. Experiments on reaction‑diffusion and modified Burgers systems show that, despite only seeing decoupled training data, the compositional models achieve low error over many time steps, with v‑parameterization improving accuracy, though a Fourier Neural Operator trained on coupled data remains the strongest baseline.<br /><strong>Summary (CN):</strong> 本文提出了一种组合扩散方法，用于长时间跨度的耦合偏微分方程（PDE）模拟，即仅在解耦数据上训练扩散模型并在推理时进行组合。实验在反应扩散和改进的 Burgers 系统上表明，尽管只使用了解耦训练数据，组合模型仍能在大量时间步上恢复耦合轨迹且误差较低，v 参数化提升了精度，但在耦合数据上训练的 Fourier 神经算子仍表现最优。<br /><strong>Keywords:</strong> compositional diffusion, coupled PDEs, long-horizon modeling, v-parameterization, Euler scheme, Fourier Neural Operator, surrogate modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Somayajulu L. N. Dhulipala, Deep Ray, Nicholas Forman</div>
Simulating coupled PDE systems is computationally intensive, and prior efforts have largely focused on training surrogates on the joint (coupled) data, which requires a large amount of data. In the paper, we study compositional diffusion approaches where diffusion models are only trained on the decoupled PDE data and are composed at inference time to recover the coupled field. Specifically, we investigate whether the compositional strategy can be feasible under long time horizons involving a large number of time steps. In addition, we compare a baseline diffusion model with that trained using the v-parameterization strategy. We also introduce a symmetric compositional scheme for the coupled fields based on the Euler scheme. We evaluate on Reaction-Diffusion and modified Burgers with longer time grids, and benchmark against a Fourier Neural Operator trained on coupled data. Despite seeing only decoupled training data, the compositional diffusion models recover coupled trajectories with low error. v-parameterization can improve accuracy over a baseline diffusion model, while the neural operator surrogate remains strongest given that it is trained on the coupled data. These results show that compositional diffusion is a viable strategy towards efficient, long-horizon modeling of coupled PDEs.
<div><strong>Authors:</strong> Somayajulu L. N. Dhulipala, Deep Ray, Nicholas Forman</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a compositional diffusion strategy for simulating long-horizon coupled partial differential equations (PDEs) by training diffusion models solely on decoupled data and composing them at inference time. Experiments on reaction‑diffusion and modified Burgers systems show that, despite only seeing decoupled training data, the compositional models achieve low error over many time steps, with v‑parameterization improving accuracy, though a Fourier Neural Operator trained on coupled data remains the strongest baseline.", "summary_cn": "本文提出了一种组合扩散方法，用于长时间跨度的耦合偏微分方程（PDE）模拟，即仅在解耦数据上训练扩散模型并在推理时进行组合。实验在反应扩散和改进的 Burgers 系统上表明，尽管只使用了解耦训练数据，组合模型仍能在大量时间步上恢复耦合轨迹且误差较低，v 参数化提升了精度，但在耦合数据上训练的 Fourier 神经算子仍表现最优。", "keywords": "compositional diffusion, coupled PDEs, long-horizon modeling, v-parameterization, Euler scheme, Fourier Neural Operator, surrogate modeling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Somayajulu L. N. Dhulipala", "Deep Ray", "Nicholas Forman"]}
]]></acme>

<pubDate>2025-10-23T02:35:25+00:00</pubDate>
</item>
<item>
<title>AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training</title>
<link>https://papers.cool/arxiv/2510.20111</link>
<guid>https://papers.cool/arxiv/2510.20111</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AsyncHZP, an asynchronous hierarchical variant of the Zero Redundancy Optimizer (ZeRO) that reshards parameters, gradients, and optimizer states across replica groups to reduce communication overhead and improve memory utilization. It also proposes a multi‑stream asynchronous scheduling system that overlaps all‑gather and reduce‑scatter operations with computation using background threads. Experiments on dense and Mixture‑of‑Experts language models show that AsyncHZP achieves state‑of‑the‑art training speed and scalability compared to traditional tensor‑parallel approaches while maintaining stability.<br /><strong>Summary (CN):</strong> 本文提出 AsyncHZP，这是一种异步层次化的 ZeRO（Zero Redundancy Optimizer）变体，通过在不同副本组之间重新分片参数、梯度和优化器状态来降低通信开销并提升内存利用率。论文还设计了多流异步调度机制，在后台线程中将参数 all‑gather 与梯度 reduce‑scatter 与计算并行执行，实现通信与计算的重叠。针对密集模型和混合专家（Mixture‑of‑Experts性的前提下，显著提升训练速度并超越传统张量并行（ND）方法的可扩展性。<br /><strong>Keywords:</strong> hierarchical ZeRO, asynchronous scheduling, large language model training, model parallelism, Mixture-of-Experts, communication optimization, zero redundancy optimizer<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu</div>
The training efficiency and scalability of language models on massive clusters currently remain a critical bottleneck. Mainstream approaches like ND parallelism are often cumbersome and complex, while flexible alternatives such as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by communication overhead. In this paper, we propose Asynchronous Hierarchical Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to achieve superior performance while maintaining simplicity and memory efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding that can lead to inefficient communication, AsyncHZP adaptively reshards parameters, gradients, and optimizer states across different replica groups. This strategy optimizes device memory utilization and significantly reduces communication overhead. In addition, we also design a multi-stream asynchronous scheduling method that executes parameter all-gather and gradient reduce-scatter operations in dedicated background threads, effectively overlapping communication with computation while incurring negligible memory fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE) models confirm that AsyncHZP maintains robust stability at scale. It consistently outperforms classic ND parallelism, achieving state-of-the-art performance without complex strategic tuning, thereby simplifying the path to efficient large-scale training.
<div><strong>Authors:</strong> Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AsyncHZP, an asynchronous hierarchical variant of the Zero Redundancy Optimizer (ZeRO) that reshards parameters, gradients, and optimizer states across replica groups to reduce communication overhead and improve memory utilization. It also proposes a multi‑stream asynchronous scheduling system that overlaps all‑gather and reduce‑scatter operations with computation using background threads. Experiments on dense and Mixture‑of‑Experts language models show that AsyncHZP achieves state‑of‑the‑art training speed and scalability compared to traditional tensor‑parallel approaches while maintaining stability.", "summary_cn": "本文提出 AsyncHZP，这是一种异步层次化的 ZeRO（Zero Redundancy Optimizer）变体，通过在不同副本组之间重新分片参数、梯度和优化器状态来降低通信开销并提升内存利用率。论文还设计了多流异步调度机制，在后台线程中将参数 all‑gather 与梯度 reduce‑scatter 与计算并行执行，实现通信与计算的重叠。针对密集模型和混合专家（Mixture‑of‑Experts性的前提下，显著提升训练速度并超越传统张量并行（ND）方法的可扩展性。", "keywords": "hierarchical ZeRO, asynchronous scheduling, large language model training, model parallelism, Mixture-of-Experts, communication optimization, zero redundancy optimizer", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Huawei Bai", "Yifan Huang", "Wenqi Shi", "Ansheng You", "Feifan Shao", "Tengfei Han", "Minghui Yu"]}
]]></acme>

<pubDate>2025-10-23T01:29:35+00:00</pubDate>
</item>
<item>
<title>Extending machine learning model for implicit solvation to free energy calculations</title>
<link>https://papers.cool/arxiv/2510.20103</link>
<guid>https://papers.cool/arxiv/2510.20103</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Lambda Solvation Neural Network (LSNN), a graph neural network–based implicit solvent model that is trained not only on force matching but also on the derivatives of alchemical variables, enabling accurate absolute free energy predictions across chemical species. Trained on ~300,000 small molecules, LSNN achieves explicit‑solvent‑level accuracy with substantial computational speedup, offering a framework for future drug‑discovery applications.<br /><strong>Summary (CN):</strong> 本文提出了 Lambda Solvation Neural Network（LSNN），一种基于图神经网络的隐式溶剂模型，除了力匹配外，还通过匹配铝化变量的导数进行训练，从而实现不同化学物种之间的绝对自由能可比预测。该模型在约30万小分子数据上训练，达到了显式溶剂模拟水平的精度并显著加快计算，为药物发现等应用提供了基础框架。<br /><strong>Keywords:</strong> graph neural network, implicit solvation, free energy calculation, force matching, alchemical derivatives, LSNN, molecular simulation, machine learning potentials<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rishabh Dey, Michael Brocidiacono, Kushal Koirala, Alexander Tropsha, Konstantin I. Popov</div>
The implicit solvent approach offers a computationally efficient framework to model solvation effects in molecular simulations. However, its accuracy often falls short compared to explicit solvent models, limiting its use in precise thermodynamic calculations. Recent advancements in machine learning (ML) present an opportunity to overcome these limitations by leveraging neural networks to develop more precise implicit solvent potentials for diverse applications. A major drawback of current ML-based methods is their reliance on force-matching alone, which can lead to energy predictions that differ by an arbitrary constant and are therefore unsuitable for absolute free energy comparisons. Here, we introduce a novel methodology with a graph neural network (GNN)-based implicit solvent model, dubbed Lambda Solvation Neural Network (LSNN). In addition to force-matching, this network was trained to match the derivatives of alchemical variables, ensuring that solvation free energies can be meaningfully compared across chemical species.. Trained on a dataset of approximately 300,000 small molecules, LSNN achieves free energy predictions with accuracy comparable to explicit-solvent alchemical simulations, while offering a computational speedup and establishing a foundational framework for future applications in drug discovery.
<div><strong>Authors:</strong> Rishabh Dey, Michael Brocidiacono, Kushal Koirala, Alexander Tropsha, Konstantin I. Popov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Lambda Solvation Neural Network (LSNN), a graph neural network–based implicit solvent model that is trained not only on force matching but also on the derivatives of alchemical variables, enabling accurate absolute free energy predictions across chemical species. Trained on ~300,000 small molecules, LSNN achieves explicit‑solvent‑level accuracy with substantial computational speedup, offering a framework for future drug‑discovery applications.", "summary_cn": "本文提出了 Lambda Solvation Neural Network（LSNN），一种基于图神经网络的隐式溶剂模型，除了力匹配外，还通过匹配铝化变量的导数进行训练，从而实现不同化学物种之间的绝对自由能可比预测。该模型在约30万小分子数据上训练，达到了显式溶剂模拟水平的精度并显著加快计算，为药物发现等应用提供了基础框架。", "keywords": "graph neural network, implicit solvation, free energy calculation, force matching, alchemical derivatives, LSNN, molecular simulation, machine learning potentials", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rishabh Dey", "Michael Brocidiacono", "Kushal Koirala", "Alexander Tropsha", "Konstantin I. Popov"]}
]]></acme>

<pubDate>2025-10-23T01:05:44+00:00</pubDate>
</item>
<item>
<title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20095</link>
<guid>https://papers.cool/arxiv/2510.20095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BIOCAP, a biological foundation model that incorporates synthetic, instance-specific captions generated by multimodal large language models to supplement traditional label supervision. By aligning images and captions within a shared latent morphospace, the approach improves species classification and text-image retrieval performance while mitigating spurious correlations. The study demonstrates that descriptive captions provide valuable semantic information beyond mere labels in biological multimodal learning.<br /><strong>Summary (CN):</strong> 本文提出 BIOCAP，一种利用多模态大型语言模型生成的合成实例级描述性字幕，补充传统标签监督的生物学基础模型。通过在共享潜在形态空间内对齐图像和字幕，该方法提升了物种分类和文本-图像检索的性能，同时抑制了虚假关联。研究表明，描述性字幕在生物多模态学习中提供了超越标签的丰富语义信息。<br /><strong>Keywords:</strong> synthetic captions, multimodal large language models, BIOCAP, biological foundation models, species classification, image-text retrieval, caption generation, dataset augmentation, multimodal supervision<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BIOCAP (i.e., BIOCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models.
<div><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BIOCAP, a biological foundation model that incorporates synthetic, instance-specific captions generated by multimodal large language models to supplement traditional label supervision. By aligning images and captions within a shared latent morphospace, the approach improves species classification and text-image retrieval performance while mitigating spurious correlations. The study demonstrates that descriptive captions provide valuable semantic information beyond mere labels in biological multimodal learning.", "summary_cn": "本文提出 BIOCAP，一种利用多模态大型语言模型生成的合成实例级描述性字幕，补充传统标签监督的生物学基础模型。通过在共享潜在形态空间内对齐图像和字幕，该方法提升了物种分类和文本-图像检索的性能，同时抑制了虚假关联。研究表明，描述性字幕在生物多模态学习中提供了超越标签的丰富语义信息。", "keywords": "synthetic captions, multimodal large language models, BIOCAP, biological foundation models, species classification, image-text retrieval, caption generation, dataset augmentation, multimodal supervision", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"]}
]]></acme>

<pubDate>2025-10-23T00:34:21+00:00</pubDate>
</item>
<item>
<title>On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers</title>
<link>https://papers.cool/arxiv/2510.20094</link>
<guid>https://papers.cool/arxiv/2510.20094</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper establishes an exact equivalence between stationary solutions of McKean-Vlasov equations on the circle and an infinite-dimensional quadratic system over Fourier coefficients, enabling explicit characterization of stationary states, local bifurcations, and resonance structures, including singular potentials. Analytic expressions are derived for various bifurcation types and their connection to discontinuous phase transitions, and global properties of the free energy landscape are proved. These results are applied to the Noisy Mean-Field Transformer model, revealing how the inverse temperature β shapes bifurcation geometry, produces multi‑mode metastable states, and triggers a sharp shift from continuous to first‑order phase behavior.<br /><strong>Summary (CN):</strong> 本文展示了圆上McKean-Vlasov方程的平稳解与傅里叶系数的无限维二次方程组之间的精确等价，从而在序列空间而非函数空间中显式描述平稳状态、局部分叉及共振结构（包括奇异势能）。作者推导了不同分叉类型（超临界、临界、亚临界、交叉）的解析表达式，并将其与不连续相变关联，同时证明了自由能全局凹凸性、最小化平稳测度的存在性与紧致性。随后将理论应用于噪声均场Transformer模型，阐明逆温度β如何影响从均匀分布的多重分叉几何、产生近似多模态的亚稳态，并在β增大时出现从连续到一级相变的锐利转变。<br /><strong>Keywords:</strong> McKean-Vlasov, stationary solutions, Fourier coefficients, bifurcation, phase transition, noisy transformer, mean-field, metastable states, statistical mechanics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 8, Safety: 3, Technicality: 9, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet</div>
We study stationary solutions of McKean-Vlasov equations on the circle. Our main contributions stem from observing an exact equivalence between solutions of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic system of equations over Fourier coefficients, which allows explicit characterization of the stationary states in a sequence space rather than a function space. This framework provides a transparent description of local bifurcations, characterizing their periodicity, and resonance structures, while accommodating singular potentials. We derive analytic expressions that characterize the emergence, form and shape (supercritical, critical, subcritical or transcritical) of bifurcations involving possibly multiple Fourier modes and connect them with discontinuous phase transitions. We also characterize, under suitable assumptions, the detailed structure of the stationary bifurcating solutions that are accurate upto an arbitrary number of Fourier modes. At the global level, we establish regularity and concavity properties of the free energy landscape, proving existence, compactness, and coexistence of globally minimizing stationary measures, further identifying discontinuous phase transitions with points of non-differentiability of the minimum free energy map. As an application, we specialize the theory to the Noisy Mean-Field Transformer model, where we show how changing the inverse temperature parameter $\beta$ affects the geometry of the infinitely many bifurcations from the uniform measure. We also explain how increasing $\beta$ can lead to a rich class of approximate multi-mode stationary solutions which can be seen as `metastable states'. Further, a sharp transition from continuous to discontinuous (first-order) phase behavior is observed as $\beta$ increases.
<div><strong>Authors:</strong> Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper establishes an exact equivalence between stationary solutions of McKean-Vlasov equations on the circle and an infinite-dimensional quadratic system over Fourier coefficients, enabling explicit characterization of stationary states, local bifurcations, and resonance structures, including singular potentials. Analytic expressions are derived for various bifurcation types and their connection to discontinuous phase transitions, and global properties of the free energy landscape are proved. These results are applied to the Noisy Mean-Field Transformer model, revealing how the inverse temperature β shapes bifurcation geometry, produces multi‑mode metastable states, and triggers a sharp shift from continuous to first‑order phase behavior.", "summary_cn": "本文展示了圆上McKean-Vlasov方程的平稳解与傅里叶系数的无限维二次方程组之间的精确等价，从而在序列空间而非函数空间中显式描述平稳状态、局部分叉及共振结构（包括奇异势能）。作者推导了不同分叉类型（超临界、临界、亚临界、交叉）的解析表达式，并将其与不连续相变关联，同时证明了自由能全局凹凸性、最小化平稳测度的存在性与紧致性。随后将理论应用于噪声均场Transformer模型，阐明逆温度β如何影响从均匀分布的多重分叉几何、产生近似多模态的亚稳态，并在β增大时出现从连续到一级相变的锐利转变。", "keywords": "McKean-Vlasov, stationary solutions, Fourier coefficients, bifurcation, phase transition, noisy transformer, mean-field, metastable states, statistical mechanics", "scoring": {"interpretability": 2, "understanding": 8, "safety": 3, "technicality": 9, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Krishnakumar Balasubramanian", "Sayan Banerjee", "Philippe Rigollet"]}
]]></acme>

<pubDate>2025-10-23T00:28:32+00:00</pubDate>
</item>
<item>
<title>LLMs can hide text in other text of the same length.ipynb</title>
<link>https://papers.cool/arxiv/2510.20075</link>
<guid>https://papers.cool/arxiv/2510.20075</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a simple protocol that uses large language models to embed a meaningful text within another coherent text of the same length, enabling covert communication while preserving surface plausibility. Experiments show that even modest 8‑billion‑parameter open‑source LLMs can encode and decode messages locally in seconds, highlighting a radical decoupling of text from authorial intent. The authors discuss the safety implications, including the potential for covert deployment of unfiltered models and the erosion of trust in written communication.<br /><strong>Summary (CN):</strong> 本文提出一种利用大型语言模型（LLM）在相同长度的另一段连上看似合乎情理的隐蔽通信。实验表明，即使是 8 十亿参数的开源 LLM，也能在本地笔记本电脑上在数秒内完成编码与解码，显示出文本与作者意图的彻底解耦。作者进一步讨论了安全风险，如通过安全模型的合规回复隐藏未过滤模型的答案，以及这对书面交流可信度的冲击。<br /><strong>Keywords:</strong> LLM steganography, text hiding, covert communication, AI safety, deceptive alignment, model misuse, encoding protocol, trust erosion, adversarial use<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.
<div><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a simple protocol that uses large language models to embed a meaningful text within another coherent text of the same length, enabling covert communication while preserving surface plausibility. Experiments show that even modest 8‑billion‑parameter open‑source LLMs can encode and decode messages locally in seconds, highlighting a radical decoupling of text from authorial intent. The authors discuss the safety implications, including the potential for covert deployment of unfiltered models and the erosion of trust in written communication.", "summary_cn": "本文提出一种利用大型语言模型（LLM）在相同长度的另一段连上看似合乎情理的隐蔽通信。实验表明，即使是 8 十亿参数的开源 LLM，也能在本地笔记本电脑上在数秒内完成编码与解码，显示出文本与作者意图的彻底解耦。作者进一步讨论了安全风险，如通过安全模型的合规回复隐藏未过滤模型的答案，以及这对书面交流可信度的冲击。", "keywords": "LLM steganography, text hiding, covert communication, AI safety, deceptive alignment, model misuse, encoding protocol, trust erosion, adversarial use", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Antonio Norelli", "Michael Bronstein"]}
]]></acme>

<pubDate>2025-10-22T23:16:50+00:00</pubDate>
</item>
<item>
<title>Endogenous Aggregation of Multiple Data Envelopment Analysis Scores for Large Data Sets</title>
<link>https://papers.cool/arxiv/2510.20052</link>
<guid>https://papers.cool/arxiv/2510.20052</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two regularized DEA models—a slack‑based measure (SBM) and a linearized goal‑programming version (GP‑SBM)—that generate both dimension‑specific and aggregate efficiency scores while handling desirable and undesirable outputs. It demonstrates computational efficiency and superior discriminatory power on large datasets, including a case study of twelve Ontario hospitals evaluated across technical, clinical, and patient‑experience dimensions. The approach outperforms conventional separate‑dimension benchmarking methods.<br /><strong>Summary (CN):</strong> 本文提出了两种正则化数据包络分析模型——基于松弛度的测度（SBM）和线性化目标规划模型（GP‑SBM），能够在处理可取和不可取输出的同时生成维度特定和整体效率得分。通过在大规模数据集上的实验以及对安大略省十二家医院在技术、临床和患者体验三个维度的案例研究，展示了该方法的计算效率和更高的辨别力，优于传统的先分别评估维度再聚合的基准方法。<br /><strong>Keywords:</strong> data envelopment analysis, DEA, slack-based measure, GP-SBM, regularization, multi-dimensional efficiency, undesirable outputs, large-scale benchmarking, organizational effectiveness, efficiency aggregation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hashem Omrani, Raha Imanirad, Adam Diamant, Utkarsh Verma, Amol Verma, Fahad Razak</div>
We propose an approach for dynamic efficiency evaluation across multiple organizational dimensions using data envelopment analysis (DEA). The method generates both dimension-specific and aggregate efficiency scores, incorporates desirable and undesirable outputs, and is suitable for large-scale problem settings. Two regularized DEA models are introduced: a slack-based measure (SBM) and a linearized version of a nonlinear goal programming model (GP-SBM). While SBM estimates an aggregate efficiency score and then distributes it across dimensions, GP-SBM first estimates dimension-level efficiencies and then derives an aggregate score. Both models utilize a regularization parameter to enhance discriminatory power while also directly integrating both desirable and undesirable outputs. We demonstrate the computational efficiency and validity of our approach on multiple datasets and apply it to a case study of twelve hospitals in Ontario, Canada, evaluating three theoretically grounded dimensions of organizational effectiveness over a 24-month period from January 2018 to December 2019: technical efficiency, clinical efficiency, and patient experience. Our numerical results show that SBM and GP-SBM better capture correlations among input/output variables and outperform conventional benchmarking methods that separately evaluate dimensions before aggregation.
<div><strong>Authors:</strong> Hashem Omrani, Raha Imanirad, Adam Diamant, Utkarsh Verma, Amol Verma, Fahad Razak</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two regularized DEA models—a slack‑based measure (SBM) and a linearized goal‑programming version (GP‑SBM)—that generate both dimension‑specific and aggregate efficiency scores while handling desirable and undesirable outputs. It demonstrates computational efficiency and superior discriminatory power on large datasets, including a case study of twelve Ontario hospitals evaluated across technical, clinical, and patient‑experience dimensions. The approach outperforms conventional separate‑dimension benchmarking methods.", "summary_cn": "本文提出了两种正则化数据包络分析模型——基于松弛度的测度（SBM）和线性化目标规划模型（GP‑SBM），能够在处理可取和不可取输出的同时生成维度特定和整体效率得分。通过在大规模数据集上的实验以及对安大略省十二家医院在技术、临床和患者体验三个维度的案例研究，展示了该方法的计算效率和更高的辨别力，优于传统的先分别评估维度再聚合的基准方法。", "keywords": "data envelopment analysis, DEA, slack-based measure, GP-SBM, regularization, multi-dimensional efficiency, undesirable outputs, large-scale benchmarking, organizational effectiveness, efficiency aggregation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hashem Omrani", "Raha Imanirad", "Adam Diamant", "Utkarsh Verma", "Amol Verma", "Fahad Razak"]}
]]></acme>

<pubDate>2025-10-22T22:03:05+00:00</pubDate>
</item>
<item>
<title>From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge</title>
<link>https://papers.cool/arxiv/2510.20043</link>
<guid>https://papers.cool/arxiv/2510.20043</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Bengali Language Cultural Knowledge (BLanCK) dataset, which captures folk traditions, culinary arts, and regional dialects to assess large language models' cultural knowledge. Experiments reveal that current multilingual models perform well on non‑cultural tasks but struggle significantly on cultural items, and that providing contextual prompts improves performance across models. The authors argue for context‑aware architectures and culturally curated training data to close this gap.<br /><strong>Summary (CN):</strong> 本文推出了孟加拉语言文化知识（BLanCK）数据集，覆盖民俗、烹饪和方言等内容，用于评估大型语言模型的文化知识水平。实验表明，现有多语言模型在非文化任务上表现良好，但在文化项目上显著不足，提供上下文提示可提升所有模型的表现。作者主张采用上下文感知的架构并进行文化定制的训练，以弥合这一差距。<br /><strong>Keywords:</strong> Bengali cultural knowledge, multilingual LLM evaluation, low-resource languages, cultural bias, BLanCK dataset, context-aware models, folk traditions, culinary arts, regional dialects<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Nafis Chowdhury, Moinul Haque, Anika Ahmed, Nazia Tasnim, Md. Istiak Hossain Shihab, Sajjadur Rahman, Farig Sadeque</div>
Recent progress in NLP research has demonstrated remarkable capabilities of large language models (LLMs) across a wide range of tasks. While recent multilingual benchmarks have advanced cultural evaluation for LLMs, critical gaps remain in capturing the nuances of low-resource cultures. Our work addresses these limitations through a Bengali Language Cultural Knowledge (BLanCK) dataset including folk traditions, culinary arts, and regional dialects. Our investigation of several multilingual language models shows that while these models perform well in non-cultural categories, they struggle significantly with cultural knowledge and performance improves substantially across all models when context is provided, emphasizing context-aware architectures and culturally curated training data.
<div><strong>Authors:</strong> Nafis Chowdhury, Moinul Haque, Anika Ahmed, Nazia Tasnim, Md. Istiak Hossain Shihab, Sajjadur Rahman, Farig Sadeque</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Bengali Language Cultural Knowledge (BLanCK) dataset, which captures folk traditions, culinary arts, and regional dialects to assess large language models' cultural knowledge. Experiments reveal that current multilingual models perform well on non‑cultural tasks but struggle significantly on cultural items, and that providing contextual prompts improves performance across models. The authors argue for context‑aware architectures and culturally curated training data to close this gap.", "summary_cn": "本文推出了孟加拉语言文化知识（BLanCK）数据集，覆盖民俗、烹饪和方言等内容，用于评估大型语言模型的文化知识水平。实验表明，现有多语言模型在非文化任务上表现良好，但在文化项目上显著不足，提供上下文提示可提升所有模型的表现。作者主张采用上下文感知的架构并进行文化定制的训练，以弥合这一差距。", "keywords": "Bengali cultural knowledge, multilingual LLM evaluation, low-resource languages, cultural bias, BLanCK dataset, context-aware models, folk traditions, culinary arts, regional dialects", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Nafis Chowdhury", "Moinul Haque", "Anika Ahmed", "Nazia Tasnim", "Md. Istiak Hossain Shihab", "Sajjadur Rahman", "Farig Sadeque"]}
]]></acme>

<pubDate>2025-10-22T21:42:59+00:00</pubDate>
</item>
<item>
<title>Throwing Vines at the Wall: Structure Learning via Random Search</title>
<link>https://papers.cool/arxiv/2510.20035</link>
<guid>https://papers.cool/arxiv/2510.20035</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces random search algorithms for learning the structure of vine copulas, accompanied by a statistical framework based on model confidence sets that offers theoretical guarantees on selection probabilities and supports ensembling. Empirical evaluations on several real-world datasets demonstrate consistent performance improvements over existing state-of-the-art methods, including the traditional greedy algorithm.<br /><strong>Summary (CN):</strong> 本文提出了用于学习 vine copulas 结构的随机搜索算法，并基于模型置信集构建统计框架，提供了结构选择概率的理论保证并支持集成。实验证明，在多个真实数据集上，该方法相较于包括传统贪心算法在内的现有最先进方法表现出一致的优势。<br /><strong>Keywords:</strong> vine copulas, structure learning, random search, model confidence sets, ensemble, multivariate dependence<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Thibault Vatter, Thomas Nagler</div>
Vine copulas offer flexible multivariate dependence modeling and have become widely used in machine learning, yet structure learning remains a key challenge. Early heuristics like the greedy algorithm of Dissmann are still considered the gold standard, but often suboptimal. We propose random search algorithms that improve structure selection and a statistical framework based on model confidence sets, which provides theoretical guarantees on selection probabilities and a powerful foundation for ensembling. Empirical results on several real-world data sets show that our methods consistently outperform state-of-the-art approaches.
<div><strong>Authors:</strong> Thibault Vatter, Thomas Nagler</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces random search algorithms for learning the structure of vine copulas, accompanied by a statistical framework based on model confidence sets that offers theoretical guarantees on selection probabilities and supports ensembling. Empirical evaluations on several real-world datasets demonstrate consistent performance improvements over existing state-of-the-art methods, including the traditional greedy algorithm.", "summary_cn": "本文提出了用于学习 vine copulas 结构的随机搜索算法，并基于模型置信集构建统计框架，提供了结构选择概率的理论保证并支持集成。实验证明，在多个真实数据集上，该方法相较于包括传统贪心算法在内的现有最先进方法表现出一致的优势。", "keywords": "vine copulas, structure learning, random search, model confidence sets, ensemble, multivariate dependence", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Thibault Vatter", "Thomas Nagler"]}
]]></acme>

<pubDate>2025-10-22T21:26:18+00:00</pubDate>
</item>
<item>
<title>On Encoding Matrices using Quantum Circuits</title>
<link>https://papers.cool/arxiv/2510.20030</link>
<guid>https://papers.cool/arxiv/2510.20030</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically studies how to encode matrices as quantum circuits using block encodings and state preparation circuits. It presents a general efficient method to construct a block encoding from a classical description stored in RAM, and low‑overhead bidirectional conversion algorithms showing the two representations are essentially equivalent, based on a constant‑depth multiplexer for higher‑order Pauli matrices and a quantum basis‑conversion algorithm.<br /><strong>Summary (CN):</strong> 本文系统研究将矩阵以块编码。提出一种在经典随机存取存储器中给定矩阵的通用高效块编码构造方法，并提供低开销、双向的块编码与状态准备电路之间的转换算法，展示两种模型本质上等价。关键技术包括常数深度的多路复用器用于同时复用高阶 Pauli 矩阵，以及在标准基和 Pauli 基之间进行矩阵展开的量子转换算法。<br /><strong>Keywords:</strong> quantum block encoding, state preparation circuits, quantum linear algebra, Pauli decomposition, quantum multiplexer, matrix encoding, quantum algorithm<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Liron Mor Yosef, Haim Avron</div>
Over a decade ago, it was demonstrated that quantum computing has the potential to revolutionize numerical linear algebra by enabling algorithms with complexity superior to what is classically achievable, e.g., the seminal HHL algorithm for solving linear systems. Efficient execution of such algorithms critically depends on representing inputs (matrices and vectors) as quantum circuits that encode or implement these inputs. For that task, two common circuit representations emerged in the literature: block encodings and state preparation circuits. In this paper, we systematically study encodings matrices in the form of block encodings and state preparation circuits. We examine methods for constructing these representations from matrices given in classical form, as well as quantum two-way conversions between circuit representations. Two key results we establish (among others) are: (a) a general method for efficiently constructing a block encoding of an arbitrary matrix given in classical form (entries stored in classical random access memory); and (b) low-overhead, bidirectional conversion algorithms between block encodings and state preparation circuits, showing that these models are essentially equivalent. From a technical perspective, two central components of our constructions are: (i) a special constant-depth multiplexer that simultaneously multiplexes all higher-order Pauli matrices of a given size, and (ii) an algorithm for performing a quantum conversion between a matrix's expansion in the standard basis and its expansion in the basis of higher-order Pauli matrices.
<div><strong>Authors:</strong> Liron Mor Yosef, Haim Avron</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically studies how to encode matrices as quantum circuits using block encodings and state preparation circuits. It presents a general efficient method to construct a block encoding from a classical description stored in RAM, and low‑overhead bidirectional conversion algorithms showing the two representations are essentially equivalent, based on a constant‑depth multiplexer for higher‑order Pauli matrices and a quantum basis‑conversion algorithm.", "summary_cn": "本文系统研究将矩阵以块编码。提出一种在经典随机存取存储器中给定矩阵的通用高效块编码构造方法，并提供低开销、双向的块编码与状态准备电路之间的转换算法，展示两种模型本质上等价。关键技术包括常数深度的多路复用器用于同时复用高阶 Pauli 矩阵，以及在标准基和 Pauli 基之间进行矩阵展开的量子转换算法。", "keywords": "quantum block encoding, state preparation circuits, quantum linear algebra, Pauli decomposition, quantum multiplexer, matrix encoding, quantum algorithm", "scoring": {"interpretability": 1, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liron Mor Yosef", "Haim Avron"]}
]]></acme>

<pubDate>2025-10-22T21:20:08+00:00</pubDate>
</item>
<item>
<title>Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators</title>
<link>https://papers.cool/arxiv/2510.20017</link>
<guid>https://papers.cool/arxiv/2510.20017</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes training neural operators to learn the mapping from dynamics and cost functionals (the "rules") to equilibrium strategies for linear‑quadratic mean field games defined on separable Hilbert spaces. It provides a statistical guarantee that a neural operator trained on a limited set of sampled rules can reliably solve unseen variants, supported by local‑Lipschitz estimates, a universal approximation theorem with controlled Lipschitz regularity, and new infinite‑dimensional sample‑complexity bounds.<br /><strong>Summary (CN):</strong> 本文提出使用神经算子学习线性‑二次均场博弈（在可分 Hilbert 空间上）的规则到均衡映射，即从动力学和代价提供统计保证，证明在少量随机采样规则上训练的神经算子能够可靠求解未见的游戏变体，核心技术包括对高度非线性映射的局部 Lipschitz 估计、具有受控 Lipschitz 常数的通用逼近定理，以及针对无限维空间的样本复杂度界。<br /><strong>Keywords:</strong> neural operators, linear-quadratic mean field games, Hilbert spaces, infinite-dimensional analysis, Lipschitz continuity, universal approximation, sample complexity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 9, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dena Firoozi, Anastasis Kratsios, Xuwei Yang</div>
Traditional mean-field game (MFG) solvers operate on an instance-by-instance basis, which becomes infeasible when many related problems must be solved (e.g., for seeking a robust description of the solution under perturbations of the dynamics or utilities, or in settings involving continuum-parameterized agents.). We overcome this by training neural operators (NOs) to learn the rules-to-equilibrium map from the problem data (``rules'': dynamics and cost functionals) of LQ MFGs defined on separable Hilbert spaces to the corresponding equilibrium strategy. Our main result is a statistical guarantee: an NO trained on a small number of randomly sampled rules reliably solves unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO parameters needed remains controlled under appropriate rule sampling during training. Our guarantee follows from three results: (i) local-Lipschitz estimates for the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation theorem using NOs with a prespecified Lipschitz regularity (unlike traditional NO results where the NO's Lipschitz constant can diverge as the approximation error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz learners in infinite dimensions, directly applicable as the Lipschitz constants of our approximating NOs are controlled in (ii).
<div><strong>Authors:</strong> Dena Firoozi, Anastasis Kratsios, Xuwei Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes training neural operators to learn the mapping from dynamics and cost functionals (the \"rules\") to equilibrium strategies for linear‑quadratic mean field games defined on separable Hilbert spaces. It provides a statistical guarantee that a neural operator trained on a limited set of sampled rules can reliably solve unseen variants, supported by local‑Lipschitz estimates, a universal approximation theorem with controlled Lipschitz regularity, and new infinite‑dimensional sample‑complexity bounds.", "summary_cn": "本文提出使用神经算子学习线性‑二次均场博弈（在可分 Hilbert 空间上）的规则到均衡映射，即从动力学和代价提供统计保证，证明在少量随机采样规则上训练的神经算子能够可靠求解未见的游戏变体，核心技术包括对高度非线性映射的局部 Lipschitz 估计、具有受控 Lipschitz 常数的通用逼近定理，以及针对无限维空间的样本复杂度界。", "keywords": "neural operators, linear-quadratic mean field games, Hilbert spaces, infinite-dimensional analysis, Lipschitz continuity, universal approximation, sample complexity", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 9, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dena Firoozi", "Anastasis Kratsios", "Xuwei Yang"]}
]]></acme>

<pubDate>2025-10-22T20:40:20+00:00</pubDate>
</item>
<item>
<title>Improving Predictive Confidence in Medical Imaging via Online Label Smoothing</title>
<link>https://papers.cool/arxiv/2510.20011</link>
<guid>https://papers.cool/arxiv/2510.20011</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Online Label Smoothing (OLS), a dynamic label smoothing technique that adjusts soft labels during training based on the model’s predictions, and evaluates it on the large‑scale RadImageNet dataset using ResNet‑, MobileNetV2 and VGG‑19. Experiments show that OLS consistently improves top‑1 and top‑5 accuracy as well as calibration compared to hard labels, conventional label smoothing and teacher‑free knowledge distillation, while also producing more compact and well‑separated feature embeddings. These results suggest OLS can enhance the reliability and trustworthiness of medical imaging classifiers.<br /><strong>Summary (CN):</strong> 本文提出在线标签平滑（OLS）方法，该方法根据模型在训练过程中的预测动态调整软标签，并在大规模 RadImageNet 数据集上使用 ResNet‑50、MobileNetV2 和 VGG‑19 进行评估。实验表明，较之硬标签、传统标签平滑以及无教师知识蒸馏，OLS 能持续提升 Top‑1 与 Top‑5 准确率及预测校准效果，同时生成更紧凑且分布更分明的特征嵌入。结果显示 OLS 有助于提升医学影像分类器的可靠性与可信度。<br /><strong>Keywords:</strong> online label smoothing, calibration, medical imaging, CNN, RadImageNet, representation learning, predictive confidence, label smoothing, deep learning, trustworthiness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry</div>
Deep learning models, especially convolutional neural networks, have achieved impressive results in medical image classification. However, these models often produce overconfident predictions, which can undermine their reliability in critical healthcare settings. While traditional label smoothing offers a simple way to reduce such overconfidence, it fails to consider relationships between classes by treating all non-target classes equally. In this study, we explore the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft labels throughout training based on the model's own prediction patterns. We evaluate OLS on the large-scale RadImageNet dataset using three widely used architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS consistently improves both Top-1 and Top-5 classification accuracy compared to standard training methods, including hard labels, conventional label smoothing, and teacher-free knowledge distillation. In addition to accuracy gains, OLS leads to more compact and well-separated feature embeddings, indicating improved representation learning. These findings suggest that OLS not only strengthens predictive performance but also enhances calibration, making it a practical and effective solution for developing trustworthy AI systems in the medical imaging domain.
<div><strong>Authors:</strong> Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Online Label Smoothing (OLS), a dynamic label smoothing technique that adjusts soft labels during training based on the model’s predictions, and evaluates it on the large‑scale RadImageNet dataset using ResNet‑, MobileNetV2 and VGG‑19. Experiments show that OLS consistently improves top‑1 and top‑5 accuracy as well as calibration compared to hard labels, conventional label smoothing and teacher‑free knowledge distillation, while also producing more compact and well‑separated feature embeddings. These results suggest OLS can enhance the reliability and trustworthiness of medical imaging classifiers.", "summary_cn": "本文提出在线标签平滑（OLS）方法，该方法根据模型在训练过程中的预测动态调整软标签，并在大规模 RadImageNet 数据集上使用 ResNet‑50、MobileNetV2 和 VGG‑19 进行评估。实验表明，较之硬标签、传统标签平滑以及无教师知识蒸馏，OLS 能持续提升 Top‑1 与 Top‑5 准确率及预测校准效果，同时生成更紧凑且分布更分明的特征嵌入。结果显示 OLS 有助于提升医学影像分类器的可靠性与可信度。", "keywords": "online label smoothing, calibration, medical imaging, CNN, RadImageNet, representation learning, predictive confidence, label smoothing, deep learning, trustworthiness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Kushan Choudhury", "Shubhrodeep Roy", "Ankur Chanda", "Shubhajit Biswas", "Somenath Kuiry"]}
]]></acme>

<pubDate>2025-10-22T20:25:14+00:00</pubDate>
</item>
<item>
<title>Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models</title>
<link>https://papers.cool/arxiv/2510.19999</link>
<guid>https://papers.cool/arxiv/2510.19999</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an Enhanced Cyclic Coordinate Descent (ECCD) framework for elastic net penalized generalized linear models, using a Taylor expansion to transform nonlinear gradient steps into efficient batched operations. By unrolling vector recurrences with a tunable integer parameter, the method achieves up to 3× speedup over state-of-the-art solvers without sacrificing convergence, and is implemented in C++ with Eigen.<br /><strong>Summary (CN):</strong> 本文提出了一种增强循环坐标下降（ECCD）框架，用于弹性网惩罚的广义线性模型，通过在当前迭代点进行泰勒展开，将非线性梯度计算转化为高效的批量运算。通过可调整数参数展开向量递推，方法在保持收敛性的同时实现了约 3 倍的加速，并使用 C++ Eigen 实现。<br /><strong>Keywords:</strong> elastic net, cyclic coordinate descent, optimization, linear models, regularization path, batched computation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yixiao Wang, Zishan Shao, Ting Jiang, Aditya Devarakonda</div>
We present a novel enhanced cyclic coordinate descent (ECCD) framework for solving generalized linear models with elastic net constraints that reduces training time in comparison to existing state-of-the-art methods. We redesign the CD method by performing a Taylor expansion around the current iterate to avoid nonlinear operations arising in the gradient computation. By introducing this approximation, we are able to unroll the vector recurrences occurring in the CD method and reformulate the resulting computations into more efficient batched computations. We show empirically that the recurrence can be unrolled by a tunable integer parameter, $s$, such that $s > 1$ yields performance improvements without affecting convergence, whereas $s = 1$ yields the original CD method. A key advantage of ECCD is that it avoids the convergence delay and numerical instability exhibited by block coordinate descent. Finally, we implement our proposed method in C++ using Eigen to accelerate linear algebra computations. Comparison of our method against existing state-of-the-art solvers shows consistent performance improvements of $3\times$ in average for regularization path variant on diverse benchmark datasets. Our implementation is available at https://github.com/Yixiao-Wang-Stats/ECCD.
<div><strong>Authors:</strong> Yixiao Wang, Zishan Shao, Ting Jiang, Aditya Devarakonda</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an Enhanced Cyclic Coordinate Descent (ECCD) framework for elastic net penalized generalized linear models, using a Taylor expansion to transform nonlinear gradient steps into efficient batched operations. By unrolling vector recurrences with a tunable integer parameter, the method achieves up to 3× speedup over state-of-the-art solvers without sacrificing convergence, and is implemented in C++ with Eigen.", "summary_cn": "本文提出了一种增强循环坐标下降（ECCD）框架，用于弹性网惩罚的广义线性模型，通过在当前迭代点进行泰勒展开，将非线性梯度计算转化为高效的批量运算。通过可调整数参数展开向量递推，方法在保持收敛性的同时实现了约 3 倍的加速，并使用 C++ Eigen 实现。", "keywords": "elastic net, cyclic coordinate descent, optimization, linear models, regularization path, batched computation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yixiao Wang", "Zishan Shao", "Ting Jiang", "Aditya Devarakonda"]}
]]></acme>

<pubDate>2025-10-22T20:01:25+00:00</pubDate>
</item>
<item>
<title>SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment</title>
<link>https://papers.cool/arxiv/2510.19979</link>
<guid>https://papers.cool/arxiv/2510.19979</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SecureInfer introduces a heterogeneous architecture that combines Trusted Execution Environments (SGX) with GPUs to protect privacy‑critical components of large language models during on‑device inference. By partitioning non‑linear layers and adapters into an enclave while encrypting linear matrix‑multiplication operations for GPU execution, the system aims to prevent model extraction attacks with modest performance overhead, demonstrated on LLaMA‑2.<br /><strong>Summary (CN):</strong> SecureInfer 提出了将可信执行环境（SGX）与 GPU 异构结合的架构，在边缘设备上对大型语言模型的隐私关键组件进行保护。它将非线性层和 LoRA 适配器等安全敏感部分放入 enclave，线性矩阵乘法在加密后交由 GPU 计算，再在 enclave 中安全，以防模型提取攻击，实验基于 LLaMA‑2 显示出合理的性能开销。<br /><strong>Keywords:</strong> secure inference, trusted execution environment, SGX, GPU offloading, model extraction protection, LLM deployment, privacy, heterogeneous architecture, LoRA adapters<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Tushar Nayan, Ziqi Zhang, Ruimin Sun</div>
With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.
<div><strong>Authors:</strong> Tushar Nayan, Ziqi Zhang, Ruimin Sun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SecureInfer introduces a heterogeneous architecture that combines Trusted Execution Environments (SGX) with GPUs to protect privacy‑critical components of large language models during on‑device inference. By partitioning non‑linear layers and adapters into an enclave while encrypting linear matrix‑multiplication operations for GPU execution, the system aims to prevent model extraction attacks with modest performance overhead, demonstrated on LLaMA‑2.", "summary_cn": "SecureInfer 提出了将可信执行环境（SGX）与 GPU 异构结合的架构，在边缘设备上对大型语言模型的隐私关键组件进行保护。它将非线性层和 LoRA 适配器等安全敏感部分放入 enclave，线性矩阵乘法在加密后交由 GPU 计算，再在 enclave 中安全，以防模型提取攻击，实验基于 LLaMA‑2 显示出合理的性能开销。", "keywords": "secure inference, trusted execution environment, SGX, GPU offloading, model extraction protection, LLM deployment, privacy, heterogeneous architecture, LoRA adapters", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tushar Nayan", "Ziqi Zhang", "Ruimin Sun"]}
]]></acme>

<pubDate>2025-10-22T19:17:31+00:00</pubDate>
</item>
<item>
<title>Guiding diffusion models to reconstruct flow fields from sparse data</title>
<link>https://papers.cool/arxiv/2510.19971</link>
<guid>https://papers.cool/arxiv/2510.19971</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes a novel sampling technique for diffusion models that guides the reverse diffusion process with sparse measurements to reconstruct high‑fidelity unsteady flow fields. It also incorporates physics knowledge via a conflict‑free update during training and demonstrates superior performance on 2‑D and 3‑D turbulent flow datasets compared to existing diffusion‑based methods. The results highlight the potential of diffusion generative models for CFD reconstruction tasks.<br /><strong>Summary (CN):</strong> 本文提出一种新颖的扩散模型采样方法，通过利用稀疏观测数据在逆扩散过程中进行引导，以重建高保真度的非稳态流场。同时在训练阶段加入冲突‑自由的物理约束更新。实验在二维和三维湍流数据上表明方法相较于现有基于扩散模型的技术在流场结构和像素误差上均有显著提升，展示了扩散模型在计算流体力学重建中的巨大潜力。<br /><strong>Keywords:</strong> diffusion models, flow reconstruction, sparse measurements, computational fluid dynamics, physics-informed learning, turbulent flow<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Marc Amorós-Trepat, Luis Medrano-Navarro, Qiang Liu, Luca Guastoni, Nils Thuerey</div>
The reconstruction of unsteady flow fields from limited measurements is a challenging and crucial task for many engineering applications. Machine learning models are gaining popularity in solving this problem due to their ability to learn complex patterns from data and generalize across diverse conditions. Among these, diffusion models have emerged as particularly powerful in generative tasks, producing high-quality samples by iteratively refining noisy inputs. In contrast to other methods, these generative models are capable of reconstructing the smallest scales of the fluid spectrum. In this work, we introduce a novel sampling method for diffusion models that enables the reconstruction of high-fidelity samples by guiding the reverse process using the available sparse data. Moreover, we enhance the reconstructions with available physics knowledge using a conflict-free update method during training. To evaluate the effectiveness of our method, we conduct experiments on 2 and 3-dimensional turbulent flow data. Our method consistently outperforms other diffusion-based methods in predicting the fluid's structure and in pixel-wise accuracy. This study underscores the remarkable potential of diffusion models in reconstructing flow field data, paving the way for their application in Computational Fluid Dynamics research.
<div><strong>Authors:</strong> Marc Amorós-Trepat, Luis Medrano-Navarro, Qiang Liu, Luca Guastoni, Nils Thuerey</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes a novel sampling technique for diffusion models that guides the reverse diffusion process with sparse measurements to reconstruct high‑fidelity unsteady flow fields. It also incorporates physics knowledge via a conflict‑free update during training and demonstrates superior performance on 2‑D and 3‑D turbulent flow datasets compared to existing diffusion‑based methods. The results highlight the potential of diffusion generative models for CFD reconstruction tasks.", "summary_cn": "本文提出一种新颖的扩散模型采样方法，通过利用稀疏观测数据在逆扩散过程中进行引导，以重建高保真度的非稳态流场。同时在训练阶段加入冲突‑自由的物理约束更新。实验在二维和三维湍流数据上表明方法相较于现有基于扩散模型的技术在流场结构和像素误差上均有显著提升，展示了扩散模型在计算流体力学重建中的巨大潜力。", "keywords": "diffusion models, flow reconstruction, sparse measurements, computational fluid dynamics, physics-informed learning, turbulent flow", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Marc Amorós-Trepat", "Luis Medrano-Navarro", "Qiang Liu", "Luca Guastoni", "Nils Thuerey"]}
]]></acme>

<pubDate>2025-10-22T19:01:50+00:00</pubDate>
</item>
<item>
<title>LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation</title>
<link>https://papers.cool/arxiv/2510.19967</link>
<guid>https://papers.cool/arxiv/2510.19967</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LyriCAR introduces a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, operating in an unsupervised manner. By designing an adaptive curriculum that presents increasingly complex translation challenges, the system improves translation quality, cross-line coherence, and global rhyme while reducing training steps by about 40% compared to baselines. Experiments on English‑Chinese lyric translation demonstrate state-of-the-art performance on both standard metrics and multi‑dimensional reward scores.<br /><strong>Summary (CN):</strong> LyriCAR 提出了一种难度感知的课程强化学习框架，用于可控的歌词翻译，完全无监督。该框架通过自适应课程设计，逐步提供更复杂的翻译挑战，从而提升翻译质量、跨行连贯性和整体押韵，并将训练步骤缩减约 40%。在英-中歌词翻译实验中，该方法在标准翻译指标和多维奖励得分上均实现了最先进的性能。<br /><strong>Keywords:</strong> lyric translation, curriculum reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, multi-dimensional reward, EN-ZH translation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.
<div><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LyriCAR introduces a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, operating in an unsupervised manner. By designing an adaptive curriculum that presents increasingly complex translation challenges, the system improves translation quality, cross-line coherence, and global rhyme while reducing training steps by about 40% compared to baselines. Experiments on English‑Chinese lyric translation demonstrate state-of-the-art performance on both standard metrics and multi‑dimensional reward scores.", "summary_cn": "LyriCAR 提出了一种难度感知的课程强化学习框架，用于可控的歌词翻译，完全无监督。该框架通过自适应课程设计，逐步提供更复杂的翻译挑战，从而提升翻译质量、跨行连贯性和整体押韵，并将训练步骤缩减约 40%。在英-中歌词翻译实验中，该方法在标准翻译指标和多维奖励得分上均实现了最先进的性能。", "keywords": "lyric translation, curriculum reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, multi-dimensional reward, EN-ZH translation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"]}
]]></acme>

<pubDate>2025-10-22T18:57:20+00:00</pubDate>
</item>
<item>
<title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
<link>https://papers.cool/arxiv/2510.19954</link>
<guid>https://papers.cool/arxiv/2510.19954</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RELATE, a schema-agnostic Perceiver-style encoder that processes heterogeneous multimodal node attributes and produces fixed-size, permutation‑invariant representations for use with any graph neural network. By sharing modality‑specific encoders and a cross‑attention aggregation module, RELATE achieves performance comparable to schema‑specific encoders while dramatically reducing parameter count, enabling multi‑dataset pretraining for relational graphs.<br /><strong>Summary (CN):</strong> 本文提出 RELATE，一种基于 Perceiver 的跨模态编码器，能够无视表结构差异，对异构多模态节点属性进行统一编码，并生成固定维度、置换不变的节点表示，可与任意图神经网络配合使用。该方法通过共享的模态专属编码器和交叉注意力聚合，保持与特定模式编码器相近的性能，同时显著降低模型参数量，支持跨数据集的预训练，为关系图数据的基础模型奠定基础。<br /><strong>Keywords:</strong> schema-agnostic encoder, multimodal relational graph, Perceiver cross-attention, heterogeneous graph neural network, foundation model, relational graph representation, multi-table data<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski</div>
Relational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.
<div><strong>Authors:</strong> Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RELATE, a schema-agnostic Perceiver-style encoder that processes heterogeneous multimodal node attributes and produces fixed-size, permutation‑invariant representations for use with any graph neural network. By sharing modality‑specific encoders and a cross‑attention aggregation module, RELATE achieves performance comparable to schema‑specific encoders while dramatically reducing parameter count, enabling multi‑dataset pretraining for relational graphs.", "summary_cn": "本文提出 RELATE，一种基于 Perceiver 的跨模态编码器，能够无视表结构差异，对异构多模态节点属性进行统一编码，并生成固定维度、置换不变的节点表示，可与任意图神经网络配合使用。该方法通过共享的模态专属编码器和交叉注意力聚合，保持与特定模式编码器相近的性能，同时显著降低模型参数量，支持跨数据集的预训练，为关系图数据的基础模型奠定基础。", "keywords": "schema-agnostic encoder, multimodal relational graph, Perceiver cross-attention, heterogeneous graph neural network, foundation model, relational graph representation, multi-table data", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Joseph Meyer", "Divyansha Lachi", "Reza Mohammadi", "Roshan Reddy Upendra", "Eva L. Dyer", "Mark Li", "Tom Palczewski"]}
]]></acme>

<pubDate>2025-10-22T18:27:49+00:00</pubDate>
</item>
<item>
<title>Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</title>
<link>https://papers.cool/arxiv/2510.19897</link>
<guid>https://papers.cool/arxiv/2510.19897</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a memory‑augmented framework for large language model agents to learn classification tasks from labeled examples without updating model parameters, using episodic memory to store instance‑level critiques and semantic memory to distill task‑level guidance. Incorporating LLM‑generated critiques improves accuracy by up to 24.8% over retrieval‑only baselines, and the authors introduce a “suggestibility” metric to analyze how different memory representations affect model learning dynamics. Experiments reveal behavioral differences between OpenAI and open‑source models on factual versus preference‑based data, highlighting the potential of reflective, memory‑driven learning for more adaptive and interpretable agents.<br /><strong>Summary (CN):</strong> 本文提出一种记忆增强框架，使基于大语言模型的智能体能够在不更新模型参数的情况下，通过标记示例学习分类任务；框架利用情景记忆存储实例层面的批评，并通过语义记忆提炼为任务层面的指导。结合 LLM 生成的批评相较仅使用检索的基线可提升最高 24.8% 的准确率，作者还引入 “suggestibility（可暗示性）” 指标来解释不同记忆表征如何影响模型学习动力学。实验展示了 OpenAI 与开源模型在处理事实型与偏好型数据时的行为差异，凸显了反思式、记忆驱动学习在构建更适应且可解释的 LLM 智能体方面的前景。<br /><strong>Keywords:</strong> memory-augmented LLM, episodic memory, semantic memory, reflective learning, LLM critiques, suggestibility metric, parameter-free adaptation, agent interpretability, classification learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.
<div><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a memory‑augmented framework for large language model agents to learn classification tasks from labeled examples without updating model parameters, using episodic memory to store instance‑level critiques and semantic memory to distill task‑level guidance. Incorporating LLM‑generated critiques improves accuracy by up to 24.8% over retrieval‑only baselines, and the authors introduce a “suggestibility” metric to analyze how different memory representations affect model learning dynamics. Experiments reveal behavioral differences between OpenAI and open‑source models on factual versus preference‑based data, highlighting the potential of reflective, memory‑driven learning for more adaptive and interpretable agents.", "summary_cn": "本文提出一种记忆增强框架，使基于大语言模型的智能体能够在不更新模型参数的情况下，通过标记示例学习分类任务；框架利用情景记忆存储实例层面的批评，并通过语义记忆提炼为任务层面的指导。结合 LLM 生成的批评相较仅使用检索的基线可提升最高 24.8% 的准确率，作者还引入 “suggestibility（可暗示性）” 指标来解释不同记忆表征如何影响模型学习动力学。实验展示了 OpenAI 与开源模型在处理事实型与偏好型数据时的行为差异，凸显了反思式、记忆驱动学习在构建更适应且可解释的 LLM 智能体方面的前景。", "keywords": "memory-augmented LLM, episodic memory, semantic memory, reflective learning, LLM critiques, suggestibility metric, parameter-free adaptation, agent interpretability, classification learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"]}
]]></acme>

<pubDate>2025-10-22T17:58:03+00:00</pubDate>
</item>
<item>
<title>Deep Sequence-to-Sequence Models for GNSS Spoofing Detection</title>
<link>https://papers.cool/arxiv/2510.19890</link>
<guid>https://papers.cool/arxiv/2510.19890</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a data generation framework for simulating GNSS spoofing attacks worldwide and applies deep sequence-to-sequence models, including LSTM and Transformer-inspired architectures, for online spoofing detection. Experiments show that the Transformer-based model with early input fusion achieves an error rate of 0.16%, outperforming other baselines.<br /><strong>Summary (CN):</strong> 本文提出了一套用于在全球范围内模拟 GNSS 欺骗攻击的数据生成框架，并使用深度序列到序列模型（包括 LSTM 与受 Transformer 启发的架构）进行在线欺骗检测。实验表明，采用早期特征融合的 Transformer 模型将错误率降低至 0.16%，优于其他基线方法。<br /><strong>Keywords:</strong> GNSS spoofing detection, sequence-to-sequence, LSTM, Transformer, online detection, data generation, cybersecurity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other<br /><strong>Authors:</strong> Jan Zelinka, Oliver Kost, Marek Hrúz</div>
We present a data generation framework designed to simulate spoofing attacks and randomly place attack scenarios worldwide. We apply deep neural network-based models for spoofing detection, utilizing Long Short-Term Memory networks and Transformer-inspired architectures. These models are specifically designed for online detection and are trained using the generated dataset. Our results demonstrate that deep learning models can accurately distinguish spoofed signals from genuine ones, achieving high detection performance. The best results are achieved by Transformer-inspired architectures with early fusion of the inputs resulting in an error rate of 0.16%.
<div><strong>Authors:</strong> Jan Zelinka, Oliver Kost, Marek Hrúz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a data generation framework for simulating GNSS spoofing attacks worldwide and applies deep sequence-to-sequence models, including LSTM and Transformer-inspired architectures, for online spoofing detection. Experiments show that the Transformer-based model with early input fusion achieves an error rate of 0.16%, outperforming other baselines.", "summary_cn": "本文提出了一套用于在全球范围内模拟 GNSS 欺骗攻击的数据生成框架，并使用深度序列到序列模型（包括 LSTM 与受 Transformer 启发的架构）进行在线欺骗检测。实验表明，采用早期特征融合的 Transformer 模型将错误率降低至 0.16%，优于其他基线方法。", "keywords": "GNSS spoofing detection, sequence-to-sequence, LSTM, Transformer, online detection, data generation, cybersecurity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}, "authors": ["Jan Zelinka", "Oliver Kost", "Marek Hrúz"]}
]]></acme>

<pubDate>2025-10-22T16:53:41+00:00</pubDate>
</item>
<item>
<title>Compressing Biology: Evaluating the Stable Diffusion VAE for Phenotypic Drug Discovery</title>
<link>https://papers.cool/arxiv/2510.19887</link>
<guid>https://papers.cool/arxiv/2510.19887</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates the Stable Diffusion variational autoencoder (SD-VAE) for reconstructing Cell Painting microscopy images, showing that reconstructions retain phenotypic signals with minimal loss. It benchmarks reconstruction quality using pixel-level, embedding-based, latent-space, and retrieval metrics, finding that general‑purpose feature extractors like InceptionV3 often match or exceed specialized models. The results provide practical guidelines for using off‑the‑shelf generative models in high‑throughput phenotypic drug discovery pipelines.<br /><strong>Summary (CN):</strong> 本文系统评估了 Stable Diffusion 变分自编码器（SD‑VAE）在 Cell Painting 显微镜图像上的重建性能，发现其重建能够以极小的损失保留表型信号。通过像素层面、嵌入层、潜在空间以及检索等多种度量指标进行基准测试，结果表明通用特征提取器（如 InceptionV3）在检索任务上常常能够匹配或超越专用模型。该研究为在高通量表型药物发现工作流中使用现成生成模型提供了实用指南。<br /><strong>Keywords:</strong> stable diffusion, variational autoencoder, phenotypic drug discovery, cell painting, microscopy images, generative models, reconstruction evaluation, embedding metrics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Télio Cropsal, Rocío Mercado</div>
High-throughput phenotypic screens generate vast microscopy image datasets that push the limits of generative models due to their large dimensionality. Despite the growing popularity of general-purpose models trained on natural images for microscopy data analysis, their suitability in this domain has not been quantitatively demonstrated. We present the first systematic evaluation of Stable Diffusion's variational autoencoder (SD-VAE) for reconstructing Cell Painting images, assessing performance across a large dataset with diverse molecular perturbations and cell types. We find that SD-VAE reconstructions preserve phenotypic signals with minimal loss, supporting its use in microscopy workflows. To benchmark reconstruction quality, we compare pixel-level, embedding-based, latent-space, and retrieval-based metrics for a biologically informed evaluation. We show that general-purpose feature extractors like InceptionV3 match or surpass publicly available bespoke models in retrieval tasks, simplifying future pipelines. Our findings offer practical guidelines for evaluating generative models on microscopy data and support the use of off-the-shelf models in phenotypic drug discovery.
<div><strong>Authors:</strong> Télio Cropsal, Rocío Mercado</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates the Stable Diffusion variational autoencoder (SD-VAE) for reconstructing Cell Painting microscopy images, showing that reconstructions retain phenotypic signals with minimal loss. It benchmarks reconstruction quality using pixel-level, embedding-based, latent-space, and retrieval metrics, finding that general‑purpose feature extractors like InceptionV3 often match or exceed specialized models. The results provide practical guidelines for using off‑the‑shelf generative models in high‑throughput phenotypic drug discovery pipelines.", "summary_cn": "本文系统评估了 Stable Diffusion 变分自编码器（SD‑VAE）在 Cell Painting 显微镜图像上的重建性能，发现其重建能够以极小的损失保留表型信号。通过像素层面、嵌入层、潜在空间以及检索等多种度量指标进行基准测试，结果表明通用特征提取器（如 InceptionV3）在检索任务上常常能够匹配或超越专用模型。该研究为在高通量表型药物发现工作流中使用现成生成模型提供了实用指南。", "keywords": "stable diffusion, variational autoencoder, phenotypic drug discovery, cell painting, microscopy images, generative models, reconstruction evaluation, embedding metrics", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Télio Cropsal", "Rocío Mercado"]}
]]></acme>

<pubDate>2025-10-22T16:16:49+00:00</pubDate>
</item>
<item>
<title>Quantifying Feature Importance for Online Content Moderation</title>
<link>https://papers.cool/arxiv/2510.19882</link>
<guid>https://papers.cool/arxiv/2510.19882</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper quantifies how 753 socio‑behavioural, linguistic, relational, and psychological features predict changes in activity, toxicity, and participation diversity of 16.8K Reddit users after a major moderation intervention. Using a greedy feature‑selection approach, the authors identify a small set of consistently informative features and show that predictive performance varies across tasks, with activity and toxicity being easier to estimate than diversity. The results inform the design of tailored moderation strategies that consider both user traits and specific intervention objectives.<br /><strong>Summary (CN):</strong> 本文量化了 753 种社会行为、语言、关系和心理特征在 Reddit 上 16.8K 名用户在一次重大内容审查干预后行为变化（活动、毒性、参与多样性）中的预测能力。通过贪婪特征选择方法，作者找出了一小批在所有任务中始终有效的特征，并发现不同任务的预测效果不同，其中活动和毒性更易预测，而多样性较难。研究结果有助于制定兼顾用户特征和干预目标的精准审查策略。<br /><strong>Keywords:</strong> feature importance, content moderation, user behavior prediction, feature selection, Reddit, toxicity, activity, participation diversity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Benedetta Tessa, Alejandro Moreo, Stefano Cresci, Tiziano Fagni, Fabrizio Sebastiani</div>
Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of "quantification", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.
<div><strong>Authors:</strong> Benedetta Tessa, Alejandro Moreo, Stefano Cresci, Tiziano Fagni, Fabrizio Sebastiani</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper quantifies how 753 socio‑behavioural, linguistic, relational, and psychological features predict changes in activity, toxicity, and participation diversity of 16.8K Reddit users after a major moderation intervention. Using a greedy feature‑selection approach, the authors identify a small set of consistently informative features and show that predictive performance varies across tasks, with activity and toxicity being easier to estimate than diversity. The results inform the design of tailored moderation strategies that consider both user traits and specific intervention objectives.", "summary_cn": "本文量化了 753 种社会行为、语言、关系和心理特征在 Reddit 上 16.8K 名用户在一次重大内容审查干预后行为变化（活动、毒性、参与多样性）中的预测能力。通过贪婪特征选择方法，作者找出了一小批在所有任务中始终有效的特征，并发现不同任务的预测效果不同，其中活动和毒性更易预测，而多样性较难。研究结果有助于制定兼顾用户特征和干预目标的精准审查策略。", "keywords": "feature importance, content moderation, user behavior prediction, feature selection, Reddit, toxicity, activity, participation diversity", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Benedetta Tessa", "Alejandro Moreo", "Stefano Cresci", "Tiziano Fagni", "Fabrizio Sebastiani"]}
]]></acme>

<pubDate>2025-10-22T14:02:30+00:00</pubDate>
</item>
<item>
<title>Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer</title>
<link>https://papers.cool/arxiv/2510.19870</link>
<guid>https://papers.cool/arxiv/2510.19870</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents Omics-GAN, a generative adversarial network framework that synthesizes multi-omics profiles (mRNA, miRNA, DNA methylation) while preserving biological relationships, and demonstrates that classifiers trained on the synthetic data achieve higher AUCs for Alzheimer's disease and several cancers compared to using original data. It also shows that the generated data retain statistical properties, enable additional biomarker discovery via enrichment analyses, and facilitate drug rep and accelerate precision medicine pipelines.<br /><strong>Summary (CN):</strong> 本文提出 Omics-GAN 框架，利用生成对抗网络合成多组学数据（mRNA、miRNA、DNA 甲基化），在保持生物学关系的同时提升阿尔茨海默病和多种癌症的预测准确率。实验表明，基于合成数据的分类器在 AUC 上均优于使用原始数据，且合成数据保持统计分布，帮助发现额外的生物标志物并通过分子对接提出药物再利用候选，如 Nilotinib（AD）和 Atovaquone（肝癌）。该方法旨在提升疾病预测并加速精准医学研究。<br /><strong>Keywords:</strong> multi-omics, GAN, synthetic data, disease prediction, Alzheimer's, cancer, biomarker discovery<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Md Selim Reza, Sabrin Afroz, Mostafizer Rahman, Md Ashad Alam</div>
Multi-omics data integration is crucial for understanding complex diseases, yet limited sample sizes, noise, and heterogeneity often reduce predictive power. To address these challenges, we introduce Omics-GAN, a Generative Adversarial Network (GAN)-based framework designed to generate high-quality synthetic multi-omics profiles while preserving biological relationships. We evaluated Omics-GAN on three omics types (mRNA, miRNA, and DNA methylation) using the ROSMAP cohort for Alzheimer's disease (AD) and TCGA datasets for colon and liver cancer. A support vector machine (SVM) classifier with repeated 5-fold cross-validation demonstrated that synthetic datasets consistently improved prediction accuracy compared to original omics profiles. The AUC of SVM for mRNA improved from 0.72 to 0.74 in AD, and from 0.68 to 0.72 in liver cancer. Synthetic miRNA enhanced classification in colon cancer from 0.59 to 0.69, while synthetic methylation data improved performance in liver cancer from 0.64 to 0.71. Boxplot analyses confirmed that synthetic data preserved statistical distributions while reducing noise and outliers. Feature selection identified significant genes overlapping with original datasets and revealed additional candidates validated by GO and KEGG enrichment analyses. Finally, molecular docking highlighted potential drug repurposing candidates, including Nilotinib for AD, Atovaquone for liver cancer, and Tecovirimat for colon cancer. Omics-GAN enhances disease prediction, preserves biological fidelity, and accelerates biomarker and drug discovery, offering a scalable strategy for precision medicine applications.
<div><strong>Authors:</strong> Md Selim Reza, Sabrin Afroz, Mostafizer Rahman, Md Ashad Alam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents Omics-GAN, a generative adversarial network framework that synthesizes multi-omics profiles (mRNA, miRNA, DNA methylation) while preserving biological relationships, and demonstrates that classifiers trained on the synthetic data achieve higher AUCs for Alzheimer's disease and several cancers compared to using original data. It also shows that the generated data retain statistical properties, enable additional biomarker discovery via enrichment analyses, and facilitate drug rep and accelerate precision medicine pipelines.", "summary_cn": "本文提出 Omics-GAN 框架，利用生成对抗网络合成多组学数据（mRNA、miRNA、DNA 甲基化），在保持生物学关系的同时提升阿尔茨海默病和多种癌症的预测准确率。实验表明，基于合成数据的分类器在 AUC 上均优于使用原始数据，且合成数据保持统计分布，帮助发现额外的生物标志物并通过分子对接提出药物再利用候选，如 Nilotinib（AD）和 Atovaquone（肝癌）。该方法旨在提升疾病预测并加速精准医学研究。", "keywords": "multi-omics, GAN, synthetic data, disease prediction, Alzheimer's, cancer, biomarker discovery", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Md Selim Reza", "Sabrin Afroz", "Mostafizer Rahman", "Md Ashad Alam"]}
]]></acme>

<pubDate>2025-10-22T05:55:49+00:00</pubDate>
</item>
<item>
<title>Artificial Intelligence Powered Identification of Potential Antidiabetic Compounds in Ficus religiosa</title>
<link>https://papers.cool/arxiv/2510.19867</link>
<guid>https://papers.cool/arxiv/2510.19867</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper applies an AI‑driven pipeline combining machine learning, graphDock) to screen phytochemicals from Ficus religiosa for inhibition of the antidiabetic target DPP‑4. Flavonoids and alkaloids are identified as promising candidates with strong predicted binding and favorable ADMET profiles, demonstrating that AI can accelerate and improve accuracy in plant‑based drug discovery.<br /><strong>Summary (CN):</strong> 本文利用人工智能驱动的流程，结合机器学习、图卷积网络（DeepBindGCN）以及分子对接（AutoDock），筛选菩提树（Ficus religiosa）中的植物化学物质以抑制抗糖尿病关键靶点 DPP‑4。研究发现黄酮类和生物碱类化合物具有强结合力和良好 ADMET 预测，展示了 AI 在植物药物发现中加速筛选并提升准确性的潜力。<br /><strong>Keywords:</strong> antidiabetic, Ficus religiosa, DPP-4 inhibition, deep learning, molecular docking, AI drug discovery, phytochemicals<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Md Ashad Alam, Md Amanullah</div>
Diabetes mellitus is a chronic metabolic disorder that necessitates novel therapeutic innovations due to its gradual progression and the onset of various metabolic complications. Research indicates that Ficus religiosa is a conventional medicinal plant that generates bioactive phytochemicals with potential antidiabetic properties. The investigation employs ecosystem-based computational approaches utilizing artificial intelligence to investigate and evaluate compounds derived from Ficus religiosa that exhibit antidiabetic properties. A comprehensive computational procedure incorporated machine learning methodologies, molecular docking techniques, and ADMET prediction systems to assess phytochemical efficacy against the significant antidiabetic enzyme dipeptidyl peptidase-4 (DPP-4). DeepBindGCN and the AutoDock software facilitated the investigation of binding interactions via deep learning technology. Flavonoids and alkaloids have emerged as attractive phytochemicals due to their strong binding interactions and advantageous pharmacological effects, as indicated by the study. The introduction of AI accelerated screening procedures and enhanced accuracy rates, demonstrating its efficacy in researching plant-based antidiabetic agents. The scientific foundation now facilitates future experimental validation of natural product therapies tailored for diabetic management.
<div><strong>Authors:</strong> Md Ashad Alam, Md Amanullah</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper applies an AI‑driven pipeline combining machine learning, graphDock) to screen phytochemicals from Ficus religiosa for inhibition of the antidiabetic target DPP‑4. Flavonoids and alkaloids are identified as promising candidates with strong predicted binding and favorable ADMET profiles, demonstrating that AI can accelerate and improve accuracy in plant‑based drug discovery.", "summary_cn": "本文利用人工智能驱动的流程，结合机器学习、图卷积网络（DeepBindGCN）以及分子对接（AutoDock），筛选菩提树（Ficus religiosa）中的植物化学物质以抑制抗糖尿病关键靶点 DPP‑4。研究发现黄酮类和生物碱类化合物具有强结合力和良好 ADMET 预测，展示了 AI 在植物药物发现中加速筛选并提升准确性的潜力。", "keywords": "antidiabetic, Ficus religiosa, DPP-4 inhibition, deep learning, molecular docking, AI drug discovery, phytochemicals", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Md Ashad Alam", "Md Amanullah"]}
]]></acme>

<pubDate>2025-10-22T02:59:32+00:00</pubDate>
</item>
<item>
<title>SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations</title>
<link>https://papers.cool/arxiv/2510.19864</link>
<guid>https://papers.cool/arxiv/2510.19864</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines Spreadsheet Operations Documentation (SOD) as the task of generating human‑readable explanations for spreadsheet manipulation code and introduces a benchmark of 111 code–summary pairs. It evaluates several LLMs (GPT‑4o, GPT‑4o‑mini, LLaMA‑3.3‑70B, Mixtral‑8x7B, Gemma2‑9B) on the benchmark using BLEU, GLEU, ROUGE‑L, and METEOR, showing that LLMs can reliably document spreadsheet operations, though challenges remain. The work highlights SOD as a step toward better reproducibility, maintainability, and collaboration in spreadsheet‑based workflows.<br /><strong>Summary (CN):</strong> 本文提出“电子表格操作文档化”（SOD）任务，即将电子表格操作代码生成可读的自然语言说明，并构建了包含 111 条代码‑摘要对的基准数据集。作者评估了多种大型语言模型（GPT‑4o、GPT‑4o‑mini、LLaMA‑3.3‑70B、Mixtral‑8x7B、Gemma2‑9B），使用 BLEU、GLEU、ROUGE‑L 和 METEOR 等指标，结果表明这些模型能够较准确地生成电子表格文档，尽管仍存在一些挑战。该工作将 SOD 视为提升电子表格可重现性、可维护性和协作性的可行前置步骤。<br /><strong>Keywords:</strong> spreadsheet documentation, large language models, SOD, natural language summarization, code-to-text, benchmark, GPT-4o, spreadsheet automation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Amila Indika, Igor Molybog</div>
Numerous knowledge workers utilize spreadsheets in business, accounting, and finance. However, a lack of systematic documentation methods for spreadsheets hinders automation, collaboration, and knowledge transfer, which risks the loss of crucial institutional knowledge. This paper introduces Spreadsheet Operations Documentation (SOD), an AI task that involves generating human-readable explanations from spreadsheet operations. Many previous studies have utilized Large Language Models (LLMs) for generating spreadsheet manipulation code; however, translating that code into natural language for SOD is a less-explored area. To address this, we present a benchmark of 111 spreadsheet manipulation code snippets, each paired with a corresponding natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and METEOR metrics. Our findings suggest that LLMs can generate accurate spreadsheet documentation, making SOD a feasible prerequisite step toward enhancing reproducibility, maintainability, and collaborative workflows in spreadsheets, although there are challenges that need to be addressed.
<div><strong>Authors:</strong> Amila Indika, Igor Molybog</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines Spreadsheet Operations Documentation (SOD) as the task of generating human‑readable explanations for spreadsheet manipulation code and introduces a benchmark of 111 code–summary pairs. It evaluates several LLMs (GPT‑4o, GPT‑4o‑mini, LLaMA‑3.3‑70B, Mixtral‑8x7B, Gemma2‑9B) on the benchmark using BLEU, GLEU, ROUGE‑L, and METEOR, showing that LLMs can reliably document spreadsheet operations, though challenges remain. The work highlights SOD as a step toward better reproducibility, maintainability, and collaboration in spreadsheet‑based workflows.", "summary_cn": "本文提出“电子表格操作文档化”（SOD）任务，即将电子表格操作代码生成可读的自然语言说明，并构建了包含 111 条代码‑摘要对的基准数据集。作者评估了多种大型语言模型（GPT‑4o、GPT‑4o‑mini、LLaMA‑3.3‑70B、Mixtral‑8x7B、Gemma2‑9B），使用 BLEU、GLEU、ROUGE‑L 和 METEOR 等指标，结果表明这些模型能够较准确地生成电子表格文档，尽管仍存在一些挑战。该工作将 SOD 视为提升电子表格可重现性、可维护性和协作性的可行前置步骤。", "keywords": "spreadsheet documentation, large language models, SOD, natural language summarization, code-to-text, benchmark, GPT-4o, spreadsheet automation", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Amila Indika", "Igor Molybog"]}
]]></acme>

<pubDate>2025-10-22T01:36:13+00:00</pubDate>
</item>
<item>
<title>Multi-Resolution Analysis of the Convective Structure of Tropical Cyclones for Short-Term Intensity Guidance</title>
<link>https://papers.cool/arxiv/2510.19854</link>
<guid>https://papers.cool/arxiv/2510.19854</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a concise, interpretable method that uses discrete wavelet transform based multi-resolution analysis (MRA) to quantify fine-scale convective structures of tropical cyclones from satellite imagery, identifying features that strongly correlate with rapid intensity changes. It demonstrates how these physically meaningful descriptors can improve short‑term (24‑hour) intensity forecasts and serve as inputs for deep‑learning models that provide intensity guidance.<br /><strong>Summary (CN):</strong> 本文提出一种简洁且可解释的方法，利用离散小波变换的多分辨率分析（MRA）量化热带气旋卫星图像中的细尺度对流结构，识别出与快速强度变化高度相关的物理特征。研究表明，这些特征可提升 24 小时短期强度预测，并可作为深度学习模型的输入，以提供强度指导。<br /><strong>Keywords:</strong> tropical cyclone, intensity forecasting, multi-resolution analysis, discrete wavelet transform, satellite imagery, convective structure, short-term guidance, deep learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Elizabeth Cucuzzella, Tria McNeely, Kimberly Wood, Ann B. Lee</div>
Accurate tropical cyclone (TC) short-term intensity forecasting with a 24-hour lead time is essential for disaster mitigation in the Atlantic TC basin. Since most TCs evolve far from land-based observing networks, satellite imagery is critical to monitoring these storms; however, these complex and high-resolution spatial structures can be challenging to qualitatively interpret in real time by forecasters. Here we propose a concise, interpretable, and descriptive approach to quantify fine TC structures with a multi-resolution analysis (MRA) by the discrete wavelet transform, enabling data analysts to identify physically meaningful structural features that strongly correlate with rapid intensity change. Furthermore, deep-learning techniques can build on this MRA for short-term intensity guidance.
<div><strong>Authors:</strong> Elizabeth Cucuzzella, Tria McNeely, Kimberly Wood, Ann B. Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a concise, interpretable method that uses discrete wavelet transform based multi-resolution analysis (MRA) to quantify fine-scale convective structures of tropical cyclones from satellite imagery, identifying features that strongly correlate with rapid intensity changes. It demonstrates how these physically meaningful descriptors can improve short‑term (24‑hour) intensity forecasts and serve as inputs for deep‑learning models that provide intensity guidance.", "summary_cn": "本文提出一种简洁且可解释的方法，利用离散小波变换的多分辨率分析（MRA）量化热带气旋卫星图像中的细尺度对流结构，识别出与快速强度变化高度相关的物理特征。研究表明，这些特征可提升 24 小时短期强度预测，并可作为深度学习模型的输入，以提供强度指导。", "keywords": "tropical cyclone, intensity forecasting, multi-resolution analysis, discrete wavelet transform, satellite imagery, convective structure, short-term guidance, deep learning", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Elizabeth Cucuzzella", "Tria McNeely", "Kimberly Wood", "Ann B. Lee"]}
]]></acme>

<pubDate>2025-10-21T18:50:42+00:00</pubDate>
</item>
<item>
<title>DAG-Math: Graph-Guided Mathematical Reasoning in LLMs</title>
<link>https://papers.cool/arxiv/2510.19842</link>
<guid>https://papers.cool/arxiv/2510.19842</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DAG-MATH, a graph‑guided framework that models chain‑of‑thought reasoning as a stochastic process over directed acyclic graphs, defining a logical closeness metric to assess how well LLM generated derivations follow rule‑consistent structures. By creating a benchmark that requires LLMs to produce CoT trajectories in this DAG format, the authors reveal significant differences in reasoning fidelity across model families even when final answer accuracy (PASS@k) is similar. This work bridges free‑form CoT and formal proof systems, offering a new diagnostic tool for evaluating LLM mathematical reasoning.<br /><strong>Summary (CN):</strong> 本文提出 DAG‑MATH 框架，将链式思考（CoT）建模为有向无环图（DAG）上的随机过程，并引入逻辑接近度指标衡量模型生成的推理路径与图结构的规则一致性。通过构建要求 LLM 按 DAG 格式输出 CoT 的基准，作者发现即使在 PASS@k 相近的情况下，不同模型家族在推理忠实度上存在显著差异。该工作在自由形式 CoT 与形式化证明系统之间提供了平衡，为评估 LLM 数学推理提供了可操作的诊断手段。<br /><strong>Keywords:</strong> DAG, chain-of-thought, logical closeness, mathematical reasoning, LLM, benchmark, interpretability, reasoning fidelity<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 8, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yuanhe Zhang, Ilja Kuzborskij, Jason D. Lee, Chenlei Leng, Fanghui Liu</div>
Large Language Models (LLMs) demonstrate strong performance on mathematical problems when prompted with Chain-of-Thought (CoT), yet it remains unclear whether this success stems from search, rote procedures, or rule-consistent reasoning. To address this, we propose modeling CoT as a certain rule-based stochastic process over directed acyclic graphs (DAGs), where nodes represent intermediate derivation states and edges encode rule applications. Within this framework, we introduce logical closeness, a metric that quantifies how well a model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG structure, providing evaluation beyond classical PASS@k metrics. Building on this, we introduce the DAG-MATH CoT format and construct a benchmark that guides LLMs to generate CoT trajectories in this format, thereby enabling the evaluation of their reasoning ability under our framework. Across standard mathematical reasoning datasets, our analysis uncovers statistically significant differences in reasoning fidelity among representative LLM families-even when PASS@k is comparable-highlighting gaps between final-answer accuracy and rule-consistent derivation. Our framework provides a balance between free-form CoT and formal proofs systems, offering actionable diagnostics for LLMs reasoning evaluation. Our benchmark and code are available at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.
<div><strong>Authors:</strong> Yuanhe Zhang, Ilja Kuzborskij, Jason D. Lee, Chenlei Leng, Fanghui Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DAG-MATH, a graph‑guided framework that models chain‑of‑thought reasoning as a stochastic process over directed acyclic graphs, defining a logical closeness metric to assess how well LLM generated derivations follow rule‑consistent structures. By creating a benchmark that requires LLMs to produce CoT trajectories in this DAG format, the authors reveal significant differences in reasoning fidelity across model families even when final answer accuracy (PASS@k) is similar. This work bridges free‑form CoT and formal proof systems, offering a new diagnostic tool for evaluating LLM mathematical reasoning.", "summary_cn": "本文提出 DAG‑MATH 框架，将链式思考（CoT）建模为有向无环图（DAG）上的随机过程，并引入逻辑接近度指标衡量模型生成的推理路径与图结构的规则一致性。通过构建要求 LLM 按 DAG 格式输出 CoT 的基准，作者发现即使在 PASS@k 相近的情况下，不同模型家族在推理忠实度上存在显著差异。该工作在自由形式 CoT 与形式化证明系统之间提供了平衡，为评估 LLM 数学推理提供了可操作的诊断手段。", "keywords": "DAG, chain-of-thought, logical closeness, mathematical reasoning, LLM, benchmark, interpretability, reasoning fidelity", "scoring": {"interpretability": 7, "understanding": 8, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yuanhe Zhang", "Ilja Kuzborskij", "Jason D. Lee", "Chenlei Leng", "Fanghui Liu"]}
]]></acme>

<pubDate>2025-10-19T21:05:17+00:00</pubDate>
</item>
<item>
<title>Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory</title>
<link>https://papers.cool/arxiv/2510.19838</link>
<guid>https://papers.cool/arxiv/2510.19838</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents Branch-and-Browse, a framework for LLM-powered autonomous web agents that integrates tree-structured subtask management, background state replay, and a page action memory to enable controllable multi-branch reasoning and improve efficiency. Evaluated on the WebArena benchmark, the method achieves a 35.8% success rate while reducing execution time by up to 40.4 compared to previous state-of-the-art approaches.<br /><strong>Summary (CN):</strong> 本文提出了 Branch-and-Browse 框架，针对基于大型语言模型的自主网页代理，结合树结构子任务管理、后台状态重放以及页面动作记忆，实现可控的多分支推理并提升效率。在 WebArena 基准测试中，该方法实现了 35.8% 的任务成功率，并将执行时间最多降低了 40.4%，相比现有最先进方法表现更佳。<br /><strong>Keywords:</strong> LLM web agents, tree-structured reasoning, action memory, web navigation, controllable exploration, WebArena, autonomous agents, efficiency<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
Autonomous web agents powered by large language models (LLMs) show strong potential for performing goal-oriented tasks such as information retrieval, report generation, and online transactions. These agents mark a key step toward practical embodied reasoning in open web environments. However, existing approaches remain limited in reasoning depth and efficiency: vanilla linear methods fail at multi-step reasoning and lack effective backtracking, while other search strategies are coarse-grained and computationally costly. We introduce Branch-and-Browse, a fine-grained web agent framework that unifies structured reasoning-acting, contextual memory, and efficient execution. It (i) employs explicit subtask management with tree-structured exploration for controllable multi-branch reasoning, (ii) bootstraps exploration through efficient web state replay with background reasoning, and (iii) leverages a page action memory to share explored actions within and across sessions. On the WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\% and reduces execution time by up to 40.4\% relative to state-of-the-art methods. These results demonstrate that Branch-and-Browse is a reliable and efficient framework for LLM-based web agents.
<div><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents Branch-and-Browse, a framework for LLM-powered autonomous web agents that integrates tree-structured subtask management, background state replay, and a page action memory to enable controllable multi-branch reasoning and improve efficiency. Evaluated on the WebArena benchmark, the method achieves a 35.8% success rate while reducing execution time by up to 40.4 compared to previous state-of-the-art approaches.", "summary_cn": "本文提出了 Branch-and-Browse 框架，针对基于大型语言模型的自主网页代理，结合树结构子任务管理、后台状态重放以及页面动作记忆，实现可控的多分支推理并提升效率。在 WebArena 基准测试中，该方法实现了 35.8% 的任务成功率，并将执行时间最多降低了 40.4%，相比现有最先进方法表现更佳。", "keywords": "LLM web agents, tree-structured reasoning, action memory, web navigation, controllable exploration, WebArena, autonomous agents, efficiency", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shiqi He", "Yue Cui", "Xinyu Ma", "Yaliang Li", "Bolin Ding", "Mosharaf Chowdhury"]}
]]></acme>

<pubDate>2025-10-18T00:45:37+00:00</pubDate>
</item>
<item>
<title>Low-Latency Neural Inference on an Edge Device for Real-Time Handwriting Recognition from EEG Signals</title>
<link>https://papers.cool/arxiv/2510.19832</link>
<guid>https://papers.cool/arxiv/2510.19832</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces EEdGeNet, a hybrid Temporal Convolutional Network and MLP architecture that decodes imagined handwriting from non‑invasive 32‑channel EEG using 85 engineered features, achieving 89.83% accuracy with sub‑second latency on an NVIDIA Jetson TX2. By selecting ten key features the latency drops to 202 ms with minimal accuracy loss, demonstrating feasible real‑time, portable BCI communication.<br /><strong>Summary (CN):</strong> 本文提出 EEdGeNet——一种结合时序卷积网络和多层感知器的混合模型，通过对 32 通道 EEG 提取的 85 项时频图特征进行解码，实现了想象手写字符的实时识别，在 NVIDIA Jetson TX2 上达到 89.83% 的准确率和约 914 ms 的单字符延迟。精选十个关键特征后，延迟降至 202 ms，准确率几乎不受影响，展示了非侵入式 BCI 在便携实时通信中的可行性。<br /><strong>Keywords:</strong> EEG, brain-computer interface, handwritten character decoding, low-latency inference, edge device, Temporal Convolutional Network, feature extraction, real-time BCI<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ovishake Sen, Raghav Soni, Darpan Virmani, Akshar Parekh, Patrick Lehman, Sarthak Jena, Adithi Katikhaneni, Adam Khalifa, Baibhab Chatterjee</div>
Brain-computer interfaces (BCIs) offer a pathway to restore communication for individuals with severe motor or speech impairments. Imagined handwriting provides an intuitive paradigm for character-level neural decoding, bridging the gap between human intention and digital communication. While invasive approaches such as electrocorticography (ECoG) achieve high accuracy, their surgical risks limit widespread adoption. Non-invasive electroencephalography (EEG) offers safer and more scalable alternatives but suffers from low signal-to-noise ratio and spatial resolution, constraining its decoding precision. This work demonstrates that advanced machine learning combined with informative EEG feature extraction can overcome these barriers, enabling real-time, high-accuracy neural decoding on portable edge devices. A 32-channel EEG dataset was collected from fifteen participants performing imagined handwriting. Signals were preprocessed with bandpass filtering and artifact subspace reconstruction, followed by extraction of 85 time-, frequency-, and graphical-domain features. A hybrid architecture, EEdGeNet, integrates a Temporal Convolutional Network with a multilayer perceptron trained on the extracted features. When deployed on an NVIDIA Jetson TX2, the system achieved 89.83 percent accuracy with 914.18 ms per-character latency. Selecting only ten key features reduced latency by 4.5 times to 202.6 ms with less than 1 percent loss in accuracy. These results establish a pathway for accurate, low-latency, and fully portable non-invasive BCIs supporting real-time communication.
<div><strong>Authors:</strong> Ovishake Sen, Raghav Soni, Darpan Virmani, Akshar Parekh, Patrick Lehman, Sarthak Jena, Adithi Katikhaneni, Adam Khalifa, Baibhab Chatterjee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces EEdGeNet, a hybrid Temporal Convolutional Network and MLP architecture that decodes imagined handwriting from non‑invasive 32‑channel EEG using 85 engineered features, achieving 89.83% accuracy with sub‑second latency on an NVIDIA Jetson TX2. By selecting ten key features the latency drops to 202 ms with minimal accuracy loss, demonstrating feasible real‑time, portable BCI communication.", "summary_cn": "本文提出 EEdGeNet——一种结合时序卷积网络和多层感知器的混合模型，通过对 32 通道 EEG 提取的 85 项时频图特征进行解码，实现了想象手写字符的实时识别，在 NVIDIA Jetson TX2 上达到 89.83% 的准确率和约 914 ms 的单字符延迟。精选十个关键特征后，延迟降至 202 ms，准确率几乎不受影响，展示了非侵入式 BCI 在便携实时通信中的可行性。", "keywords": "EEG, brain-computer interface, handwritten character decoding, low-latency inference, edge device, Temporal Convolutional Network, feature extraction, real-time BCI", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ovishake Sen", "Raghav Soni", "Darpan Virmani", "Akshar Parekh", "Patrick Lehman", "Sarthak Jena", "Adithi Katikhaneni", "Adam Khalifa", "Baibhab Chatterjee"]}
]]></acme>

<pubDate>2025-10-07T21:20:50+00:00</pubDate>
</item>
<item>
<title>SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks</title>
<link>https://papers.cool/arxiv/2510.19829</link>
<guid>https://papers.cool/arxiv/2510.19829</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents SSL-SE-EEG, a framework that combines self-supervised learning with squeeze-and-excitation networks to transform EEG signals into 2D representations and improve robustness to noise and data scarcity. Experiments on four public EEG datasets show state-of-the-art classification accuracies, demonstrating its suitability for real-time brain-computer interface applications.<br /><strong>Summary (CN):</strong> 本文提出 SSL-SE-EEG 框架，将自监督学习与 Squeeze‑and‑Excitation 网络结合，将 EEG 信号转化为二维图像表示，从而提升特征提取的噪声鲁棒性并降低对标注数据的依赖。对 MindBigData、TUH‑AB、SEED‑IV 与 BCI‑IV 四个数据集的实验显示其分类准确率达到业界领先水平，适用于实时脑机接口应用。<br /><strong>Keywords:</strong> self-supervised learning, squeeze-and-excitation networks, EEG, brain-computer interface, noise robustness, feature extraction, deep learning, biomedical signal processing<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Meghna Roy Chowdhury, Yi Ding, Shreyas Sen</div>
Electroencephalography (EEG) plays a crucial role in brain-computer interfaces (BCIs) and neurological diagnostics, but its real-world deployment faces challenges due to noise artifacts, missing data, and high annotation costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance feature extraction, improve noise robustness, and reduce reliance on labeled data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG signals into structured 2D image representations, suitable for deep learning. Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB), making it well-suited for real-time BCI applications. By enabling low-power, scalable EEG processing, SSL-SE-EEG presents a promising solution for biomedical signal analysis, neural engineering, and next-generation BCIs.
<div><strong>Authors:</strong> Meghna Roy Chowdhury, Yi Ding, Shreyas Sen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents SSL-SE-EEG, a framework that combines self-supervised learning with squeeze-and-excitation networks to transform EEG signals into 2D representations and improve robustness to noise and data scarcity. Experiments on four public EEG datasets show state-of-the-art classification accuracies, demonstrating its suitability for real-time brain-computer interface applications.", "summary_cn": "本文提出 SSL-SE-EEG 框架，将自监督学习与 Squeeze‑and‑Excitation 网络结合，将 EEG 信号转化为二维图像表示，从而提升特征提取的噪声鲁棒性并降低对标注数据的依赖。对 MindBigData、TUH‑AB、SEED‑IV 与 BCI‑IV 四个数据集的实验显示其分类准确率达到业界领先水平，适用于实时脑机接口应用。", "keywords": "self-supervised learning, squeeze-and-excitation networks, EEG, brain-computer interface, noise robustness, feature extraction, deep learning, biomedical signal processing", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Meghna Roy Chowdhury", "Yi Ding", "Shreyas Sen"]}
]]></acme>

<pubDate>2025-10-07T06:37:34+00:00</pubDate>
</item>
<item>
<title>Neurotremor: A wearable Supportive Device for Supporting Upper Limb Muscle Function</title>
<link>https://papers.cool/arxiv/2510.19826</link>
<guid>https://papers.cool/arxiv/2510.19826</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents Neurotremor, a wearable prototype that assists upper‑limb muscle function by fusing surface EMG, inertial measurement, and flex/force sensors with an on‑device INT8 TensorFlow Lite micro model. Real‑time features are extracted and control commands are bounded by a control‑barrier‑function safety envelope, leading to reduced tremor and improved movement metrics in a pilot feasibility study with healthy volunteers. The system operates at 100 Hz with low latency, full session completion, and no device‑related adverse events, with patient trials planned.<br /><strong>Summary (CN):</strong> 本文介绍了 Neurotremor种可穿戴原型，通过融合表面肌电 (sEMG)、惯性测量单元 (IMU) 与弯曲/力传感器，并在 MStickC+ ESP32‑S3 上运行 INT8 TensorFlow Lite Micro 模型，实现上肢肌肉功能的辅助。系统在 250 ms 窗口内实时提取特征，并通过控制屏障函数 (control‑barrier‑function) 安全包络约束控制指令，在健康志愿者的可行性试验中显著降低颤抖并提升运动范围和重复次数。该装置以 100 Hz 运行，延迟约 8.7 ms，实验期间无设备相关不良事件，计划在伦理审查下进行患者试验。<br /><strong>Keywords:</strong> wearable assistive device, upper limb, sEMG, sensor fusion, TensorFlow Lite Micro, control barrier function, embedded inference, rehabilitation, real-time control<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 4, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Aueaphum Aueawattthanaphisut, Thanyanee Srichaisak, Arissa Ieochai</div>
A sensor-fused wearable assistance prototype for upper-limb function (triceps brachii and extensor pollicis brevis) is presented. The device integrates surface electromyography (sEMG), an inertial measurement unit (IMU), and flex/force sensors on an M5StickC plus an ESP32-S3 compute hub. Signals are band-pass and notch filtered; features (RMS, MAV, zero-crossings, and 4-12 Hz tremor-band power) are computed in 250 ms windows and fed to an INT8 TensorFlow Lite Micro model. Control commands are bounded by a control-barrier-function safety envelope and delivered within game-based tasks with lightweight personalization. In a pilot technical feasibility evaluation with healthy volunteers (n = 12) performing three ADL-oriented tasks, tremor prominence decreased (Delta TI = -0.092, 95% CI [-0.102, -0.079]), range of motion increased (+12.65%, 95% CI [+8.43, +13.89]), repetitions rose (+2.99 min^-1, 95% CI [+2.61, +3.35]), and the EMG median-frequency slope became less negative (Delta = +0.100 Hz/min, 95% CI [+0.083, +0.127]). The sensing-to-assist loop ran at 100 Hz with 8.7 ms median on-device latency, 100% session completion, and 0 device-related adverse events. These results demonstrate technical feasibility of embedded, sensor-fused assistance for upper-limb function; formal patient studies under IRB oversight are planned.
<div><strong>Authors:</strong> Aueaphum Aueawattthanaphisut, Thanyanee Srichaisak, Arissa Ieochai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents Neurotremor, a wearable prototype that assists upper‑limb muscle function by fusing surface EMG, inertial measurement, and flex/force sensors with an on‑device INT8 TensorFlow Lite micro model. Real‑time features are extracted and control commands are bounded by a control‑barrier‑function safety envelope, leading to reduced tremor and improved movement metrics in a pilot feasibility study with healthy volunteers. The system operates at 100 Hz with low latency, full session completion, and no device‑related adverse events, with patient trials planned.", "summary_cn": "本文介绍了 Neurotremor种可穿戴原型，通过融合表面肌电 (sEMG)、惯性测量单元 (IMU) 与弯曲/力传感器，并在 MStickC+ ESP32‑S3 上运行 INT8 TensorFlow Lite Micro 模型，实现上肢肌肉功能的辅助。系统在 250 ms 窗口内实时提取特征，并通过控制屏障函数 (control‑barrier‑function) 安全包络约束控制指令，在健康志愿者的可行性试验中显著降低颤抖并提升运动范围和重复次数。该装置以 100 Hz 运行，延迟约 8.7 ms，实验期间无设备相关不良事件，计划在伦理审查下进行患者试验。", "keywords": "wearable assistive device, upper limb, sEMG, sensor fusion, TensorFlow Lite Micro, control barrier function, embedded inference, rehabilitation, real-time control", "scoring": {"interpretability": 2, "understanding": 4, "safety": 4, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Aueaphum Aueawattthanaphisut", "Thanyanee Srichaisak", "Arissa Ieochai"]}
]]></acme>

<pubDate>2025-10-02T00:07:09+00:00</pubDate>
</item>
<item>
<title>Spectral Thresholds in Correlated Spiked Models and Fundamental Limits of Partial Least Squares</title>
<link>https://papers.cool/arxiv/2510.17561</link>
<guid>https://papers.cool/arxiv/2510.17561</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a rigorous random matrix theory analysis of spiked cross-covariance models where signals in two high-dimensional data channels are partially aligned, a setting underlying Partial Least Squares (PLS). It shows that the leading singular values of the sample cross-covariance matrix exhibit a BBP-type phase transition and derives exact thresholds for the emergence of informative components, revealing a fundamental performance gap between PLS and the Bayes-optimal estimator. The results delineate regimes where PLS cannot recover any signal despite detectability being theoretically possible, thus clarifying the theoretical limits of PLS for multi‑modal inference.<br /><strong>Summary (CN):</strong> 本文对两个高维数据通道中部分对齐信号的尖刺交叉协方差模型进行严格的随机矩阵理论分析，该模型是偏最小二乘（PLS）方法的标准生成设定。研究表明样本交叉协方差矩阵的主奇异值出现 BBP 类型的相变，并精确刻画了信息成分出现的阈值，揭示了 PLS 与贝叶斯最优估计器之间的根本性能差距。结果指出在某些信噪比和相关性 regime 下，PLS 无法恢复任何信号，即使理论上可以检测到，从而阐明了 PLS 在多模态推断中的理论极限。<br /><strong>Keywords:</strong> spiked covariance, cross-covariance, partial least squares, random matrix theory, BBP phase transition, signal recovery, high-dimensional statistics, multi-modal learning, Bayes-optimal estimator<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 9, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Pierre Mergny, Lenka Zdeborová</div>
We provide a rigorous random matrix theory analysis of spiked cross-covariance models where the signals across two high-dimensional data channels are partially aligned. These models are motivated by multi-modal learning and form the standard generative setting underlying Partial Least Squares (PLS), a widely used yet theoretically underdeveloped method. We show that the leading singular values of the sample cross-covariance matrix undergo a Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the precise thresholds for the emergence of informative components. Our results yield the first sharp asymptotic description of the signal recovery capabilities of PLS in this setting, revealing a fundamental performance gap between PLS and the Bayes-optimal estimator. In particular, we identify the SNR and correlation regimes where PLS fails to recover any signal, despite detectability being possible in principle. These findings clarify the theoretical limits of PLS and provide guidance for the design of reliable multi-modal inference methods in high dimensions.
<div><strong>Authors:</strong> Pierre Mergny, Lenka Zdeborová</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a rigorous random matrix theory analysis of spiked cross-covariance models where signals in two high-dimensional data channels are partially aligned, a setting underlying Partial Least Squares (PLS). It shows that the leading singular values of the sample cross-covariance matrix exhibit a BBP-type phase transition and derives exact thresholds for the emergence of informative components, revealing a fundamental performance gap between PLS and the Bayes-optimal estimator. The results delineate regimes where PLS cannot recover any signal despite detectability being theoretically possible, thus clarifying the theoretical limits of PLS for multi‑modal inference.", "summary_cn": "本文对两个高维数据通道中部分对齐信号的尖刺交叉协方差模型进行严格的随机矩阵理论分析，该模型是偏最小二乘（PLS）方法的标准生成设定。研究表明样本交叉协方差矩阵的主奇异值出现 BBP 类型的相变，并精确刻画了信息成分出现的阈值，揭示了 PLS 与贝叶斯最优估计器之间的根本性能差距。结果指出在某些信噪比和相关性 regime 下，PLS 无法恢复任何信号，即使理论上可以检测到，从而阐明了 PLS 在多模态推断中的理论极限。", "keywords": "spiked covariance, cross-covariance, partial least squares, random matrix theory, BBP phase transition, signal recovery, high-dimensional statistics, multi-modal learning, Bayes-optimal estimator", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 9, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Pierre Mergny", "Lenka Zdeborová"]}
]]></acme>

<pubDate>2025-10-20T14:08:58+00:00</pubDate>
</item>
<item>
<title>Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</title>
<link>https://papers.cool/arxiv/2510.18874</link>
<guid>https://papers.cool/arxiv/2510.18874</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper compares catastrophic forgetting in language models when fine‑tuned with supervised learning versus reinforcement learning, finding that RL with on‑policy data preserves prior knowledge better while achieving strong task performance. A simple mixture‑of‑distributions analysis shows the mode‑seeking nature of RL as the cause, and experiments confirm that on‑policy data—not KL regularization or advantage estimation—drives reduced forgetting. The authors suggest that (approximate) on‑policy data can be an efficient practical tool for mitigating forgetting during post‑training.<br /><strong>Summary (CN):</strong> 本文比较了在语言模型的后训练阶段使用监督微调（SFT）与强化学习（RL）时出现的灾难性遗忘现象，发现使用 RL 并利用 on‑policy 数据能够更好地保留已有知识，同时实现较高的目标任务性能。通过一个简化的混合分布模型，作者解释了 RL 的模式寻找特性是降低遗忘的根本原因，并通过实验证明，on‑policy 数据而非 KL 正则或优势估计是实现鲁棒性的关键。最后指出，获取（近似）on‑policy 数据是一种高效的实用手段，可用于缓解后训练过程中的遗忘。<br /><strong>Keywords:</strong> catastrophic forgetting, reinforcement learning, supervised fine-tuning, on-policy data, language models, continual learning, mixture model, retention, KL regularization, advantage estimation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.
<div><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper compares catastrophic forgetting in language models when fine‑tuned with supervised learning versus reinforcement learning, finding that RL with on‑policy data preserves prior knowledge better while achieving strong task performance. A simple mixture‑of‑distributions analysis shows the mode‑seeking nature of RL as the cause, and experiments confirm that on‑policy data—not KL regularization or advantage estimation—drives reduced forgetting. The authors suggest that (approximate) on‑policy data can be an efficient practical tool for mitigating forgetting during post‑training.", "summary_cn": "本文比较了在语言模型的后训练阶段使用监督微调（SFT）与强化学习（RL）时出现的灾难性遗忘现象，发现使用 RL 并利用 on‑policy 数据能够更好地保留已有知识，同时实现较高的目标任务性能。通过一个简化的混合分布模型，作者解释了 RL 的模式寻找特性是降低遗忘的根本原因，并通过实验证明，on‑policy 数据而非 KL 正则或优势估计是实现鲁棒性的关键。最后指出，获取（近似）on‑policy 数据是一种高效的实用手段，可用于缓解后训练过程中的遗忘。", "keywords": "catastrophic forgetting, reinforcement learning, supervised fine-tuning, on-policy data, language models, continual learning, mixture model, retention, KL regularization, advantage estimation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Howard Chen", "Noam Razin", "Karthik Narasimhan", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-21T17:59:41+00:00</pubDate>
</item>
<item>
<title>A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure</title>
<link>https://papers.cool/arxiv/2510.18841</link>
<guid>https://papers.cool/arxiv/2510.18841</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a hybrid enumeration and optimization framework for generating optimal counterfactual explanations in a clinical setting, applied to post‑acute sequelae of COVID‑19 in patients with pre‑existing heart failure. By integrating regularized predictive models with the NICE and Multi‑Objective Counterfactual algorithms, the method efficiently explores high‑dimensional intervention spaces and produces patient‑specific, interpretable counterfactuals that indicate how changes in comorbidities or treatments could affect predicted heart‑failure admissions. Experiments on over 2700 patients achieve high discriminative performance (AUROC 0.88) and demonstrate the approach’s utility for personalized risk estimation and intervention analysis.<br /><strong>Summary (CN):</strong> 本文提出一种混合枚举与优化框架，用于在临床环境中生成最优反事实解释，聚焦于新冠后遗症患者中已有心衰的病例。通过将正则化预测模型与 NICE 与多目标反事实（MOC）算法相结合，能够高效搜索高维干预空间，生成患者特定的可解释反事实，量化改变共病模式或治疗因素对心衰住院风险的影响。对2700余名患者的实验显示模型具有较高的判别能力（AUROC 0.88），验证了该方法在个性化风险评估和干预分析中的价值。<br /><strong>Keywords:</strong> counterfactual explanations, causal inference, personalized medicine, heart failure, COVID-19, optimization, enumeration, predictive modeling, interpretable AI, healthcare<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</div>
Counterfactual inference provides a mathematical framework for reasoning about hypothetical outcomes under alternative interventions, bridging causal reasoning and predictive modeling. We present a counterfactual inference framework for individualized risk estimation and intervention analysis, illustrated through a clinical application to post-acute sequelae of COVID-19 (PASC) among patients with pre-existing heart failure (HF). Using longitudinal diagnosis, laboratory, and medication data from a large health-system cohort, we integrate regularized predictive modeling with counterfactual search to identify actionable pathways to PASC-related HF hospital admissions. The framework combines exact enumeration with optimization-based methods, including the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional intervention spaces. Applied to more than 2700 individuals with confirmed SARS-CoV-2 infection and prior HF, the model achieved strong discriminative performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable, patient-specific counterfactuals that quantify how modifying comorbidity patterns or treatment factors could alter predicted outcomes. This work demonstrates how counterfactual reasoning can be formalized as an optimization problem over predictive functions, offering a rigorous, interpretable, and computationally efficient approach to personalized inference in complex biomedical systems.
<div><strong>Authors:</strong> Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a hybrid enumeration and optimization framework for generating optimal counterfactual explanations in a clinical setting, applied to post‑acute sequelae of COVID‑19 in patients with pre‑existing heart failure. By integrating regularized predictive models with the NICE and Multi‑Objective Counterfactual algorithms, the method efficiently explores high‑dimensional intervention spaces and produces patient‑specific, interpretable counterfactuals that indicate how changes in comorbidities or treatments could affect predicted heart‑failure admissions. Experiments on over 2700 patients achieve high discriminative performance (AUROC 0.88) and demonstrate the approach’s utility for personalized risk estimation and intervention analysis.", "summary_cn": "本文提出一种混合枚举与优化框架，用于在临床环境中生成最优反事实解释，聚焦于新冠后遗症患者中已有心衰的病例。通过将正则化预测模型与 NICE 与多目标反事实（MOC）算法相结合，能够高效搜索高维干预空间，生成患者特定的可解释反事实，量化改变共病模式或治疗因素对心衰住院风险的影响。对2700余名患者的实验显示模型具有较高的判别能力（AUROC 0.88），验证了该方法在个性化风险评估和干预分析中的价值。", "keywords": "counterfactual explanations, causal inference, personalized medicine, heart failure, COVID-19, optimization, enumeration, predictive modeling, interpretable AI, healthcare", "scoring": {"interpretability": 7, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jingya Cheng", "Alaleh Azhir", "Jiazi Tian", "Hossein Estiri"]}
]]></acme>

<pubDate>2025-10-21T17:35:12+00:00</pubDate>
</item>
<item>
<title>Actor-Free Continuous Control via Structurally Maximizable Q-Functions</title>
<link>https://papers.cool/arxiv/2510.18828</link>
<guid>https://papers.cool/arxiv/2510.18828</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes an actor‑free, purely value‑based framework for continuous‑control reinforcement learning by introducing structurally maximizable Q‑functions, enabling efficient maximization of Q‑values without a separate actor. The approach achieves performance and sample efficiency comparable to state‑of‑the‑art actor‑critic baselines, especially in environments with constrained, non‑smooth action spaces. Code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出一种无演员、纯价值的连续控制强化学习框架，通过结构化可最大化的 Q 函数实现对 Q 值的高效最大化，无需单独的演员网络。该方法在性能和样本效率上与最先进的演员‑评论家基线相当，特别在约束且非光滑的动作空间环境中表现更佳。已公开代码。<br /><strong>Keywords:</strong> value-based RL, continuous control, Q-function maximization, actor-free, off-policy learning, structural maximization, sample efficiency, constrained action spaces, reinforcement learning, Q-learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yigit Korkmaz, Urvi Bhuwania, Ayush Jain, Erdem Bıyık</div>
Value-based algorithms are a cornerstone of off-policy reinforcement learning due to their simplicity and training stability. However, their use has traditionally been restricted to discrete action spaces, as they rely on estimating Q-values for individual state-action pairs. In continuous action spaces, evaluating the Q-value over the entire action space becomes computationally infeasible. To address this, actor-critic methods are typically employed, where a critic is trained on off-policy data to estimate Q-values, and an actor is trained to maximize the critic's output. Despite their popularity, these methods often suffer from instability during training. In this work, we propose a purely value-based framework for continuous control that revisits structural maximization of Q-functions, introducing a set of key architectural and algorithmic choices to enable efficient and stable learning. We evaluate the proposed actor-free Q-learning approach on a range of standard simulation tasks, demonstrating performance and sample efficiency on par with state-of-the-art baselines, without the cost of learning a separate actor. Particularly, in environments with constrained action spaces, where the value functions are typically non-smooth, our method with structural maximization outperforms traditional actor-critic methods with gradient-based maximization. We have released our code at https://github.com/USC-Lira/Q3C.
<div><strong>Authors:</strong> Yigit Korkmaz, Urvi Bhuwania, Ayush Jain, Erdem Bıyık</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes an actor‑free, purely value‑based framework for continuous‑control reinforcement learning by introducing structurally maximizable Q‑functions, enabling efficient maximization of Q‑values without a separate actor. The approach achieves performance and sample efficiency comparable to state‑of‑the‑art actor‑critic baselines, especially in environments with constrained, non‑smooth action spaces. Code is released publicly.", "summary_cn": "本文提出一种无演员、纯价值的连续控制强化学习框架，通过结构化可最大化的 Q 函数实现对 Q 值的高效最大化，无需单独的演员网络。该方法在性能和样本效率上与最先进的演员‑评论家基线相当，特别在约束且非光滑的动作空间环境中表现更佳。已公开代码。", "keywords": "value-based RL, continuous control, Q-function maximization, actor-free, off-policy learning, structural maximization, sample efficiency, constrained action spaces, reinforcement learning, Q-learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yigit Korkmaz", "Urvi Bhuwania", "Ayush Jain", "Erdem Bıyık"]}
]]></acme>

<pubDate>2025-10-21T17:24:27+00:00</pubDate>
</item>
<item>
<title>BO4Mob: Bayesian Optimization Benchmarks for High-Dimensional Urban Mobility Problem</title>
<link>https://papers.cool/arxiv/2510.18824</link>
<guid>https://papers.cool/arxiv/2510.18824</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BO4Mob is a benchmark suite for high-dimensional Bayesian Optimization applied to origin-destination travel demand estimation in large urban road networks, featuring up to 10,100 continuous variables and realistic stochastic traffic simulations based on San Jose, CA. The benchmark includes five real-world scenarios and evaluates three state-of-the-art BO algorithms against two non-BO baselines, highlighting challenges of expensive, non-differentiable objective evaluations. It aims to foster scalable optimization methods for data-driven urban mobility models and digital twins.<br /><strong>Summary (CN):</strong> BO4Mob 是一个针对大规模城市道路网络中起点‑终点（OD）出行需求估计的高维贝叶斯优化基准套件，涵盖最高 10,100 维连续变量，并使用基于加州圣何塞的真实交通仿真，捕捉非线性、随机的交通动态。该基准提供五种实际场景，比较了三种最新的 BO 算法与两种非 BO 基线，以展示昂贵、不可微目标评估的挑战。旨在推动可扩展的优化方法在数据驱动的城市出行模型和数字孪生中的应用。<br /><strong>Keywords:</strong> Bayesian optimization, high-dimensional optimization, urban mobility, origin-destination demand estimation, traffic simulation, benchmark, digital twin<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Seunghee Ryu, Donghoon Kwon, Seongjin Choi, Aryan Deshwal, Seungmo Kang, Carolina Osorio</div>
We introduce \textbf{BO4Mob}, a new benchmark framework for high-dimensional Bayesian Optimization (BO), driven by the challenge of origin-destination (OD) travel demand estimation in large urban road networks. Estimating OD travel demand from limited traffic sensor data is a difficult inverse optimization problem, particularly in real-world, large-scale transportation networks. This problem involves optimizing over high-dimensional continuous spaces where each objective evaluation is computationally expensive, stochastic, and non-differentiable. BO4Mob comprises five scenarios based on real-world San Jose, CA road networks, with input dimensions scaling up to 10,100. These scenarios utilize high-resolution, open-source traffic simulations that incorporate realistic nonlinear and stochastic dynamics. We demonstrate the benchmark's utility by evaluating five optimization methods: three state-of-the-art BO algorithms and two non-BO baselines. This benchmark is designed to support both the development of scalable optimization algorithms and their application for the design of data-driven urban mobility models, including high-resolution digital twins of metropolitan road networks. Code and documentation are available at https://github.com/UMN-Choi-Lab/BO4Mob.
<div><strong>Authors:</strong> Seunghee Ryu, Donghoon Kwon, Seongjin Choi, Aryan Deshwal, Seungmo Kang, Carolina Osorio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BO4Mob is a benchmark suite for high-dimensional Bayesian Optimization applied to origin-destination travel demand estimation in large urban road networks, featuring up to 10,100 continuous variables and realistic stochastic traffic simulations based on San Jose, CA. The benchmark includes five real-world scenarios and evaluates three state-of-the-art BO algorithms against two non-BO baselines, highlighting challenges of expensive, non-differentiable objective evaluations. It aims to foster scalable optimization methods for data-driven urban mobility models and digital twins.", "summary_cn": "BO4Mob 是一个针对大规模城市道路网络中起点‑终点（OD）出行需求估计的高维贝叶斯优化基准套件，涵盖最高 10,100 维连续变量，并使用基于加州圣何塞的真实交通仿真，捕捉非线性、随机的交通动态。该基准提供五种实际场景，比较了三种最新的 BO 算法与两种非 BO 基线，以展示昂贵、不可微目标评估的挑战。旨在推动可扩展的优化方法在数据驱动的城市出行模型和数字孪生中的应用。", "keywords": "Bayesian optimization, high-dimensional optimization, urban mobility, origin-destination demand estimation, traffic simulation, benchmark, digital twin", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Seunghee Ryu", "Donghoon Kwon", "Seongjin Choi", "Aryan Deshwal", "Seungmo Kang", "Carolina Osorio"]}
]]></acme>

<pubDate>2025-10-21T17:22:28+00:00</pubDate>
</item>
<item>
<title>Search Self-play: Pushing the Frontier of Agent Capability without Supervision</title>
<link>https://papers.cool/arxiv/2510.18821</link>
<guid>https://papers.cool/arxiv/2510.18821</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Search Self‑Play (SSP), a framework where a large language model simultaneously generates deep search queries (task proposer) and solves them (problem solver), using the proposer’s retrieved documents as ground‑truth for verifiable rewards. By iteratively co‑evolving both roles through competition and cooperation, SSP achieves substantial performance gains on various search benchmarks without any human‑provided supervision. Experiments show the method works both from scratch and with continuous reinforcement‑learning updates.<br /><strong>Summary (CN):</strong> 本文提出搜索自博弈（Search Self‑Play, SSP）框架，让大型语言模型同时担任任务提议者和问题求解者，通过提议者的检索文档生成可靠的真实答案，以实现可验证奖励的强化学习。提议者与求解者在竞争与合作中共同进化，显著提升搜索代理在多项基准上的表现，无需任何人工监督。实验表明该方法在从零训练和持续 RL 训练两种设置下均有效。<br /><strong>Keywords:</strong> search self-play, RL with verifiable rewards, LLM agents, task synthesis, retrieval-augmented generation, unsupervised RL training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang</div>
Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, RLVR highly depends on well-crafted task queries and corresponding ground-truth answers to provide accurate rewards, which requires massive human efforts and hinders the RL scaling processes, especially under agentic scenarios. Although a few recent works explore task synthesis methods, the difficulty of generated agentic tasks can hardly be controlled to provide effective RL training advantages. To achieve agentic RLVR with higher scalability, we explore self-play training for deep search agents, in which the learning LLM utilizes multi-turn search engine calling and acts simultaneously as both a task proposer and a problem solver. The task proposer aims to generate deep search queries with well-defined ground-truth answers and increasing task difficulty. The problem solver tries to handle the generated search queries and output the correct answer predictions. To ensure that each generated search query has accurate ground truth, we collect all the searching results from the proposer's trajectory as external knowledge, then conduct retrieval-augmentation generation (RAG) to test whether the proposed query can be correctly answered with all necessary search documents provided. In this search self-play (SSP) game, the proposer and the solver co-evolve their agent capabilities through both competition and cooperation. With substantial experimental results, we find that SSP can significantly improve search agents' performance uniformly on various benchmarks without any supervision under both from-scratch and continuous RL training setups. The code is at https://github.com/Alibaba-Quark/SSP.
<div><strong>Authors:</strong> Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Search Self‑Play (SSP), a framework where a large language model simultaneously generates deep search queries (task proposer) and solves them (problem solver), using the proposer’s retrieved documents as ground‑truth for verifiable rewards. By iteratively co‑evolving both roles through competition and cooperation, SSP achieves substantial performance gains on various search benchmarks without any human‑provided supervision. Experiments show the method works both from scratch and with continuous reinforcement‑learning updates.", "summary_cn": "本文提出搜索自博弈（Search Self‑Play, SSP）框架，让大型语言模型同时担任任务提议者和问题求解者，通过提议者的检索文档生成可靠的真实答案，以实现可验证奖励的强化学习。提议者与求解者在竞争与合作中共同进化，显著提升搜索代理在多项基准上的表现，无需任何人工监督。实验表明该方法在从零训练和持续 RL 训练两种设置下均有效。", "keywords": "search self-play, RL with verifiable rewards, LLM agents, task synthesis, retrieval-augmented generation, unsupervised RL training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Hongliang Lu", "Yuhang Wen", "Pengyu Cheng", "Ruijin Ding", "Haotian Xu", "Jiaqi Guo", "Chutian Wang", "Haonan Chen", "Xiaoxi Jiang", "Guanjun Jiang"]}
]]></acme>

<pubDate>2025-10-21T17:19:35+00:00</pubDate>
</item>
<item>
<title>Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards</title>
<link>https://papers.cool/arxiv/2510.18814</link>
<guid>https://papers.cool/arxiv/2510.18814</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Online Supervised Fine‑Tuning (OSFT), a simple reward‑free paradigm where a large language model generates its own answers and is immediately fine‑tuned on this self‑generated data. Experiments on challenging mathematical reasoning benchmarks show that OSFT attains performance comparable to strong reinforcement‑learning‑with‑verifiable‑rewards methods such as GRPO while using only a single rollout per example. An ablation study attributes the gains to the model's latent preferences learned during pretraining, highlighting OSFT as an efficient alternative to complex reward‑based training.<br /><strong>Summary (CN):</strong> 本文提出在线监督微调（OSFT）范式，即让大型语言模型自行生成答案并立即在这些自生成数据上进行微调，整个过程无需奖励信号。实验在挑战性的数学推理基准上显示，OSFT 只使用一次采样即可实现与强大的可验证奖励强化学习方法（如 GRPO）相当的性能。消融实验表明，效果提升来源于模型在预训练阶段已学习的潜在偏好，凸显 OSFT 作为一种高效的、无需复杂奖励机制的训练替代方案。<br /><strong>Keywords:</strong> online supervised finetuning, self-tuning, LLM reasoning, reward-free training, RLVR, GRPO, mathematical reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</div>
We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the model's own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at https://github.com/ElementQi/OnlineSFT.
<div><strong>Authors:</strong> Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Online Supervised Fine‑Tuning (OSFT), a simple reward‑free paradigm where a large language model generates its own answers and is immediately fine‑tuned on this self‑generated data. Experiments on challenging mathematical reasoning benchmarks show that OSFT attains performance comparable to strong reinforcement‑learning‑with‑verifiable‑rewards methods such as GRPO while using only a single rollout per example. An ablation study attributes the gains to the model's latent preferences learned during pretraining, highlighting OSFT as an efficient alternative to complex reward‑based training.", "summary_cn": "本文提出在线监督微调（OSFT）范式，即让大型语言模型自行生成答案并立即在这些自生成数据上进行微调，整个过程无需奖励信号。实验在挑战性的数学推理基准上显示，OSFT 只使用一次采样即可实现与强大的可验证奖励强化学习方法（如 GRPO）相当的性能。消融实验表明，效果提升来源于模型在预训练阶段已学习的潜在偏好，凸显 OSFT 作为一种高效的、无需复杂奖励机制的训练替代方案。", "keywords": "online supervised finetuning, self-tuning, LLM reasoning, reward-free training, RLVR, GRPO, mathematical reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mengqi Li", "Lei Zhao", "Anthony Man-Cho So", "Ruoyu Sun", "Xiao Li"]}
]]></acme>

<pubDate>2025-10-21T17:15:56+00:00</pubDate>
</item>
<item>
<title>A Unified Perspective on Optimization in Machine Learning and Neuroscience: From Gradient Descent to Neural Adaptation</title>
<link>https://papers.cool/arxiv/2510.18812</link>
<guid>https://papers.cool/arxiv/2510.18812</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The review unifies optimization theory across machine learning and neuroscience, contrasting gradient-based methods that rely on backpropagation with derivative‑free (zeroth‑order) approaches that use only function evaluations and randomness. It surveys how zeroth‑order techniques can approximate gradients for neural network training and draws parallels between these methods and biological learning mechanisms such as random exploration and feedback‑guided adaptation, emphasizing implications for neuromorphic hardware.<br /><strong>Summary (CN):</strong> 本文综述了机器学习与神经科学中的优化方法，比较了依赖反向传播的梯度基方法与仅需函数评估的零阶（zeroth‑order）优化。文章阐述了零阶技巧如何在神经网络训练中近似梯度，并将其与生物学习中的随机探索和反馈引导的适应机制相类比，探讨了对神经形态硬件的潜在意义。<br /><strong>Keywords:</strong> optimization, gradient descent, backpropagation, zeroth-order optimization, neural adaptation, biological learning, neuromorphic hardware, stochastic optimization, derivative-free methods<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jesús García Fernández, Nasir Ahmad, Marcel van Gerven</div>
Iterative optimization is central to modern artificial intelligence (AI) and provides a crucial framework for understanding adaptive systems. This review provides a unified perspective on this subject, bridging classic theory with neural network training and biological learning. Although gradient-based methods, powered by the efficient but biologically implausible backpropagation (BP), dominate machine learning, their computational demands can hinder scalability in high-dimensional settings. In contrast, derivative-free or zeroth-order (ZO) optimization feature computationally lighter approaches that rely only on function evaluations and randomness. While generally less sample efficient, recent breakthroughs demonstrate that modern ZO methods can effectively approximate gradients and achieve performance competitive with BP in neural network models. This ZO paradigm is also particularly relevant for biology. Its core principles of random exploration (probing) and feedback-guided adaptation (reinforcing) parallel key mechanisms of biological learning, offering a mathematically principled perspective on how the brain learns. In this review, we begin by categorizing optimization approaches based on the order of derivative information they utilize, ranging from first-, second-, and higher-order gradient-based to ZO methods. We then explore how these methods are adapted to the unique challenges of neural network training and the resulting learning dynamics. Finally, we build upon these insights to view biological learning through an optimization lens, arguing that a ZO paradigm leverages the brain's intrinsic noise as a computational resource. This framework not only illuminates our understanding of natural intelligence but also holds vast implications for neuromorphic hardware, helping us design fast and energy-efficient AI systems that exploit intrinsic hardware noise.
<div><strong>Authors:</strong> Jesús García Fernández, Nasir Ahmad, Marcel van Gerven</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The review unifies optimization theory across machine learning and neuroscience, contrasting gradient-based methods that rely on backpropagation with derivative‑free (zeroth‑order) approaches that use only function evaluations and randomness. It surveys how zeroth‑order techniques can approximate gradients for neural network training and draws parallels between these methods and biological learning mechanisms such as random exploration and feedback‑guided adaptation, emphasizing implications for neuromorphic hardware.", "summary_cn": "本文综述了机器学习与神经科学中的优化方法，比较了依赖反向传播的梯度基方法与仅需函数评估的零阶（zeroth‑order）优化。文章阐述了零阶技巧如何在神经网络训练中近似梯度，并将其与生物学习中的随机探索和反馈引导的适应机制相类比，探讨了对神经形态硬件的潜在意义。", "keywords": "optimization, gradient descent, backpropagation, zeroth-order optimization, neural adaptation, biological learning, neuromorphic hardware, stochastic optimization, derivative-free methods", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jesús García Fernández", "Nasir Ahmad", "Marcel van Gerven"]}
]]></acme>

<pubDate>2025-10-21T17:10:15+00:00</pubDate>
</item>
<item>
<title>When LRP Diverges from Leave-One-Out in Transformers</title>
<link>https://papers.cool/arxiv/2510.18810</link>
<guid>https://papers.cool/arxiv/2510.18810</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates why Layer‑Wise Relevance Propagation (LRP) often fails to match the intuitive feature‑importance scores produced by Leave‑One‑Out (LOO) in modern Transformer models. It shows analytically and empirically that bilinear propagation rules used in recent AttnLRP violate the implementation invariance axiom, and demonstrates that ignoring relevance flow through the softmax (propagating only through value matrices) markedly improves alignment with LOO, especially in middle‑to‑late layers.<br /><strong>Summary (CN):</strong> 本文研究了在现代 Transformer 中，层级相关传播 (LRP) 为什么常常与直观的留一法 (LOO) 特征重要性得分不一致。研究表明，近期 AttnLRP 中使用的双线性传播规则违背实现不变性公理；此外，实验发现如果在软最大层不传播相关性，仅通过值矩阵进行反向传播，可显著提升 LRP 与 LOO 的对齐，尤其在中后层表现更佳。<br /><strong>Keywords:</strong> LRP, leave-one-out, transformers, feature importance, implementation invariance, softmax propagation, interpretability, attention mechanisms<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Weiqiu You, Siqi Zeng, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao</div>
Leave-One-Out (LOO) provides an intuitive measure of feature importance but is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP) offers a potentially efficient alternative, its axiomatic soundness in modern Transformers remains largely under-examined. In this work, we first show that the bilinear propagation rules used in recent advances of AttnLRP violate the implementation invariance axiom. We prove this analytically and confirm it empirically in linear attention layers. Second, we also revisit CP-LRP as a diagnostic baseline and find that bypassing relevance propagation through the softmax layer -- backpropagating relevance only through the value matrices -- significantly improves alignment with LOO, particularly in middle-to-late Transformer layers. Overall, our results suggest that (i) bilinear factorization sensitivity and (ii) softmax propagation error potentially jointly undermine LRP's ability to approximate LOO in Transformers.
<div><strong>Authors:</strong> Weiqiu You, Siqi Zeng, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates why Layer‑Wise Relevance Propagation (LRP) often fails to match the intuitive feature‑importance scores produced by Leave‑One‑Out (LOO) in modern Transformer models. It shows analytically and empirically that bilinear propagation rules used in recent AttnLRP violate the implementation invariance axiom, and demonstrates that ignoring relevance flow through the softmax (propagating only through value matrices) markedly improves alignment with LOO, especially in middle‑to‑late layers.", "summary_cn": "本文研究了在现代 Transformer 中，层级相关传播 (LRP) 为什么常常与直观的留一法 (LOO) 特征重要性得分不一致。研究表明，近期 AttnLRP 中使用的双线性传播规则违背实现不变性公理；此外，实验发现如果在软最大层不传播相关性，仅通过值矩阵进行反向传播，可显著提升 LRP 与 LOO 的对齐，尤其在中后层表现更佳。", "keywords": "LRP, leave-one-out, transformers, feature importance, implementation invariance, softmax propagation, interpretability, attention mechanisms", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Weiqiu You", "Siqi Zeng", "Yao-Hung Hubert Tsai", "Makoto Yamada", "Han Zhao"]}
]]></acme>

<pubDate>2025-10-21T17:06:05+00:00</pubDate>
</item>
<item>
<title>On Biologically Plausible Learning in Continuous Time</title>
<link>https://papers.cool/arxiv/2510.18808</link>
<guid>https://papers.cool/arxiv/2510.18808</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a continuous‑time neural model that unifies several biologically plausible learning rules (SGD, FA, DFA, KP) and eliminates the need for separate inference and learning phases. Through analysis and simulations it shows that successful learning requires temporal overlap between inputs and error signals, predicting that synaptic plasticity windows must be orders of magnitude longer than stimulus durations. This leads to a testable prediction that seconds‑scale eligibility traces are necessary for error‑driven learning in cortical circuits.<br /><strong>Summary (CN):</strong> 本文提出一种连续时间神经模型，统一了多种生物学上可行的学习规则（如 SGD、反馈对齐（FA）、直接反馈对齐（DFA）和 Kolen‑Pollack（KP）），并消除了推理与学习阶段的分离。通过理论分析和仿真研究，展示了学习成功依赖于输入与误差信号的时间重叠，预测突触可塑性窗口必须比刺激持续时间长若干数量级。该模型提出了可检验的预言：在皮层电路中，错误驱动学习需要秒级的资格迹（eligibility traces）。<br /><strong>Keywords:</strong> continuous-time learning, biologically plausible learning, eligibility traces, feedback alignment, stochastic gradient descent, Kolen-Pollack, synaptic plasticity, temporal overlap, neural dynamics, error-driven learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Marc Gong Bacvanski, Liu Ziyin, Tomaso Poggio</div>
Biological learning unfolds continuously in time, yet most algorithmic models rely on discrete updates and separate inference and learning phases. We study a continuous-time neural model that unifies several biologically plausible learning algorithms and removes the need for phase separation. Rules including stochastic gradient descent (SGD), feedback alignment (FA), direct feedback alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of the dynamics. Simulations show that these continuous-time networks stably learn at biological timescales, even under temporal mismatches and integration noise. Through analysis and simulation, we show that learning depends on temporal overlap: a synapse updates correctly only when its input and the corresponding error signal coincide in time. When inputs are held constant, learning strength declines linearly as the delay between input and error approaches the stimulus duration, explaining observed robustness and failure across network depths. Critically, robust learning requires the synaptic plasticity timescale to exceed the stimulus duration by one to two orders of magnitude. For typical cortical stimuli (tens of milliseconds), this places the functional plasticity window in the few-second range, a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits.
<div><strong>Authors:</strong> Marc Gong Bacvanski, Liu Ziyin, Tomaso Poggio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a continuous‑time neural model that unifies several biologically plausible learning rules (SGD, FA, DFA, KP) and eliminates the need for separate inference and learning phases. Through analysis and simulations it shows that successful learning requires temporal overlap between inputs and error signals, predicting that synaptic plasticity windows must be orders of magnitude longer than stimulus durations. This leads to a testable prediction that seconds‑scale eligibility traces are necessary for error‑driven learning in cortical circuits.", "summary_cn": "本文提出一种连续时间神经模型，统一了多种生物学上可行的学习规则（如 SGD、反馈对齐（FA）、直接反馈对齐（DFA）和 Kolen‑Pollack（KP）），并消除了推理与学习阶段的分离。通过理论分析和仿真研究，展示了学习成功依赖于输入与误差信号的时间重叠，预测突触可塑性窗口必须比刺激持续时间长若干数量级。该模型提出了可检验的预言：在皮层电路中，错误驱动学习需要秒级的资格迹（eligibility traces）。", "keywords": "continuous-time learning, biologically plausible learning, eligibility traces, feedback alignment, stochastic gradient descent, Kolen-Pollack, synaptic plasticity, temporal overlap, neural dynamics, error-driven learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Marc Gong Bacvanski", "Liu Ziyin", "Tomaso Poggio"]}
]]></acme>

<pubDate>2025-10-21T17:04:06+00:00</pubDate>
</item>
<item>
<title>Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams</title>
<link>https://papers.cool/arxiv/2510.18786</link>
<guid>https://papers.cool/arxiv/2510.18786</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SB-SETM, a novel online extension of the Embedded Topic Model that uses a truncated stick-breaking process to adaptively determine the number of active topics and merges topic embeddings across time steps via a continuous optimal transport formulation. This enables effective analysis of evolving document streams, demonstrated on simulated data and a real-world news corpus covering the Russian-Ukrainian war. Experiments show SB-SETM outperforms existing baselines in tracking topic dynamics over time.<br /><strong>Summary (CN):</strong> 本文提出 SB-SETM，一种基于嵌入式主题模型的在线扩展，通过截断 Stick-Breaking 过程自适应推断每个时间步的活跃主题数，并利用连续最优传输方法合并跨时间的主题嵌入，实现对文档流的动态分析。实验在模拟情景和涵盖 2022-2023 年俄乌战争的新闻数据集上验证了其相较于现有基线的优越性能。<br /><strong>Keywords:</strong> online topic modeling, stick-breaking, embedded topic model, optimal transport, streaming documents, dynamic topics, latent space alignment, document streams<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Federica Granese, Serena Villata, Charles Bouveyron</div>
Online topic models are unsupervised algorithms to identify latent topics in data streams that continuously evolve over time. Although these methods naturally align with real-world scenarios, they have received considerably less attention from the community compared to their offline counterparts, due to specific additional challenges. To tackle these issues, we present SB-SETM, an innovative model extending the Embedded Topic Model (ETM) to process data streams by merging models formed on successive partial document batches. To this end, SB-SETM (i) leverages a truncated stick-breaking construction for the topic-per-document distribution, enabling the model to automatically infer from the data the appropriate number of active topics at each timestep; and (ii) introduces a merging strategy for topic embeddings based on a continuous formulation of optimal transport adapted to the high dimensionality of the latent topic space. Numerical experiments show SB-SETM outperforming baselines on simulated scenarios. We extensively test it on a real-world corpus of news articles covering the Russian-Ukrainian war throughout 2022-2023.
<div><strong>Authors:</strong> Federica Granese, Serena Villata, Charles Bouveyron</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SB-SETM, a novel online extension of the Embedded Topic Model that uses a truncated stick-breaking process to adaptively determine the number of active topics and merges topic embeddings across time steps via a continuous optimal transport formulation. This enables effective analysis of evolving document streams, demonstrated on simulated data and a real-world news corpus covering the Russian-Ukrainian war. Experiments show SB-SETM outperforms existing baselines in tracking topic dynamics over time.", "summary_cn": "本文提出 SB-SETM，一种基于嵌入式主题模型的在线扩展，通过截断 Stick-Breaking 过程自适应推断每个时间步的活跃主题数，并利用连续最优传输方法合并跨时间的主题嵌入，实现对文档流的动态分析。实验在模拟情景和涵盖 2022-2023 年俄乌战争的新闻数据集上验证了其相较于现有基线的优越性能。", "keywords": "online topic modeling, stick-breaking, embedded topic model, optimal transport, streaming documents, dynamic topics, latent space alignment, document streams", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Federica Granese", "Serena Villata", "Charles Bouveyron"]}
]]></acme>

<pubDate>2025-10-21T16:40:14+00:00</pubDate>
</item>
<item>
<title>CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training</title>
<link>https://papers.cool/arxiv/2510.18784</link>
<guid>https://papers.cool/arxiv/2510.18784</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes CAGE, a curvature-aware gradient correction for quantization-aware training that augments the straight-through estimator to mitigate loss increases caused by low-bit quantization. By formulating QAT as a multi‑objective optimization problem and deriving a Pareto‑optimal correction term using local curvature, CAGE provides optimizer‑agnostic, theoretically‑grounded updates with strong convergence guarantees. Experiments on Llama‑style models up to 800M parameters show that CAGE recovers more than 10% of the quantization‑induced loss in the W4A4 regime compared to existing outlier‑mitigation methods.<br /><strong>Summary (CN):</strong> 本文提出了 CAGE，一种基于曲率的梯度校正方法，用于量化感知训练（QAT），在直通估计器（STE）梯度上加入校正以抵消低比特量化导致的损失增加。通过将 QAT 建模为多目标优化并利用局部曲率信息推导出 Pareto 最优校正项，CAGE 在保持优化器无关的同时提供了理论收敛保证。实验在最高 800M 参数的 Llama 系列模型上进行，显示在 W4A4 设定下相较于现有异常值处理方法，CAGE 能恢复超过 10% 的量化引起的损失。<br /><strong>Keywords:</strong> quantization-aware training, curvature-aware gradient, straight-through estimator, Pareto-optimal, low-bit quantization, optimizer-agnostic, Llama, W4A4<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Soroush Tabesh, Mher Safaryan, Dan Alistarh</div>
Despite significant work on low-bit quantization-aware training (QAT), there is still a large accuracy gap between such techniques and native training. To address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new QAT method that augments the straight-through estimator (STE) gradient with a curvature-aware correction designed to counteract the loss increase induced by quantization. CAGE is derived from a multi-objective view of QAT that balances loss minimization with adherence to quantization constraints, yielding a principled correction term that depends on local curvature information. On the theoretical side, we introduce the notion of Pareto-optimal solutions for quantized optimization, and establish that CAGE yields strong convergence guarantees in the smooth non-convex setting. In terms of implementation, our approach is optimizer-agnostic, but we provide a highly-efficient implementation that leverages Adam statistics. When pre-training Llama-style models of up to 800M-parameters, CAGE recovers over 10% of the quantization-induced loss increase in the W4A4 regime over outlier-mitigation methods. These results indicate that curvature-aware gradient corrections can bridge the remaining performance gap beyond current outlier-handling methods.
<div><strong>Authors:</strong> Soroush Tabesh, Mher Safaryan, Dan Alistarh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes CAGE, a curvature-aware gradient correction for quantization-aware training that augments the straight-through estimator to mitigate loss increases caused by low-bit quantization. By formulating QAT as a multi‑objective optimization problem and deriving a Pareto‑optimal correction term using local curvature, CAGE provides optimizer‑agnostic, theoretically‑grounded updates with strong convergence guarantees. Experiments on Llama‑style models up to 800M parameters show that CAGE recovers more than 10% of the quantization‑induced loss in the W4A4 regime compared to existing outlier‑mitigation methods.", "summary_cn": "本文提出了 CAGE，一种基于曲率的梯度校正方法，用于量化感知训练（QAT），在直通估计器（STE）梯度上加入校正以抵消低比特量化导致的损失增加。通过将 QAT 建模为多目标优化并利用局部曲率信息推导出 Pareto 最优校正项，CAGE 在保持优化器无关的同时提供了理论收敛保证。实验在最高 800M 参数的 Llama 系列模型上进行，显示在 W4A4 设定下相较于现有异常值处理方法，CAGE 能恢复超过 10% 的量化引起的损失。", "keywords": "quantization-aware training, curvature-aware gradient, straight-through estimator, Pareto-optimal, low-bit quantization, optimizer-agnostic, Llama, W4A4", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Soroush Tabesh", "Mher Safaryan", "Dan Alistarh"]}
]]></acme>

<pubDate>2025-10-21T16:33:57+00:00</pubDate>
</item>
<item>
<title>Enhancing Fractional Gradient Descent with Learned Optimizers</title>
<link>https://papers.cool/arxiv/2510.18783</link>
<guid>https://papers.cool/arxiv/2510.18783</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces L2O-CFGD, a meta-learned optimizer that dynamically tunes the hyperparameters of Caputo fractional gradient descent, addressing convergence and scheduling challenges in non-convex settings. Experiments show that the learned schedule outperforms static hyperparameter choices and can rival fully black-box meta-learned optimizers on several tasks. The approach also provides insights into leveraging the history-dependent nature of fractional differentials for optimization.<br /><strong>Summary (CN):</strong> 本文提出 L2O-CFGD，一种元学习优化器，用于动态调节 Caputo 分数梯度下降（Caputo Fractional Gradient Descent, CFGD）的超参数，以解决非凸情境下的收敛性和调度难题。实验表明，该学习得到的调度方案优于通过广泛搜索得到的静态超参数，并在部分任务上可与全黑盒元学习优化器相媲美。该方法还帮助理解如何利用分数微分的历史依赖性来提升优化性能。<br /><strong>Keywords:</strong> fractional gradient descent, learned optimizer, meta-learning, Caputo derivative, hyperparameter scheduling, optimization, neural network training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jan Sobotka, Petr Šimánek, Pavel Kordík</div>
Fractional Gradient Descent (FGD) offers a novel and promising way to accelerate optimization by incorporating fractional calculus into machine learning. Although FGD has shown encouraging initial results across various optimization tasks, it faces significant challenges with convergence behavior and hyperparameter selection. Moreover, the impact of its hyperparameters is not fully understood, and scheduling them is particularly difficult in non-convex settings such as neural network training. To address these issues, we propose a novel approach called Learning to Optimize Caputo Fractional Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule outperforms CFGD with static hyperparameters found through an extensive search and, in some tasks, achieves performance comparable to a fully black-box meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for researchers to identify high-performing hyperparameters and gain insights on how to leverage the history-dependence of the fractional differential in optimization.
<div><strong>Authors:</strong> Jan Sobotka, Petr Šimánek, Pavel Kordík</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces L2O-CFGD, a meta-learned optimizer that dynamically tunes the hyperparameters of Caputo fractional gradient descent, addressing convergence and scheduling challenges in non-convex settings. Experiments show that the learned schedule outperforms static hyperparameter choices and can rival fully black-box meta-learned optimizers on several tasks. The approach also provides insights into leveraging the history-dependent nature of fractional differentials for optimization.", "summary_cn": "本文提出 L2O-CFGD，一种元学习优化器，用于动态调节 Caputo 分数梯度下降（Caputo Fractional Gradient Descent, CFGD）的超参数，以解决非凸情境下的收敛性和调度难题。实验表明，该学习得到的调度方案优于通过广泛搜索得到的静态超参数，并在部分任务上可与全黑盒元学习优化器相媲美。该方法还帮助理解如何利用分数微分的历史依赖性来提升优化性能。", "keywords": "fractional gradient descent, learned optimizer, meta-learning, Caputo derivative, hyperparameter scheduling, optimization, neural network training", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jan Sobotka", "Petr Šimánek", "Pavel Kordík"]}
]]></acme>

<pubDate>2025-10-21T16:33:20+00:00</pubDate>
</item>
<item>
<title>Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference</title>
<link>https://papers.cool/arxiv/2510.18768</link>
<guid>https://papers.cool/arxiv/2510.18768</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies key desiderata for synthetic medical datasets used in treatment effect analysis—preserving covariate distribution, treatment assignment, and outcome generation—and introduces evaluation metrics for these criteria. Based on this, it proposes STEAM, a generative method that explicitly models the data-generating process to satisfy the desiderata, achieving state-of-the-art performance on the proposed metrics, especially as the underlying causal complexity grows.<br /><strong>Summary (CN):</strong> 本文确定了用于医学治疗效应分析的合成数据的关键需求：保持协变量分布、治疗分配机制和结果生成机制，并提出了相应的评估指标。在此基础上，作者提出了 STEAM 方法，该生成模型显式模拟数据生成过程以满足这些需求，在所设指标上实现了最新水平的表现，尤其在真实数据生成过程复杂度提升时表现突出。<br /><strong>Keywords:</strong> synthetic data, causal inference, treatment effect, generative models, STEAM, medical data, evaluation metrics, covariate distribution, treatment assignment, outcome generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Harry Amad, Zhaozhi Qian, Dennis Frauen, Julianna Piskorz, Stefan Feuerriegel, Mihaela van der Schaar</div>
Causal inference is essential for developing and evaluating medical interventions, yet real-world medical datasets are often difficult to access due to regulatory barriers. This makes synthetic data a potentially valuable asset that enables these medical analyses, along with the development of new inference methods themselves. Generative models can produce synthetic data that closely approximate real data distributions, yet existing methods do not consider the unique challenges that downstream causal inference tasks, and specifically those focused on treatments, pose. We establish a set of desiderata that synthetic data containing treatments should satisfy to maximise downstream utility: preservation of (i) the covariate distribution, (ii) the treatment assignment mechanism, and (iii) the outcome generation mechanism. Based on these desiderata, we propose a set of evaluation metrics to assess such synthetic data. Finally, we present STEAM: a novel method for generating Synthetic data for Treatment Effect Analysis in Medicine that mimics the data-generating process of data containing treatments and optimises for our desiderata. We empirically demonstrate that STEAM achieves state-of-the-art performance across our metrics as compared to existing generative models, particularly as the complexity of the true data-generating process increases.
<div><strong>Authors:</strong> Harry Amad, Zhaozhi Qian, Dennis Frauen, Julianna Piskorz, Stefan Feuerriegel, Mihaela van der Schaar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies key desiderata for synthetic medical datasets used in treatment effect analysis—preserving covariate distribution, treatment assignment, and outcome generation—and introduces evaluation metrics for these criteria. Based on this, it proposes STEAM, a generative method that explicitly models the data-generating process to satisfy the desiderata, achieving state-of-the-art performance on the proposed metrics, especially as the underlying causal complexity grows.", "summary_cn": "本文确定了用于医学治疗效应分析的合成数据的关键需求：保持协变量分布、治疗分配机制和结果生成机制，并提出了相应的评估指标。在此基础上，作者提出了 STEAM 方法，该生成模型显式模拟数据生成过程以满足这些需求，在所设指标上实现了最新水平的表现，尤其在真实数据生成过程复杂度提升时表现突出。", "keywords": "synthetic data, causal inference, treatment effect, generative models, STEAM, medical data, evaluation metrics, covariate distribution, treatment assignment, outcome generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Harry Amad", "Zhaozhi Qian", "Dennis Frauen", "Julianna Piskorz", "Stefan Feuerriegel", "Mihaela van der Schaar"]}
]]></acme>

<pubDate>2025-10-21T16:16:00+00:00</pubDate>
</item>
<item>
<title>Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</title>
<link>https://papers.cool/arxiv/2510.18713</link>
<guid>https://papers.cool/arxiv/2510.18713</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies online preference-based reinforcement learning with ranking feedback, introducing the M-AUPO algorithm that selects multiple actions to maximize average uncertainty under a Plackett-Luce model. It provides the first theoretical guarantees showing that larger feedback subsets improve sample efficiency, achieving a suboptimality bound that scales with the inverse square root of subset size and a matching lower bound. These results advance the understanding of how richer preference information can accelerate alignment of large language models.<br /><strong>Summary (CN):</strong> 本文研究了在线基于偏好的强化学习（PbRL）在使用排名反馈时的样本效率提升，提出了在 Plackett-Luce 模型下通过最大化子集平均不确定性来选择多动作的 M-AUPO 算法。论文给出了首次理论保证，证明更大的反馈子集能够直接提升样本效率，提供了随子集大小的平方根倒数缩放的次优差距上界以及相匹配的下界。该工作深化了我们对利用更丰富偏好信息加速大语言模型对齐的理解。<br /><strong>Keywords:</strong> preference-based reinforcement learning, Plackett-Luce, ranking feedback, sample efficiency, online learning, LLM alignment, subset selection, M-AUPO<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.
<div><strong>Authors:</strong> Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies online preference-based reinforcement learning with ranking feedback, introducing the M-AUPO algorithm that selects multiple actions to maximize average uncertainty under a Plackett-Luce model. It provides the first theoretical guarantees showing that larger feedback subsets improve sample efficiency, achieving a suboptimality bound that scales with the inverse square root of subset size and a matching lower bound. These results advance the understanding of how richer preference information can accelerate alignment of large language models.", "summary_cn": "本文研究了在线基于偏好的强化学习（PbRL）在使用排名反馈时的样本效率提升，提出了在 Plackett-Luce 模型下通过最大化子集平均不确定性来选择多动作的 M-AUPO 算法。论文给出了首次理论保证，证明更大的反馈子集能够直接提升样本效率，提供了随子集大小的平方根倒数缩放的次优差距上界以及相匹配的下界。该工作深化了我们对利用更丰富偏好信息加速大语言模型对齐的理解。", "keywords": "preference-based reinforcement learning, Plackett-Luce, ranking feedback, sample efficiency, online learning, LLM alignment, subset selection, M-AUPO", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Joongkyu Lee", "Seouh-won Yi", "Min-hwan Oh"]}
]]></acme>

<pubDate>2025-10-21T15:11:01+00:00</pubDate>
</item>
<item>
<title>OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales</title>
<link>https://papers.cool/arxiv/2510.18707</link>
<guid>https://papers.cool/arxiv/2510.18707</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> OmniCast introduces a masked latent diffusion model that combines a VAE encoder with a diffusion‑based transformer to produce probabilistic weather forecasts across medium‑range to subseasonal‑to‑seasonal horizons. By masking future latent tokens during training and jointly sampling space‑time tokens during inference, the approach reduces error accumulation and achieves state‑of‑the‑art performance while being orders of magnitude faster than autoregressive baselines. The model also demonstrates stable rollouts up to a century ahead, highlighting its scalability for long‑term climate prediction.<br /><strong>Summary (CN):</strong> OmniCast 提出了一种掩码潜在扩散模型，将 VAE 编码器与基于扩散的 Transformer 相结合，以生成跨中期到亚季节‑季节时间尺度的概率天气预报。通过在训练期间掩码未来潜在标记并在推理时联合采样时空标记，该方法降低了自回归模型的误差累积，并在保持预测精度的同时实现了数十倍的速度提升。实验表明，该模型能够稳定生成最长达 100 年的天气演化序列，展现了在长期气候预测中的可扩展性。<br /><strong>Keywords:</strong> masked diffusion, latent diffusion model, weather forecasting, subseasonal-to-seasonal, transformer, VAE, probabilistic forecasting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tung Nguyen, Tuan Pham, Troy Arcomano, Veerabhadra Kotamarthi, Ian Foster, Sandeep Madireddy, Aditya Grover</div>
Accurate weather forecasting across time scales is critical for anticipating and mitigating the impacts of climate change. Recent data-driven methods based on deep learning have achieved significant success in the medium range, but struggle at longer subseasonal-to-seasonal (S2S) horizons due to error accumulation in their autoregressive approach. In this work, we propose OmniCast, a scalable and skillful probabilistic model that unifies weather forecasting across timescales. OmniCast consists of two components: a VAE model that encodes raw weather data into a continuous, lower-dimensional latent space, and a diffusion-based transformer model that generates a sequence of future latent tokens given the initial conditioning tokens. During training, we mask random future tokens and train the transformer to estimate their distribution given conditioning and visible tokens using a per-token diffusion head. During inference, the transformer generates the full sequence of future tokens by iteratively unmasking random subsets of tokens. This joint sampling across space and time mitigates compounding errors from autoregressive approaches. The low-dimensional latent space enables modeling long sequences of future latent states, allowing the transformer to learn weather dynamics beyond initial conditions. OmniCast performs competitively with leading probabilistic methods at the medium-range timescale while being 10x to 20x faster, and achieves state-of-the-art performance at the subseasonal-to-seasonal scale across accuracy, physics-based, and probabilistic metrics. Furthermore, we demonstrate that OmniCast can generate stable rollouts up to 100 years ahead. Code and model checkpoints are available at https://github.com/tung-nd/omnicast.
<div><strong>Authors:</strong> Tung Nguyen, Tuan Pham, Troy Arcomano, Veerabhadra Kotamarthi, Ian Foster, Sandeep Madireddy, Aditya Grover</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "OmniCast introduces a masked latent diffusion model that combines a VAE encoder with a diffusion‑based transformer to produce probabilistic weather forecasts across medium‑range to subseasonal‑to‑seasonal horizons. By masking future latent tokens during training and jointly sampling space‑time tokens during inference, the approach reduces error accumulation and achieves state‑of‑the‑art performance while being orders of magnitude faster than autoregressive baselines. The model also demonstrates stable rollouts up to a century ahead, highlighting its scalability for long‑term climate prediction.", "summary_cn": "OmniCast 提出了一种掩码潜在扩散模型，将 VAE 编码器与基于扩散的 Transformer 相结合，以生成跨中期到亚季节‑季节时间尺度的概率天气预报。通过在训练期间掩码未来潜在标记并在推理时联合采样时空标记，该方法降低了自回归模型的误差累积，并在保持预测精度的同时实现了数十倍的速度提升。实验表明，该模型能够稳定生成最长达 100 年的天气演化序列，展现了在长期气候预测中的可扩展性。", "keywords": "masked diffusion, latent diffusion model, weather forecasting, subseasonal-to-seasonal, transformer, VAE, probabilistic forecasting", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tung Nguyen", "Tuan Pham", "Troy Arcomano", "Veerabhadra Kotamarthi", "Ian Foster", "Sandeep Madireddy", "Aditya Grover"]}
]]></acme>

<pubDate>2025-10-20T17:48:27+00:00</pubDate>
</item>
<item>
<title>Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach</title>
<link>https://papers.cool/arxiv/2510.18687</link>
<guid>https://papers.cool/arxiv/2510.18687</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Bayesian value function to enable tractable optimal policies in reinforcement learning when agents have access to imperfect multi-step transition predictions. It proposes a Bellman-Jensen Gap analysis to quantify the value of such predictions and presents BOLA, a two-stage model-based RL algorithm that learns offline and adapts online, with theoretical sample-efficiency guarantees and empirical validation on synthetic tasks and a wind-energy storage control problem.<br /><strong>Summary (CN):</strong> 本文提出了贝叶斯价值函数，以在强化学习中处理具有不完美多步转移预测的代理，实现可求解的最优策略。通过 Bellman-Jensen Gap 分析量化这些预测的价值，并引入 BOLA（Bayesian Offline Learning with Online Adaptation）两阶段模型式 RL 算法，在离线学习后进行轻量级在线适配，理论上保持样本效率，并在合成 MDP 和实际风能储能控制任务上进行验证。<br /><strong>Keywords:</strong> reinforcement learning, multi-step predictions, Bayesian value function, Bellman-Jensen Gap, model-based RL, sample efficiency, offline learning, online adaptation, wind energy storage<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Chenbei Lu, Zaiwei Chen, Tongxin Li, Chenye Wu, Adam Wierman</div>
Traditional reinforcement learning (RL) assumes the agents make decisions based on Markov decision processes (MDPs) with one-step transition models. In many real-world applications, such as energy management and stock investment, agents can access multi-step predictions of future states, which provide additional advantages for decision making. However, multi-step predictions are inherently high-dimensional: naively embedding these predictions into an MDP leads to an exponential blow-up in state space and the curse of dimensionality. Moreover, existing RL theory provides few tools to analyze prediction-augmented MDPs, as it typically works on one-step transition kernels and cannot accommodate multi-step predictions with errors or partial action-coverage. We address these challenges with three key innovations: First, we propose the \emph{Bayesian value function} to characterize the optimal prediction-aware policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis on the Bayesian value function, which enables characterizing the value of imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with Online Adaptation), a two-stage model-based RL algorithm that separates offline Bayesian value learning from lightweight online adaptation to real-time predictions. We prove that BOLA remains sample-efficient even under imperfect predictions. We validate our theory and algorithm on synthetic MDPs and a real-world wind energy storage control problem.
<div><strong>Authors:</strong> Chenbei Lu, Zaiwei Chen, Tongxin Li, Chenye Wu, Adam Wierman</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Bayesian value function to enable tractable optimal policies in reinforcement learning when agents have access to imperfect multi-step transition predictions. It proposes a Bellman-Jensen Gap analysis to quantify the value of such predictions and presents BOLA, a two-stage model-based RL algorithm that learns offline and adapts online, with theoretical sample-efficiency guarantees and empirical validation on synthetic tasks and a wind-energy storage control problem.", "summary_cn": "本文提出了贝叶斯价值函数，以在强化学习中处理具有不完美多步转移预测的代理，实现可求解的最优策略。通过 Bellman-Jensen Gap 分析量化这些预测的价值，并引入 BOLA（Bayesian Offline Learning with Online Adaptation）两阶段模型式 RL 算法，在离线学习后进行轻量级在线适配，理论上保持样本效率，并在合成 MDP 和实际风能储能控制任务上进行验证。", "keywords": "reinforcement learning, multi-step predictions, Bayesian value function, Bellman-Jensen Gap, model-based RL, sample efficiency, offline learning, online adaptation, wind energy storage", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Chenbei Lu", "Zaiwei Chen", "Tongxin Li", "Chenye Wu", "Adam Wierman"]}
]]></acme>

<pubDate>2025-10-21T14:47:08+00:00</pubDate>
</item>
<item>
<title>Learning Task-Agnostic Representations through Multi-Teacher Distillation</title>
<link>https://papers.cool/arxiv/2510.18680</link>
<guid>https://papers.cool/arxiv/2510.18680</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a task-agnostic distillation framework that uses a majority-vote objective to combine diverse teacher embeddings across modalities, deriving a loss bounded by mutual information between student and teachers. Experiments on text, vision, and molecular data show that the resulting student representations improve performance on various downstream tasks such as classification, clustering, and regression without requiring task-specific labels.<br /><strong>Summary (CN):</strong> 本文提出一种任务无关的多教师蒸馏框架，采用“多数投票”目标函数，将文本、视觉和分子等不同模态的教师嵌入进行融合，并证明该目标函数被学生与教师嵌入之间的互信息所界定。实验表明，得到的学生表示能够在无需任务标签的情况下提升分类、聚类和回归等多种下游任务的性能。<br /><strong>Keywords:</strong> multi-teacher distillation, task-agnostic representation, embedding, mutual information, knowledge distillation, modality-agnostic, representation learning, downstream tasks, classification, clustering<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Philippe Formont, Maxime Darrin, Banafsheh Karimian, Jackie CK Cheung, Eric Granger, Ismail Ben Ayed, Mohammadhadi Shateri, Pablo Piantanida</div>
Casting complex inputs into tractable representations is a critical step across various fields. Diverse embedding models emerge from differences in architectures, loss functions, input modalities and datasets, each capturing unique aspects of the input. Multi-teacher distillation leverages this diversity to enrich representations but often remains tailored to specific tasks. In this paper, we introduce a task-agnostic framework based on a ``majority vote" objective function. We demonstrate that this function is bounded by the mutual information between student and teachers' embeddings, leading to a task-agnostic distillation loss that eliminates dependence on task-specific labels or prior knowledge. Our evaluations across text, vision models, and molecular modeling show that our method effectively leverages teacher diversity, resulting in representations enabling better performance for a wide range of downstream tasks such as classification, clustering, or regression. Additionally, we train and release state-of-the-art embedding models, enhancing downstream performance in various modalities.
<div><strong>Authors:</strong> Philippe Formont, Maxime Darrin, Banafsheh Karimian, Jackie CK Cheung, Eric Granger, Ismail Ben Ayed, Mohammadhadi Shateri, Pablo Piantanida</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a task-agnostic distillation framework that uses a majority-vote objective to combine diverse teacher embeddings across modalities, deriving a loss bounded by mutual information between student and teachers. Experiments on text, vision, and molecular data show that the resulting student representations improve performance on various downstream tasks such as classification, clustering, and regression without requiring task-specific labels.", "summary_cn": "本文提出一种任务无关的多教师蒸馏框架，采用“多数投票”目标函数，将文本、视觉和分子等不同模态的教师嵌入进行融合，并证明该目标函数被学生与教师嵌入之间的互信息所界定。实验表明，得到的学生表示能够在无需任务标签的情况下提升分类、聚类和回归等多种下游任务的性能。", "keywords": "multi-teacher distillation, task-agnostic representation, embedding, mutual information, knowledge distillation, modality-agnostic, representation learning, downstream tasks, classification, clustering", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Philippe Formont", "Maxime Darrin", "Banafsheh Karimian", "Jackie CK Cheung", "Eric Granger", "Ismail Ben Ayed", "Mohammadhadi Shateri", "Pablo Piantanida"]}
]]></acme>

<pubDate>2025-10-21T14:36:33+00:00</pubDate>
</item>
<item>
<title>Reasoning Language Model Inference Serving Unveiled: An Empirical Study</title>
<link>https://papers.cool/arxiv/2510.18672</link>
<guid>https://papers.cool/arxiv/2510.18672</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper empirically investigates the inference serving performance of reasoning large language models (RLLMs), revealing distinct serving behaviors such as high memory usage, request stragglers, adaptive runtime, and domain preference compared to traditional LLMs. It evaluates the applicability of common inference optimizations, finding that model quantization and speculative decoding improve efficiency with minor accuracy loss, while prefix caching and KV‑cache quantization can hurt performance for smaller RLLMs. The study validates these findings under realistic workloads modeled by a Gamma distribution, providing practical insights for deploying RLLMs.<br /><strong>Summary (CN):</strong> 本文对推理大语言模型（RLLM）的推理服务性能进行实证研究，指出其相较于传统 LLM 存在显著的内存占用波动、请求延迟、运行时间自适应和领域偏好等特征。评估常用的推理优化技术后发现，模型量化和投机解码可提升效率且对准确率影响有限，而前缀缓存和 KV 缓存量化在小规模 RLLM 上可能导致性能或准确率下降。通过 Gamma 分布模拟的真实工作负载实验验证了这些结论，为 RLLM 部署提供了实用参考。<br /><strong>Keywords:</strong> reasoning language model, inference serving, model quantization, speculative decoding, KV cache, memory usage, straggler requests, real-world workload<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu</div>
The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.
<div><strong>Authors:</strong> Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper empirically investigates the inference serving performance of reasoning large language models (RLLMs), revealing distinct serving behaviors such as high memory usage, request stragglers, adaptive runtime, and domain preference compared to traditional LLMs. It evaluates the applicability of common inference optimizations, finding that model quantization and speculative decoding improve efficiency with minor accuracy loss, while prefix caching and KV‑cache quantization can hurt performance for smaller RLLMs. The study validates these findings under realistic workloads modeled by a Gamma distribution, providing practical insights for deploying RLLMs.", "summary_cn": "本文对推理大语言模型（RLLM）的推理服务性能进行实证研究，指出其相较于传统 LLM 存在显著的内存占用波动、请求延迟、运行时间自适应和领域偏好等特征。评估常用的推理优化技术后发现，模型量化和投机解码可提升效率且对准确率影响有限，而前缀缓存和 KV 缓存量化在小规模 RLLM 上可能导致性能或准确率下降。通过 Gamma 分布模拟的真实工作负载实验验证了这些结论，为 RLLM 部署提供了实用参考。", "keywords": "reasoning language model, inference serving, model quantization, speculative decoding, KV cache, memory usage, straggler requests, real-world workload", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Qi Li", "Junpan Wu", "Xiang Liu", "Yuxin Wang", "Zeyu Li", "Zhenheng Tang", "Yuhan Chen", "Shaohuai Shi", "Xiaowen Chu"]}
]]></acme>

<pubDate>2025-10-21T14:25:51+00:00</pubDate>
</item>
<item>
<title>Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches</title>
<link>https://papers.cool/arxiv/2510.18668</link>
<guid>https://papers.cool/arxiv/2510.18668</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper presents a tiny convolutional neural network that fuses ECG and PCG signals early to classify cardiovascular health conditions on resource‑constrained edge devices, achieving a three‑fold reduction in memory and computational cost while keeping accuracy competitive with state‑of‑the‑art models. The authors evaluate the model on the PhysioNet 2016 Challenge dataset and demonstrate its low‑energy inference on a microcontroller‑based sensor patch – showing feasibility for on‑device monitoring.<br /><strong>Summary (CN):</strong> 本文 : 在 ECG 和 PCG 信号上构建 tiny CNN 进行二元分类，旨 在 资源受限的 边缘设备上实现 低功耗、低内存的心血管监测。作者使用 PhysioNet 2016 数据集进行训练验证，并在微控制器 传感器补丁上展示 能耗优势，证明 可在设备端进行实时监测。<br /><strong>Keywords:</strong> cardiovascular monitoring, ECG, PCG, multi-modal CNN, edge AI, low-power inference<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mustafa Fuad Rifet Ibrahim, Tunc Alkanat, Maurice Meijer, Felix Manthey, Alexander Schlaefer, Peer Stelldinger</div>
The vast majority of cardiovascular diseases may be preventable if early signs and risk factors are detected. Cardiovascular monitoring with body-worn sensor devices like sensor patches allows for the detection of such signs while preserving the freedom and comfort of patients. However, the analysis of the sensor data must be robust, reliable, efficient, and highly accurate. Deep learning methods can automate data interpretation, reducing the workload of clinicians. In this work, we analyze the feasibility of applying deep learning models to the classification of synchronized electrocardiogram (ECG) and phonocardiogram (PCG) recordings on resource-constrained medical edge devices. We propose a convolutional neural network with early fusion of data to solve a binary classification problem. We train and validate our model on the synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset. Our approach reduces memory footprint and compute cost by three orders of magnitude compared to the state-of-the-art while maintaining competitive accuracy. We demonstrate the applicability of our proposed model on medical edge devices by analyzing energy consumption on a microcontroller and an experimental sensor device setup, confirming that on-device inference can be more energy-efficient than continuous data streaming.
<div><strong>Authors:</strong> Mustafa Fuad Rifet Ibrahim, Tunc Alkanat, Maurice Meijer, Felix Manthey, Alexander Schlaefer, Peer Stelldinger</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper presents a tiny convolutional neural network that fuses ECG and PCG signals early to classify cardiovascular health conditions on resource‑constrained edge devices, achieving a three‑fold reduction in memory and computational cost while keeping accuracy competitive with state‑of‑the‑art models. The authors evaluate the model on the PhysioNet 2016 Challenge dataset and demonstrate its low‑energy inference on a microcontroller‑based sensor patch – showing feasibility for on‑device monitoring.", "summary_cn": "本文 : 在 ECG 和 PCG 信号上构建 tiny CNN 进行二元分类，旨 在 资源受限的 边缘设备上实现 低功耗、低内存的心血管监测。作者使用 PhysioNet 2016 数据集进行训练验证，并在微控制器 传感器补丁上展示 能耗优势，证明 可在设备端进行实时监测。", "keywords": "cardiovascular monitoring, ECG, PCG, multi-modal CNN, edge AI, low-power inference", "scoring": {"interpretability": 4, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mustafa Fuad Rifet Ibrahim", "Tunc Alkanat", "Maurice Meijer", "Felix Manthey", "Alexander Schlaefer", "Peer Stelldinger"]}
]]></acme>

<pubDate>2025-10-21T14:23:20+00:00</pubDate>
</item>
<item>
<title>Learning Time-Varying Turn-Taking Behavior in Group Conversations</title>
<link>https://papers.cool/arxiv/2510.18649</link>
<guid>https://papers.cool/arxiv/2510.18649</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a flexible probabilistic model that predicts turn‑taking patterns in group conversations using individuals' personality traits and their recent speaking history. Unlike prior models that assume a universal formulation, this approach learns how a speaker’s inclination changes over time after they last spoke, and is evaluated on synthetic and real‑world datasets. Results show that conventional behavioral models can be unrealistic, highlighting the benefit of a data‑driven yet theoretically grounded method.<br /><strong>Summary (CN):</strong> 本文提出了一种灵活的概率模型，利用个体的性格特质和近期发言历史来预测群体对话中的轮换发言模式。不同于以往假设统一公式的模型，该方法能够学习说话者在上一次发言之后随时间变化的发言倾向，并在合成数据和真实对话数据上进行评估。实验结果表明传统行为模型并非总是现实的，凸显了数据驱动且理论扎实的方法的优势。<br /><strong>Keywords:</strong> turn-taking, probabilistic model, group conversation, personality traits, temporal dynamics, conversational modeling, synthetic data, real-world data, behavioral modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Madeline Navarro, Lisa O'Bryan, Santiago Segarra</div>
We propose a flexible probabilistic model for predicting turn-taking patterns in group conversations based solely on individual characteristics and past speaking behavior. Many models of conversation dynamics cannot yield insights that generalize beyond a single group. Moreover, past works often aim to characterize speaking behavior through a universal formulation that may not be suitable for all groups. We thus develop a generalization of prior conversation models that predicts speaking turns among individuals in any group based on their individual characteristics, that is, personality traits, and prior speaking behavior. Importantly, our approach provides the novel ability to learn how speaking inclination varies based on when individuals last spoke. We apply our model to synthetic and real-world conversation data to verify the proposed approach and characterize real group interactions. Our results demonstrate that previous behavioral models may not always be realistic, motivating our data-driven yet theoretically grounded approach.
<div><strong>Authors:</strong> Madeline Navarro, Lisa O'Bryan, Santiago Segarra</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a flexible probabilistic model that predicts turn‑taking patterns in group conversations using individuals' personality traits and their recent speaking history. Unlike prior models that assume a universal formulation, this approach learns how a speaker’s inclination changes over time after they last spoke, and is evaluated on synthetic and real‑world datasets. Results show that conventional behavioral models can be unrealistic, highlighting the benefit of a data‑driven yet theoretically grounded method.", "summary_cn": "本文提出了一种灵活的概率模型，利用个体的性格特质和近期发言历史来预测群体对话中的轮换发言模式。不同于以往假设统一公式的模型，该方法能够学习说话者在上一次发言之后随时间变化的发言倾向，并在合成数据和真实对话数据上进行评估。实验结果表明传统行为模型并非总是现实的，凸显了数据驱动且理论扎实的方法的优势。", "keywords": "turn-taking, probabilistic model, group conversation, personality traits, temporal dynamics, conversational modeling, synthetic data, real-world data, behavioral modeling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Madeline Navarro", "Lisa O'Bryan", "Santiago Segarra"]}
]]></acme>

<pubDate>2025-10-21T13:58:43+00:00</pubDate>
</item>
<item>
<title>Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction</title>
<link>https://papers.cool/arxiv/2510.18648</link>
<guid>https://papers.cool/arxiv/2510.18648</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a physics-informed deep learning framework that estimates fine-scale crop drought stress and sensitivity to water scarcity, and uses these estimates to predict crop yields. By integrating multispectral satellite imagery, meteorological data, and a novel physics-based loss with a deep ensemble approach, it achieves up to 0.82 R2, outperforming LSTM and Transformer baselines while providing explainable insights into the water-yield relationship.<br /><strong>Summary (CN):</strong> 本文提出一种融合物理约束的深度学习框架，用于在细尺度上估计作物干旱胁迫及对缺水的敏感性，并基于此预测作物产量。通过结合多光谱卫星影像、气象数据以及新颖的 physics-informed loss 与深度集成方法，模型实现了最高 0.82 的 R2，超越 LSTM 与 Transformer 基线，同时提供了关于水分与产量关系的可解释性洞察。<br /><strong>Keywords:</strong> drought stress, physics-informed learning, crop yield prediction, satellite imagery, deep ensembles, explainable AI, water scarcity, agricultural AI<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Miro Miranda, Marcela Charfuelan, Matias Valdenegro Toro, Andreas Dengel</div>
Water is essential for agricultural productivity. Assessing water shortages and reduced yield potential is a critical factor in decision-making for ensuring agricultural productivity and food security. Crop simulation models, which align with physical processes, offer intrinsic explainability but often perform poorly. Conversely, machine learning models for crop yield modeling are powerful and scalable, yet they commonly operate as black boxes and lack adherence to the physical principles of crop growth. This study bridges this gap by coupling the advantages of both worlds. We postulate that the crop yield is inherently defined by the water availability. Therefore, we formulate crop yield as a function of temporal water scarcity and predict both the crop drought stress and the sensitivity to water scarcity at fine-scale resolution. Sequentially modeling the crop yield response to water enables accurate yield prediction. To enforce physical consistency, a novel physics-informed loss function is proposed. We leverage multispectral satellite imagery, meteorological data, and fine-scale yield data. Further, to account for the uncertainty within the model, we build upon a deep ensemble approach. Our method surpasses state-of-the-art models like LSTM and Transformers in crop yield prediction with a coefficient of determination ($R^2$-score) of up to 0.82 while offering high explainability. This method offers decision support for industry, policymakers, and farmers in building a more resilient agriculture in times of changing climate conditions.
<div><strong>Authors:</strong> Miro Miranda, Marcela Charfuelan, Matias Valdenegro Toro, Andreas Dengel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a physics-informed deep learning framework that estimates fine-scale crop drought stress and sensitivity to water scarcity, and uses these estimates to predict crop yields. By integrating multispectral satellite imagery, meteorological data, and a novel physics-based loss with a deep ensemble approach, it achieves up to 0.82 R2, outperforming LSTM and Transformer baselines while providing explainable insights into the water-yield relationship.", "summary_cn": "本文提出一种融合物理约束的深度学习框架，用于在细尺度上估计作物干旱胁迫及对缺水的敏感性，并基于此预测作物产量。通过结合多光谱卫星影像、气象数据以及新颖的 physics-informed loss 与深度集成方法，模型实现了最高 0.82 的 R2，超越 LSTM 与 Transformer 基线，同时提供了关于水分与产量关系的可解释性洞察。", "keywords": "drought stress, physics-informed learning, crop yield prediction, satellite imagery, deep ensembles, explainable AI, water scarcity, agricultural AI", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Miro Miranda", "Marcela Charfuelan", "Matias Valdenegro Toro", "Andreas Dengel"]}
]]></acme>

<pubDate>2025-10-21T13:58:04+00:00</pubDate>
</item>
<item>
<title>Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions</title>
<link>https://papers.cool/arxiv/2510.18638</link>
<guid>https://papers.cool/arxiv/2510.18638</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how single‑layer linear self‑attention transformers learn Markovian dynamical functions in an in‑context learning setting. It derives a closed‑form global minimizer, shows that recovering exact transformer parameters that achieve this optimum is NP‑hard, and interprets multilayer LSA as a preconditioned gradient‑descent optimizer for multiple objectives. Empirical experiments on simplified transformers validate the theoretical predictions.<br /><strong>Summary (CN):</strong> 本文研究单层线性自注意力 Transformer 在上下文学习设置中学习马尔可夫动力学函数的能力。作者给出全局最小解的闭式表达式，证明在一般情况下恢复实现该最优解的 Transformer 参数是 NP‑hard，并将多层 LSA 解释为对多个目标进行预条件化梯度下降的优化器。通过简化的 Transformer 实验验证了理论结果。<br /><strong>Keywords:</strong> transformers, in-context learning, linear self-attention, Markovian dynamics, NP-hardness, optimization landscape, gradient descent interpretation, theoretical analysis<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yanna Ding, Songtao Lu, Yingdong Lu, Tomasz Nowicki, Jianxi Gao</div>
Transformer architectures can solve unseen tasks based on input-output pairs in a given prompt due to in-context learning (ICL). Existing theoretical studies on ICL have mainly focused on linear regression tasks, often with i.i.d. inputs. To understand how transformers express ICL when modeling dynamics-driven functions, we investigate Markovian function learning through a structured ICL setup, where we characterize the loss landscape to reveal underlying optimization behaviors. Specifically, we (1) provide the closed-form expression of the global minimizer (in an enlarged parameter space) for a single-layer linear self-attention (LSA) model; (2) prove that recovering transformer parameters that realize the optimal solution is NP-hard in general, revealing a fundamental limitation of one-layer LSA in representing structured dynamical functions; and (3) supply a novel interpretation of a multilayer LSA as performing preconditioned gradient descent to optimize multiple objectives beyond the square loss. These theoretical results are numerically validated using simplified transformers.
<div><strong>Authors:</strong> Yanna Ding, Songtao Lu, Yingdong Lu, Tomasz Nowicki, Jianxi Gao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how single‑layer linear self‑attention transformers learn Markovian dynamical functions in an in‑context learning setting. It derives a closed‑form global minimizer, shows that recovering exact transformer parameters that achieve this optimum is NP‑hard, and interprets multilayer LSA as a preconditioned gradient‑descent optimizer for multiple objectives. Empirical experiments on simplified transformers validate the theoretical predictions.", "summary_cn": "本文研究单层线性自注意力 Transformer 在上下文学习设置中学习马尔可夫动力学函数的能力。作者给出全局最小解的闭式表达式，证明在一般情况下恢复实现该最优解的 Transformer 参数是 NP‑hard，并将多层 LSA 解释为对多个目标进行预条件化梯度下降的优化器。通过简化的 Transformer 实验验证了理论结果。", "keywords": "transformers, in-context learning, linear self-attention, Markovian dynamics, NP-hardness, optimization landscape, gradient descent interpretation, theoretical analysis", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yanna Ding", "Songtao Lu", "Yingdong Lu", "Tomasz Nowicki", "Jianxi Gao"]}
]]></acme>

<pubDate>2025-10-21T13:42:48+00:00</pubDate>
</item>
<item>
<title>Hardness of Learning Regular Languages in the Next Symbol Prediction Setting</title>
<link>https://papers.cool/arxiv/2510.18634</link>
<guid>https://papers.cool/arxiv/2510.18634</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper formalizes the Next Symbol Prediction (NSP) learning setting—where a learner receives positive strings together with prefix membership and admissible next-symbol information—and studies its PAC learnability. It proves that even with these richer labels, learning concept classes such as deterministic finite automata (DFAs) and Boolean formulas remains computationally hard, providing a reduction that makes most extra labels uninformative and, under standard cryptographic assumptions, establishing hardness of DFA learning in the NSP setting.<br /><strong>Summary (CN):</strong> 本文形式化了“下一个符号预测”(NSP)学习设置——学习者在获得正例字符串的同时，还得到每个前缀是否属于语言以及可导致接受字符串的下一符号信息，并对其在 PAC 学习框架下的可学习性进行研究。研究表明，即使标签更丰富，学习确定性有限自动机（DFA）和布尔公式等概念类仍然在计算上是困难的，作者通过构造几乎所有额外标签都是无信息的归约，且在标准密码学假设下证明了 DFA 在 NSP 设置下的学习硬度。<br /><strong>Keywords:</strong> regular languages, DFA learning, next symbol prediction, PAC learning, computational hardness, cryptographic assumptions<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Satwik Bhattamishra, Phil Blunsom, Varun Kanade</div>
We study the learnability of languages in the Next Symbol Prediction (NSP) setting, where a learner receives only positive examples from a language together with, for every prefix, (i) whether the prefix itself is in the language and (ii) which next symbols can lead to an accepting string. This setting has been used in prior works to empirically analyze neural sequence models, and additionally, we observe that efficient algorithms for the NSP setting can be used to learn the (truncated) support of language models. We formalize the setting so as to make it amenable to PAC-learning analysis. While the setting provides a much richer set of labels than the conventional classification setting, we show that learning concept classes such as DFAs and Boolean formulas remains computationally hard. The proof is via a construction that makes almost all additional labels uninformative, yielding a reduction from the conventional learning problem to learning with NSP labels. Under cryptographic assumptions, the reduction implies that the problem of learning DFAs is computationally hard in the NSP setting.
<div><strong>Authors:</strong> Satwik Bhattamishra, Phil Blunsom, Varun Kanade</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper formalizes the Next Symbol Prediction (NSP) learning setting—where a learner receives positive strings together with prefix membership and admissible next-symbol information—and studies its PAC learnability. It proves that even with these richer labels, learning concept classes such as deterministic finite automata (DFAs) and Boolean formulas remains computationally hard, providing a reduction that makes most extra labels uninformative and, under standard cryptographic assumptions, establishing hardness of DFA learning in the NSP setting.", "summary_cn": "本文形式化了“下一个符号预测”(NSP)学习设置——学习者在获得正例字符串的同时，还得到每个前缀是否属于语言以及可导致接受字符串的下一符号信息，并对其在 PAC 学习框架下的可学习性进行研究。研究表明，即使标签更丰富，学习确定性有限自动机（DFA）和布尔公式等概念类仍然在计算上是困难的，作者通过构造几乎所有额外标签都是无信息的归约，且在标准密码学假设下证明了 DFA 在 NSP 设置下的学习硬度。", "keywords": "regular languages, DFA learning, next symbol prediction, PAC learning, computational hardness, cryptographic assumptions", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Satwik Bhattamishra", "Phil Blunsom", "Varun Kanade"]}
]]></acme>

<pubDate>2025-10-21T13:37:34+00:00</pubDate>
</item>
<item>
<title>A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees</title>
<link>https://papers.cool/arxiv/2510.18615</link>
<guid>https://papers.cool/arxiv/2510.18615</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a rectification-based method for distilling boosted-ensemble models into single decision trees, aiming to balance predictive performance with interpretability. By iteratively correcting the decision tree using the residuals of the boosted model, the approach yields compact trees that retain much of the original accuracy. Empirical evaluations demonstrate that this rectification distillation outperforms naïve retraining baselines.<br /><strong>Summary (CN):</strong> 本文提出一种基于校正（rectification）的蒸馏方法，将提升树（boosted trees）转化为单棵决策树，以在预测性能和可解释性之间取得平衡。该方法通过利用提升模型的残差对决策树进行迭代校正，生成既紧凑又保持原模型大部分准确性的树结构。实验结果显示，该校正蒸馏在效果上优于传统的重新训练蒸馏基线。<br /><strong>Keywords:</strong> boosted trees, decision trees, model distillation, rectification, interpretability, tree ensembles, machine learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
We present a new approach for distilling boosted trees into decision trees, in the objective of generating an ML model offering an acceptable compromise in terms of predictive performance and interpretability. We explain how the correction approach called rectification can be used to implement such a distillation process. We show empirically that this approach provides interesting results, in comparison with an approach to distillation achieved by retraining the model.
<div><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a rectification-based method for distilling boosted-ensemble models into single decision trees, aiming to balance predictive performance with interpretability. By iteratively correcting the decision tree using the residuals of the boosted model, the approach yields compact trees that retain much of the original accuracy. Empirical evaluations demonstrate that this rectification distillation outperforms naïve retraining baselines.", "summary_cn": "本文提出一种基于校正（rectification）的蒸馏方法，将提升树（boosted trees）转化为单棵决策树，以在预测性能和可解释性之间取得平衡。该方法通过利用提升模型的残差对决策树进行迭代校正，生成既紧凑又保持原模型大部分准确性的树结构。实验结果显示，该校正蒸馏在效果上优于传统的重新训练蒸馏基线。", "keywords": "boosted trees, decision trees, model distillation, rectification, interpretability, tree ensembles, machine learning", "scoring": {"interpretability": 7, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Gilles Audemard", "Sylvie Coste-Marquis", "Pierre Marquis", "Mehdi Sabiri", "Nicolas Szczepanski"]}
]]></acme>

<pubDate>2025-10-21T13:14:04+00:00</pubDate>
</item>
<item>
<title>Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data</title>
<link>https://papers.cool/arxiv/2510.18611</link>
<guid>https://papers.cool/arxiv/2510.18611</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Unrolled-SINDy, an unrolling scheme that stabilizes explicit methods for discovering governing PDEs from sparsely sampled temporal data by decoupling the numerical time step from the data sampling rate. The approach can be implemented via a closed‑form iterative update or gradient descent and demonstrates improved parameter recovery on both classic SINDy and the noise‑robust iNeuralSINDy across Euler and RK4 discretizations. Experiments show that the unrolled method enables the identification of equations that standard SINDy cannot recover due to large local truncation errors.<br /><strong>Summary (CN):</strong> 本文提出 Unrolled‑SINDy 方法，通过将数值时间步长与稀疏采样的观测数据率解耦，实现了显式数值方法在 PDE 发现任务中的稳定性提升。该方案可采用闭式迭代或梯度下降实现，并在传统 SINDy 与抗噪 iNeuralSINDy 以及 Euler、RK4 等离散方案上展示了更好的参数恢复能力，使得原本因局部截断误差过大而无法识别的方程得以重建。实验验证了该解卷方法在稀疏数据情形下的通用性和有效性。<br /><strong>Keywords:</strong> PDE discovery, SINDy, unrolling, sparse data, stability, gradient descent, iNeuralSINDy, system identification, explicit methods, numerical integration<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Fayad Ali Banna, Antoine Caradot, Eduardo Brandao, Jean-Philippe Colombier, Rémi Emonet, Marc Sebban</div>
Identifying from observation data the governing differential equations of a physical dynamics is a key challenge in machine learning. Although approaches based on SINDy have shown great promise in this area, they still fail to address a whole class of real world problems where the data is sparsely sampled in time. In this article, we introduce Unrolled-SINDy, a simple methodology that leverages an unrolling scheme to improve the stability of explicit methods for PDE discovery. By decorrelating the numerical time step size from the sampling rate of the available data, our approach enables the recovery of equation parameters that would not be the minimizers of the original SINDy optimization problem due to large local truncation errors. Our method can be exploited either through an iterative closed-form approach or by a gradient descent scheme. Experiments show the versatility of our method. On both traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with different numerical schemes (Euler, RK4), our proposed unrolling scheme allows to tackle problems not accessible to non-unrolled methods.
<div><strong>Authors:</strong> Fayad Ali Banna, Antoine Caradot, Eduardo Brandao, Jean-Philippe Colombier, Rémi Emonet, Marc Sebban</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Unrolled-SINDy, an unrolling scheme that stabilizes explicit methods for discovering governing PDEs from sparsely sampled temporal data by decoupling the numerical time step from the data sampling rate. The approach can be implemented via a closed‑form iterative update or gradient descent and demonstrates improved parameter recovery on both classic SINDy and the noise‑robust iNeuralSINDy across Euler and RK4 discretizations. Experiments show that the unrolled method enables the identification of equations that standard SINDy cannot recover due to large local truncation errors.", "summary_cn": "本文提出 Unrolled‑SINDy 方法，通过将数值时间步长与稀疏采样的观测数据率解耦，实现了显式数值方法在 PDE 发现任务中的稳定性提升。该方案可采用闭式迭代或梯度下降实现，并在传统 SINDy 与抗噪 iNeuralSINDy 以及 Euler、RK4 等离散方案上展示了更好的参数恢复能力，使得原本因局部截断误差过大而无法识别的方程得以重建。实验验证了该解卷方法在稀疏数据情形下的通用性和有效性。", "keywords": "PDE discovery, SINDy, unrolling, sparse data, stability, gradient descent, iNeuralSINDy, system identification, explicit methods, numerical integration", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Fayad Ali Banna", "Antoine Caradot", "Eduardo Brandao", "Jean-Philippe Colombier", "Rémi Emonet", "Marc Sebban"]}
]]></acme>

<pubDate>2025-10-21T13:09:34+00:00</pubDate>
</item>
<item>
<title>Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing</title>
<link>https://papers.cool/arxiv/2510.18591</link>
<guid>https://papers.cool/arxiv/2510.18591</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes RobLight, a verification tool for graph neural networks that improves structural robustness certification by replacing heavyweight mixed‑integer solvers with efficient polynomial‑time partial SAT solvers, which may be incomplete but run much faster. Experiments on multiple GNN variants and datasets show that this lightweight approach can match or exceed state‑of‑the‑art robustness detection while reducing computational cost.<br /><strong>Summary (CN):</strong> 本文提出了 RobLight，一种用于图神经网络结构鲁棒性验证的工具，通过用高效的多项式时间部分 SAT 求解器替代传统的混合整数求解器，从而在可能不完整的情况下显著提升验证速度。对多种 GNN 变体和数据集的实验表明，该轻量化方法在计算成本降低的同时，能够匹配或超越现有最先进的鲁棒性检测效果。<br /><strong>Keywords:</strong> graph neural networks, adversarial robustness, verification, satisfiability testing, partial solvers, robustness certification, structural attacks<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Chia-Hsuan Lu, Tony Tan, Michael Benedikt</div>
Graph neural networks (GNNs) are the predominant architecture for learning over graphs. As with any machine learning model, and important issue is the detection of adversarial attacks, where an adversary can change the output with a small perturbation of the input. Techniques for solving the adversarial robustness problem - determining whether such an attack exists - were originally developed for image classification, but there are variants for many other machine learning architectures. In the case of graph learning, the attack model usually considers changes to the graph structure in addition to or instead of the numerical features of the input, and the state of the art techniques in the area proceed via reduction to constraint solving, working on top of powerful solvers, e.g. for mixed integer programming. We show that it is possible to improve on the state of the art in structural robustness by replacing the use of powerful solvers by calls to efficient partial solvers, which run in polynomial time but may be incomplete. We evaluate our tool RobLight on a diverse set of GNN variants and datasets.
<div><strong>Authors:</strong> Chia-Hsuan Lu, Tony Tan, Michael Benedikt</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes RobLight, a verification tool for graph neural networks that improves structural robustness certification by replacing heavyweight mixed‑integer solvers with efficient polynomial‑time partial SAT solvers, which may be incomplete but run much faster. Experiments on multiple GNN variants and datasets show that this lightweight approach can match or exceed state‑of‑the‑art robustness detection while reducing computational cost.", "summary_cn": "本文提出了 RobLight，一种用于图神经网络结构鲁棒性验证的工具，通过用高效的多项式时间部分 SAT 求解器替代传统的混合整数求解器，从而在可能不完整的情况下显著提升验证速度。对多种 GNN 变体和数据集的实验表明，该轻量化方法在计算成本降低的同时，能够匹配或超越现有最先进的鲁棒性检测效果。", "keywords": "graph neural networks, adversarial robustness, verification, satisfiability testing, partial solvers, robustness certification, structural attacks", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Chia-Hsuan Lu", "Tony Tan", "Michael Benedikt"]}
]]></acme>

<pubDate>2025-10-21T12:45:51+00:00</pubDate>
</item>
<item>
<title>HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search</title>
<link>https://papers.cool/arxiv/2510.18575</link>
<guid>https://papers.cool/arxiv/2510.18575</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HeFS (Helper-Enhanced Feature Selection) is a framework that refines feature subsets generated by existing algorithms by searching the residual feature space for a Helper Set of complementary features. It uses a biased initialization, ratio‑guided mutation within a genetic algorithm, and Pareto‑based multi‑objective optimization to jointly maximize predictive accuracy and feature complementarity. Experiments on 18 benchmark datasets, including gastric cancer classification and drug toxicity prediction, show consistent performance gains over state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> HeFS（Helper‑Enhanced Feature Selection）框架通过在剩余特征空间中寻找可补充原有子集的“Helper Set”，对现有特征选择算法的结果进行优化。该方法在遗传算法中引入偏置初始化和比例引导的变异，并采用基于 Pareto 的多目标优化，同时最大化预测精度和特征互补性。实验证明，在包括胃癌分类、药物毒性预测等18 个基准数据集上，HeFS 能持续发现被忽视的有价值特征并提升性能。<br /><strong>Keywords:</strong> feature selection, genetic algorithm, Pareto optimization, helper set, combinatorial optimization, high-dimensional data<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yusi Fan, Tian Wang, Zhiying Yan, Chang Liu, Qiong Zhou, Qi Lu, Zhehao Guo, Ziqi Deng, Wenyu Zhu, Ruochi Zhang, Fengfeng Zhou</div>
Feature selection is a combinatorial optimization problem that is NP-hard. Conventional approaches often employ heuristic or greedy strategies, which are prone to premature convergence and may fail to capture subtle yet informative features. This limitation becomes especially critical in high-dimensional datasets, where complex and interdependent feature relationships prevail. We introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine feature subsets produced by existing algorithms. HeFS systematically searches the residual feature space to identify a Helper Set - features that complement the original subset and improve classification performance. The approach employs a biased initialization scheme and a ratio-guided mutation mechanism within a genetic algorithm, coupled with Pareto-based multi-objective optimization to jointly maximize predictive accuracy and feature complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS consistently identifies overlooked yet informative features and achieves superior performance over state-of-the-art methods, including in challenging domains such as gastric cancer classification, drug toxicity prediction, and computer science applications. The code and datasets are available at https://healthinformaticslab.org/supp/.
<div><strong>Authors:</strong> Yusi Fan, Tian Wang, Zhiying Yan, Chang Liu, Qiong Zhou, Qi Lu, Zhehao Guo, Ziqi Deng, Wenyu Zhu, Ruochi Zhang, Fengfeng Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HeFS (Helper-Enhanced Feature Selection) is a framework that refines feature subsets generated by existing algorithms by searching the residual feature space for a Helper Set of complementary features. It uses a biased initialization, ratio‑guided mutation within a genetic algorithm, and Pareto‑based multi‑objective optimization to jointly maximize predictive accuracy and feature complementarity. Experiments on 18 benchmark datasets, including gastric cancer classification and drug toxicity prediction, show consistent performance gains over state‑of‑the‑art methods.", "summary_cn": "HeFS（Helper‑Enhanced Feature Selection）框架通过在剩余特征空间中寻找可补充原有子集的“Helper Set”，对现有特征选择算法的结果进行优化。该方法在遗传算法中引入偏置初始化和比例引导的变异，并采用基于 Pareto 的多目标优化，同时最大化预测精度和特征互补性。实验证明，在包括胃癌分类、药物毒性预测等18 个基准数据集上，HeFS 能持续发现被忽视的有价值特征并提升性能。", "keywords": "feature selection, genetic algorithm, Pareto optimization, helper set, combinatorial optimization, high-dimensional data", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yusi Fan", "Tian Wang", "Zhiying Yan", "Chang Liu", "Qiong Zhou", "Qi Lu", "Zhehao Guo", "Ziqi Deng", "Wenyu Zhu", "Ruochi Zhang", "Fengfeng Zhou"]}
]]></acme>

<pubDate>2025-10-21T12:30:22+00:00</pubDate>
</item>
<item>
<title>RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</title>
<link>https://papers.cool/arxiv/2510.18559</link>
<guid>https://papers.cool/arxiv/2510.18559</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents RAISE, a unified framework that scores AI models across explainability, fairness, robustness, and sustainability, aggregating them into a single Responsibility Score. It evaluates three deep learning models on structured datasets from finance, healthcare, and socioeconomics, revealing trade-offs such as high explainability and fairness for a Transformer at high environmental cost. The work highlights the need for multi-dimensional evaluation for responsible model selection.<br /><strong>Summary (CN):</strong> 本文提出 RAISE 框架，用于在可解释性、公平性、鲁棒性和可持续性四个维度上对 AI 模型进行评分，并汇总为一个整体责任分数。作者在金融、医疗和社会经济结构化数据上评估了多层感知机、表格 ResNet 和特征分词 Transformer，揭示了如 Transformer 在可解释性和公平性上表现突出但环境成本高的权衡。该研究强调在负责任的模型选择中进行多维度评估的必要性。<br /><strong>Keywords:</strong> responsible AI, evaluation framework, explainability, fairness, robustness, sustainability, responsibility score, model assessment, multi-dimensional evaluation<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 5, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Loc Phuc Truong Nguyen, Hung Thanh Do</div>
As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: https://github.com/raise-framework/raise.
<div><strong>Authors:</strong> Loc Phuc Truong Nguyen, Hung Thanh Do</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents RAISE, a unified framework that scores AI models across explainability, fairness, robustness, and sustainability, aggregating them into a single Responsibility Score. It evaluates three deep learning models on structured datasets from finance, healthcare, and socioeconomics, revealing trade-offs such as high explainability and fairness for a Transformer at high environmental cost. The work highlights the need for multi-dimensional evaluation for responsible model selection.", "summary_cn": "本文提出 RAISE 框架，用于在可解释性、公平性、鲁棒性和可持续性四个维度上对 AI 模型进行评分，并汇总为一个整体责任分数。作者在金融、医疗和社会经济结构化数据上评估了多层感知机、表格 ResNet 和特征分词 Transformer，揭示了如 Transformer 在可解释性和公平性上表现突出但环境成本高的权衡。该研究强调在负责任的模型选择中进行多维度评估的必要性。", "keywords": "responsible AI, evaluation framework, explainability, fairness, robustness, sustainability, responsibility score, model assessment, multi-dimensional evaluation", "scoring": {"interpretability": 6, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Loc Phuc Truong Nguyen", "Hung Thanh Do"]}
]]></acme>

<pubDate>2025-10-21T12:15:13+00:00</pubDate>
</item>
<item>
<title>Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</title>
<link>https://papers.cool/arxiv/2510.18541</link>
<guid>https://papers.cool/arxiv/2510.18541</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces T-MTB, a backdoor technique for large language models that constructs composite triggers composed of common tokens, enabling the backdoor to survive knowledge distillation into student models. By leveraging triggers that appear frequently in distillation datasets, the poisoned teacher remains stealthy while the backdoor reliably transfers during compression, exposing severe security risks in scenarios such as jailbreaking and content manipulation across multiple model families.<br /><strong>Summary (CN):</strong> 本文提出了 T-MTB，这是一种针对大型语言模型的后门技术，通过使用由常见词组成的复合触发词，使后门能够在知识蒸馏过程中成功转移到学生模型。由于这些触发词在蒸馏数据集中经常出现，受污染的教师模型保持隐蔽，而后门在压缩过程中仍能被有效传递，揭示了在越狱和内容调制等情境下的严重安全风险，实验覆盖了多个模型系列。<br /><strong>Keywords:</strong> backdoor, knowledge distillation, large language model, trigger, transferability, jailbreak, content modulation, security, model compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</div>
LLMs are often used by downstream users as teacher models for knowledge distillation, compressing their capabilities into memory-efficient models. However, as these teacher models may stem from untrusted parties, distillation can raise unexpected security risks. In this paper, we investigate the security implications of knowledge distillation from backdoored teacher models. First, we show that prior backdoors mostly do not transfer onto student models. Our key insight is that this is because existing LLM backdooring methods choose trigger tokens that rarely occur in usual contexts. We argue that this underestimates the security risks of knowledge distillation and introduce a new backdooring technique, T-MTB, that enables the construction and study of transferable backdoors. T-MTB carefully constructs a composite backdoor trigger, made up of several specific tokens that often occur individually in anticipated distillation datasets. As such, the poisoned teacher remains stealthy, while during distillation the individual presence of these tokens provides enough signal for the backdoor to transfer onto the student. Using T-MTB, we demonstrate and extensively study the security risks of transferable backdoors across two attack scenarios, jailbreaking and content modulation, and across four model families of LLMs.
<div><strong>Authors:</strong> Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces T-MTB, a backdoor technique for large language models that constructs composite triggers composed of common tokens, enabling the backdoor to survive knowledge distillation into student models. By leveraging triggers that appear frequently in distillation datasets, the poisoned teacher remains stealthy while the backdoor reliably transfers during compression, exposing severe security risks in scenarios such as jailbreaking and content manipulation across multiple model families.", "summary_cn": "本文提出了 T-MTB，这是一种针对大型语言模型的后门技术，通过使用由常见词组成的复合触发词，使后门能够在知识蒸馏过程中成功转移到学生模型。由于这些触发词在蒸馏数据集中经常出现，受污染的教师模型保持隐蔽，而后门在压缩过程中仍能被有效传递，揭示了在越狱和内容调制等情境下的严重安全风险，实验覆盖了多个模型系列。", "keywords": "backdoor, knowledge distillation, large language model, trigger, transferability, jailbreak, content modulation, security, model compression", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Giovanni De Muri", "Mark Vero", "Robin Staab", "Martin Vechev"]}
]]></acme>

<pubDate>2025-10-21T11:39:45+00:00</pubDate>
</item>
<item>
<title>Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints</title>
<link>https://papers.cool/arxiv/2510.18520</link>
<guid>https://papers.cool/arxiv/2510.18520</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Partial VOROS, a cost-aware performance metric for binary classifiers that incorporates precision and capacity constraints into ROC analysis. It defines a feasible region in ROC space for classifiers meeting these constraints and proposes the partial area of lesser classifiers, which is averaged over cost parameters to obtain the partial volume over the ROC surface. Experiments on mortality risk prediction using the MIMIC-IV dataset demonstrate that this metric better ranks classifiers for hospital alert systems than existing alternatives.<br /><strong>Summary (CN):</strong> 本文提出了 Partial VOROS，一种考虑精确率和容量约束的成本感知二分类性能指标，将这些约束映射到 ROC 空间的可行区域，并定义了较差分类器的部分面积，然后在成本参数范围内平均得到 ROC 曲面的部分体积。通过在 MIMIC-IV 数据集上预测死亡风险的实验，展示该指标在医院警报系统中比传统指标更能有效排序分类器。<br /><strong>Keywords:</strong> binary classification, ROC, Partial VOROS, cost-aware metric, precision constraint, capacity constraint, healthcare alerts, MIMIC-IV<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Christopher Ratigan, Kyle Heuton, Carissa Wang, Lenore Cowen, Michael C. Hughes</div>
The ROC curve is widely used to assess binary classification performance. Yet for some applications such as alert systems for hospitalized patient monitoring, conventional ROC analysis cannot capture crucial factors that impact deployment, such as enforcing a minimum precision constraint to avoid false alarm fatigue or imposing an upper bound on the number of predicted positives to represent the capacity of hospital staff. The usual area under the curve metric also does not reflect asymmetric costs for false positives and false negatives. In this paper we address all three of these issues. First, we show how the subset of classifiers that meet given precision and capacity constraints can be represented as a feasible region in ROC space. We establish the geometry of this feasible region. We then define the partial area of lesser classifiers, a performance metric that is monotonic with cost and only accounts for the feasible portion of ROC space. Averaging this area over a desired range of cost parameters results in the partial volume over the ROC surface, or partial VOROS. In experiments predicting mortality risk using vital sign history on the MIMIC-IV dataset, we show this cost-aware metric is better than alternatives for ranking classifiers in hospital alert applications.
<div><strong>Authors:</strong> Christopher Ratigan, Kyle Heuton, Carissa Wang, Lenore Cowen, Michael C. Hughes</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Partial VOROS, a cost-aware performance metric for binary classifiers that incorporates precision and capacity constraints into ROC analysis. It defines a feasible region in ROC space for classifiers meeting these constraints and proposes the partial area of lesser classifiers, which is averaged over cost parameters to obtain the partial volume over the ROC surface. Experiments on mortality risk prediction using the MIMIC-IV dataset demonstrate that this metric better ranks classifiers for hospital alert systems than existing alternatives.", "summary_cn": "本文提出了 Partial VOROS，一种考虑精确率和容量约束的成本感知二分类性能指标，将这些约束映射到 ROC 空间的可行区域，并定义了较差分类器的部分面积，然后在成本参数范围内平均得到 ROC 曲面的部分体积。通过在 MIMIC-IV 数据集上预测死亡风险的实验，展示该指标在医院警报系统中比传统指标更能有效排序分类器。", "keywords": "binary classification, ROC, Partial VOROS, cost-aware metric, precision constraint, capacity constraint, healthcare alerts, MIMIC-IV", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Christopher Ratigan", "Kyle Heuton", "Carissa Wang", "Lenore Cowen", "Michael C. Hughes"]}
]]></acme>

<pubDate>2025-10-21T11:00:02+00:00</pubDate>
</item>
<item>
<title>Alibaba International E-commerce Product Search Competition DILAB Team Technical Report</title>
<link>https://papers.cool/arxiv/2510.18499</link>
<guid>https://papers.cool/arxiv/2510.18499</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The report describes the multilingual e‑commerce product search system built by the DILAB team for the Alibaba International Search competition, which achieved 5th place with an overall score of 0.8819. The system uses a multi‑stage pipeline that combines data refinement, lightweight preprocessing, language tagging, and adaptive modeling with multiple architectures and fine‑tuning strategies to handle query‑category and query‑item tasks across many languages. The authors emphasize the importance of systematic data curation and iterative validation for robust multilingual search performance.<br /><strong>Summary (CN):</strong> 本文介绍了 DILAB 团队为阿里巴巴国际电商搜索大赛构建的多语言商品搜索系统，该系统在最终排行榜上获得第 5 名，整体得分 0.8819。系统采用多阶段流水线，融合数据精炼、轻量预处理、语言标签和自适应建模，使用多种架构和微调策略来同时处理查询‑类别（QC）和查询‑商品（QI）任务，支持多语言和多领域。作者强调系统化的数据策划和迭代验证对实现稳健的多语言搜索性能至关重要。<br /><strong>Keywords:</strong> multilingual search, e-commerce product search, data refinement, language tagging, adaptive modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hyewon Lee, Junghyun Oh, Minkyung Song, Soyoung Park, Seunghoon Han</div>
This study presents the multilingual e-commerce search system developed by the DILAB team, which achieved 5th place on the final leaderboard with a competitive overall score of 0.8819, demonstrating stable and high-performing results across evaluation metrics. To address challenges in multilingual query-item understanding, we designed a multi-stage pipeline integrating data refinement, lightweight preprocessing, and adaptive modeling. The data refinement stage enhanced dataset consistency and category coverage, while language tagging and noise filtering improved input quality. In the modeling phase, multiple architectures and fine-tuning strategies were explored, and hyperparameters optimized using curated validation sets to balance performance across query-category (QC) and query-item (QI) tasks. The proposed framework exhibited robustness and adaptability across languages and domains, highlighting the effectiveness of systematic data curation and iterative evaluation for multilingual search systems. The source code is available at https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.
<div><strong>Authors:</strong> Hyewon Lee, Junghyun Oh, Minkyung Song, Soyoung Park, Seunghoon Han</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The report describes the multilingual e‑commerce product search system built by the DILAB team for the Alibaba International Search competition, which achieved 5th place with an overall score of 0.8819. The system uses a multi‑stage pipeline that combines data refinement, lightweight preprocessing, language tagging, and adaptive modeling with multiple architectures and fine‑tuning strategies to handle query‑category and query‑item tasks across many languages. The authors emphasize the importance of systematic data curation and iterative validation for robust multilingual search performance.", "summary_cn": "本文介绍了 DILAB 团队为阿里巴巴国际电商搜索大赛构建的多语言商品搜索系统，该系统在最终排行榜上获得第 5 名，整体得分 0.8819。系统采用多阶段流水线，融合数据精炼、轻量预处理、语言标签和自适应建模，使用多种架构和微调策略来同时处理查询‑类别（QC）和查询‑商品（QI）任务，支持多语言和多领域。作者强调系统化的数据策划和迭代验证对实现稳健的多语言搜索性能至关重要。", "keywords": "multilingual search, e-commerce product search, data refinement, language tagging, adaptive modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hyewon Lee", "Junghyun Oh", "Minkyung Song", "Soyoung Park", "Seunghoon Han"]}
]]></acme>

<pubDate>2025-10-21T10:36:02+00:00</pubDate>
</item>
<item>
<title>Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18485</link>
<guid>https://papers.cool/arxiv/2510.18485</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces COPPOL, a conformal‑prediction‑driven method that yields calibrated semantic‑segmentation hazard maps with finite‑sample safety guarantees and incorporates them into risk‑aware cost fields for reinforcement‑learning navigation. Across two satellite‑derived benchmarks, COPPOL achieves up to six‑fold higher hazard coverage and roughly 50% fewer safety violations while maintaining robustness under distributional shift.<br /><strong>Summary (CN):</strong> 本文提出 COPPOL，一种基于共形预测的感知到策略学习方法，生成具有有限样本安全保证的校准语义分割危害图，并将其转化为风险感知的代价场用于强化学习导航。在两个卫星衍生基准上，COPPOL 提升了最高 6 倍的危害覆盖率，并将导航中的安全违规降低约 50%，同时在分布漂移情况下保持鲁棒性。<br /><strong>Keywords:</strong> conformal prediction, semantic segmentation, safety reinforcement learning, uncertainty quantification, hazard mapping, distributional shift, risk-aware planning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
Reliable navigation in safety-critical environments requires both accurate hazard perception and principled uncertainty handling to strengthen downstream safety handling. Despite the effectiveness of existing approaches, they assume perfect hazard detection capabilities, while uncertainty-aware perception approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven perception-to-policy learning approach that integrates distribution-free, finite-sample safety guarantees into semantic segmentation, yielding calibrated hazard maps with rigorous bounds for missed detections. These maps induce risk-aware cost fields for downstream RL planning. Across two satellite-derived benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative baselines, achieving near-complete detection of unsafe regions while reducing hazardous violations during navigation (up to approx 50%). More importantly, our approach remains robust to distributional shift, preserving both safety and efficiency.
<div><strong>Authors:</strong> Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces COPPOL, a conformal‑prediction‑driven method that yields calibrated semantic‑segmentation hazard maps with finite‑sample safety guarantees and incorporates them into risk‑aware cost fields for reinforcement‑learning navigation. Across two satellite‑derived benchmarks, COPPOL achieves up to six‑fold higher hazard coverage and roughly 50% fewer safety violations while maintaining robustness under distributional shift.", "summary_cn": "本文提出 COPPOL，一种基于共形预测的感知到策略学习方法，生成具有有限样本安全保证的校准语义分割危害图，并将其转化为风险感知的代价场用于强化学习导航。在两个卫星衍生基准上，COPPOL 提升了最高 6 倍的危害覆盖率，并将导航中的安全违规降低约 50%，同时在分布漂移情况下保持鲁棒性。", "keywords": "conformal prediction, semantic segmentation, safety reinforcement learning, uncertainty quantification, hazard mapping, distributional shift, risk-aware planning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Daniel Bethell", "Simos Gerasimou", "Radu Calinescu", "Calum Imrie"]}
]]></acme>

<pubDate>2025-10-21T10:07:04+00:00</pubDate>
</item>
<item>
<title>Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation</title>
<link>https://papers.cool/arxiv/2510.18478</link>
<guid>https://papers.cool/arxiv/2510.18478</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Uncertain Safety Critic (USC), which integrates uncertainty‑aware modulation into safety‑critic training to concentrate conservatism in uncertain, high‑cost regions while keeping gradients sharp in safe areas. Experiments demonstrate that USC cuts safety violations by about 40% and improves cost‑gradient prediction error by roughly 83%, effectively breaking the usual safety‑performance trade‑off in reinforcement learning.<br /><strong>Summary (CN):</strong> 本文提出了不确定安全评论家（USC），在安全评论家的训练中加入不确定性感知调制，使保守性集中于不确定且高代价的区域，同时在安全区域保持梯度的锐利性。实验表明，USC 将安全违规率降低约40%，并将成本梯度预测误差降低约83%，从而有效突破强化学习中安全与性能之间的传统权衡。<br /><strong>Keywords:</strong> safe reinforcement learning, safety critic, uncertainty-aware modulation, reward-safety trade-off, conservative safety, RL safety, uncertainty estimation, policy optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.
<div><strong>Authors:</strong> Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Uncertain Safety Critic (USC), which integrates uncertainty‑aware modulation into safety‑critic training to concentrate conservatism in uncertain, high‑cost regions while keeping gradients sharp in safe areas. Experiments demonstrate that USC cuts safety violations by about 40% and improves cost‑gradient prediction error by roughly 83%, effectively breaking the usual safety‑performance trade‑off in reinforcement learning.", "summary_cn": "本文提出了不确定安全评论家（USC），在安全评论家的训练中加入不确定性感知调制，使保守性集中于不确定且高代价的区域，同时在安全区域保持梯度的锐利性。实验表明，USC 将安全违规率降低约40%，并将成本梯度预测误差降低约83%，从而有效突破强化学习中安全与性能之间的传统权衡。", "keywords": "safe reinforcement learning, safety critic, uncertainty-aware modulation, reward-safety trade-off, conservative safety, RL safety, uncertainty estimation, policy optimization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Daniel Bethell", "Simos Gerasimou", "Radu Calinescu", "Calum Imrie"]}
]]></acme>

<pubDate>2025-10-21T09:57:44+00:00</pubDate>
</item>
<item>
<title>Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</title>
<link>https://papers.cool/arxiv/2510.18473</link>
<guid>https://papers.cool/arxiv/2510.18473</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a benchmark for fairness-aware graph neural networks (GNNs) applied to large-scale knowledge graphs (YAGO, DBpedia, Wikidata). It evaluates both in‑processing and preprocessing mitigation methods across various GNN backbones and early‑stopping settings, revealing distinct trade‑offs between prediction accuracy and fairness metrics compared to previous graph datasets. Key findings include the strong influence of backbone choice and early stopping on performance, preprocessing methods boosting fairness, and in‑processing methods improving accuracy.<br /><strong>Summary (CN):</strong> 本文针对大规模知识图谱（YAGO、DBpedia、Wikidata）开展了公平感知图神经网络（GNN）的基准评估，比较了不同的在训练中处理和预处理方法在多种GNN骨干网络和提前停止条件下的表现。结果显示，与以往图数据集相比，知识图谱在预测准确率与公平度量之间呈现更明显的权衡，且骨干网络和提前停止策略显著影响性能；预处理方法通常提升公平性，而在训练中处理的方法则更有利于预测准确度。<br /><strong>Keywords:</strong> fairness-aware GNN, knowledge graph, bias mitigation, graph neural network, benchmarking, fairness metrics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Yuya Sasaki</div>
Graph neural networks (GNNs) are powerful tools for learning from graph-structured data but often produce biased predictions with respect to sensitive attributes. Fairness-aware GNNs have been actively studied for mitigating biased predictions. However, no prior studies have evaluated fairness-aware GNNs on knowledge graphs, which are one of the most important graphs in many applications, such as recommender systems. Therefore, we introduce a benchmarking study on knowledge graphs. We generate new graphs from three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly larger than the existing graph datasets used in fairness studies. We benchmark inprocessing and preprocessing methods in different GNN backbones and early stopping conditions. We find several key insights: (i) knowledge graphs show different trends from existing datasets; clearer trade-offs between prediction accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii) the performance is largely affected by not only fairness-aware GNN methods but also GNN backbones and early stopping conditions, and (iii) preprocessing methods often improve fairness metrics, while inprocessing methods improve prediction accuracy.
<div><strong>Authors:</strong> Yuya Sasaki</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a benchmark for fairness-aware graph neural networks (GNNs) applied to large-scale knowledge graphs (YAGO, DBpedia, Wikidata). It evaluates both in‑processing and preprocessing mitigation methods across various GNN backbones and early‑stopping settings, revealing distinct trade‑offs between prediction accuracy and fairness metrics compared to previous graph datasets. Key findings include the strong influence of backbone choice and early stopping on performance, preprocessing methods boosting fairness, and in‑processing methods improving accuracy.", "summary_cn": "本文针对大规模知识图谱（YAGO、DBpedia、Wikidata）开展了公平感知图神经网络（GNN）的基准评估，比较了不同的在训练中处理和预处理方法在多种GNN骨干网络和提前停止条件下的表现。结果显示，与以往图数据集相比，知识图谱在预测准确率与公平度量之间呈现更明显的权衡，且骨干网络和提前停止策略显著影响性能；预处理方法通常提升公平性，而在训练中处理的方法则更有利于预测准确度。", "keywords": "fairness-aware GNN, knowledge graph, bias mitigation, graph neural network, benchmarking, fairness metrics", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Yuya Sasaki"]}
]]></acme>

<pubDate>2025-10-21T09:51:42+00:00</pubDate>
</item>
<item>
<title>Simple and Efficient Heterogeneous Temporal Graph Neural Network</title>
<link>https://papers.cool/arxiv/2510.18467</link>
<guid>https://papers.cool/arxiv/2510.18467</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SE-HTGNN, a simple and efficient heterogeneous temporal graph neural network that integrates temporal modeling directly into spatial attention via a dynamic attention mechanism, retaining historical attention information to improve representation learning. It also leverages large language models to prompt the network, capturing implicit node-type properties as prior knowledge. Experiments show up to a 10× speed‑up over state‑of‑the‑art baselines while achieving superior forecasting accuracy.<br /><strong>Summary (CN):</strong> 本文提出了 SE-HTGNN，一种简洁高效的异构时序图神经网络，通过动态注意力机制将时序建模直接融合到空间注意力中，保留历史快照的注意力信息以提升表示学习效果。并利用大语言模型对 SE-HTGNN 进行提示，捕获节点类型的隐含属性作为先验知识。实验表明该方法在保持最佳预测精度的同时，实现了相较于最先进基线高达 10 倍的加速。<br /><strong>Keywords:</strong> heterogeneous temporal graph, dynamic attention, graph neural network, temporal modeling, LLM prompting, efficiency, representation learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao</div>
Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy.
<div><strong>Authors:</strong> Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SE-HTGNN, a simple and efficient heterogeneous temporal graph neural network that integrates temporal modeling directly into spatial attention via a dynamic attention mechanism, retaining historical attention information to improve representation learning. It also leverages large language models to prompt the network, capturing implicit node-type properties as prior knowledge. Experiments show up to a 10× speed‑up over state‑of‑the‑art baselines while achieving superior forecasting accuracy.", "summary_cn": "本文提出了 SE-HTGNN，一种简洁高效的异构时序图神经网络，通过动态注意力机制将时序建模直接融合到空间注意力中，保留历史快照的注意力信息以提升表示学习效果。并利用大语言模型对 SE-HTGNN 进行提示，捕获节点类型的隐含属性作为先验知识。实验表明该方法在保持最佳预测精度的同时，实现了相较于最先进基线高达 10 倍的加速。", "keywords": "heterogeneous temporal graph, dynamic attention, graph neural network, temporal modeling, LLM prompting, efficiency, representation learning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yili Wang", "Tairan Huang", "Changlong He", "Qiutong Li", "Jianliang Gao"]}
]]></acme>

<pubDate>2025-10-21T09:43:08+00:00</pubDate>
</item>
<item>
<title>Learning Boltzmann Generators via Constrained Mass Transport</title>
<link>https://papers.cool/arxiv/2510.18460</link>
<guid>https://papers.cool/arxiv/2510.18460</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Learning Boltzmann Generators via Constrained Mass Transport proposes a variational framework that generates intermediate distributions while constraining KL divergence and entropy decay between steps, reducing mode collapse and mass teleportation. The method achieves over 2.5× higher effective sample size than prior variational baselines on standard benchmarks and a new ELIL tetrapeptide system without using molecular dynamics samples. Experiments demonstrate improved distributional overlap and stability across multimodal high‑dimensional targets.<br /><strong>Summary (CN):</strong> 本文提出了受约束质量传输（Constrained Mass Transport）框架，用于在生成玻尔兹曼生成器（Boltzmann Generators）时通过约束相邻步骤之间的 KL 散度和熵衰减来生成中间分布，从而缓解模式崩溃和质量瞬移问题。该方法在标准基准以及首次引入的 ELIL 四肽系统上实现了超过 2.5 倍的有效样本量提升，且无需使用分子动力学采样。实验表明该方法能够提升分布重叠度并在高维多模态目标上保持更好的采样稳定性。<br /><strong>Keywords:</strong> Boltzmann generators, constrained mass transport, variational inference, KL divergence constraint, entropy decay, mode collapse, effective sample size, molecular sampling, ELIL tetrapeptide<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Christopher von Klitzing, Denis Blessing, Henrik Schopmans, Pascal Friederich, Gerhard Neumann</div>
Efficient sampling from high-dimensional and multimodal unnormalized probability distributions is a central challenge in many areas of science and machine learning. We focus on Boltzmann generators (BGs) that aim to sample the Boltzmann distribution of physical systems, such as molecules, at a given temperature. Classical variational approaches that minimize the reverse Kullback-Leibler divergence are prone to mode collapse, while annealing-based methods, commonly using geometric schedules, can suffer from mass teleportation and rely heavily on schedule tuning. We introduce Constrained Mass Transport (CMT), a variational framework that generates intermediate distributions under constraints on both the KL divergence and the entropy decay between successive steps. These constraints enhance distributional overlap, mitigate mass teleportation, and counteract premature convergence. Across standard BG benchmarks and the here introduced ELIL tetrapeptide, the largest system studied to date without access to samples from molecular dynamics, CMT consistently surpasses state-of-the-art variational methods, achieving more than 2.5x higher effective sample size while avoiding mode collapse.
<div><strong>Authors:</strong> Christopher von Klitzing, Denis Blessing, Henrik Schopmans, Pascal Friederich, Gerhard Neumann</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Learning Boltzmann Generators via Constrained Mass Transport proposes a variational framework that generates intermediate distributions while constraining KL divergence and entropy decay between steps, reducing mode collapse and mass teleportation. The method achieves over 2.5× higher effective sample size than prior variational baselines on standard benchmarks and a new ELIL tetrapeptide system without using molecular dynamics samples. Experiments demonstrate improved distributional overlap and stability across multimodal high‑dimensional targets.", "summary_cn": "本文提出了受约束质量传输（Constrained Mass Transport）框架，用于在生成玻尔兹曼生成器（Boltzmann Generators）时通过约束相邻步骤之间的 KL 散度和熵衰减来生成中间分布，从而缓解模式崩溃和质量瞬移问题。该方法在标准基准以及首次引入的 ELIL 四肽系统上实现了超过 2.5 倍的有效样本量提升，且无需使用分子动力学采样。实验表明该方法能够提升分布重叠度并在高维多模态目标上保持更好的采样稳定性。", "keywords": "Boltzmann generators, constrained mass transport, variational inference, KL divergence constraint, entropy decay, mode collapse, effective sample size, molecular sampling, ELIL tetrapeptide", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Christopher von Klitzing", "Denis Blessing", "Henrik Schopmans", "Pascal Friederich", "Gerhard Neumann"]}
]]></acme>

<pubDate>2025-10-21T09:34:01+00:00</pubDate>
</item>
<item>
<title>Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization</title>
<link>https://papers.cool/arxiv/2510.18410</link>
<guid>https://papers.cool/arxiv/2510.18410</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Momentum-Adaptive Gradient Dropout (MAGDrop), a regularization technique that modulates dropout rates on activations using current gradients and accumulated momentum, and provides a tightened PAC-Bayes generalization bound reflecting this adaptivity. Empirical results on MNIST and CIFAR-10 show MAGDrop improves test accuracy and reduces generalization gaps compared to standard dropout and other adaptive regularizers. The work bridges theoretical analysis with practical regularization, offering a framework aimed at improving deep network generalization for high‑stakes applications.<br /><strong>Summary (CN):</strong> 本文提出动量自适应梯度 Dropout（MAGDrop），一种依据当前梯度和累计动量动态调整激活层 dropout 率的正则化方法，并推导了反映该自适应性的紧致 PAC‑Bayes 泛化上界。实验在 MNIST 与 CIFAR‑10 上显示，MAGDrop 相较于标准 dropout 与其他自适应正则化器提升了测试准确率并显著缩小了泛化差距。该工作将理论分析与实用正则化相结合，为提升深度网络在高风险应用场景下的泛化能力提供了框架。<br /><strong>Keywords:</strong> generalization, deep neural networks, adaptive regularization, dropout, momentum, PAC-Bayes, overfitting, theoretical bounds, empirical evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Adeel Safder</div>
Deep neural networks (DNNs) achieve remarkable performance but often suffer from overfitting due to their high capacity. We introduce Momentum-Adaptive Gradient Dropout (MAGDrop), a novel regularization method that dynamically adjusts dropout rates on activations based on current gradients and accumulated momentum, enhancing stability in non-convex optimization landscapes. To theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes generalization bound that accounts for its adaptive nature, achieving up to 20% sharper bounds compared to standard approaches by leveraging momentum-driven perturbation control. Empirically, the activation-based MAGDrop outperforms baseline regularization techniques, including standard dropout and adaptive gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively. Our work bridges theoretical insights and practical advancements, offering a robust framework for enhancing DNN generalization suitable for high-stakes applications.
<div><strong>Authors:</strong> Adeel Safder</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Momentum-Adaptive Gradient Dropout (MAGDrop), a regularization technique that modulates dropout rates on activations using current gradients and accumulated momentum, and provides a tightened PAC-Bayes generalization bound reflecting this adaptivity. Empirical results on MNIST and CIFAR-10 show MAGDrop improves test accuracy and reduces generalization gaps compared to standard dropout and other adaptive regularizers. The work bridges theoretical analysis with practical regularization, offering a framework aimed at improving deep network generalization for high‑stakes applications.", "summary_cn": "本文提出动量自适应梯度 Dropout（MAGDrop），一种依据当前梯度和累计动量动态调整激活层 dropout 率的正则化方法，并推导了反映该自适应性的紧致 PAC‑Bayes 泛化上界。实验在 MNIST 与 CIFAR‑10 上显示，MAGDrop 相较于标准 dropout 与其他自适应正则化器提升了测试准确率并显著缩小了泛化差距。该工作将理论分析与实用正则化相结合，为提升深度网络在高风险应用场景下的泛化能力提供了框架。", "keywords": "generalization, deep neural networks, adaptive regularization, dropout, momentum, PAC-Bayes, overfitting, theoretical bounds, empirical evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Adeel Safder"]}
]]></acme>

<pubDate>2025-10-21T08:36:56+00:00</pubDate>
</item>
<item>
<title>Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</title>
<link>https://papers.cool/arxiv/2510.18406</link>
<guid>https://papers.cool/arxiv/2510.18406</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a method for learning from N‑tuple data where only the total number of positive instances (M) per tuple is known, deriving an unbiased risk estimator and providing theoretical generalization and consistency guarantees, along with practical ReLU‑based corrections for stability.<br /><strong>Summary (CN):</strong> 本文提出一种在仅观察到每个 N‑tuple 中正例数量 (M) 的情况下进行学习的方法，推导出无偏风险估计器并给出理论上的泛化与一致性保证，同时引入基于 ReLU 的修正以提升有限样本的稳定性。<br /><strong>Keywords:</strong> weak supervision, N-tuple learning, unbiased risk estimator, multi-instance learning, Rademacher complexity, statistical consistency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</div>
Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
<div><strong>Authors:</strong> Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a method for learning from N‑tuple data where only the total number of positive instances (M) per tuple is known, deriving an unbiased risk estimator and providing theoretical generalization and consistency guarantees, along with practical ReLU‑based corrections for stability.", "summary_cn": "本文提出一种在仅观察到每个 N‑tuple 中正例数量 (M) 的情况下进行学习的方法，推导出无偏风险估计器并给出理论上的泛化与一致性保证，同时引入基于 ReLU 的修正以提升有限样本的稳定性。", "keywords": "weak supervision, N-tuple learning, unbiased risk estimator, multi-instance learning, Rademacher complexity, statistical consistency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Miao Zhang", "Junpeng Li", "ChangChun HUa", "Yana Yang"]}
]]></acme>

<pubDate>2025-10-21T08:28:07+00:00</pubDate>
</item>
<item>
<title>Approximation Rates of Shallow Neural Networks: Barron Spaces, Activation Functions and Optimality Analysis</title>
<link>https://papers.cool/arxiv/2510.18388</link>
<guid>https://papers.cool/arxiv/2510.18388</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies the approximation capabilities of shallow neural networks whose activation functions are powers of exponential functions, focusing on how the rates depend on input dimension and target function smoothness within Barron spaces. It proves that optimal rates cannot be achieved with \ell^{1}-bounded coefficients or insufficient smoothness, and establishes optimal rates in various norms for functions in Barron and Sobolev spaces, highlighting the inherent curse of dimensionality.<br /><strong>Summary (CN):</strong> 本文研究了使用指数函数幂次作为激活函数的浅层神经网络的近似能力，重点考察在 Barron 空间中维度和目标函数光滑度对近似速率的影响。研究表明在 \ell^{1} 系数受限或光滑度不足的情况下无法实现最优速率，并为 Barron 空间和 Sobolev 空间中的函数在多种范数下给出最优近似速率，确认了维度诅咒的存在。<br /><strong>Keywords:</strong> shallow neural networks, Barron space, approximation rates, activation functions, ReLU^k, curse of dimensionality, Sobolev spaces, theoretical analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jian Lu, Xiaohuang Huang</div>
This paper investigates the approximation properties of shallow neural networks with activation functions that are powers of exponential functions. It focuses on the dependence of the approximation rate on the dimension and the smoothness of the function being approximated within the Barron function space. We examine the approximation rates of ReLU$^{k}$ activation functions, proving that the optimal rate cannot be achieved under $\ell^{1}$-bounded coefficients or insufficient smoothness conditions. We also establish optimal approximation rates in various norms for functions in Barron spaces and Sobolev spaces, confirming the curse of dimensionality. Our results clarify the limits of shallow neural networks' approximation capabilities and offer insights into the selection of activation functions and network structures.
<div><strong>Authors:</strong> Jian Lu, Xiaohuang Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies the approximation capabilities of shallow neural networks whose activation functions are powers of exponential functions, focusing on how the rates depend on input dimension and target function smoothness within Barron spaces. It proves that optimal rates cannot be achieved with \\ell^{1}-bounded coefficients or insufficient smoothness, and establishes optimal rates in various norms for functions in Barron and Sobolev spaces, highlighting the inherent curse of dimensionality.", "summary_cn": "本文研究了使用指数函数幂次作为激活函数的浅层神经网络的近似能力，重点考察在 Barron 空间中维度和目标函数光滑度对近似速率的影响。研究表明在 \\ell^{1} 系数受限或光滑度不足的情况下无法实现最优速率，并为 Barron 空间和 Sobolev 空间中的函数在多种范数下给出最优近似速率，确认了维度诅咒的存在。", "keywords": "shallow neural networks, Barron space, approximation rates, activation functions, ReLU^k, curse of dimensionality, Sobolev spaces, theoretical analysis", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jian Lu", "Xiaohuang Huang"]}
]]></acme>

<pubDate>2025-10-21T08:08:35+00:00</pubDate>
</item>
<item>
<title>Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study</title>
<link>https://papers.cool/arxiv/2510.18370</link>
<guid>https://papers.cool/arxiv/2510.18370</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first systematic empirical study of diversification techniques for training multiple graph neural network experts within a mixture‑of‑experts ensemble. By evaluating 20 strategies—including random re‑initialization, hyperparameter variation, architectural changes, directionality modeling, and data partitioning—across 14 node‑classification benchmarks, the authors analyze expert diversity, complementarity, and overall ensemble performance, offering practical guidance for building effective GNN MoE systems.<br /><strong>Summary (CN):</strong> 本文首次系统性地研究了在图神经网络（GNN）混合专家（Mixture‑of‑Experts）框架中训练多样化专家的技术。通过在 14 个节点分类基准上评估 20 种多样化策略（包括随机重新初始化、超参数调节、架构变化、方向建模和数据划分），分析了专家的多样性、互补性以及整体集成性能，并提供了构建高效 GNN MoE 系统的实用指导。<br /><strong>Keywords:</strong> graph neural networks, mixture of experts, ensemble learning, expert diversification, node classification, graph heterogeneity, systematic study, performance improvement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Gangda Deng, Yuxin Yang, Ömer Faruk Akgül, Hanqing Zeng, Yinglong Xia, Rajgopal Kannan, Viktor Prasanna</div>
Graph Neural Networks (GNNs) have become essential tools for learning on relational data, yet the performance of a single GNN is often limited by the heterogeneity present in real-world graphs. Recent advances in Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple, explicitly diverse GNNs with distinct generalization patterns can significantly improve performance. In this work, we present the first systematic empirical study of expert-level diversification techniques for GNN ensembles. Evaluating 20 diversification strategies -- including random re-initialization, hyperparameter tuning, architectural variation, directionality modeling, and training data partitioning -- across 14 node classification benchmarks, we construct and analyze over 200 ensemble variants. Our comprehensive evaluation examines each technique in terms of expert diversity, complementarity, and ensemble performance. We also uncovers mechanistic insights into training maximally diverse experts. These findings provide actionable guidance for expert training and the design of effective MoE frameworks on graph data. Our code is available at https://github.com/Hydrapse/bench-gnn-diversification.
<div><strong>Authors:</strong> Gangda Deng, Yuxin Yang, Ömer Faruk Akgül, Hanqing Zeng, Yinglong Xia, Rajgopal Kannan, Viktor Prasanna</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first systematic empirical study of diversification techniques for training multiple graph neural network experts within a mixture‑of‑experts ensemble. By evaluating 20 strategies—including random re‑initialization, hyperparameter variation, architectural changes, directionality modeling, and data partitioning—across 14 node‑classification benchmarks, the authors analyze expert diversity, complementarity, and overall ensemble performance, offering practical guidance for building effective GNN MoE systems.", "summary_cn": "本文首次系统性地研究了在图神经网络（GNN）混合专家（Mixture‑of‑Experts）框架中训练多样化专家的技术。通过在 14 个节点分类基准上评估 20 种多样化策略（包括随机重新初始化、超参数调节、架构变化、方向建模和数据划分），分析了专家的多样性、互补性以及整体集成性能，并提供了构建高效 GNN MoE 系统的实用指导。", "keywords": "graph neural networks, mixture of experts, ensemble learning, expert diversification, node classification, graph heterogeneity, systematic study, performance improvement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gangda Deng", "Yuxin Yang", "Ömer Faruk Akgül", "Hanqing Zeng", "Yinglong Xia", "Rajgopal Kannan", "Viktor Prasanna"]}
]]></acme>

<pubDate>2025-10-21T07:40:51+00:00</pubDate>
</item>
<item>
<title>Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming</title>
<link>https://papers.cool/arxiv/2510.18363</link>
<guid>https://papers.cool/arxiv/2510.18363</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GraphRTA, a framework for unsupervised open-set graph domain adaptation that reprograms both the target graph structure and the model by pruning domain-specific parameters, and adds an extra classifier dimension for unknown classes to eliminate threshold tuning. Experiments on multiple datasets show competitive performance against recent baselines.<br /><strong>Summary (CN):</strong> 本文提出了 GraphRTA 框架，用于无监督开放集合图域适应，通过修改目标图的结构和节点特征以及剪枝模型的域特定参数来实现图和模型的双重重编程，并在分类器中加入未知类别维度，以免除手动阈值设置。实验在多个公开数据集上表明该方法相较于最新基线取得了满意的性能。<br /><strong>Keywords:</strong> open-set graph domain adaptation, graph reprogramming, model pruning, unknown class detection, unsupervised learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zhen Zhang, Bingsheng He</div>
Unsupervised Graph Domain Adaptation has become a promising paradigm for transferring knowledge from a fully labeled source graph to an unlabeled target graph. Existing graph domain adaptation models primarily focus on the closed-set setting, where the source and target domains share the same label spaces. However, this assumption might not be practical in the real-world scenarios, as the target domain might include classes that are not present in the source domain. In this paper, we investigate the problem of unsupervised open-set graph domain adaptation, where the goal is to not only correctly classify target nodes into the known classes, but also recognize previously unseen node types into the unknown class. Towards this end, we propose a novel framework called GraphRTA, which conducts reprogramming on both the graph and model sides. Specifically, we reprogram the graph by modifying target graph structure and node features, which facilitates better separation of known and unknown classes. Meanwhile, we also perform model reprogramming by pruning domain-specific parameters to reduce bias towards the source graph while preserving parameters that capture transferable patterns across graphs. Additionally, we extend the classifier with an extra dimension for the unknown class, thus eliminating the need of manually specified threshold in open-set recognition. Comprehensive experiments on several public datasets demonstrate that our proposed model can achieve satisfied performance compared with recent state-of-the-art baselines. Our source codes and datasets are publicly available at https://github.com/cszhangzhen/GraphRTA.
<div><strong>Authors:</strong> Zhen Zhang, Bingsheng He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GraphRTA, a framework for unsupervised open-set graph domain adaptation that reprograms both the target graph structure and the model by pruning domain-specific parameters, and adds an extra classifier dimension for unknown classes to eliminate threshold tuning. Experiments on multiple datasets show competitive performance against recent baselines.", "summary_cn": "本文提出了 GraphRTA 框架，用于无监督开放集合图域适应，通过修改目标图的结构和节点特征以及剪枝模型的域特定参数来实现图和模型的双重重编程，并在分类器中加入未知类别维度，以免除手动阈值设置。实验在多个公开数据集上表明该方法相较于最新基线取得了满意的性能。", "keywords": "open-set graph domain adaptation, graph reprogramming, model pruning, unknown class detection, unsupervised learning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zhen Zhang", "Bingsheng He"]}
]]></acme>

<pubDate>2025-10-21T07:34:58+00:00</pubDate>
</item>
<item>
<title>Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding</title>
<link>https://papers.cool/arxiv/2510.18360</link>
<guid>https://papers.cool/arxiv/2510.18360</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FGP, a pre‑training approach that trains a neural architecture encoder to reconstruct a surrogate representation of information flow, allowing the encoder to capture flow information without specialized model structures. Experiments show that FGP improves encoder performance by up to 106% in Precision‑1% compared to the same encoder trained only with supervised learning, while being computationally faster than existing flow‑based encoders.<br /><strong>Summary (CN):</strong> 本文提出了 FGP，一种预训练方法，使神经网络结构编码器学习重建信息流的代理表示，从而在无需专门模型结构的情况下捕获信息流特征。实验表明，与仅使用监督学习的同一编码器相比，FGP 在 Precision‑1% 指标上提升最高达 106%，且处理速度快于现有基于信息流的编码器。<br /><strong>Keywords:</strong> neural architecture encoding, information flow, pretraining, flow surrogate, performance prediction, neural architecture search, representation learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sunwoo Kim, Hyunjin Hwang, Kijung Shin</div>
The performance of a deep learning model on a specific task and dataset depends heavily on its neural architecture, motivating considerable efforts to rapidly and accurately identify architectures suited to the target task and dataset. To achieve this, researchers use machine learning models-typically neural architecture encoders-to predict the performance of a neural architecture. Many state-of-the-art encoders aim to capture information flow within a neural architecture, which reflects how information moves through the forward pass and backpropagation, via a specialized model structure. However, due to their complicated structures, these flow-based encoders are significantly slower to process neural architectures compared to simpler encoders, presenting a notable practical challenge. To address this, we propose FGP, a novel pre-training method for neural architecture encoding that trains an encoder to capture the information flow without requiring specialized model structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed representation of the neural architecture's information flow. Our experiments show that FGP boosts encoder performance by up to 106% in Precision-1%, compared to the same encoder trained solely with supervised learning.
<div><strong>Authors:</strong> Sunwoo Kim, Hyunjin Hwang, Kijung Shin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FGP, a pre‑training approach that trains a neural architecture encoder to reconstruct a surrogate representation of information flow, allowing the encoder to capture flow information without specialized model structures. Experiments show that FGP improves encoder performance by up to 106% in Precision‑1% compared to the same encoder trained only with supervised learning, while being computationally faster than existing flow‑based encoders.", "summary_cn": "本文提出了 FGP，一种预训练方法，使神经网络结构编码器学习重建信息流的代理表示，从而在无需专门模型结构的情况下捕获信息流特征。实验表明，与仅使用监督学习的同一编码器相比，FGP 在 Precision‑1% 指标上提升最高达 106%，且处理速度快于现有基于信息流的编码器。", "keywords": "neural architecture encoding, information flow, pretraining, flow surrogate, performance prediction, neural architecture search, representation learning", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sunwoo Kim", "Hyunjin Hwang", "Kijung Shin"]}
]]></acme>

<pubDate>2025-10-21T07:29:15+00:00</pubDate>
</item>
<item>
<title>Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers</title>
<link>https://papers.cool/arxiv/2510.18358</link>
<guid>https://papers.cool/arxiv/2510.18358</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Hydra Ensembles, an efficient transformer‑based ensemble that creates diverse members by pruning attention heads and combines them with a novel grouped multi‑head attention, achieving uncertainty quantification performance comparable to or better than Deep Ensembles while keeping inference speed close to a single model. Extensive experiments on image and text classification, including zero‑shot ImageNet‑1k, demonstrate that the method preserves calibration and provides state‑of‑the‑art results without additional training analysis of pruning strategies shows that naïve pruning harms calibration, whereas the proposed approach maintains robust uncertainty estimates.<br /><strong>Summary (CN):</strong> 本文提出了 Hydra Ensembles，一种通过剪枝注意力头创建多样化成员并使用分组全连接层的多头注意力进行融合的高效 Transformer 集成方法，实现了与 Deep Ensembles 相当或更佳的不确定性量化性能，同时推理速度接近单模型。广泛的图像和文本分类实验（包括 ImageNet‑1k 零样本分类）表明，该方法在无需额外训练的情况下保持良好校准并达到最新水平。对剪枝的深入分析显示，传统剪枝会损害校准，而本文的方法能够保持稳健的不确定性估计。<br /><strong>Keywords:</strong> uncertainty quantification, deep ensembles, transformer pruning, attention head pruning, Hydra Ensembles, model calibration, efficient inference, zero-shot classification<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Firas Gabetni, Giuseppe Curci, Andrea Pilzer, Subhankar Roy, Elisa Ricci, Gianni Franchi</div>
Uncertainty quantification (UQ) is essential for deploying deep neural networks in safety-critical settings. Although methods like Deep Ensembles achieve strong UQ performance, their high computational and memory costs hinder scalability to large models. We introduce Hydra Ensembles, an efficient transformer-based ensemble that prunes attention heads to create diverse members and merges them via a new multi-head attention with grouped fully-connected layers. This yields a compact model with inference speed close to a single network, matching or surpassing Deep Ensembles in UQ performance without retraining from scratch. We also provide an in-depth analysis of pruning, showing that naive approaches can harm calibration, whereas Hydra Ensembles preserves robust uncertainty. Experiments on image and text classification tasks, with various architectures, show consistent gains over Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our approach surpasses state of the art methods, even without requiring additional training.
<div><strong>Authors:</strong> Firas Gabetni, Giuseppe Curci, Andrea Pilzer, Subhankar Roy, Elisa Ricci, Gianni Franchi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Hydra Ensembles, an efficient transformer‑based ensemble that creates diverse members by pruning attention heads and combines them with a novel grouped multi‑head attention, achieving uncertainty quantification performance comparable to or better than Deep Ensembles while keeping inference speed close to a single model. Extensive experiments on image and text classification, including zero‑shot ImageNet‑1k, demonstrate that the method preserves calibration and provides state‑of‑the‑art results without additional training analysis of pruning strategies shows that naïve pruning harms calibration, whereas the proposed approach maintains robust uncertainty estimates.", "summary_cn": "本文提出了 Hydra Ensembles，一种通过剪枝注意力头创建多样化成员并使用分组全连接层的多头注意力进行融合的高效 Transformer 集成方法，实现了与 Deep Ensembles 相当或更佳的不确定性量化性能，同时推理速度接近单模型。广泛的图像和文本分类实验（包括 ImageNet‑1k 零样本分类）表明，该方法在无需额外训练的情况下保持良好校准并达到最新水平。对剪枝的深入分析显示，传统剪枝会损害校准，而本文的方法能够保持稳健的不确定性估计。", "keywords": "uncertainty quantification, deep ensembles, transformer pruning, attention head pruning, Hydra Ensembles, model calibration, efficient inference, zero-shot classification", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Firas Gabetni", "Giuseppe Curci", "Andrea Pilzer", "Subhankar Roy", "Elisa Ricci", "Gianni Franchi"]}
]]></acme>

<pubDate>2025-10-21T07:26:38+00:00</pubDate>
</item>
<item>
<title>Computable universal online learning</title>
<link>https://papers.cool/arxiv/2510.18352</link>
<guid>https://papers.cool/arxiv/2510.18352</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates when universal online learning can be realized by an actual computer program, distinguishing computable from non‑computable learning strategies and providing exact characterizations for classes that are learnable under computable‑universal and agnostic settings. It also analyses proper universal online learning variants, highlighting limits of computability in online binary classification theory.<br /><strong>Summary (CN):</strong> 本文探讨了通用在线学习能否由实际计算机程序实现，区分可计算与不可计算的学习策略，并对在可计算‑通用和容错（agnostic）设置下可学习的类别给出精确刻画。同时分析了正则通用在线学习的变体，凸显了在线二元分类理论中计算能力的局限。<br /><strong>Keywords:</strong> online learning, universal learning, computable learning, binary classification, agnostic learning, inductive inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dariusz Kalociński, Tomasz Steifer</div>
Understanding when learning is possible is a fundamental task in the theory of machine learning. However, many characterizations known from the literature deal with abstract learning as a mathematical object and ignore the crucial question: when can learning be implemented as a computer program? We address this question for universal online learning, a generalist theoretical model of online binary classification, recently characterized by Bousquet et al. (STOC'21). In this model, there is no hypothesis fixed in advance; instead, Adversary -- playing the role of Nature -- can change their mind as long as local consistency with the given class of hypotheses is maintained. We require Learner to achieve a finite number of mistakes while using a strategy that can be implemented as a computer program. We show that universal online learning does not imply computable universal online learning, even if the class of hypotheses is relatively easy from a computability-theoretic perspective. We then study the agnostic variant of computable universal online learning and provide an exact characterization of classes that are learnable in this sense. We also consider a variant of proper universal online learning and show exactly when it is possible. Together, our results give a more realistic perspective on the existing theory of online binary classification and the related problem of inductive inference.
<div><strong>Authors:</strong> Dariusz Kalociński, Tomasz Steifer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates when universal online learning can be realized by an actual computer program, distinguishing computable from non‑computable learning strategies and providing exact characterizations for classes that are learnable under computable‑universal and agnostic settings. It also analyses proper universal online learning variants, highlighting limits of computability in online binary classification theory.", "summary_cn": "本文探讨了通用在线学习能否由实际计算机程序实现，区分可计算与不可计算的学习策略，并对在可计算‑通用和容错（agnostic）设置下可学习的类别给出精确刻画。同时分析了正则通用在线学习的变体，凸显了在线二元分类理论中计算能力的局限。", "keywords": "online learning, universal learning, computable learning, binary classification, agnostic learning, inductive inference", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dariusz Kalociński", "Tomasz Steifer"]}
]]></acme>

<pubDate>2025-10-21T07:21:32+00:00</pubDate>
</item>
<item>
<title>Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs</title>
<link>https://papers.cool/arxiv/2510.18340</link>
<guid>https://papers.cool/arxiv/2510.18340</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes the classical policy gradient method for infinite-horizon MDPs with undiscounted total reward (gamma = 1). It shows that the classification of states into recurrent and transient remains invariant for policies that assign strictly positive probability to every action, and introduces a new transient visitation measure to replace the ill-defined state visitation measure used with discounts. These insights provide convergence guarantees for policy gradient algorithms in the undiscounted setting, which is relevant for large language model training.<br /><strong>Summary (CN):</strong> 本文研究了在无限时域、累计总回报（gamma = 1）下的策略梯度方法。作者证明，对于对每个动作都给出正概率的策略（如软最大输出），状态的循环/瞬时划分保持不变，并提出了“瞬时访问度量”（transient visitation measure）来取代在折扣为1时不适用的传统访问度量，从而为无折扣环境中的策略梯度收敛提供了理论保障。<br /><strong>Keywords:</strong> policy gradient, undiscounted MDP, transient visitation measure, recurrent states, reinforcement learning theory, infinite-horizon, softmax policies<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jongmin Lee, Ernest K. Ryu</div>
The classical policy gradient method is the theoretical and conceptual foundation of modern policy-based reinforcement learning (RL) algorithms. Most rigorous analyses of such methods, particularly those establishing convergence guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a recent line of work on policy-based RL for large language models uses the undiscounted total-reward setting with $\gamma = 1$, rendering much of the existing theory inapplicable. In this paper, we provide analyses of the policy gradient method for undiscounted expected total-reward infinite-horizon MDPs based on two key insights: (i) the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action (as is typical in deep RL models employing a softmax output layer) and (ii) the classical state visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced with a new object that we call the transient visitation measure.
<div><strong>Authors:</strong> Jongmin Lee, Ernest K. Ryu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes the classical policy gradient method for infinite-horizon MDPs with undiscounted total reward (gamma = 1). It shows that the classification of states into recurrent and transient remains invariant for policies that assign strictly positive probability to every action, and introduces a new transient visitation measure to replace the ill-defined state visitation measure used with discounts. These insights provide convergence guarantees for policy gradient algorithms in the undiscounted setting, which is relevant for large language model training.", "summary_cn": "本文研究了在无限时域、累计总回报（gamma = 1）下的策略梯度方法。作者证明，对于对每个动作都给出正概率的策略（如软最大输出），状态的循环/瞬时划分保持不变，并提出了“瞬时访问度量”（transient visitation measure）来取代在折扣为1时不适用的传统访问度量，从而为无折扣环境中的策略梯度收敛提供了理论保障。", "keywords": "policy gradient, undiscounted MDP, transient visitation measure, recurrent states, reinforcement learning theory, infinite-horizon, softmax policies", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jongmin Lee", "Ernest K. Ryu"]}
]]></acme>

<pubDate>2025-10-21T06:46:21+00:00</pubDate>
</item>
<item>
<title>Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</title>
<link>https://papers.cool/arxiv/2510.18328</link>
<guid>https://papers.cool/arxiv/2510.18328</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Time-Conditioned Contraction Matching (TCCM), a semi-supervised anomaly detection method for tabular data that learns a velocity field toward a fixed target and provides a lightweight, scalable training objective. TCCM offers an efficient one-step scoring strategy, feature-wise explainability, and provable Lipschitz robustness, achieving strong performance on the ADBench benchmark with reduced inference cost. Experiments demonstrate its advantages over diffusion‑based models, especially on high‑dimensional and large‑scale datasets.<br /><strong>Summary (CN):</strong> 本文提出了时间条件收缩匹配（TCCM），一种用于表格数据的半监督异常检测方法，通过学习指向固定目标（原点）的速度场，实现轻量级、可扩展的训练目标。TCCM 提供了一步得分策略，使异常分数具备特征层面的可解释性，并在输入空间上具备 Lipschitz 连续性，提供理论鲁棒性保证，在 ADBench 基准上表现优异，尤其在高维大规模数据集上优于基于扩散的模型。<br /><strong>Keywords:</strong> anomaly detection, flow matching, time-conditioned contraction, explainability, provable robustness, tabular data, one-step scoring, velocity field<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</div>
We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.
<div><strong>Authors:</strong> Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Time-Conditioned Contraction Matching (TCCM), a semi-supervised anomaly detection method for tabular data that learns a velocity field toward a fixed target and provides a lightweight, scalable training objective. TCCM offers an efficient one-step scoring strategy, feature-wise explainability, and provable Lipschitz robustness, achieving strong performance on the ADBench benchmark with reduced inference cost. Experiments demonstrate its advantages over diffusion‑based models, especially on high‑dimensional and large‑scale datasets.", "summary_cn": "本文提出了时间条件收缩匹配（TCCM），一种用于表格数据的半监督异常检测方法，通过学习指向固定目标（原点）的速度场，实现轻量级、可扩展的训练目标。TCCM 提供了一步得分策略，使异常分数具备特征层面的可解释性，并在输入空间上具备 Lipschitz 连续性，提供理论鲁棒性保证，在 ADBench 基准上表现优异，尤其在高维大规模数据集上优于基于扩散的模型。", "keywords": "anomaly detection, flow matching, time-conditioned contraction, explainability, provable robustness, tabular data, one-step scoring, velocity field", "scoring": {"interpretability": 7, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Zhong Li", "Qi Huang", "Yuxuan Zhu", "Lincen Yang", "Mohammad Mohammadi Amiri", "Niki van Stein", "Matthijs van Leeuwen"]}
]]></acme>

<pubDate>2025-10-21T06:26:38+00:00</pubDate>
</item>
<item>
<title>Uncertainty Estimation by Flexible Evidential Deep Learning</title>
<link>https://papers.cool/arxiv/2510.18322</link>
<guid>https://papers.cool/arxiv/2510.18322</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Flexible Evidential Deep Learning (F-EDL), which generalizes standard evidential deep learning by predicting a flexible Dirichlet distribution over class probabilities, offering a more expressive representation of uncertainty. The authors provide theoretical advantages of F-EDL and demonstrate state-of-the-art performance on uncertainty quantification across classical, long-tailed, and noisy in-distribution scenarios.<br /><strong>Summary (CN):</strong> 本文提出了灵活证据深度学习（F-EDL），通过对类别概率预测灵活的 Dirichlet 分布，扩展了传统证据深度学习的不确定性表达能力。作者给出了理论优势，并在经典、长尾和噪声分布等场景中展示了其在不确定性量化方面的最先进性能。<br /><strong>Keywords:</strong> uncertainty quantification, evidential deep learning, flexible Dirichlet distribution, classification, out-of-distribution, long-tailed, noisy labels<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Taeseong Yoon, Heeyoung Kim</div>
Uncertainty quantification (UQ) is crucial for deploying machine learning models in high-stakes applications, where overconfident predictions can lead to serious consequences. An effective UQ method must balance computational efficiency with the ability to generalize across diverse scenarios. Evidential deep learning (EDL) achieves efficiency by modeling uncertainty through the prediction of a Dirichlet distribution over class probabilities. However, the restrictive assumption of Dirichlet-distributed class probabilities limits EDL's robustness, particularly in complex or unforeseen situations. To address this, we propose \textit{flexible evidential deep learning} ($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet distribution -- a generalization of the Dirichlet distribution -- over class probabilities. This approach provides a more expressive and adaptive representation of uncertainty, significantly enhancing UQ generalization and reliability under challenging scenarios. We theoretically establish several advantages of $\mathcal{F}$-EDL and empirically demonstrate its state-of-the-art UQ performance across diverse evaluation settings, including classical, long-tailed, and noisy in-distribution scenarios.
<div><strong>Authors:</strong> Taeseong Yoon, Heeyoung Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Flexible Evidential Deep Learning (F-EDL), which generalizes standard evidential deep learning by predicting a flexible Dirichlet distribution over class probabilities, offering a more expressive representation of uncertainty. The authors provide theoretical advantages of F-EDL and demonstrate state-of-the-art performance on uncertainty quantification across classical, long-tailed, and noisy in-distribution scenarios.", "summary_cn": "本文提出了灵活证据深度学习（F-EDL），通过对类别概率预测灵活的 Dirichlet 分布，扩展了传统证据深度学习的不确定性表达能力。作者给出了理论优势，并在经典、长尾和噪声分布等场景中展示了其在不确定性量化方面的最先进性能。", "keywords": "uncertainty quantification, evidential deep learning, flexible Dirichlet distribution, classification, out-of-distribution, long-tailed, noisy labels", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Taeseong Yoon", "Heeyoung Kim"]}
]]></acme>

<pubDate>2025-10-21T06:12:33+00:00</pubDate>
</item>
<item>
<title>Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task</title>
<link>https://papers.cool/arxiv/2510.18315</link>
<guid>https://papers.cool/arxiv/2510.18315</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how the embedding dimension influences the emergence of an internal world model in a transformer trained via reinforcement learning to perform bubble-sort-style adjacent swaps. While high accuracy is achieved even with very small embeddings, larger dimensions produce more faithful, consistent, and robust internal representations, revealed through two mechanisms: the last row of the attention matrix encodes global token order, and the chosen transposition aligns with the largest adjacent difference of these encoded values. These findings provide quantitative evidence that larger embedding dimensions strengthen structured internal representations and improve interpretability.<br /><strong>Summary (CN):</strong> 本文研究了嵌入维度如何影响通过强化学习训练的 transformer 在冒泡排序式相邻交换任务中内部 world model 的形成。即使在极小的嵌入维度下模型也能达到高准确率，但更高的维度会产生更忠实、一致且鲁棒的内部表示，具体表现为两条机制：注意力权重矩阵的最后一行单调编码全局 token 顺序，选择的置换对齐于这些编码值的最大相邻差异。这些结果提供了量化证据，表明更高的嵌入维度能强化结构化内部表示并提升 interpretability。<br /><strong>Keywords:</strong> embedding dimension, transformer, world model, bubble sort, internal representation, attention matrix, algorithmic task, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Brady Bhalla, Honglu Fan, Nancy Chen, Tony Yue YU</div>
We investigate how embedding dimension affects the emergence of an internal "world model" in a transformer trained with reinforcement learning to perform bubble-sort-style adjacent swaps. Models achieve high accuracy even with very small embedding dimensions, but larger dimensions yield more faithful, consistent, and robust internal representations. In particular, higher embedding dimensions strengthen the formation of structured internal representation and lead to better interpretability. After hundreds of experiments, we observe two consistent mechanisms: (1) the last row of the attention weight matrix monotonically encodes the global ordering of tokens; and (2) the selected transposition aligns with the largest adjacent difference of these encoded values. Our results provide quantitative evidence that transformers build structured internal world models and that model size improves representation quality in addition to end performance. We release our metrics and analyses, which can be used to probe similar algorithmic tasks.
<div><strong>Authors:</strong> Brady Bhalla, Honglu Fan, Nancy Chen, Tony Yue YU</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how the embedding dimension influences the emergence of an internal world model in a transformer trained via reinforcement learning to perform bubble-sort-style adjacent swaps. While high accuracy is achieved even with very small embeddings, larger dimensions produce more faithful, consistent, and robust internal representations, revealed through two mechanisms: the last row of the attention matrix encodes global token order, and the chosen transposition aligns with the largest adjacent difference of these encoded values. These findings provide quantitative evidence that larger embedding dimensions strengthen structured internal representations and improve interpretability.", "summary_cn": "本文研究了嵌入维度如何影响通过强化学习训练的 transformer 在冒泡排序式相邻交换任务中内部 world model 的形成。即使在极小的嵌入维度下模型也能达到高准确率，但更高的维度会产生更忠实、一致且鲁棒的内部表示，具体表现为两条机制：注意力权重矩阵的最后一行单调编码全局 token 顺序，选择的置换对齐于这些编码值的最大相邻差异。这些结果提供了量化证据，表明更高的嵌入维度能强化结构化内部表示并提升 interpretability。", "keywords": "embedding dimension, transformer, world model, bubble sort, internal representation, attention matrix, algorithmic task, reinforcement learning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Brady Bhalla", "Honglu Fan", "Nancy Chen", "Tony Yue YU"]}
]]></acme>

<pubDate>2025-10-21T05:51:02+00:00</pubDate>
</item>
<item>
<title>Towards Identifiability of Hierarchical Temporal Causal Representation Learning</title>
<link>https://papers.cool/arxiv/2510.18310</link>
<guid>https://papers.cool/arxiv/2510.18310</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Causally Hierarchical Latent Dynamic (CHiLD) framework, which establishes identifiability of multi‑layer latent variables in time‑series data using three conditionally independent observations and leverages sparsity to recover hierarchical causal dynamics. It presents theoretical guarantees, a variational inference‑based generative model with contextual encoders and flow‑based hierarchical priors, and validates the approach on synthetic and real‑world datasets.<br /><strong>Summary (CN):</strong> 本文提出因果层次潜在动态（CHiLD）框架，通过利用三个条件独立的观测以及层次结构的稀疏性，实现时间序列数据中多层潜在变量的唯一可识别性。文中给出理论保证，构建基于变分推断的生成模型，结合上下文编码器和基于流的层次先验，并在合成及真实数据集上验证了方法的有效性。<br /><strong>Keywords:</strong> hierarchical latent dynamics, causal representation learning, identifiability, temporal causal models, variational inference, flow-based prior, CHiLD, multi-layer latent variables<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</div>
Modeling hierarchical latent dynamics behind time series data is critical for capturing temporal dependencies across multiple levels of abstraction in real-world tasks. However, existing temporal causal representation learning methods fail to capture such dynamics, as they fail to recover the joint distribution of hierarchical latent variables from \textit{single-timestep observed variables}. Interestingly, we find that the joint distribution of hierarchical latent variables can be uniquely determined using three conditionally independent observations. Building on this insight, we propose a Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our approach first employs temporal contextual observed variables to identify the joint distribution of multi-layer latent variables. Sequentially, we exploit the natural sparsity of the hierarchical structure among latent variables to identify latent variables within each layer. Guided by the theoretical results, we develop a time series generative model grounded in variational inference. This model incorporates a contextual encoder to reconstruct multi-layer latent variables and normalize flow-based hierarchical prior networks to impose the independent noise condition of hierarchical latent dynamics. Empirical evaluations on both synthetic and real-world datasets validate our theoretical claims and demonstrate the effectiveness of CHiLD in modeling hierarchical latent dynamics.
<div><strong>Authors:</strong> Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Causally Hierarchical Latent Dynamic (CHiLD) framework, which establishes identifiability of multi‑layer latent variables in time‑series data using three conditionally independent observations and leverages sparsity to recover hierarchical causal dynamics. It presents theoretical guarantees, a variational inference‑based generative model with contextual encoders and flow‑based hierarchical priors, and validates the approach on synthetic and real‑world datasets.", "summary_cn": "本文提出因果层次潜在动态（CHiLD）框架，通过利用三个条件独立的观测以及层次结构的稀疏性，实现时间序列数据中多层潜在变量的唯一可识别性。文中给出理论保证，构建基于变分推断的生成模型，结合上下文编码器和基于流的层次先验，并在合成及真实数据集上验证了方法的有效性。", "keywords": "hierarchical latent dynamics, causal representation learning, identifiability, temporal causal models, variational inference, flow-based prior, CHiLD, multi-layer latent variables", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Zijian Li", "Minghao Fu", "Junxian Huang", "Yifan Shen", "Ruichu Cai", "Yuewen Sun", "Guangyi Chen", "Kun Zhang"]}
]]></acme>

<pubDate>2025-10-21T05:40:17+00:00</pubDate>
</item>
<item>
<title>Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications</title>
<link>https://papers.cool/arxiv/2510.18299</link>
<guid>https://papers.cool/arxiv/2510.18299</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two physics-informed bandit algorithms, pretc and prgreedy, that exploit the sparse multipath structure of millimeter-wave channels to efficiently identify optimal beam directions. pretc performs an initial random exploration followed by exploitation based on estimated rewards, while prgreedy continuously updates estimates online. Experiments on synthetic DeepMIMO and real-world DeepSense6G datasets show both methods outperform existing beam-alignment approaches across diverse scenarios.<br /><strong>Summary (CN):</strong> 本文提出两种物理信息驱动的 Bandit 算法 pretc 和 prgreedy，利用毫米波信道的稀疏多路径特性高效寻找最优波束方向。pretc 先进行随机探索，再基于估计的奖励函数进行利用；prgreedy 则在在线过程中持续更新估计并选择当前最佳波束。实验在合成 DeepMIMO 数据集和真实 DeepSense6G 数据集上均显示两种方法在多种场景下优于现有波束对准方案，表现出良好的通用性和鲁棒性。<br /><strong>Keywords:</strong> mmWave communications, beam alignment, bandit algorithms, physics-informed, sparse multipath, phase retrieval, DeepMIMO, DeepSense6G, online estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Hao Qin, Thang Duong, Ming Li, Chicheng Zhang</div>
In millimeter wave (mmWave) communications, beam alignment and tracking are crucial to combat the significant path loss. As scanning the entire directional space is inefficient, designing an efficient and robust method to identify the optimal beam directions is essential. Since traditional bandit algorithms require a long time horizon to converge under large beam spaces, many existing works propose efficient bandit algorithms for beam alignment by relying on unimodality or multimodality assumptions on the reward function's structure. However, such assumptions often do not hold (or cannot be strictly satisfied) in practice, which causes such algorithms to converge to choosing suboptimal beams. In this work, we propose two physics-informed bandit algorithms \textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property of mmWave channels - a generic but realistic assumption - which is connected to the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each path as black boxes and maintain optimal estimates of them based on sampled historical rewards. \textit{pretc} starts with a random exploration phase and then commits to the optimal beam under the estimated reward function. \textit{prgreedy} performs such estimation in an online manner and chooses the best beam under current estimates. Our algorithms can also be easily adapted to beam tracking in the mobile setting. Through experiments using both the synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we demonstrate that both algorithms outperform existing approaches in a wide range of scenarios across diverse channel environments, showing their generalizability and robustness.
<div><strong>Authors:</strong> Hao Qin, Thang Duong, Ming Li, Chicheng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two physics-informed bandit algorithms, pretc and prgreedy, that exploit the sparse multipath structure of millimeter-wave channels to efficiently identify optimal beam directions. pretc performs an initial random exploration followed by exploitation based on estimated rewards, while prgreedy continuously updates estimates online. Experiments on synthetic DeepMIMO and real-world DeepSense6G datasets show both methods outperform existing beam-alignment approaches across diverse scenarios.", "summary_cn": "本文提出两种物理信息驱动的 Bandit 算法 pretc 和 prgreedy，利用毫米波信道的稀疏多路径特性高效寻找最优波束方向。pretc 先进行随机探索，再基于估计的奖励函数进行利用；prgreedy 则在在线过程中持续更新估计并选择当前最佳波束。实验在合成 DeepMIMO 数据集和真实 DeepSense6G 数据集上均显示两种方法在多种场景下优于现有波束对准方案，表现出良好的通用性和鲁棒性。", "keywords": "mmWave communications, beam alignment, bandit algorithms, physics-informed, sparse multipath, phase retrieval, DeepMIMO, DeepSense6G, online estimation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Hao Qin", "Thang Duong", "Ming Li", "Chicheng Zhang"]}
]]></acme>

<pubDate>2025-10-21T05:07:07+00:00</pubDate>
</item>
<item>
<title>Online Time Series Forecasting with Theoretical Guarantees</title>
<link>https://papers.cool/arxiv/2510.18281</link>
<guid>https://papers.cool/arxiv/2510.18281</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a theoretical framework (TOT) for online time‑series forecasting under latent distribution shifts, proving that providing a forecaster with latent variables tightens Bayes risk and that this benefit persists despite estimation uncertainty. It proposes a model‑agnostic blueprint using a temporal decoder and two noise estimators to infer latent variables and combines them with empirical results on synthetic and real benchmarks that support the theory. The approach demonstrates general performance improvements across multiple baseline methods.<br /><strong>Summary (CN):</strong> 本文提出了一种面向在线时间序列预测的理论框架（TOT），在潜在分布漂移下证明向预测器提供潜在变量可以收紧贝叶斯风险，且该收益在潜在变量估计不确定性下仍然维持并随可识别性提升而增长。文中进一步设计了基于时间解码器和两个独立噪声估计器的模型无关蓝图，用于推断潜在变量并匹配观测分布，并通过合成数据与真实基准实验验证了理论主张，展示了对多种基线模型的普遍性能提升。<br /><strong>Keywords:</strong> online forecasting, latent variables, time series, Bayes risk, distribution shift, theoretical guarantees, causal inference, temporal decoder<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zijian Li, Changze Zhou, Minghao Fu, Sanjay Manjunath, Fan Feng, Guangyi Chen, Yingyao Hu, Ruichu Cai, Kun Zhang</div>
This paper is concerned with online time series forecasting, where unknown distribution shifts occur over time, i.e., latent variables influence the mapping from historical to future observations. To develop an automated way of online time series forecasting, we propose a Theoretical framework for Online Time-series forecasting (TOT in short) with theoretical guarantees. Specifically, we prove that supplying a forecaster with latent variables tightens the Bayes risk, the benefit endures under estimation uncertainty of latent variables and grows as the latent variables achieve a more precise identifiability. To better introduce latent variables into online forecasting algorithms, we further propose to identify latent variables with minimal adjacent observations. Based on these results, we devise a model-agnostic blueprint by employing a temporal decoder to match the distribution of observed variables and two independent noise estimators to model the causal inference of latent variables and mixing procedures of observed variables, respectively. Experiment results on synthetic data support our theoretical claims. Moreover, plug-in implementations built on several baselines yield general improvement across multiple benchmarks, highlighting the effectiveness in real-world applications.
<div><strong>Authors:</strong> Zijian Li, Changze Zhou, Minghao Fu, Sanjay Manjunath, Fan Feng, Guangyi Chen, Yingyao Hu, Ruichu Cai, Kun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a theoretical framework (TOT) for online time‑series forecasting under latent distribution shifts, proving that providing a forecaster with latent variables tightens Bayes risk and that this benefit persists despite estimation uncertainty. It proposes a model‑agnostic blueprint using a temporal decoder and two noise estimators to infer latent variables and combines them with empirical results on synthetic and real benchmarks that support the theory. The approach demonstrates general performance improvements across multiple baseline methods.", "summary_cn": "本文提出了一种面向在线时间序列预测的理论框架（TOT），在潜在分布漂移下证明向预测器提供潜在变量可以收紧贝叶斯风险，且该收益在潜在变量估计不确定性下仍然维持并随可识别性提升而增长。文中进一步设计了基于时间解码器和两个独立噪声估计器的模型无关蓝图，用于推断潜在变量并匹配观测分布，并通过合成数据与真实基准实验验证了理论主张，展示了对多种基线模型的普遍性能提升。", "keywords": "online forecasting, latent variables, time series, Bayes risk, distribution shift, theoretical guarantees, causal inference, temporal decoder", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zijian Li", "Changze Zhou", "Minghao Fu", "Sanjay Manjunath", "Fan Feng", "Guangyi Chen", "Yingyao Hu", "Ruichu Cai", "Kun Zhang"]}
]]></acme>

<pubDate>2025-10-21T04:12:11+00:00</pubDate>
</item>
<item>
<title>From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation</title>
<link>https://papers.cool/arxiv/2510.18263</link>
<guid>https://papers.cool/arxiv/2510.18263</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Customized-GRPO, a reinforcement‑learning framework that improves subject‑driven image generation by addressing the trade‑off between identity preservation and prompt adherence. It introduces Synergy‑Aware Reward Shaping to resolve conflicting reward signals and Time‑Aware Dynamic Weighting to align optimization with the diffusion process timeline, achieving better fidelity‑editability balance than naive GRPO baselines.<br /><strong>Summary (CN):</strong> 本文提出了 Customized‑GRPO 框架，通过强化学习提升以主题为驱动的图像生成质量，解决身份保持（忠实度）与文本提示遵循（可编辑性）之间的权衡。创新点包括协同感知奖励塑形（SARS），用于消除冲突奖励并放大协同信号，以及时间感知动态加权（TDW），根据扩散过程的不同阶段调整优化力度，从而在保持关键特征的同时更准确地满足复杂文本提示。<br /><strong>Keywords:</strong> reinforcement learning, subject-driven image generation, diffusion models, reward shaping, time-aware weighting, synergy, fidelity-editability tradeoff, GRPO<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</div>
Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.
<div><strong>Authors:</strong> Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Customized-GRPO, a reinforcement‑learning framework that improves subject‑driven image generation by addressing the trade‑off between identity preservation and prompt adherence. It introduces Synergy‑Aware Reward Shaping to resolve conflicting reward signals and Time‑Aware Dynamic Weighting to align optimization with the diffusion process timeline, achieving better fidelity‑editability balance than naive GRPO baselines.", "summary_cn": "本文提出了 Customized‑GRPO 框架，通过强化学习提升以主题为驱动的图像生成质量，解决身份保持（忠实度）与文本提示遵循（可编辑性）之间的权衡。创新点包括协同感知奖励塑形（SARS），用于消除冲突奖励并放大协同信号，以及时间感知动态加权（TDW），根据扩散过程的不同阶段调整优化力度，从而在保持关键特征的同时更准确地满足复杂文本提示。", "keywords": "reinforcement learning, subject-driven image generation, diffusion models, reward shaping, time-aware weighting, synergy, fidelity-editability tradeoff, GRPO", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziwei Huang", "Ying Shu", "Hao Fang", "Quanyu Long", "Wenya Wang", "Qiushi Guo", "Tiezheng Ge", "Leilei Gan"]}
]]></acme>

<pubDate>2025-10-21T03:32:26+00:00</pubDate>
</item>
<item>
<title>NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective</title>
<link>https://papers.cool/arxiv/2510.18258</link>
<guid>https://papers.cool/arxiv/2510.18258</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NTKMTL, a multi-task learning framework that leverages Neural Tangent Kernel (NTK) theory to analyze and balance the convergence speeds of different tasks, thereby mitigating task imbalance. By extending the NTK matrix to the multi-task setting and applying spectral analysis, the authors develop NTKMTL and its efficient variant NTKMTL-SR, achieving state-of-the-art results on supervised and reinforcement learning benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 NTKMTL 框架，利用神经切线核（NTK）理论分析并平衡多任务学习中各任务的收敛速度，以缓解任务不平衡问题。作者构建了多任务的扩展 NTK 矩阵并进行谱分析，进一步提出高效变体 NTKMTL‑SR，在监督学习和强化学习基准上实现了最新性能。<br /><strong>Keywords:</strong> multi-task learning, task imbalance, neural tangent kernel, spectral analysis, NTKMTL, representation sharing, training dynamics, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Junchi Yan</div>
Multi-Task Learning (MTL) enables a single model to learn multiple tasks simultaneously, leveraging knowledge transfer among tasks for enhanced generalization, and has been widely applied across various domains. However, task imbalance remains a major challenge in MTL. Although balancing the convergence speeds of different tasks is an effective approach to address this issue, it is highly challenging to accurately characterize the training dynamics and convergence speeds of multiple tasks within the complex MTL system. To this end, we attempt to analyze the training dynamics in MTL by leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method, NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt spectral analysis to balance the convergence speeds of multiple tasks, thereby mitigating task imbalance. Based on the approximation via shared representation, we further propose NTKMTL-SR, achieving training efficiency while maintaining competitive performance. Extensive experiments demonstrate that our methods achieve state-of-the-art performance across a wide range of benchmarks, including both multi-task supervised learning and multi-task reinforcement learning. Source code is available at https://github.com/jianke0604/NTKMTL.
<div><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Junchi Yan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NTKMTL, a multi-task learning framework that leverages Neural Tangent Kernel (NTK) theory to analyze and balance the convergence speeds of different tasks, thereby mitigating task imbalance. By extending the NTK matrix to the multi-task setting and applying spectral analysis, the authors develop NTKMTL and its efficient variant NTKMTL-SR, achieving state-of-the-art results on supervised and reinforcement learning benchmarks.", "summary_cn": "本文提出 NTKMTL 框架，利用神经切线核（NTK）理论分析并平衡多任务学习中各任务的收敛速度，以缓解任务不平衡问题。作者构建了多任务的扩展 NTK 矩阵并进行谱分析，进一步提出高效变体 NTKMTL‑SR，在监督学习和强化学习基准上实现了最新性能。", "keywords": "multi-task learning, task imbalance, neural tangent kernel, spectral analysis, NTKMTL, representation sharing, training dynamics, reinforcement learning", "scoring": {"interpretability": 4, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Junchi Yan"]}
]]></acme>

<pubDate>2025-10-21T03:29:40+00:00</pubDate>
</item>
<item>
<title>Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs</title>
<link>https://papers.cool/arxiv/2510.18245</link>
<guid>https://papers.cool/arxiv/2510.18245</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a conditional scaling law that extends the Chinchilla framework by incorporating architectural factors such as hidden size, mlp-to-attention ratio, and grouped-query attention, enabling the prediction of inference-efficient and accurate LLM designs. By training over 200 models ranging from 80M to 3B parameters, the authors demonstrate that architectures selected via this law achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to strong baselines. This work provides a systematic approach to balance model performance and inference cost.<br /><strong>Summary (CN):</strong> 本文在 Chinchilla 框架基础上加入隐藏层大小、MLP 与注意力比例以及分组查询注意力（GQA）等架构信息，提出条件尺度律，用于预测兼具推理效率和准确性的 LLM 结构。通过训练 200 多个参数规模在 80M 到 3B、训练数据在 80 亿到 1000 亿 tokens 的模型，验证该尺度律能够可靠地选出最优架构，使模型在相同训练预算下相较 LLaMA-3.2 提升最高 2.1% 的准确率并实现 42% 的推理吞吐提升。该研究提供了在性能与推理成本之间系统权衡的方法。<br /><strong>Keywords:</strong> scaling laws, model architecture, inference efficiency, conditional scaling law, Chinchilla, grouped-query attention, mlp-to-attention ratio, hidden size, LLM performance, throughput<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park</div>
Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.
<div><strong>Authors:</strong> Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a conditional scaling law that extends the Chinchilla framework by incorporating architectural factors such as hidden size, mlp-to-attention ratio, and grouped-query attention, enabling the prediction of inference-efficient and accurate LLM designs. By training over 200 models ranging from 80M to 3B parameters, the authors demonstrate that architectures selected via this law achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to strong baselines. This work provides a systematic approach to balance model performance and inference cost.", "summary_cn": "本文在 Chinchilla 框架基础上加入隐藏层大小、MLP 与注意力比例以及分组查询注意力（GQA）等架构信息，提出条件尺度律，用于预测兼具推理效率和准确性的 LLM 结构。通过训练 200 多个参数规模在 80M 到 3B、训练数据在 80 亿到 1000 亿 tokens 的模型，验证该尺度律能够可靠地选出最优架构，使模型在相同训练预算下相较 LLaMA-3.2 提升最高 2.1% 的准确率并实现 42% 的推理吞吐提升。该研究提供了在性能与推理成本之间系统权衡的方法。", "keywords": "scaling laws, model architecture, inference efficiency, conditional scaling law, Chinchilla, grouped-query attention, mlp-to-attention ratio, hidden size, LLM performance, throughput", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Song Bian", "Tao Yu", "Shivaram Venkataraman", "Youngsuk Park"]}
]]></acme>

<pubDate>2025-10-21T03:08:48+00:00</pubDate>
</item>
<item>
<title>Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment</title>
<link>https://papers.cool/arxiv/2510.18240</link>
<guid>https://papers.cool/arxiv/2510.18240</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies the Dual-level Noisy Correspondence (DNC) problem in multi-modal entity alignment, where both intra-entity (entity-attribute) and inter-graph (entity-entity, attribute-attribute) correspondences may be corrupted. It proposes the RULE framework that estimates reliability of correspondences, mitigates intra-entity noise during attribute fusion, avoids overfitting to noisy inter-graph links, and adds a correspondence reasoning module to uncover attribute-attribute connections, achieving strong results on five benchmarks.<br /><strong>Summary (CN):</strong> 本文指出在多模态实体对齐中存在的双层噪声对应（Dual-level Noisy Correspondence, DNC）问题，即实体属性内部对应以及图间对应都可能出现错误。为此提出了 RULE 框架，通过两步原则估计对应的可靠性，在属性融合时降低内部噪声影响，防止对噪声图间对应的过拟合，并加入对应推理模块发现跨图属性关联，从而在五个基准上取得显著提升。<br /><strong>Keywords:</strong> multi-modal entity alignment, noisy correspondence, knowledge graph, attribute fusion, reliability estimation, correspondence reasoning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haobin Li, Yijie Lin, Peng Hu, Mouxing Yang, Xi Peng</div>
Multi-modal entity alignment (MMEA) aims to identify equivalent entities across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is described by attributes from various modalities. Existing methods typically assume that both intra-entity and inter-graph correspondences are faultless, which is often violated in real-world MMKGs due to the reliance on expert annotations. In this paper, we reveal and study a highly practical yet under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC). DNC refers to misalignments in both intra-entity (entity-attribute) and inter-graph (entity-entity and attribute-attribute) correspondences. To address the DNC problem, we propose a robust MMEA framework termed RULE. RULE first estimates the reliability of both intra-entity and inter-graph correspondences via a dedicated two-fold principle. Leveraging the estimated reliabilities, RULE mitigates the negative impact of intra-entity noise during attribute fusion and prevents overfitting to noisy inter-graph correspondences during inter-graph discrepancy elimination. Beyond the training-time designs, RULE further incorporates a correspondence reasoning module that uncovers the underlying attribute-attribute connection across graphs, guaranteeing more accurate equivalent entity identification. Extensive experiments on five benchmarks verify the effectiveness of our method against the DNC compared with seven state-of-the-art methods.The code is available at \href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}
<div><strong>Authors:</strong> Haobin Li, Yijie Lin, Peng Hu, Mouxing Yang, Xi Peng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies the Dual-level Noisy Correspondence (DNC) problem in multi-modal entity alignment, where both intra-entity (entity-attribute) and inter-graph (entity-entity, attribute-attribute) correspondences may be corrupted. It proposes the RULE framework that estimates reliability of correspondences, mitigates intra-entity noise during attribute fusion, avoids overfitting to noisy inter-graph links, and adds a correspondence reasoning module to uncover attribute-attribute connections, achieving strong results on five benchmarks.", "summary_cn": "本文指出在多模态实体对齐中存在的双层噪声对应（Dual-level Noisy Correspondence, DNC）问题，即实体属性内部对应以及图间对应都可能出现错误。为此提出了 RULE 框架，通过两步原则估计对应的可靠性，在属性融合时降低内部噪声影响，防止对噪声图间对应的过拟合，并加入对应推理模块发现跨图属性关联，从而在五个基准上取得显著提升。", "keywords": "multi-modal entity alignment, noisy correspondence, knowledge graph, attribute fusion, reliability estimation, correspondence reasoning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haobin Li", "Yijie Lin", "Peng Hu", "Mouxing Yang", "Xi Peng"]}
]]></acme>

<pubDate>2025-10-21T03:00:11+00:00</pubDate>
</item>
<item>
<title>Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards</title>
<link>https://papers.cool/arxiv/2510.18238</link>
<guid>https://papers.cool/arxiv/2510.18238</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that current review criteria for AI/ML for social impact overemphasize projects that combine deployment with novel methodological contributions, which can discourage valuable single-front contributions. It proposes expanding the notion of social impact beyond deployment and adopting more rigorous impact evaluations for deployed systems to sustain a healthy research ecosystem.<br /><strong>Summary (CN):</strong> 本文指出当前 AI/ML 社会影响研究的评审标准过于强调同时实现部署和创新方法的项目，这可能抑制仅在单一方向（如纯应用或纯方法）上有价值的工作。作者主张在评估标准中拓宽对社会影响的定义，超越仅关注部署，并对已部署系统进行更严格的影响评估，以促进社会影响 AI 研究生态的可持续发展。<br /><strong>Keywords:</strong> AI for social impact, evaluation standards, deployment, methodological innovation, impact assessment, research ecosystem, social impact metrics, AI policy<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Bryan Wilder, Angela Zhou</div>
There has been increasing research interest in AI/ML for social impact, and correspondingly more publication venues have refined review criteria for practice-driven AI/ML research. However, these review guidelines tend to most concretely recognize projects that simultaneously achieve deployment and novel ML methodological innovation. We argue that this introduces incentives for researchers that undermine the sustainability of a broader research ecosystem of social impact, which benefits from projects that make contributions on single front (applied or methodological) that may better meet project partner needs. Our position is that researchers and reviewers in machine learning for social impact must simultaneously adopt: 1) a more expansive conception of social impacts beyond deployment and 2) more rigorous evaluations of the impact of deployed systems.
<div><strong>Authors:</strong> Bryan Wilder, Angela Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that current review criteria for AI/ML for social impact overemphasize projects that combine deployment with novel methodological contributions, which can discourage valuable single-front contributions. It proposes expanding the notion of social impact beyond deployment and adopting more rigorous impact evaluations for deployed systems to sustain a healthy research ecosystem.", "summary_cn": "本文指出当前 AI/ML 社会影响研究的评审标准过于强调同时实现部署和创新方法的项目，这可能抑制仅在单一方向（如纯应用或纯方法）上有价值的工作。作者主张在评估标准中拓宽对社会影响的定义，超越仅关注部署，并对已部署系统进行更严格的影响评估，以促进社会影响 AI 研究生态的可持续发展。", "keywords": "AI for social impact, evaluation standards, deployment, methodological innovation, impact assessment, research ecosystem, social impact metrics, AI policy", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Bryan Wilder", "Angela Zhou"]}
]]></acme>

<pubDate>2025-10-21T02:51:03+00:00</pubDate>
</item>
<item>
<title>ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control</title>
<link>https://papers.cool/arxiv/2510.18232</link>
<guid>https://papers.cool/arxiv/2510.18232</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ACTG-ARL, a hierarchical framework for differentially private synthetic text generation that separates feature learning from conditional generation, and enhances control via Anchored Reinforcement Learning (ARL) to improve instruction-following while preventing reward hacking. ACTG combines a DP tabular synthesizer with a DP fine-tuned conditional generator, achieving a 20% MAUVE improvement over prior methods. The approach delivers higher quality DP text and finer-grained conditional control under strong privacy guarantees.<br /><strong>Summary (CN):</strong> 本文提出 ACTG-ARL，一种层次化的差分隐私（DP）合成文本生成框架，将特征学习与条件文本生成分离，并通过锚定强化学习（ARL）提升指令遵循能力且防止奖励黑客行为。ACTG 将 DP 表格合成器与 DP 微调的条件生成器结合，在保持强隐私保证的前提下，使合成文本质量提升约 20%（MAUVE），并实现更细粒度的控制。<br /><strong>Keywords:</strong> differential privacy, conditional text generation, anchored reinforcement learning, synthetic data, privacy-preserving generation, DP tabular synthesizer, MAUVE<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Yuzheng Hu, Ryan McKenna, Da Yu, Shanshan Wu, Han Zhao, Zheng Xu, Peter Kairouz</div>
Generating high-quality synthetic text under differential privacy (DP) is critical for training and evaluating language models without compromising user privacy. Prior work on synthesizing DP datasets often fail to preserve key statistical attributes, suffer utility loss from the noise required by DP, and lack fine-grained control over generation. To address these challenges, we make two contributions. First, we introduce a hierarchical framework that decomposes DP synthetic text generation into two subtasks: feature learning and conditional text generation. This design explicitly incorporates learned features into the generation process and simplifies the end-to-end synthesis task. Through systematic ablations, we identify the most effective configuration: a rich tabular schema as feature, a DP tabular synthesizer, and a DP fine-tuned conditional generator, which we term ACTG (Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL), a post-training method that improves the instruction-following ability of ACTG for conditional generation. ARL combines RL to boost control with an SFT anchor on best-of-$N$ data to prevent reward hacking. Together, these components form our end-to-end algorithm ACTG-ARL, which advances both the quality of DP synthetic text (+20% MAUVE over prior work) and the control of the conditional generator under strong privacy guarantees.
<div><strong>Authors:</strong> Yuzheng Hu, Ryan McKenna, Da Yu, Shanshan Wu, Han Zhao, Zheng Xu, Peter Kairouz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ACTG-ARL, a hierarchical framework for differentially private synthetic text generation that separates feature learning from conditional generation, and enhances control via Anchored Reinforcement Learning (ARL) to improve instruction-following while preventing reward hacking. ACTG combines a DP tabular synthesizer with a DP fine-tuned conditional generator, achieving a 20% MAUVE improvement over prior methods. The approach delivers higher quality DP text and finer-grained conditional control under strong privacy guarantees.", "summary_cn": "本文提出 ACTG-ARL，一种层次化的差分隐私（DP）合成文本生成框架，将特征学习与条件文本生成分离，并通过锚定强化学习（ARL）提升指令遵循能力且防止奖励黑客行为。ACTG 将 DP 表格合成器与 DP 微调的条件生成器结合，在保持强隐私保证的前提下，使合成文本质量提升约 20%（MAUVE），并实现更细粒度的控制。", "keywords": "differential privacy, conditional text generation, anchored reinforcement learning, synthetic data, privacy-preserving generation, DP tabular synthesizer, MAUVE", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Yuzheng Hu", "Ryan McKenna", "Da Yu", "Shanshan Wu", "Han Zhao", "Zheng Xu", "Peter Kairouz"]}
]]></acme>

<pubDate>2025-10-21T02:31:31+00:00</pubDate>
</item>
<item>
<title>Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations</title>
<link>https://papers.cool/arxiv/2510.18228</link>
<guid>https://papers.cool/arxiv/2510.18228</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces P-GAP, a zeroth-order optimization method for fast fine-tuning of large language models that aligns perturbations with projected gradients in a low-dimensional space, reducing variance and the number of perturbed parameters. Experiments show that P-GAP achieves higher accuracy on classification and generation tasks while requiring significantly fewer training iterations and GPU hours compared to existing baselines.<br /><strong>Summary (CN):</strong> 本文提出了 P-GAP，一种用于快速微调大语言模型的零阶优化方法，通过在低维空间中对齐投影梯度方向的扰动，从而降低梯度估计方差并减少扰动参数数量。实验表明，P-GAP 在分类和生成任务上均提升了准确率，同时相较于现有基线显著减少了训练迭代次数和 GPU 计算时间。<br /><strong>Keywords:</strong> zeroth-order optimization, projected gradient-aligned perturbations, LLM fine-tuning, resource-efficient training, gradient variance reduction, P-GAP<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang</div>
Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow convergence and suboptimal performance on large-scale models. In this work, we propose P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with Projected Gradient-Aligned Perturbations. Specifically, we first estimate a low-dimensional gradient space and then align perturbations in projected gradients' direction within the space. This approach enables reduced the number of perturbed parameters and decreased variance, therefore accelerated convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP consistently surpasses the baselines, achieving up to 6% increase in accuracy on classification tasks and up to 12% higher accuracy on generation tasks, with up to about 81% less training iterations and 70% less GPU hours. These results demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM fine-tuning.
<div><strong>Authors:</strong> Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces P-GAP, a zeroth-order optimization method for fast fine-tuning of large language models that aligns perturbations with projected gradients in a low-dimensional space, reducing variance and the number of perturbed parameters. Experiments show that P-GAP achieves higher accuracy on classification and generation tasks while requiring significantly fewer training iterations and GPU hours compared to existing baselines.", "summary_cn": "本文提出了 P-GAP，一种用于快速微调大语言模型的零阶优化方法，通过在低维空间中对齐投影梯度方向的扰动，从而降低梯度估计方差并减少扰动参数数量。实验表明，P-GAP 在分类和生成任务上均提升了准确率，同时相较于现有基线显著减少了训练迭代次数和 GPU 计算时间。", "keywords": "zeroth-order optimization, projected gradient-aligned perturbations, LLM fine-tuning, resource-efficient training, gradient variance reduction, P-GAP", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhendong Mi", "Qitao Tan", "Grace Li Zhang", "Zhaozhuo Xu", "Geng Yuan", "Shaoyi Huang"]}
]]></acme>

<pubDate>2025-10-21T02:19:11+00:00</pubDate>
</item>
<item>
<title>Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs</title>
<link>https://papers.cool/arxiv/2510.18225</link>
<guid>https://papers.cool/arxiv/2510.18225</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies cooperative underwater target detection with multiple AUVs, focusing on the trade‑off between cooperation efficiency and communication covertness. It formulates a joint trajectory‑and‑power optimization problem and solves it via a hierarchical action‑management framework that uses macro‑level PPO for agent selection and micro‑level multi‑agent PPO for decentralized trajectory and power decisions under a centralized‑training/decentralized‑execution paradigm. The approach enables adaptive, covert cooperation while respecting energy and mobility constraints, offering theoretical and practical insights for secure underwater operations.<br /><strong>Summary (CN):</strong> 本文研究了多 AUV 在水下协同目标检测中的合作效率与通信隐蔽性之间的权衡。通过将轨迹与功率控制的联合优化问题建模为层次化行动管理框架，在宏观层使用 PPO 进行任务分配的马尔可夫决策过程建模，在微观层采用多智能体 PPO 依据局部观测动态调整轨迹和发射功率，实现集中训练、分散执行的隐蔽协作。该方法在满足能量和运动约束的前提下提供了高效安全的水下作业理论与实践方案。<br /><strong>Keywords:</strong> underwater autonomous vehicles, cooperative target detection, communication covertness, trajectory optimization, power control, hierarchical reinforcement learning, proximal policy optimization, multi-agent RL, covert communication<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Xueyao Zhang, Bo Yang, Zhiwen Yu, Xuelin Cao, Wei Xiang, Bin Guo, Liang Wang, Billy Pik Lik Lau, George C. Alexandropoulos, Jun Luo, Mérouane Debbah, Zhu Han, Chau Yuen</div>
This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimization problem, and then present an innovative hierarchical action management framework to solve it. According to the hierarchical formulation, at the macro level, the master AUV models the agent selection process as a Markov decision process and deploys the proximal policy optimization algorithm for strategic task allocation. At the micro level, each selected agent's decentralized decision-making is modeled as a partially observable Markov decision process, and a multi-agent proximal policy optimization algorithm is used to dynamically adjust its trajectory and transmission power based on its local observations. Under the centralized training and decentralized execution paradigm, our target detection framework enables adaptive covert cooperation while satisfying both energy and mobility constraints. By comprehensively modeling the considered system, the involved signals and tasks, as well as energy consumption, theoretical insights and practical solutions for the efficient and secure operation of multiple AUVs are provided, offering significant implications for the execution of underwater covert communication tasks.
<div><strong>Authors:</strong> Xueyao Zhang, Bo Yang, Zhiwen Yu, Xuelin Cao, Wei Xiang, Bin Guo, Liang Wang, Billy Pik Lik Lau, George C. Alexandropoulos, Jun Luo, Mérouane Debbah, Zhu Han, Chau Yuen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies cooperative underwater target detection with multiple AUVs, focusing on the trade‑off between cooperation efficiency and communication covertness. It formulates a joint trajectory‑and‑power optimization problem and solves it via a hierarchical action‑management framework that uses macro‑level PPO for agent selection and micro‑level multi‑agent PPO for decentralized trajectory and power decisions under a centralized‑training/decentralized‑execution paradigm. The approach enables adaptive, covert cooperation while respecting energy and mobility constraints, offering theoretical and practical insights for secure underwater operations.", "summary_cn": "本文研究了多 AUV 在水下协同目标检测中的合作效率与通信隐蔽性之间的权衡。通过将轨迹与功率控制的联合优化问题建模为层次化行动管理框架，在宏观层使用 PPO 进行任务分配的马尔可夫决策过程建模，在微观层采用多智能体 PPO 依据局部观测动态调整轨迹和发射功率，实现集中训练、分散执行的隐蔽协作。该方法在满足能量和运动约束的前提下提供了高效安全的水下作业理论与实践方案。", "keywords": "underwater autonomous vehicles, cooperative target detection, communication covertness, trajectory optimization, power control, hierarchical reinforcement learning, proximal policy optimization, multi-agent RL, covert communication", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Xueyao Zhang", "Bo Yang", "Zhiwen Yu", "Xuelin Cao", "Wei Xiang", "Bin Guo", "Liang Wang", "Billy Pik Lik Lau", "George C. Alexandropoulos", "Jun Luo", "Mérouane Debbah", "Zhu Han", "Chau Yuen"]}
]]></acme>

<pubDate>2025-10-21T02:14:11+00:00</pubDate>
</item>
<item>
<title>Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks</title>
<link>https://papers.cool/arxiv/2510.18195</link>
<guid>https://papers.cool/arxiv/2510.18195</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multistage ensemble framework that uses physics-informed neural networks (PINNs) to learn the optimal cost-to-go and corresponding control signals by solving the Hamilton-Jacobi-Bellman equation without stabilizer terms. It demonstrates closed-loop control of a continuous nonlinear two-state system under noisy conditions using either a singular learned control signal or an ensemble policy.<br /><strong>Summary (CN):</strong> 本文提出一种多阶段集成框架，利用物理信息神经网络（PINNs）在不使用稳定子项的情况下学习最优代价函数并求解相应的控制信号，通过求解Hamilton-Jacobi-Bellman方程实现。实验在噪声扰动和不同初始条件下，对一个两状态连续非线性系统展示了单一控制信号和集成控制策略的闭环控制效果。<br /><strong>Keywords:</strong> physics-informed neural networks, Hamilton-Jacobi-Bellman, optimal control, ensemble learning, closed-loop control, nonlinear systems<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Jostein Barry-Straume, Adwait D. Verulkar, Arash Sarshar, Andrey A. Popov, Adrian Sandu</div>
The objective of designing a control system is to steer a dynamical system with a control signal, guiding it to exhibit the desired behavior. The Hamilton-Jacobi-Bellman (HJB) partial differential equation offers a framework for optimal control system design. However, numerical solutions to this equation are computationally intensive, and analytical solutions are frequently unavailable. Knowledge-guided machine learning methodologies, such as physics-informed neural networks (PINNs), offer new alternative approaches that can alleviate the difficulties of solving the HJB equation numerically. This work presents a multistage ensemble framework to learn the optimal cost-to-go, and subsequently the corresponding optimal control signal, through the HJB equation. Prior PINN-based approaches rely on a stabilizing the HJB enforcement during training. Our framework does not use stabilizer terms and offers a means of controlling the nonlinear system, via either a singular learned control signal or an ensemble control signal policy. Success is demonstrated in closed-loop control, using both ensemble- and singular-control, of a steady-state time-invariant two-state continuous nonlinear system with an infinite time horizon, accounting of noisy, perturbed system states and varying initial conditions.
<div><strong>Authors:</strong> Jostein Barry-Straume, Adwait D. Verulkar, Arash Sarshar, Andrey A. Popov, Adrian Sandu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multistage ensemble framework that uses physics-informed neural networks (PINNs) to learn the optimal cost-to-go and corresponding control signals by solving the Hamilton-Jacobi-Bellman equation without stabilizer terms. It demonstrates closed-loop control of a continuous nonlinear two-state system under noisy conditions using either a singular learned control signal or an ensemble policy.", "summary_cn": "本文提出一种多阶段集成框架，利用物理信息神经网络（PINNs）在不使用稳定子项的情况下学习最优代价函数并求解相应的控制信号，通过求解Hamilton-Jacobi-Bellman方程实现。实验在噪声扰动和不同初始条件下，对一个两状态连续非线性系统展示了单一控制信号和集成控制策略的闭环控制效果。", "keywords": "physics-informed neural networks, Hamilton-Jacobi-Bellman, optimal control, ensemble learning, closed-loop control, nonlinear systems", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Jostein Barry-Straume", "Adwait D. Verulkar", "Arash Sarshar", "Andrey A. Popov", "Adrian Sandu"]}
]]></acme>

<pubDate>2025-10-21T00:41:41+00:00</pubDate>
</item>
<item>
<title>ActivationReasoning: Logical Reasoning in Latent Activation Spaces</title>
<link>https://papers.cool/arxiv/2510.18184</link>
<guid>https://papers.cool/arxiv/2510.18184</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent activation space of large language models by using sparse autoencoders to identify concepts, mapping activating concepts to logical propositions, and applying logical rules to infer higher‑order structures and steer model behavior. Experiments on multi‑hop reasoning, abstraction, and context‑sensitive safety tasks show that AR improves transparency, robustness, and controllability while scaling with reasoning complexity. The approach aims to make internal reasoning more interpretable and aligned with desired outcomes.<br /><strong>Summary (CN):</strong> 本文提出了 ActivationReasoning（AR）框架，通过稀疏自编码器识别潜在概念，将激活概念映射为逻辑命题，并在潜在激活空间中应用逻辑规则推导更高层结构，从而引导模型行为。实验在多跳推理、抽象化以及上下文敏感安全任务上展示了 AR 能提升透明度、鲁棒性和可控性，并随推理复杂度良好扩展。该方法旨在让模型内部推理更具可解释性并与期望行为对齐。<br /><strong>Keywords:</strong> latent activation reasoning, sparse autoencoders, mechanistic interpretability, logical reasoning, model control, AI alignment, safety, concept activation, multi-hop reasoning, probing<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 6, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Lukas Helff, Ruben Härle, Wolfgang Stammer, Felix Friedrich, Manuel Brack, Antonia Wüst, Hikaru Shindo, Patrick Schramowski, Kristian Kersting</div>
Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are fragile and passive, offering no mechanism for systematic reasoning or model control. To address this, we introduce ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent space of LLMs. It proceeds in three stages: (1) Finding latent representations, first latent concept representations are identified (e.g., via SAEs) and organized into a dictionary; (2) Activating propositions, at inference time AR detects activating concepts and maps them to logical propositions; and (3)Logical reasoning, applying logical rules over these propositions to infer higher-order structures, compose new concepts, and steer model behavior. We evaluate AR on multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept cues (Rail2Country), reasoning over natural and diverse language (ProverQA), and context-sensitive safety (BeaverTails). Across all tasks, AR scales robustly with reasoning complexity, generalizes to abstract and context-sensitive tasks, and transfers across model backbones. These results demonstrate that grounding logical structure in latent activations not only improves transparency but also enables structured reasoning, reliable control, and alignment with desired behaviors, providing a path toward more reliable and auditable AI.
<div><strong>Authors:</strong> Lukas Helff, Ruben Härle, Wolfgang Stammer, Felix Friedrich, Manuel Brack, Antonia Wüst, Hikaru Shindo, Patrick Schramowski, Kristian Kersting</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent activation space of large language models by using sparse autoencoders to identify concepts, mapping activating concepts to logical propositions, and applying logical rules to infer higher‑order structures and steer model behavior. Experiments on multi‑hop reasoning, abstraction, and context‑sensitive safety tasks show that AR improves transparency, robustness, and controllability while scaling with reasoning complexity. The approach aims to make internal reasoning more interpretable and aligned with desired outcomes.", "summary_cn": "本文提出了 ActivationReasoning（AR）框架，通过稀疏自编码器识别潜在概念，将激活概念映射为逻辑命题，并在潜在激活空间中应用逻辑规则推导更高层结构，从而引导模型行为。实验在多跳推理、抽象化以及上下文敏感安全任务上展示了 AR 能提升透明度、鲁棒性和可控性，并随推理复杂度良好扩展。该方法旨在让模型内部推理更具可解释性并与期望行为对齐。", "keywords": "latent activation reasoning, sparse autoencoders, mechanistic interpretability, logical reasoning, model control, AI alignment, safety, concept activation, multi-hop reasoning, probing", "scoring": {"interpretability": 8, "understanding": 8, "safety": 6, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Lukas Helff", "Ruben Härle", "Wolfgang Stammer", "Felix Friedrich", "Manuel Brack", "Antonia Wüst", "Hikaru Shindo", "Patrick Schramowski", "Kristian Kersting"]}
]]></acme>

<pubDate>2025-10-21T00:21:04+00:00</pubDate>
</item>
<item>
<title>Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria</title>
<link>https://papers.cool/arxiv/2510.18183</link>
<guid>https://papers.cool/arxiv/2510.18183</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Nash Policy Gradient (NashPG), a policy‑gradient algorithm that keeps regularization strength fixed and iteratively refines a reference policy, guaranteeing strictly monotonic improvement and convergence to an exact Nash equilibrium in two‑player zero‑sum games without requiring a uniqueness assumption. Theoretical analysis shows convergence, and experiments on classic benchmark games as well as large‑scale domains such as Battleship and No‑Limit Texas Hold'em demonstrate comparable or lower exploitability and higher Elo ratings than prior model‑free methods.<br /><strong>Summary (CN):</strong> 本文提出了 Nash Policy Gradient (NashPG) 方法，在固定正则化强度的情况下通过迭代改进参考策略，实现严格单调改进并在二人零和游戏中收敛到精确的纳什均衡，且不依赖唯一性假设。理论分析证明了收敛性，实验证明在传统基准游戏以及 Battleship 和无限注德州扑克等大规模领域中，NashPG 的可利用性更低且 Elo 评分更高，优于以往的无模型方法。<br /><strong>Keywords:</strong> Nash equilibrium, policy gradient, multi-agent reinforcement learning, regularization, imperfect-information games, convergence, exploitability, zero-sum games, algorithmic robustness, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Eason Yu, Tzu Hao Liu, Yunke Wang, Clément L. Canonne, Nguyen H. Tran, Chang Xu</div>
Finding Nash equilibria in imperfect-information games remains a central challenge in multi-agent reinforcement learning. While regularization-based methods have recently achieved last-iteration convergence to a regularized equilibrium, they require the regularization strength to shrink toward zero to approximate a Nash equilibrium, often leading to unstable learning in practice. Instead, we fix the regularization strength at a large value for robustness and achieve convergence by iteratively refining the reference policy. Our main theoretical result shows that this procedure guarantees strictly monotonic improvement and convergence to an exact Nash equilibrium in two-player zero-sum games, without requiring a uniqueness assumption. Building on this framework, we develop a practical algorithm, Nash Policy Gradient (NashPG), which preserves the generalizability of policy gradient methods while relying solely on the current and reference policies. Empirically, NashPG achieves comparable or lower exploitability than prior model-free methods on classic benchmark games and scales to large domains such as Battleship and No-Limit Texas Hold'em, where NashPG consistently attains higher Elo ratings.
<div><strong>Authors:</strong> Eason Yu, Tzu Hao Liu, Yunke Wang, Clément L. Canonne, Nguyen H. Tran, Chang Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Nash Policy Gradient (NashPG), a policy‑gradient algorithm that keeps regularization strength fixed and iteratively refines a reference policy, guaranteeing strictly monotonic improvement and convergence to an exact Nash equilibrium in two‑player zero‑sum games without requiring a uniqueness assumption. Theoretical analysis shows convergence, and experiments on classic benchmark games as well as large‑scale domains such as Battleship and No‑Limit Texas Hold'em demonstrate comparable or lower exploitability and higher Elo ratings than prior model‑free methods.", "summary_cn": "本文提出了 Nash Policy Gradient (NashPG) 方法，在固定正则化强度的情况下通过迭代改进参考策略，实现严格单调改进并在二人零和游戏中收敛到精确的纳什均衡，且不依赖唯一性假设。理论分析证明了收敛性，实验证明在传统基准游戏以及 Battleship 和无限注德州扑克等大规模领域中，NashPG 的可利用性更低且 Elo 评分更高，优于以往的无模型方法。", "keywords": "Nash equilibrium, policy gradient, multi-agent reinforcement learning, regularization, imperfect-information games, convergence, exploitability, zero-sum games, algorithmic robustness, reinforcement learning", "scoring": {"interpretability": 1, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Eason Yu", "Tzu Hao Liu", "Yunke Wang", "Clément L. Canonne", "Nguyen H. Tran", "Chang Xu"]}
]]></acme>

<pubDate>2025-10-21T00:14:45+00:00</pubDate>
</item>
<item>
<title>Rethinking PCA Through Duality</title>
<link>https://papers.cool/arxiv/2510.18130</link>
<guid>https://papers.cool/arxiv/2510.18130</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper revisits principal component analysis (PCA) by applying the difference‑of‑convex (DC) framework, introducing novel dual formulations that are kernelizable and support out‑of‑sample extensions. It shows that simultaneous iteration is an instance of the difference‑of‑convex algorithm, proposes new PCA algorithms, and presents a kernelizable robust PCA variant minimizing an \(l_1\) reconstruction error. Empirical comparisons with state‑of‑the‑art methods are also provided.<br /><strong>Summary (CN):</strong> 本文通过差分凸（DC）框架重新审视主成分分析（PCA），提出可核化并支持样本外扩展的全新对偶形式。作者证明同步迭代是差分凸算法（DCA）的一个实例，进而设计了新的 PCA 算法，并引入一种最小化 \(l_1\) 重构误差的核化鲁棒 PCA 变体。最后给出与最新方法的实验对比。<br /><strong>Keywords:</strong> PCA, dual formulation, difference-of-convex algorithm, kernel PCA, robust PCA, optimization, QR algorithm, out-of-sample extension<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jan Quan, Johan Suykens, Panagiotis Patrinos</div>
Motivated by the recently shown connection between self-attention and (kernel) principal component analysis (PCA), we revisit the fundamentals of PCA. Using the difference-of-convex (DC) framework, we present several novel formulations and provide new theoretical insights. In particular, we show the kernelizability and out-of-sample applicability for a PCA-like family of problems. Moreover, we uncover that simultaneous iteration, which is connected to the classical QR algorithm, is an instance of the difference-of-convex algorithm (DCA), offering an optimization perspective on this longstanding method. Further, we describe new algorithms for PCA and empirically compare them with state-of-the-art methods. Lastly, we introduce a kernelizable dual formulation for a robust variant of PCA that minimizes the $l_1$ deviation of the reconstruction errors.
<div><strong>Authors:</strong> Jan Quan, Johan Suykens, Panagiotis Patrinos</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper revisits principal component analysis (PCA) by applying the difference‑of‑convex (DC) framework, introducing novel dual formulations that are kernelizable and support out‑of‑sample extensions. It shows that simultaneous iteration is an instance of the difference‑of‑convex algorithm, proposes new PCA algorithms, and presents a kernelizable robust PCA variant minimizing an \\(l_1\\) reconstruction error. Empirical comparisons with state‑of‑the‑art methods are also provided.", "summary_cn": "本文通过差分凸（DC）框架重新审视主成分分析（PCA），提出可核化并支持样本外扩展的全新对偶形式。作者证明同步迭代是差分凸算法（DCA）的一个实例，进而设计了新的 PCA 算法，并引入一种最小化 \\(l_1\\) 重构误差的核化鲁棒 PCA 变体。最后给出与最新方法的实验对比。", "keywords": "PCA, dual formulation, difference-of-convex algorithm, kernel PCA, robust PCA, optimization, QR algorithm, out-of-sample extension", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jan Quan", "Johan Suykens", "Panagiotis Patrinos"]}
]]></acme>

<pubDate>2025-10-20T21:56:14+00:00</pubDate>
</item>
<item>
<title>HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</title>
<link>https://papers.cool/arxiv/2510.18122</link>
<guid>https://papers.cool/arxiv/2510.18122</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces HyperDiffusionFields (HyDiF), a framework that represents 3D molecular conformers as continuous vector fields (Molecular Directional Fields) modeled by molecule-specific neural implicit fields generated by a hypernetwork. The hypernetwork is trained as a denoising diffusion model, enabling generation of molecular fields and supporting tasks such as molecular inpainting and property prediction. Experiments show the method scales to larger biomolecules and provides fine-grained spatial features beyond traditional graph or point‑cloud approaches.<br /><strong>Summary (CN):</strong> 本文提出 HyperDiffusionFields（HyDiF）框架，将三维分子构象表示为连续的向量场（分子方向场），通过分子特定的隐式神经场（MNF）实现，且这些隐式神经场的权重由一个以分子为条件的超网络生成。该超网络以去噪扩散模型方式训练，使得能够在函数空间中采样分子场，并支持诸如分子填补的结构条件生成以及细粒度的分子属性预测。实验表明该方法能够扩展到更大的生物分子，并提供比传统图或点云方法更精细的空间特征。<br /><strong>Keywords:</strong> diffusion models, hypernetworks, implicit neural fields, molecular directional field, molecular neural fields, generative modeling, molecular property prediction, 3D conformer modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan</div>
We introduce HyperDiffusionFields (HyDiF), a framework that models 3D molecular conformers as continuous fields rather than discrete atomic coordinates or graphs. At the core of our approach is the Molecular Directional Field (MDF), a vector field that maps any point in space to the direction of the nearest atom of a particular type. We represent MDFs using molecule-specific neural implicit fields, which we call Molecular Neural Fields (MNFs). To enable learning across molecules and facilitate generalization, we adopt an approach where a shared hypernetwork, conditioned on a molecule, generates the weights of the given molecule's MNF. To endow the model with generative capabilities, we train the hypernetwork as a denoising diffusion model, enabling sampling in the function space of molecular fields. Our design naturally extends to a masked diffusion mechanism to support structure-conditioned generation tasks, such as molecular inpainting, by selectively noising regions of the field. Beyond generation, the localized and continuous nature of MDFs enables spatially fine-grained feature extraction for molecular property prediction, something not easily achievable with graph or point cloud based methods. Furthermore, we demonstrate that our approach scales to larger biomolecules, illustrating a promising direction for field-based molecular modeling.
<div><strong>Authors:</strong> Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces HyperDiffusionFields (HyDiF), a framework that represents 3D molecular conformers as continuous vector fields (Molecular Directional Fields) modeled by molecule-specific neural implicit fields generated by a hypernetwork. The hypernetwork is trained as a denoising diffusion model, enabling generation of molecular fields and supporting tasks such as molecular inpainting and property prediction. Experiments show the method scales to larger biomolecules and provides fine-grained spatial features beyond traditional graph or point‑cloud approaches.", "summary_cn": "本文提出 HyperDiffusionFields（HyDiF）框架，将三维分子构象表示为连续的向量场（分子方向场），通过分子特定的隐式神经场（MNF）实现，且这些隐式神经场的权重由一个以分子为条件的超网络生成。该超网络以去噪扩散模型方式训练，使得能够在函数空间中采样分子场，并支持诸如分子填补的结构条件生成以及细粒度的分子属性预测。实验表明该方法能够扩展到更大的生物分子，并提供比传统图或点云方法更精细的空间特征。", "keywords": "diffusion models, hypernetworks, implicit neural fields, molecular directional field, molecular neural fields, generative modeling, molecular property prediction, 3D conformer modeling", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sudarshan Babu", "Phillip Lo", "Xiao Zhang", "Aadi Srivastava", "Ali Davariashtiyani", "Jason Perera", "Michael Maire", "Aly A. Khan"]}
]]></acme>

<pubDate>2025-10-20T21:41:10+00:00</pubDate>
</item>
<item>
<title>Efficient Long-context Language Model Training by Core Attention Disaggregation</title>
<link>https://papers.cool/arxiv/2510.18121</link>
<guid>https://papers.cool/arxiv/2510.18121</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Core Attention Disaggregation (CAD), a method that separates the stateless core attention computation from the rest of a large language model and runs it on dedicated attention servers. By partitioning attention into token-level tasks and dynamically rebatching them, CAD balances compute across devices, reduces stragglers, and improves training throughput up to 1.35× on 512 GPUs with context lengths up to 512k tokens.<br /><strong>Summary (CN):</strong> 本文提出核心注意力解耦（CAD）技术，将无状态的核心注意力计算从模型主体中分离并在专用注意力服务器上执行。通过将注意力分割为基于 token 的任务并动态重批，CAD 实现了计算负载平衡，消除数据和流水线并行的瓶颈，在 512 张 GPU、上下文长度达 512k Token 的情况下提升训练吞吐量至最高 1.35 倍。<br /><strong>Keywords:</strong> long-context, language model, attention disaggregation, core attention, distributed training, compute balancing, pipeline parallelism, DistCA, efficiency, scaling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu, Yibo Zhu, Yimin Jiang, Ion Stoica, Eric Xing, Hao Zhang</div>
We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.
<div><strong>Authors:</strong> Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu, Yibo Zhu, Yimin Jiang, Ion Stoica, Eric Xing, Hao Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Core Attention Disaggregation (CAD), a method that separates the stateless core attention computation from the rest of a large language model and runs it on dedicated attention servers. By partitioning attention into token-level tasks and dynamically rebatching them, CAD balances compute across devices, reduces stragglers, and improves training throughput up to 1.35× on 512 GPUs with context lengths up to 512k tokens.", "summary_cn": "本文提出核心注意力解耦（CAD）技术，将无状态的核心注意力计算从模型主体中分离并在专用注意力服务器上执行。通过将注意力分割为基于 token 的任务并动态重批，CAD 实现了计算负载平衡，消除数据和流水线并行的瓶颈，在 512 张 GPU、上下文长度达 512k Token 的情况下提升训练吞吐量至最高 1.35 倍。", "keywords": "long-context, language model, attention disaggregation, core attention, distributed training, compute balancing, pipeline parallelism, DistCA, efficiency, scaling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yonghao Zhuang", "Junda Chen", "Bo Pang", "Yi Gu", "Yibo Zhu", "Yimin Jiang", "Ion Stoica", "Eric Xing", "Hao Zhang"]}
]]></acme>

<pubDate>2025-10-20T21:40:51+00:00</pubDate>
</item>
<item>
<title>Gradient Variance Reveals Failure Modes in Flow-Based Generative Models</title>
<link>https://papers.cool/arxiv/2510.18118</link>
<guid>https://papers.cool/arxiv/2510.18118</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that the straight‑path objective of Rectified Flow models hides a fundamental failure mode: under deterministic training, low gradient variance causes the model to memorize arbitrary training pairings, even when interpolating lines intersect. By analyzing Gaussian‑to‑Gaussian transport and proving the existence of a memorizing vector field, the authors demonstrate that deterministic integration at inference reproduces these exact pairings, while adding small noise restores generalization, as confirmed on CelebA.<br /><strong>Summary (CN):</strong> 本文指出，Rectified Flow 的直线路径目标在确定性训练下会导致梯度方差低，从而使模型记忆任意的训练配对，即便插值直线相交。通过对高斯到高斯传输的理论分析与证明记忆向量场的存在，作者展示了确定性积分在推理时会精确复制训练配对，而加入微小噪声则能恢复模型的泛化能力，实验证明了该现象（以 CelebA 数据集为例）。<br /><strong>Keywords:</strong> flow-based generative models, rectified flows, gradient variance, memorization, deterministic training, ODE vector fields, generalization, stochastic training, CelebA, failure modes<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Teodora Reu, Sixtine Dromigny, Michael Bronstein, Francisco Vargas</div>
Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective conceals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization.
<div><strong>Authors:</strong> Teodora Reu, Sixtine Dromigny, Michael Bronstein, Francisco Vargas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that the straight‑path objective of Rectified Flow models hides a fundamental failure mode: under deterministic training, low gradient variance causes the model to memorize arbitrary training pairings, even when interpolating lines intersect. By analyzing Gaussian‑to‑Gaussian transport and proving the existence of a memorizing vector field, the authors demonstrate that deterministic integration at inference reproduces these exact pairings, while adding small noise restores generalization, as confirmed on CelebA.", "summary_cn": "本文指出，Rectified Flow 的直线路径目标在确定性训练下会导致梯度方差低，从而使模型记忆任意的训练配对，即便插值直线相交。通过对高斯到高斯传输的理论分析与证明记忆向量场的存在，作者展示了确定性积分在推理时会精确复制训练配对，而加入微小噪声则能恢复模型的泛化能力，实验证明了该现象（以 CelebA 数据集为例）。", "keywords": "flow-based generative models, rectified flows, gradient variance, memorization, deterministic training, ODE vector fields, generalization, stochastic training, CelebA, failure modes", "scoring": {"interpretability": 6, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Teodora Reu", "Sixtine Dromigny", "Michael Bronstein", "Francisco Vargas"]}
]]></acme>

<pubDate>2025-10-20T21:37:11+00:00</pubDate>
</item>
<item>
<title>Latent Discrete Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.18114</link>
<guid>https://papers.cool/arxiv/2510.18114</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Latent Discrete Diffusion Models (LDDMs), which combine a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings to capture cross‑token dependencies. Two variants are proposed—FUJI‑LDDMs that jointly denoise tokens and latents, and SEQ‑LDDMs that resolve latents first and then the discrete chain—and ELBO‑based training objectives are derived. Experiments on unconditional language generation show that LDDMs outperform existing masked discrete diffusion baselines, especially under low‑step sampling budgets.<br /><strong>Summary (CN):</strong> 本文提出潜在离散扩散模型 (LDDM)，将对 token 的掩码离散扩散与对潜在嵌入的连续扩散相耦合，以捕获跨 token 的依赖关系。介绍了两种实现：FUJI‑LDDM 同时对 token 与潜在进行联合去噪，SEQ‑LDDM 先解码潜在再条件化离散链，并给出基于 ELBO 的目标函数。实验表明，在无条件语言生成任务中，LDDM 在整体质量和低步采样预算下均优于现有的掩码离散扩散基线。<br /><strong>Keywords:</strong> latent diffusion, discrete diffusion, masked diffusion, language modeling, joint denoising, ELBO, generative modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dario Shariatian, Alain Durmus, Stefano Peluchetti</div>
We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \emph{Latent Discrete Diffusion Models} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.
<div><strong>Authors:</strong> Dario Shariatian, Alain Durmus, Stefano Peluchetti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Latent Discrete Diffusion Models (LDDMs), which combine a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings to capture cross‑token dependencies. Two variants are proposed—FUJI‑LDDMs that jointly denoise tokens and latents, and SEQ‑LDDMs that resolve latents first and then the discrete chain—and ELBO‑based training objectives are derived. Experiments on unconditional language generation show that LDDMs outperform existing masked discrete diffusion baselines, especially under low‑step sampling budgets.", "summary_cn": "本文提出潜在离散扩散模型 (LDDM)，将对 token 的掩码离散扩散与对潜在嵌入的连续扩散相耦合，以捕获跨 token 的依赖关系。介绍了两种实现：FUJI‑LDDM 同时对 token 与潜在进行联合去噪，SEQ‑LDDM 先解码潜在再条件化离散链，并给出基于 ELBO 的目标函数。实验表明，在无条件语言生成任务中，LDDM 在整体质量和低步采样预算下均优于现有的掩码离散扩散基线。", "keywords": "latent diffusion, discrete diffusion, masked diffusion, language modeling, joint denoising, ELBO, generative modeling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dario Shariatian", "Alain Durmus", "Stefano Peluchetti"]}
]]></acme>

<pubDate>2025-10-20T21:26:52+00:00</pubDate>
</item>
<item>
<title>Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV</title>
<link>https://papers.cool/arxiv/2510.18103</link>
<guid>https://papers.cool/arxiv/2510.18103</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper develops a meta‑model that combines structured clinical variables with textual features extracted from discharge summaries and radiology reports to predict in‑hospital mortality of cardiac‑arrest ICU patients in the MIMIC‑IV database. Using LASSO and XGBoost for feature selection and a logistic regression on the top features, augmented with TF‑IDF and BERT embeddings, the final model achieves an AUC of 0.918, markedly outperforming a structured‑data‑only baseline. Decision‑curve analysis shows a broader range of clinically useful threshold probabilities, highlighting the prognostic value of unstructured notes in interpretable risk models.<br /><strong>Summary (CN):</strong> 本文构建了一个元模型，将结构化临床变量与出院摘要和影像学报告中的文本特征相结合，以预测 MIMIC‑IV 数据库中心脏骤停 ICU 患者的院内死亡率。通过 LASSO 与 XGBoost 进行特征选择，再使用逻辑回归结合 TF‑IDF 与 BERT 嵌入的顶级特征，最终模型的 AUC 达到 0.918，显著优于仅使用结构化数据的基线。决策曲线分析表明模型在更宽阄阈值概率范围内提供了更大的临床净收益，突显了非结构化笔记在可解释风险预测模型中的预后价值。<br /><strong>Keywords:</strong> mortality prediction, ICU, MIMIC-IV, structured clinical data, unstructured text, BERT embeddings, logistic regression, feature selection, AUC, decision curve analysis<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nursultan Mamatov, Philipp Kellmeyer</div>
Accurate early prediction of in-hospital mortality in intensive care units (ICUs) is essential for timely clinical intervention and efficient resource allocation. This study develops and evaluates machine learning models that integrate both structured clinical data and unstructured textual information, specifically discharge summaries and radiology reports, from the MIMIC-IV database. We used LASSO and XGBoost for feature selection, followed by a multivariate logistic regression trained on the top features identified by both models. Incorporating textual features using TF-IDF and BERT embeddings significantly improved predictive performance. The final logistic regression model, which combined structured and textual input, achieved an AUC of 0.918, compared to 0.753 when using structured data alone, a relative improvement 22%. The analysis of the decision curve demonstrated a superior standardized net benefit in a wide range of threshold probabilities (0.2-0.8), confirming the clinical utility of the model. These results underscore the added prognostic value of unstructured clinical notes and support their integration into interpretable feature-driven risk prediction models for ICU patients.
<div><strong>Authors:</strong> Nursultan Mamatov, Philipp Kellmeyer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper develops a meta‑model that combines structured clinical variables with textual features extracted from discharge summaries and radiology reports to predict in‑hospital mortality of cardiac‑arrest ICU patients in the MIMIC‑IV database. Using LASSO and XGBoost for feature selection and a logistic regression on the top features, augmented with TF‑IDF and BERT embeddings, the final model achieves an AUC of 0.918, markedly outperforming a structured‑data‑only baseline. Decision‑curve analysis shows a broader range of clinically useful threshold probabilities, highlighting the prognostic value of unstructured notes in interpretable risk models.", "summary_cn": "本文构建了一个元模型，将结构化临床变量与出院摘要和影像学报告中的文本特征相结合，以预测 MIMIC‑IV 数据库中心脏骤停 ICU 患者的院内死亡率。通过 LASSO 与 XGBoost 进行特征选择，再使用逻辑回归结合 TF‑IDF 与 BERT 嵌入的顶级特征，最终模型的 AUC 达到 0.918，显著优于仅使用结构化数据的基线。决策曲线分析表明模型在更宽阄阈值概率范围内提供了更大的临床净收益，突显了非结构化笔记在可解释风险预测模型中的预后价值。", "keywords": "mortality prediction, ICU, MIMIC-IV, structured clinical data, unstructured text, BERT embeddings, logistic regression, feature selection, AUC, decision curve analysis", "scoring": {"interpretability": 5, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nursultan Mamatov", "Philipp Kellmeyer"]}
]]></acme>

<pubDate>2025-10-20T20:56:45+00:00</pubDate>
</item>
<item>
<title>Provably Optimal Reinforcement Learning under Safety Filtering</title>
<link>https://papers.cool/arxiv/2510.18082</link>
<guid>https://papers.cool/arxiv/2510.18082</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a formal safety-critical MDP framework and proves that training reinforcement‑learning agents with a sufficiently permissive safety filter retains asymptotic optimality, eliminating the perceived safety‑performance trade‑off. It shows that learning in the filtered MDP is categorically safe, that standard RL convergence results apply, and that optimal policies in the filtered MDP achieve the same return as the best safe policy in the original problem. Empirical validation on Safety Gymnasium demonstrates zero safety violations during training while matching or exceeding the performance of unfiltered baselines.<br /><strong>Summary (CN):</strong> 本文提出了安全关键马尔可夫决策过程（SC‑MDP）框架，并证明在使用足够宽松的安全过滤器进行强化学习训练时，仍能保持渐近最优性，从而消除安全‑性能权衡的误解。研究表明，在过滤后的 MDP 中学习可以确保类别性安全，标准的强化学习收敛性同样适用，并且在相同过滤器下的最优策略能够达到原始 SC‑MDP 中最佳安全策略的回报。实验在 Safety Gymnasium 上验证了训练期间零违规，同时最终性能与或超过未过滤的基线。<br /><strong>Keywords:</strong> safe reinforcement learning, safety filtering, SC-MDP, provable optimality, control, RL convergence, safety-critical MDP, zero-violation training, performance guarantees<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 9, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Donggeon David Oh, Duy P. Nguyen, Haimin Hu, Jaime F. Fisac</div>
Recent advances in reinforcement learning (RL) enable its use on increasingly complex tasks, but the lack of formal safety guarantees still limits its application in safety-critical settings. A common practical approach is to augment the RL policy with a safety filter that overrides unsafe actions to prevent failures during both training and deployment. However, safety filtering is often perceived as sacrificing performance and hindering the learning process. We show that this perceived safety-performance tradeoff is not inherent and prove, for the first time, that enforcing safety with a sufficiently permissive safety filter does not degrade asymptotic performance. We formalize RL safety with a safety-critical Markov decision process (SC-MDP), which requires categorical, rather than high-probability, avoidance of catastrophic failure states. Additionally, we define an associated filtered MDP in which all actions result in safe effects, thanks to a safety filter that is considered to be a part of the environment. Our main theorem establishes that (i) learning in the filtered MDP is safe categorically, (ii) standard RL convergence carries over to the filtered MDP, and (iii) any policy that is optimal in the filtered MDP-when executed through the same filter-achieves the same asymptotic return as the best safe policy in the SC-MDP, yielding a complete separation between safety enforcement and performance optimization. We validate the theory on Safety Gymnasium with representative tasks and constraints, observing zero violations during training and final performance matching or exceeding unfiltered baselines. Together, these results shed light on a long-standing question in safety-filtered learning and provide a simple, principled recipe for safe RL: train and deploy RL policies with the most permissive safety filter that is available.
<div><strong>Authors:</strong> Donggeon David Oh, Duy P. Nguyen, Haimin Hu, Jaime F. Fisac</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a formal safety-critical MDP framework and proves that training reinforcement‑learning agents with a sufficiently permissive safety filter retains asymptotic optimality, eliminating the perceived safety‑performance trade‑off. It shows that learning in the filtered MDP is categorically safe, that standard RL convergence results apply, and that optimal policies in the filtered MDP achieve the same return as the best safe policy in the original problem. Empirical validation on Safety Gymnasium demonstrates zero safety violations during training while matching or exceeding the performance of unfiltered baselines.", "summary_cn": "本文提出了安全关键马尔可夫决策过程（SC‑MDP）框架，并证明在使用足够宽松的安全过滤器进行强化学习训练时，仍能保持渐近最优性，从而消除安全‑性能权衡的误解。研究表明，在过滤后的 MDP 中学习可以确保类别性安全，标准的强化学习收敛性同样适用，并且在相同过滤器下的最优策略能够达到原始 SC‑MDP 中最佳安全策略的回报。实验在 Safety Gymnasium 上验证了训练期间零违规，同时最终性能与或超过未过滤的基线。", "keywords": "safe reinforcement learning, safety filtering, SC-MDP, provable optimality, control, RL convergence, safety-critical MDP, zero-violation training, performance guarantees", "scoring": {"interpretability": 2, "understanding": 7, "safety": 9, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Donggeon David Oh", "Duy P. Nguyen", "Haimin Hu", "Jaime F. Fisac"]}
]]></acme>

<pubDate>2025-10-20T20:20:10+00:00</pubDate>
</item>
<item>
<title>Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</title>
<link>https://papers.cool/arxiv/2510.18081</link>
<guid>https://papers.cool/arxiv/2510.18081</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Any-Depth Alignment (ADA), an inference-time technique that periodically re‑inserts alignment‑rich header tokens during generation, forcing large language models to reassess harmfulness and recover refusals at any depth. ADA achieves near‑100% refusal rates against strong adversarial pre‑fill and prompt attacks across multiple open‑source model families without modifying model parameters, while preserving utility on benign tasks. The method reveals that alignment priors are concentrated in early assistant tokens and can be leveraged to maintain safety throughout generation.<br /><strong>Summary (CN):</strong> 本文提出 Any‑Depth Alignment（ADA）作为一种推理时防御方法，通过在生成过程中周期性重新注入包含强对齐先验的助手头部标记，使大语言模型在任意深度重新评估有害性并恢复拒绝。ADA 在多个开源模型上实现了接近 100% 的拒绝率，能够抵御强大的对抗性前置与提示攻击且无需修改模型参数，同时在正常任务上保持效用。该工作揭示了对齐能力主要集中在助手开头的标记中，可利用这些标记在整个生成过程保持安全。<br /><strong>Keywords:</strong> LLM alignment, inference-time defense, adversarial prompt attacks, refusal tokens, shallow alignment, safety, token prompting, robustness<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</div>
Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).
<div><strong>Authors:</strong> Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Any-Depth Alignment (ADA), an inference-time technique that periodically re‑inserts alignment‑rich header tokens during generation, forcing large language models to reassess harmfulness and recover refusals at any depth. ADA achieves near‑100% refusal rates against strong adversarial pre‑fill and prompt attacks across multiple open‑source model families without modifying model parameters, while preserving utility on benign tasks. The method reveals that alignment priors are concentrated in early assistant tokens and can be leveraged to maintain safety throughout generation.", "summary_cn": "本文提出 Any‑Depth Alignment（ADA）作为一种推理时防御方法，通过在生成过程中周期性重新注入包含强对齐先验的助手头部标记，使大语言模型在任意深度重新评估有害性并恢复拒绝。ADA 在多个开源模型上实现了接近 100% 的拒绝率，能够抵御强大的对抗性前置与提示攻击且无需修改模型参数，同时在正常任务上保持效用。该工作揭示了对齐能力主要集中在助手开头的标记中，可利用这些标记在整个生成过程保持安全。", "keywords": "LLM alignment, inference-time defense, adversarial prompt attacks, refusal tokens, shallow alignment, safety, token prompting, robustness", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiawei Zhang", "Andrew Estornell", "David D. Baek", "Bo Li", "Xiaojun Xu"]}
]]></acme>

<pubDate>2025-10-20T20:18:59+00:00</pubDate>
</item>
<item>
<title>MEG-GPT: A transformer-based foundation model for magnetoencephalography data</title>
<link>https://papers.cool/arxiv/2510.18080</link>
<guid>https://papers.cool/arxiv/2510.18080</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MEG-GPT, a transformer‑based foundation model for magnetoencephalography (MEG) data that uses a novel continuous‑tokenizer and next‑time‑point prediction. Trained on a large eyes‑closed resting‑state dataset, the model can generate realistic spatio‑spectral MEG signals and improves zero‑shot and fine‑tuned decoding performance across sessions and subjects. The work demonstrates the potential of large‑scale foundation models for electrophysiological data and neural decoding applications.<br /><strong>Summary (CN):</strong> 本文提出 MEG‑GPT，一种基于 Transformer 的磁共振脑电图（MEG）基础模型，采用新颖的连续数据分词器和下一个时间点预测任务。在大规模闭眼静息数据上进行训练后，模型能够生成具有真实时空频谱特性的 MEG 信号，并在跨会话、跨受试者的解码任务中显著提升零样本以及微调后的准确率。该工作展示了大规模基础模型在电生理数据和神经解码中的潜力。<br /><strong>Keywords:</strong> MEG, transformer, foundation model, time-attention, neural decoding, tokenization, cross-subject generalization, brain dynamics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 3, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</div>
Modelling the complex spatiotemporal patterns of large-scale brain dynamics is crucial for neuroscience, but traditional methods fail to capture the rich structure in modalities such as magnetoencephalography (MEG). Recent advances in deep learning have enabled significant progress in other domains, such as language and vision, by using foundation models at scale. Here, we introduce MEG-GPT, a transformer based foundation model that uses time-attention and next time-point prediction. To facilitate this, we also introduce a novel data-driven tokeniser for continuous MEG data, which preserves the high temporal resolution of continuous MEG signals without lossy transformations. We trained MEG-GPT on tokenised brain region time-courses extracted from a large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that the learnt model can generate data with realistic spatio-spectral properties, including transient events and population variability. Critically, it performs well in downstream decoding tasks, improving downstream supervised prediction task, showing improved zero-shot generalisation across sessions (improving accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49) compared to a baseline methods. Furthermore, we show the model can be efficiently fine-tuned on a smaller labelled dataset to boost performance in cross-subject decoding scenarios. This work establishes a powerful foundation model for electrophysiological data, paving the way for applications in computational neuroscience and neural decoding.
<div><strong>Authors:</strong> Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MEG-GPT, a transformer‑based foundation model for magnetoencephalography (MEG) data that uses a novel continuous‑tokenizer and next‑time‑point prediction. Trained on a large eyes‑closed resting‑state dataset, the model can generate realistic spatio‑spectral MEG signals and improves zero‑shot and fine‑tuned decoding performance across sessions and subjects. The work demonstrates the potential of large‑scale foundation models for electrophysiological data and neural decoding applications.", "summary_cn": "本文提出 MEG‑GPT，一种基于 Transformer 的磁共振脑电图（MEG）基础模型，采用新颖的连续数据分词器和下一个时间点预测任务。在大规模闭眼静息数据上进行训练后，模型能够生成具有真实时空频谱特性的 MEG 信号，并在跨会话、跨受试者的解码任务中显著提升零样本以及微调后的准确率。该工作展示了大规模基础模型在电生理数据和神经解码中的潜力。", "keywords": "MEG, transformer, foundation model, time-attention, neural decoding, tokenization, cross-subject generalization, brain dynamics", "scoring": {"interpretability": 3, "understanding": 3, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rukuang Huang", "Sungjun Cho", "Chetan Gohil", "Oiwi Parker Jones", "Mark Woolrich"]}
]]></acme>

<pubDate>2025-10-20T20:18:38+00:00</pubDate>
</item>
<item>
<title>Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods</title>
<link>https://papers.cool/arxiv/2510.18075</link>
<guid>https://papers.cool/arxiv/2510.18075</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a publicly available, laboratory‑scale batch distillation dataset containing 119 experiments with both fault‑free and intentionally induced anomalies. The data include multivariate time‑series sensor readings, online NMR concentration profiles, video, audio, uncertainty estimates, extensive metadata, and expert annotations based on a newly developed anomaly ontology. This resource is intended to facilitate the development and evaluation of advanced machine‑learning‑based anomaly detection, interpretability, and mitigation methods for chemical processes.<br /><strong>Summary (CN):</strong> 本文发布了一个公开的实验室规模批式蒸馏数据集，包含119次实验的正常和人为诱导异常数据。数据集提供多传感器时间序列、在线核磁共振(NMR)浓度曲线、视频、音频、测量不确定性、丰富的元数据以及基于新建异常本体的专家标注。该资源旨在促进机器学习异常检测、可解释性以及异常缓解方法在化工过程中的研发与评估。<br /><strong>Keywords:</strong> batch distillation, anomaly detection, dataset, chemical process, machine learning, time-series, NMR spectroscopy, interpretability, fault detection, process control<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Justus Arweiler, Indra Jungjohann, Aparna Muraleedharan, Heike Leitte, Jakob Burger, Kerstin Münnemann, Fabian Jirasek, Hans Hasse</div>
Machine learning (ML) holds great potential to advance anomaly detection (AD) in chemical processes. However, the development of ML-based methods is hindered by the lack of openly available experimental data. To address this gap, we have set up a laboratory-scale batch distillation plant and operated it to generate an extensive experimental database, covering fault-free experiments and experiments in which anomalies were intentionally induced, for training advanced ML-based AD methods. In total, 119 experiments were conducted across a wide range of operating conditions and mixtures. Most experiments containing anomalies were paired with a corresponding fault-free one. The database that we provide here includes time-series data from numerous sensors and actuators, along with estimates of measurement uncertainty. In addition, unconventional data sources -- such as concentration profiles obtained via online benchtop NMR spectroscopy and video and audio recordings -- are provided. Extensive metadata and expert annotations of all experiments are included. The anomaly annotations are based on an ontology developed in this work. The data are organized in a structured database and made freely available via doi.org/10.5281/zenodo.17395544. This new database paves the way for the development of advanced ML-based AD methods. As it includes information on the causes of anomalies, it further enables the development of interpretable and explainable ML approaches, as well as methods for anomaly mitigation.
<div><strong>Authors:</strong> Justus Arweiler, Indra Jungjohann, Aparna Muraleedharan, Heike Leitte, Jakob Burger, Kerstin Münnemann, Fabian Jirasek, Hans Hasse</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a publicly available, laboratory‑scale batch distillation dataset containing 119 experiments with both fault‑free and intentionally induced anomalies. The data include multivariate time‑series sensor readings, online NMR concentration profiles, video, audio, uncertainty estimates, extensive metadata, and expert annotations based on a newly developed anomaly ontology. This resource is intended to facilitate the development and evaluation of advanced machine‑learning‑based anomaly detection, interpretability, and mitigation methods for chemical processes.", "summary_cn": "本文发布了一个公开的实验室规模批式蒸馏数据集，包含119次实验的正常和人为诱导异常数据。数据集提供多传感器时间序列、在线核磁共振(NMR)浓度曲线、视频、音频、测量不确定性、丰富的元数据以及基于新建异常本体的专家标注。该资源旨在促进机器学习异常检测、可解释性以及异常缓解方法在化工过程中的研发与评估。", "keywords": "batch distillation, anomaly detection, dataset, chemical process, machine learning, time-series, NMR spectroscopy, interpretability, fault detection, process control", "scoring": {"interpretability": 5, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Justus Arweiler", "Indra Jungjohann", "Aparna Muraleedharan", "Heike Leitte", "Jakob Burger", "Kerstin Münnemann", "Fabian Jirasek", "Hans Hasse"]}
]]></acme>

<pubDate>2025-10-20T20:13:31+00:00</pubDate>
</item>
<item>
<title>R2L: Reliable Reinforcement Learning: Guaranteed Return &amp; Reliable Policies in Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18074</link>
<guid>https://papers.cool/arxiv/2510.18074</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a reliable reinforcement learning formulation that maximizes the probability that cumulative return exceeds a given threshold, and shows how this can be transformed into a standard RL problem via state augmentation, enabling the use of existing algorithms such as Q‑learning and Dueling Double DQN. Theoretical analysis proves equivalence of the formulations, and experiments on a reliable routing task demonstrate policies that balance efficiency and success probability, highlighting applicability to stochastic and safety‑critical domains.<br /><strong>Summary (CN):</strong> 本文提出一种可靠强化学习的形式化目标，即最大化累计回报超过预设阈值的概率，并通过状态增强将其转化为标准强化学习问题，从而可以直接使用 Q‑学习、双 DQN 等现有算法。理论结果证明了两种形式的等价性，实验在可靠路由任务上展示了能够在效率与成功概率之间取得平衡的策略，凸显了该方法在随机且安全关键环境中的潜在价值。<br /><strong>Keywords:</strong> reliable reinforcement learning, probability of return, risk-sensitive RL, safety-critical RL, robust RL, Q-learning, Dueling Double DQN, reliable routing, performance guarantees<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Nadir Farhi</div>
In this work, we address the problem of determining reliable policies in reinforcement learning (RL), with a focus on optimization under uncertainty and the need for performance guarantees. While classical RL algorithms aim at maximizing the expected return, many real-world applications - such as routing, resource allocation, or sequential decision-making under risk - require strategies that ensure not only high average performance but also a guaranteed probability of success. To this end, we propose a novel formulation in which the objective is to maximize the probability that the cumulative return exceeds a prescribed threshold. We demonstrate that this reliable RL problem can be reformulated, via a state-augmented representation, into a standard RL problem, thereby allowing the use of existing RL and deep RL algorithms without the need for entirely new algorithmic frameworks. Theoretical results establish the equivalence of the two formulations and show that reliable strategies can be derived by appropriately adapting well-known methods such as Q-learning or Dueling Double DQN. To illustrate the practical relevance of the approach, we consider the problem of reliable routing, where the goal is not to minimize the expected travel time but rather to maximize the probability of reaching the destination within a given time budget. Numerical experiments confirm that the proposed formulation leads to policies that effectively balance efficiency and reliability, highlighting the potential of reliable RL for applications in stochastic and safety-critical environments.
<div><strong>Authors:</strong> Nadir Farhi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a reliable reinforcement learning formulation that maximizes the probability that cumulative return exceeds a given threshold, and shows how this can be transformed into a standard RL problem via state augmentation, enabling the use of existing algorithms such as Q‑learning and Dueling Double DQN. Theoretical analysis proves equivalence of the formulations, and experiments on a reliable routing task demonstrate policies that balance efficiency and success probability, highlighting applicability to stochastic and safety‑critical domains.", "summary_cn": "本文提出一种可靠强化学习的形式化目标，即最大化累计回报超过预设阈值的概率，并通过状态增强将其转化为标准强化学习问题，从而可以直接使用 Q‑学习、双 DQN 等现有算法。理论结果证明了两种形式的等价性，实验在可靠路由任务上展示了能够在效率与成功概率之间取得平衡的策略，凸显了该方法在随机且安全关键环境中的潜在价值。", "keywords": "reliable reinforcement learning, probability of return, risk-sensitive RL, safety-critical RL, robust RL, Q-learning, Dueling Double DQN, reliable routing, performance guarantees", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Nadir Farhi"]}
]]></acme>

<pubDate>2025-10-20T20:08:41+00:00</pubDate>
</item>
<item>
<title>Fine-tuning Flow Matching Generative Models with Intermediate Feedback</title>
<link>https://papers.cool/arxiv/2510.18072</link>
<guid>https://papers.cool/arxiv/2510.18072</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AC-Flow, an actor-critic framework designed to fine‑tune continuous‑time flow matching generative models using intermediate feedback. It employs reward shaping, advantage clipping with a warm‑up phase, and a generalized critic weighting scheme with Wasserstein regularization to stabilize training and preserve diversity. Experiments on Stable Diffusion 3 show state‑of‑the‑art performance on text‑to‑image alignment and generalization to unseen human preference models.<br /><strong>Summary (CN):</strong> 本文提出了 AC-Flow，一种基于 actor‑critic 的框架，用于利用中间反馈微调连续时间流匹配生成模型。通过奖励整形、优势裁剪加热身阶段以及带 Wasserstein 正则化的广义批评者加权机制，实现了训练的稳定性并保持生成多样性。实验在 Stable Diffusion 3 上展示了在文本到图像对齐任务以及对未见人类偏好模型的泛化方面的先进表现。<br /><strong>Keywords:</strong> flow matching, generative models, actor-critic, reward shaping, text-to-image alignment, Wasserstein regularization, fine-tuning, preference modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</div>
Flow-based generative models have shown remarkable success in text-to-image generation, yet fine-tuning them with intermediate feedback remains challenging, especially for continuous-time flow matching models. Most existing approaches solely learn from outcome rewards, struggling with the credit assignment problem. Alternative methods that attempt to learn a critic via direct regression on cumulative rewards often face training instabilities and model collapse in online settings. We present AC-Flow, a robust actor-critic framework that addresses these challenges through three key innovations: (1) reward shaping that provides well-normalized learning signals to enable stable intermediate value learning and gradient control, (2) a novel dual-stability mechanism that combines advantage clipping to prevent destructive policy updates with a warm-up phase that allows the critic to mature before influencing the actor, and (3) a scalable generalized critic weighting scheme that extends traditional reward-weighted methods while preserving model diversity through Wasserstein regularization. Through extensive experiments on Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art performance in text-to-image alignment tasks and generalization to unseen human preference models. Our results demonstrate that even with a computationally efficient critic model, we can robustly finetune flow models without compromising generative quality, diversity, or stability.
<div><strong>Authors:</strong> Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AC-Flow, an actor-critic framework designed to fine‑tune continuous‑time flow matching generative models using intermediate feedback. It employs reward shaping, advantage clipping with a warm‑up phase, and a generalized critic weighting scheme with Wasserstein regularization to stabilize training and preserve diversity. Experiments on Stable Diffusion 3 show state‑of‑the‑art performance on text‑to‑image alignment and generalization to unseen human preference models.", "summary_cn": "本文提出了 AC-Flow，一种基于 actor‑critic 的框架，用于利用中间反馈微调连续时间流匹配生成模型。通过奖励整形、优势裁剪加热身阶段以及带 Wasserstein 正则化的广义批评者加权机制，实现了训练的稳定性并保持生成多样性。实验在 Stable Diffusion 3 上展示了在文本到图像对齐任务以及对未见人类偏好模型的泛化方面的先进表现。", "keywords": "flow matching, generative models, actor-critic, reward shaping, text-to-image alignment, Wasserstein regularization, fine-tuning, preference modeling", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiajun Fan", "Chaoran Cheng", "Shuaike Shen", "Xiangxin Zhou", "Ge Liu"]}
]]></acme>

<pubDate>2025-10-20T20:08:03+00:00</pubDate>
</item>
<item>
<title>SPACeR: Self-Play Anchoring with Centralized Reference Models</title>
<link>https://papers.cool/arxiv/2510.18060</link>
<guid>https://papers.cool/arxiv/2510.18060</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SPACeR combines a pretrained tokenized autoregressive motion model as a centralized reference policy that supplies likelihood and KL‑divergence rewards during decentralized self‑play reinforcement learning, producing human‑like traffic agents that are up to 10× faster and 50× smaller than large diffusion models while achieving competitive performance on the Waymo Sim Agents Challenge and effective closed‑loop ego‑planning evaluation.<br /><strong>Summary (CN):</strong> SPACeR利用预训练的基于令牌的自动回归运动模型作为中心参考策略，在去中心化的自博弈强化学习中提供似然奖励和 KL 散度，从而锚定策略到人类驾驶分布，实现了比大型生成模型快 10 倍、参数体积小 50 倍的逼真交通代理，并在 Waymo Sim Agents 挑战及闭环规划评估中取得竞争性能。<br /><strong>Keywords:</strong> self-play, reinforcement learning, tokenized motion model, imitation learning, autonomous driving, human-like behavior, likelihood reward, KL anchoring, Waymo Sim Agents, scalable traffic simulation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.
<div><strong>Authors:</strong> Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SPACeR combines a pretrained tokenized autoregressive motion model as a centralized reference policy that supplies likelihood and KL‑divergence rewards during decentralized self‑play reinforcement learning, producing human‑like traffic agents that are up to 10× faster and 50× smaller than large diffusion models while achieving competitive performance on the Waymo Sim Agents Challenge and effective closed‑loop ego‑planning evaluation.", "summary_cn": "SPACeR利用预训练的基于令牌的自动回归运动模型作为中心参考策略，在去中心化的自博弈强化学习中提供似然奖励和 KL 散度，从而锚定策略到人类驾驶分布，实现了比大型生成模型快 10 倍、参数体积小 50 倍的逼真交通代理，并在 Waymo Sim Agents 挑战及闭环规划评估中取得竞争性能。", "keywords": "self-play, reinforcement learning, tokenized motion model, imitation learning, autonomous driving, human-like behavior, likelihood reward, KL anchoring, Waymo Sim Agents, scalable traffic simulation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Wei-Jer Chang", "Akshay Rangesh", "Kevin Joseph", "Matthew Strong", "Masayoshi Tomizuka", "Yihan Hu", "Wei Zhan"]}
]]></acme>

<pubDate>2025-10-20T19:53:02+00:00</pubDate>
</item>
<item>
<title>Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</title>
<link>https://papers.cool/arxiv/2510.18053</link>
<guid>https://papers.cool/arxiv/2510.18053</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Divergence Regularized Policy Optimization (ADRPO), which dynamically adjusts divergence regularization strength based on advantage estimates, allowing fine‑tuning of generative models to balance exploration and exploitation. ADRPO, instantiated with Wasserstein‑2 regularization for diffusion models and KL regularization for LLMs, achieves superior semantic alignment, diversity, and compositional control compared to fixed‑regularization baselines across text‑to‑image, language, and audio‑reasoning tasks. The method also mitigates reward‑hacking and instability, enabling smaller models to outperform much larger commercial systems.<br /><strong>Summary (CN):</strong> 本文提出自适应散度正则化策略优化（ADRPO），通过依据优势估计动态调节散度正则化强度，实现生成模型微调过程中的探索与利用平衡。该方法在文本到图像、语言模型以及音频推理等多模态任务中，使用 Wasserstein‑2 或 KL 正则化，实现了比固定正则化基线更好的语义对齐、多样性和组合控制，并减轻了奖励黑客和训练不稳定的问题，使得较小模型能够超越更大商业模型。<br /><strong>Keywords:</strong> adaptive regularization, policy optimization, reinforcement learning fine-tuning, generative models, Wasserstein-2, KL regularization, exploration-exploitation, text-to-image, large language models, reward hacking mitigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</div>
Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.
<div><strong>Authors:</strong> Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Divergence Regularized Policy Optimization (ADRPO), which dynamically adjusts divergence regularization strength based on advantage estimates, allowing fine‑tuning of generative models to balance exploration and exploitation. ADRPO, instantiated with Wasserstein‑2 regularization for diffusion models and KL regularization for LLMs, achieves superior semantic alignment, diversity, and compositional control compared to fixed‑regularization baselines across text‑to‑image, language, and audio‑reasoning tasks. The method also mitigates reward‑hacking and instability, enabling smaller models to outperform much larger commercial systems.", "summary_cn": "本文提出自适应散度正则化策略优化（ADRPO），通过依据优势估计动态调节散度正则化强度，实现生成模型微调过程中的探索与利用平衡。该方法在文本到图像、语言模型以及音频推理等多模态任务中，使用 Wasserstein‑2 或 KL 正则化，实现了比固定正则化基线更好的语义对齐、多样性和组合控制，并减轻了奖励黑客和训练不稳定的问题，使得较小模型能够超越更大商业模型。", "keywords": "adaptive regularization, policy optimization, reinforcement learning fine-tuning, generative models, Wasserstein-2, KL regularization, exploration-exploitation, text-to-image, large language models, reward hacking mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiajun Fan", "Tong Wei", "Chaoran Cheng", "Yuxin Chen", "Ge Liu"]}
]]></acme>

<pubDate>2025-10-20T19:46:02+00:00</pubDate>
</item>
<item>
<title>Measure-Theoretic Anti-Causal Representation Learning</title>
<link>https://papers.cool/arxiv/2510.18052</link>
<guid>https://papers.cool/arxiv/2510.18052</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Anti-Causal Invariant Abstractions (ACIA), a measure-theoretic framework for learning representations in the anti-causal setting where labels generate features. ACIA uses a two-level design with low-level representations of the label-to-observation process and high-level representations that capture stable causal patterns across environments, providing theoretical out-of-distribution generalization guarantees and handling both perfect and imperfect interventions. Experiments on synthetic and medical datasets show ACIA outperforms existing methods in accuracy and invariance.<br /><strong>Summary (CN):</strong> 本文提出了 Anti-Causal Invariant Abstractions (ACIA)，一种基于测度论的反因果表征学习框架，假设标签产生特征。ACIA 采用两层设计，低层表征捕获标签到观测的生成过程，高层表征学习跨环境的稳定因果模式，提供了分布外泛化的理论保证并能处理完美或不完美的干预。实验在合成数据和医学数据上显示 ACIA 在准确率和不变性指标上 consistently 优于现有方法。<br /><strong>Keywords:</strong> anti-causal representation learning, invariant abstractions, measure-theoretic framework, out-of-distribution generalization, interventional kernels, causal invariance, robust learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Arman Behnam, Binghui Wang</div>
Causal representation learning in the anti-causal setting (labels cause features rather than the reverse) presents unique challenges requiring specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a novel measure-theoretic framework for anti-causal representation learning. ACIA employs a two-level design, low-level representations capture how labels generate observations, while high-level representations learn stable causal patterns across environment-specific variations. ACIA addresses key limitations of existing approaches by accommodating prefect and imperfect interventions through interventional kernels, eliminating dependency on explicit causal structures, handling high-dimensional data effectively, and providing theoretical guarantees for out-of-distribution generalization. Experiments on synthetic and real-world medical datasets demonstrate that ACIA consistently outperforms state-of-the-art methods in both accuracy and invariance metrics. Furthermore, our theoretical results establish tight bounds on performance gaps between training and unseen environments, confirming the efficacy of our approach for robust anti-causal learning.
<div><strong>Authors:</strong> Arman Behnam, Binghui Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Anti-Causal Invariant Abstractions (ACIA), a measure-theoretic framework for learning representations in the anti-causal setting where labels generate features. ACIA uses a two-level design with low-level representations of the label-to-observation process and high-level representations that capture stable causal patterns across environments, providing theoretical out-of-distribution generalization guarantees and handling both perfect and imperfect interventions. Experiments on synthetic and medical datasets show ACIA outperforms existing methods in accuracy and invariance.", "summary_cn": "本文提出了 Anti-Causal Invariant Abstractions (ACIA)，一种基于测度论的反因果表征学习框架，假设标签产生特征。ACIA 采用两层设计，低层表征捕获标签到观测的生成过程，高层表征学习跨环境的稳定因果模式，提供了分布外泛化的理论保证并能处理完美或不完美的干预。实验在合成数据和医学数据上显示 ACIA 在准确率和不变性指标上 consistently 优于现有方法。", "keywords": "anti-causal representation learning, invariant abstractions, measure-theoretic framework, out-of-distribution generalization, interventional kernels, causal invariance, robust learning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Arman Behnam", "Binghui Wang"]}
]]></acme>

<pubDate>2025-10-16T22:13:05+00:00</pubDate>
</item>
<item>
<title>Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network</title>
<link>https://papers.cool/arxiv/2510.18041</link>
<guid>https://papers.cool/arxiv/2510.18041</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a Spatio‑Temporal Operator Network (STONe) that learns a non‑autoregressive mapping from sparse ground‑based neutron sensor data to high‑altitude radiation dose fields, enabling accurate 180‑day forecasts with millisecond inference speed. It tackles cross‑domain, long‑term forecasting challenges in scientific‑machine‑learning contexts, showing that operator learning can work without domain alignment or recurrent propagation. The work is evaluated on 23 years of global neutron measurements and demonstrates applicability to broader physical, climate and energy systems.<br /><strong>Summary (CN):</strong> 本文提出一种跨域长时序预测模型 STONe，利用稀疏地面中子传感器数据直接预测高空辐射剂量场，实现 180 天的精准预报并具备毫秒级推理速度。该方法解决了科学机器学习中跨域、长时序预测的难题，突破了传统算子学习需域对齐或递归的限制。利用 23 年全球中子数据进行实验，展示了在物理、气候和能源系统中的广泛适用性。<br /><strong>Keywords:</strong> cross-domain forecasting, neural operator, spatio-temporal modeling, radiation dose prediction, sparse sensors, long-term prediction, scientific machine learning, STONe<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jay Phil Yoo, Kazuma Kobayashi, Souvik Chakraborty, Syed Bahauddin Alam</div>
Forecasting unobservable physical quantities from sparse, cross-domain sensor data is a central unsolved problem in scientific machine learning. Existing neural operators and large-scale forecasters rely on dense, co-located input-output fields and short temporal contexts, assumptions that fail in real-world systems where sensing and prediction occur on distinct physical manifolds and over long timescales. We introduce the Spatio-Temporal Operator Network (STONe), a non-autoregressive neural operator that learns a stable functional mapping between heterogeneous domains. By directly inferring high-altitude radiation dose fields from sparse ground-based neutron measurements, STONe demonstrates that operator learning can generalize beyond shared-domain settings. It defines a nonlinear operator between sensor and target manifolds that remains stable over long forecasting horizons without iterative recurrence. This challenges the conventional view that operator learning requires domain alignment or autoregressive propagation. Trained on 23 years of global neutron data, STONe achieves accurate 180-day forecasts with millisecond inference latency. The framework establishes a general principle for cross-domain operator inference, enabling real-time prediction of complex spatiotemporal fields in physics, climate, and energy systems.
<div><strong>Authors:</strong> Jay Phil Yoo, Kazuma Kobayashi, Souvik Chakraborty, Syed Bahauddin Alam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a Spatio‑Temporal Operator Network (STONe) that learns a non‑autoregressive mapping from sparse ground‑based neutron sensor data to high‑altitude radiation dose fields, enabling accurate 180‑day forecasts with millisecond inference speed. It tackles cross‑domain, long‑term forecasting challenges in scientific‑machine‑learning contexts, showing that operator learning can work without domain alignment or recurrent propagation. The work is evaluated on 23 years of global neutron measurements and demonstrates applicability to broader physical, climate and energy systems.", "summary_cn": "本文提出一种跨域长时序预测模型 STONe，利用稀疏地面中子传感器数据直接预测高空辐射剂量场，实现 180 天的精准预报并具备毫秒级推理速度。该方法解决了科学机器学习中跨域、长时序预测的难题，突破了传统算子学习需域对齐或递归的限制。利用 23 年全球中子数据进行实验，展示了在物理、气候和能源系统中的广泛适用性。", "keywords": "cross-domain forecasting, neural operator, spatio-temporal modeling, radiation dose prediction, sparse sensors, long-term prediction, scientific machine learning, STONe", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jay Phil Yoo", "Kazuma Kobayashi", "Souvik Chakraborty", "Syed Bahauddin Alam"]}
]]></acme>

<pubDate>2025-10-20T19:27:00+00:00</pubDate>
</item>
<item>
<title>Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity</title>
<link>https://papers.cool/arxiv/2510.18037</link>
<guid>https://papers.cool/arxiv/2510.18037</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates eight probabilistic deep learning models, including two foundation models, against four classical statistical approaches for forecasting spontaneous neural activity recorded via widefield imaging in mouse cortex. Results show that several deep learning models consistently outperform classical methods, with the best model providing informative forecasts up to 1.5 seconds ahead, highlighting potential for closed-loop neural control and deeper insight into neural dynamics.<br /><strong>Summary (CN):</strong> 本文系统评估了八种概率深度学习模型（含两个基础模型）在小鼠皮层宽视野成像记录的自发神经活动时间序列预测任务中的表现，并与四种经典统计模型及两种基线方法进行比较。实验表明，多种深度学习模型在各预测时长上均优于传统方法，最佳模型能够提供约1.5秒的有效预测，显示出对闭环神经控制和神经活动内在时间结构探索的潜在价值。<br /><strong>Keywords:</strong> probabilistic forecasting, neural activity, time series, deep learning, widefield imaging, benchmark, foundation models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz</div>
Neural activity forecasting is central to understanding neural systems and enabling closed-loop control. While deep learning has recently advanced the state-of-the-art in the time series forecasting literature, its application to neural activity forecasting remains limited. To bridge this gap, we systematically evaluated eight probabilistic deep learning models, including two foundation models, that have demonstrated strong performance on general forecasting benchmarks. We compared them against four classical statistical models and two baseline methods on spontaneous neural activity recorded from mouse cortex via widefield imaging. Across prediction horizons, several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future. Our findings point toward future control applications and open new avenues for probing the intrinsic temporal structure of neural activity.
<div><strong>Authors:</strong> Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates eight probabilistic deep learning models, including two foundation models, against four classical statistical approaches for forecasting spontaneous neural activity recorded via widefield imaging in mouse cortex. Results show that several deep learning models consistently outperform classical methods, with the best model providing informative forecasts up to 1.5 seconds ahead, highlighting potential for closed-loop neural control and deeper insight into neural dynamics.", "summary_cn": "本文系统评估了八种概率深度学习模型（含两个基础模型）在小鼠皮层宽视野成像记录的自发神经活动时间序列预测任务中的表现，并与四种经典统计模型及两种基线方法进行比较。实验表明，多种深度学习模型在各预测时长上均优于传统方法，最佳模型能够提供约1.5秒的有效预测，显示出对闭环神经控制和神经活动内在时间结构探索的潜在价值。", "keywords": "probabilistic forecasting, neural activity, time series, deep learning, widefield imaging, benchmark, foundation models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziyu Lu", "Anna J. Li", "Alexander E. Ladd", "Pascha Matveev", "Aditya Deole", "Eric Shea-Brown", "J. Nathan Kutz", "Nicholas A. Steinmetz"]}
]]></acme>

<pubDate>2025-10-20T19:19:29+00:00</pubDate>
</item>
<item>
<title>Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data</title>
<link>https://papers.cool/arxiv/2510.18004</link>
<guid>https://papers.cool/arxiv/2510.18004</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces A-DATSC, an Attention-Guided Deep Adversarial Temporal Subspace Clustering model that combines a U-Net‑style generator with ConvLSTM2D layers and a graph‑attention transformer self‑expressive network to better capture local and global spatiotemporal dependencies in multivariate data, and uses an adversarial discriminator to verify clustering quality, achieving state‑of‑the‑art results on three real‑world datasets.<br /><strong>Summary (CN):</strong> 本文提出 A-DATSC 模型，通过 U‑Net 结构的生成器融合 ConvLSTM2D 层以及基于图注意力变换器的自表达网络，捕获多变量时空数据的局部空间关系和全局长短程依赖，并采用对抗式判别器验证聚类质量，在三个真实数据集上实现了领先的聚类性能。<br /><strong>Keywords:</strong> subspace clustering, attention transformer, ConvLSTM, adversarial learning, spatiotemporal data, graph attention, self-expressive network<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Francis Ndikum Nji, Vandana Janeja, Jianwu Wang</div>
Deep subspace clustering models are vital for applications such as snowmelt detection, sea ice tracking, crop health monitoring, infectious disease modeling, network load prediction, and land-use planning, where multivariate spatiotemporal data exhibit complex temporal dependencies and reside on multiple nonlinear manifolds beyond the capability of traditional clustering methods. These models project data into a latent space where samples lie in linear subspaces and exploit the self-expressiveness property to uncover intrinsic relationships. Despite their success, existing methods face major limitations: they use shallow autoencoders that ignore clustering errors, emphasize global features while neglecting local structure, fail to model long-range dependencies and positional information, and are rarely applied to 4D spatiotemporal data. To address these issues, we propose A-DATSC (Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model combining a deep subspace clustering generator and a quality-verifying discriminator. The generator, inspired by U-Net, preserves spatial and temporal integrity through stacked TimeDistributed ConvLSTM2D layers, reducing parameters and enhancing generalization. A graph attention transformer based self-expressive network captures local spatial relationships, global dependencies, and both short- and long-range correlations. Experiments on three real-world multivariate spatiotemporal datasets show that A-DATSC achieves substantially superior clustering performance compared to state-of-the-art deep subspace clustering models.
<div><strong>Authors:</strong> Francis Ndikum Nji, Vandana Janeja, Jianwu Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces A-DATSC, an Attention-Guided Deep Adversarial Temporal Subspace Clustering model that combines a U-Net‑style generator with ConvLSTM2D layers and a graph‑attention transformer self‑expressive network to better capture local and global spatiotemporal dependencies in multivariate data, and uses an adversarial discriminator to verify clustering quality, achieving state‑of‑the‑art results on three real‑world datasets.", "summary_cn": "本文提出 A-DATSC 模型，通过 U‑Net 结构的生成器融合 ConvLSTM2D 层以及基于图注意力变换器的自表达网络，捕获多变量时空数据的局部空间关系和全局长短程依赖，并采用对抗式判别器验证聚类质量，在三个真实数据集上实现了领先的聚类性能。", "keywords": "subspace clustering, attention transformer, ConvLSTM, adversarial learning, spatiotemporal data, graph attention, self-expressive network", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Francis Ndikum Nji", "Vandana Janeja", "Jianwu Wang"]}
]]></acme>

<pubDate>2025-10-20T18:38:26+00:00</pubDate>
</item>
<item>
<title>Demystifying Transition Matching: When and Why It Can Beat Flow Matching</title>
<link>https://papers.cool/arxiv/2510.17991</link>
<guid>https://papers.cool/arxiv/2510.17991</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes when Transition Matching (TM) outperforms Flow Matching (FM) in generative sampling, proving that TM yields lower KL divergence for unimodal Gaussian targets and faster convergence under a fixed compute budget. It extends the theory to Gaussian mixtures, showing TM benefits in regimes with well‑separated modes and non‑negligible variance, and validates the findings with experiments on synthetic and real image/video data.<br /><strong>Summary (CN):</strong> 本文研究了何时以及为何过渡匹配（TM）在生成式采样中优于流匹配（FM），并证明在单峰高斯目标下 TM 能获得更低的 KL 散度并在固定计算预算下收敛更快。进一步将分析推广至高斯混合模型，指出在模式分离充分且方差不趋于零的情形下 TM 具有优势，并通过合成高斯分布及真实图像/视频生成实验进行验证。<br /><strong>Keywords:</strong> transition matching, flow matching, generative models, KL divergence, Gaussian mixture, sampling dynamics, convergence analysis, unimodal Gaussian<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</div>
Flow Matching (FM) underpins many state-of-the-art generative models, yet recent results indicate that Transition Matching (TM) can achieve higher quality with fewer sampling steps. This work answers the question of when and why TM outperforms FM. First, when the target is a unimodal Gaussian distribution, we prove that TM attains strictly lower KL divergence than FM for finite number of steps. The improvement arises from stochastic difference latent updates in TM, which preserve target covariance that deterministic FM underestimates. We then characterize convergence rates, showing that TM achieves faster convergence than FM under a fixed compute budget, establishing its advantage in the unimodal Gaussian setting. Second, we extend the analysis to Gaussian mixtures and identify local-unimodality regimes in which the sampling dynamics approximate the unimodal case, where TM can outperform FM. The approximation error decreases as the minimal distance between component means increases, highlighting that TM is favored when the modes are well separated. However, when the target variance approaches zero, each TM update converges to the FM update, and the performance advantage of TM diminishes. In summary, we show that TM outperforms FM when the target distribution has well-separated modes and non-negligible variances. We validate our theoretical results with controlled experiments on Gaussian distributions, and extend the comparison to real-world applications in image and video generation.
<div><strong>Authors:</strong> Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes when Transition Matching (TM) outperforms Flow Matching (FM) in generative sampling, proving that TM yields lower KL divergence for unimodal Gaussian targets and faster convergence under a fixed compute budget. It extends the theory to Gaussian mixtures, showing TM benefits in regimes with well‑separated modes and non‑negligible variance, and validates the findings with experiments on synthetic and real image/video data.", "summary_cn": "本文研究了何时以及为何过渡匹配（TM）在生成式采样中优于流匹配（FM），并证明在单峰高斯目标下 TM 能获得更低的 KL 散度并在固定计算预算下收敛更快。进一步将分析推广至高斯混合模型，指出在模式分离充分且方差不趋于零的情形下 TM 具有优势，并通过合成高斯分布及真实图像/视频生成实验进行验证。", "keywords": "transition matching, flow matching, generative models, KL divergence, Gaussian mixture, sampling dynamics, convergence analysis, unimodal Gaussian", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jaihoon Kim", "Rajarshi Saha", "Minhyuk Sung", "Youngsuk Park"]}
]]></acme>

<pubDate>2025-10-20T18:11:29+00:00</pubDate>
</item>
<item>
<title>UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts</title>
<link>https://papers.cool/arxiv/2510.17937</link>
<guid>https://papers.cool/arxiv/2510.17937</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> UniRL-Zero introduces a reinforcement learning framework that jointly trains language model and diffusion model experts within a unified multimodal architecture, enabling improved understanding, reasoning, and multimedia generation. The paper defines six RL scenarios for such unified models and provides baseline experiments demonstrating the benefits of integrated policy learning across language and visual generation tasks.<br /><strong>Summary (CN):</strong> UniRL-Zero 提出一种强化学习框架，在统一的多模态模型中联合训练语言模型和扩散模型专家，从而提升理解、推理以及多媒体生成能力。论文定义了六种统一模型的强化学习场景，并提供基准实验，展示了跨语言与视觉生成任务的综合策略学习优势。<br /><strong>Keywords:</strong> unified model, reinforcement learning, multimodal, language model, diffusion model, joint training<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</div>
We present UniRL-Zero, a unified reinforcement learning (RL) framework that boosts, multimodal language model understanding and reasoning, diffusion model multimedia generation, and their beneficial interaction capabilities within a unified model. Our work defines six scenarios for unified model reinforcement learning, providing systematic baselines for reinforcement learning of unified understanding and generation model. Our code is available at https://github.com/G-U-N/UniRL.
<div><strong>Authors:</strong> Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "UniRL-Zero introduces a reinforcement learning framework that jointly trains language model and diffusion model experts within a unified multimodal architecture, enabling improved understanding, reasoning, and multimedia generation. The paper defines six RL scenarios for such unified models and provides baseline experiments demonstrating the benefits of integrated policy learning across language and visual generation tasks.", "summary_cn": "UniRL-Zero 提出一种强化学习框架，在统一的多模态模型中联合训练语言模型和扩散模型专家，从而提升理解、推理以及多媒体生成能力。论文定义了六种统一模型的强化学习场景，并提供基准实验，展示了跨语言与视觉生成任务的综合策略学习优势。", "keywords": "unified model, reinforcement learning, multimodal, language model, diffusion model, joint training", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"]}
]]></acme>

<pubDate>2025-10-20T16:02:16+00:00</pubDate>
</item>
<item>
<title>From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference</title>
<link>https://papers.cool/arxiv/2510.17933</link>
<guid>https://papers.cool/arxiv/2510.17933</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Parameter‑Space Changepoint Detection (Param‑CPD), a two‑stage method that first learns amortized Bayesian posteriors over governing parameters of a nonlinear system using simulation‑based inference, and then applies a conventional changepoint detection algorithm to the inferred parameter trajectory. Experiments on piecewise‑constant Lorenz‑63 dynamics show improved F1 scores, reduced localization error, and fewer false positives compared to observation‑space baselines, and the authors provide identifiability and calibration analyses to justify the benefits of operating in parameter space.<br /><strong>Summary (CN):</strong> 本文提出了参数空间变点检测（Param‑CPD）框架，先通过仿真推断（simulation‑based inference）训练神经后验估计器，实现对非线性系统控制参数的贝叶斯推断，然后在得到的参数轨迹上使用传统变点检测算法。实验证明，在具有分段常数参数的 Lorenz‑63 系统上，Param‑CPD 相比基于观测空间的方法提升了 F1 分数、降低了定位误差并减少误报，且通过可辨识性和校准分析解释了参数空间提供更清晰检测信号的原因。<br /><strong>Keywords:</strong> changepoint detection, simulation-based inference, Bayesian posterior estimation, parameter space, nonlinear dynamics, Lorenz-63, amortized inference<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Xiangbo Deng, Cheng Chen, Peng Yang</div>
Detecting regime shifts in chaotic time series is hard because observation-space signals are entangled with intrinsic variability. We propose Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework that first amortizes Bayesian inference of governing parameters with a neural posterior estimator trained by simulation-based inference, and then applies a standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63 with piecewise-constant parameters, Param--CPD improves F1, reduces localization error, and lowers false positives compared to observation--space baselines. We further verify identifiability and calibration of the inferred posteriors on stationary trajectories, explaining why parameter space offers a cleaner detection signal. Robustness analyses over tolerance, window length, and noise indicate consistent gains. Our results show that operating in a physically interpretable parameter space enables accurate and interpretable changepoint detection in nonlinear dynamical systems.
<div><strong>Authors:</strong> Xiangbo Deng, Cheng Chen, Peng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Parameter‑Space Changepoint Detection (Param‑CPD), a two‑stage method that first learns amortized Bayesian posteriors over governing parameters of a nonlinear system using simulation‑based inference, and then applies a conventional changepoint detection algorithm to the inferred parameter trajectory. Experiments on piecewise‑constant Lorenz‑63 dynamics show improved F1 scores, reduced localization error, and fewer false positives compared to observation‑space baselines, and the authors provide identifiability and calibration analyses to justify the benefits of operating in parameter space.", "summary_cn": "本文提出了参数空间变点检测（Param‑CPD）框架，先通过仿真推断（simulation‑based inference）训练神经后验估计器，实现对非线性系统控制参数的贝叶斯推断，然后在得到的参数轨迹上使用传统变点检测算法。实验证明，在具有分段常数参数的 Lorenz‑63 系统上，Param‑CPD 相比基于观测空间的方法提升了 F1 分数、降低了定位误差并减少误报，且通过可辨识性和校准分析解释了参数空间提供更清晰检测信号的原因。", "keywords": "changepoint detection, simulation-based inference, Bayesian posterior estimation, parameter space, nonlinear dynamics, Lorenz-63, amortized inference", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xiangbo Deng", "Cheng Chen", "Peng Yang"]}
]]></acme>

<pubDate>2025-10-20T15:29:31+00:00</pubDate>
</item>
<item>
<title>EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</title>
<link>https://papers.cool/arxiv/2510.17928</link>
<guid>https://papers.cool/arxiv/2510.17928</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> EvoSyn proposes an evolutionary, task-agnostic framework that jointly synthesizes problems, candidate solutions, and verification artifacts for language models, using a consistency-based evaluator to discover strategies that enforce agreement between human annotations and generated checks. By turning filtering into principled synthesis, the method produces coherent, verifiable training instances that improve performance on RL with verifiable rewards and model distillation across diverse benchmarks such as LiveCodeBench and AgentBench-OS.<br /><strong>Summary (CN):</strong> EvoSyn 提出了一个进化式、任务无关的数据合成框架，能够同时生成问题、候选解答和验证机制，并通过一致性评估器发现策略，使人工标注与生成检查保持一致，从而实现可验证的学习。该方法将过滤升级为原则化的合成，产生连贯的可验证训练实例，并在 RLVR 与模型蒸馏等场景下显著提升 LiveCodeBench 与 AgentBench-OS 等基准的表现。<br /><strong>Keywords:</strong> evolutionary data synthesis, verifiable learning, consistency evaluator, strategy-guided generation, language model distillation, RL with verifiable rewards, synthetic verification, generalization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</div>
Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.
<div><strong>Authors:</strong> He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "EvoSyn proposes an evolutionary, task-agnostic framework that jointly synthesizes problems, candidate solutions, and verification artifacts for language models, using a consistency-based evaluator to discover strategies that enforce agreement between human annotations and generated checks. By turning filtering into principled synthesis, the method produces coherent, verifiable training instances that improve performance on RL with verifiable rewards and model distillation across diverse benchmarks such as LiveCodeBench and AgentBench-OS.", "summary_cn": "EvoSyn 提出了一个进化式、任务无关的数据合成框架，能够同时生成问题、候选解答和验证机制，并通过一致性评估器发现策略，使人工标注与生成检查保持一致，从而实现可验证的学习。该方法将过滤升级为原则化的合成，产生连贯的可验证训练实例，并在 RLVR 与模型蒸馏等场景下显著提升 LiveCodeBench 与 AgentBench-OS 等基准的表现。", "keywords": "evolutionary data synthesis, verifiable learning, consistency evaluator, strategy-guided generation, language model distillation, RL with verifiable rewards, synthetic verification, generalization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["He Du", "Bowen Li", "Aijun Yang", "Siyang He", "Qipeng Guo", "Dacheng Tao"]}
]]></acme>

<pubDate>2025-10-20T11:56:35+00:00</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.17923</link>
<guid>https://papers.cool/arxiv/2510.17923</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces COMPASS, a composite self-scoring reward mechanism for test-time reinforcement learning of large language models that operates without external supervision. COMPASS combines a Dual-Calibration Answer Reward to create trustworthy pseudo‑labels and a Decisive Path Reward to directly optimize the quality of reasoning chains, leading to consistent performance gains on diverse reasoning tasks.<br /><strong>Summary (CN):</strong> 本文提出了 COMPASS，一种在无外部监督下进行测试时强化学习的复合自评分奖励机制，专为大语言模型设计。COMPASS 通过双校准答案奖励（DCAR）生成可信的伪标签，并利用决定性路径奖励（DPR）直接优化推理链质量，从而在多种推理任务和模型上实现了显著且稳定的性能提升。<br /><strong>Keywords:</strong> test-time reinforcement learning, self-scoring reward, composite reward, LLM reasoning, dual-calibration answer reward, decisive path reward, autonomous learning, RL without labels<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</div>
Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
<div><strong>Authors:</strong> Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces COMPASS, a composite self-scoring reward mechanism for test-time reinforcement learning of large language models that operates without external supervision. COMPASS combines a Dual-Calibration Answer Reward to create trustworthy pseudo‑labels and a Decisive Path Reward to directly optimize the quality of reasoning chains, leading to consistent performance gains on diverse reasoning tasks.", "summary_cn": "本文提出了 COMPASS，一种在无外部监督下进行测试时强化学习的复合自评分奖励机制，专为大语言模型设计。COMPASS 通过双校准答案奖励（DCAR）生成可信的伪标签，并利用决定性路径奖励（DPR）直接优化推理链质量，从而在多种推理任务和模型上实现了显著且稳定的性能提升。", "keywords": "test-time reinforcement learning, self-scoring reward, composite reward, LLM reasoning, dual-calibration answer reward, decisive path reward, autonomous learning, RL without labels", "scoring": {"interpretability": 5, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Chenwei Tang", "Jingyu Xing", "Xinyu Liu", "Wei Ju", "Jiancheng Lv", "Deng Xiong", "Ziyue Qiao"]}
]]></acme>

<pubDate>2025-10-20T07:53:51+00:00</pubDate>
</item>
<item>
<title>Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection</title>
<link>https://papers.cool/arxiv/2510.17917</link>
<guid>https://papers.cool/arxiv/2510.17917</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies data unlearning in diffusion models and shows that forgetting varies across diffusion timesteps and frequency bands. By selectively targeting specific time‑frequency ranges during unlearning, the authors improve generation quality while effectively removing targeted training samples. They also introduce a normalized SSCD metric to evaluate both deletion effectiveness and sample quality.<br /><strong>Summary (CN):</strong> 本文研究扩散模型中的数据删除，发现遗忘在不同扩散时间步和频率范围上表现不均匀。通过在训练时仅针对特定时间‑频率范围进行删除，提升了生成质量并有效去除目标训练样本。作者还提出了一种归一化的 SSCD 指标，用于同时评估删除效果和样本质量。<br /><strong>Keywords:</strong> data unlearning, diffusion models, time-frequency selection, privacy, model forgetting, SSCD, gradient-based unlearning, preference optimization<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Jinseong Park, Mijung Park</div>
Data unlearning aims to remove the influence of specific training samples from a trained model without requiring full retraining. Unlike concept unlearning, data unlearning in diffusion models remains underexplored and often suffers from quality degradation or incomplete forgetting. To address this, we first observe that most existing methods attempt to unlearn the samples at all diffusion time steps equally, leading to poor-quality generation. We argue that forgetting occurs disproportionately across time and frequency, depending on the model and scenarios. By selectively focusing on specific time-frequency ranges during training, we achieve samples with higher aesthetic quality and lower noise. We validate this improvement by applying our time-frequency selective approach to diverse settings, including gradient-based and preference optimization objectives, as well as both image-level and text-to-image tasks. Finally, to evaluate both deletion and quality of unlearned data samples, we propose a simple normalized version of SSCD. Together, our analysis and methods establish a clearer understanding of the unique challenges in data unlearning for diffusion models, providing practical strategies to improve both evaluation and unlearning performance.
<div><strong>Authors:</strong> Jinseong Park, Mijung Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies data unlearning in diffusion models and shows that forgetting varies across diffusion timesteps and frequency bands. By selectively targeting specific time‑frequency ranges during unlearning, the authors improve generation quality while effectively removing targeted training samples. They also introduce a normalized SSCD metric to evaluate both deletion effectiveness and sample quality.", "summary_cn": "本文研究扩散模型中的数据删除，发现遗忘在不同扩散时间步和频率范围上表现不均匀。通过在训练时仅针对特定时间‑频率范围进行删除，提升了生成质量并有效去除目标训练样本。作者还提出了一种归一化的 SSCD 指标，用于同时评估删除效果和样本质量。", "keywords": "data unlearning, diffusion models, time-frequency selection, privacy, model forgetting, SSCD, gradient-based unlearning, preference optimization", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Jinseong Park", "Mijung Park"]}
]]></acme>

<pubDate>2025-10-20T02:00:12+00:00</pubDate>
</item>
<item>
<title>Uncertainty-Aware Post-Hoc Calibration: Mitigating Confidently Incorrect Predictions Beyond Calibration Metrics</title>
<link>https://papers.cool/arxiv/2510.17915</link>
<guid>https://papers.cool/arxiv/2510.17915</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an uncertainty-aware post-hoc calibration framework that first stratifies predictions into putatively correct and incorrect groups using proximity‑based conformal prediction, then applies a dual isotonic regression strategy—standard isotonic regression for the correct group and an underconfidence‑regularized isotonic regression for the incorrect group. This instance‑level adaptivity reduces confidently incorrect predictions while maintaining competitive Expected Calibration Error, demonstrated on CIFAR‑10/100 with BiT and CoAtNet backbones. The method requires no model retraining and bridges calibration with uncertainty‑aware decision‑making.<br /><strong>Summary (CN):</strong> 本文提出一种不确定性感知的后置校准框架，利用基于相似度的共形预测将预测划分为可能正确和可能错误两类，再对这两类分别使用标准等距回归和欠置信正则化等距回归进行双重校准，从而降低高置信错误预测并保持竞争性的期望校准误差（Expected Calibration Error）。在 CIFAR‑10/100 上使用 BiT 与 CoAtNet 骨干网络的实验表明，该方法无需重新训练模型即可提升概率对齐与不确定性感知决策性能。<br /><strong>Keywords:</strong> post-hoc calibration, uncertainty quantification, isotonic regression, conformal prediction, confidently incorrect predictions, neural network reliability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Hassan Gharoun, Mohammad Sadegh Khorshidi, Kasra Ranjbarigderi, Fang Chen, Amir H. Gandomi</div>
Despite extensive research on neural network calibration, existing methods typically apply global transformations that treat all predictions uniformly, overlooking the heterogeneous reliability of individual predictions. Furthermore, the relationship between improved calibration and effective uncertainty-aware decision-making remains largely unexplored. This paper presents a post-hoc calibration framework that leverages prediction reliability assessment to jointly enhance calibration quality and uncertainty-aware decision-making. The framework employs proximity-based conformal prediction to stratify calibration samples into putatively correct and putatively incorrect groups based on semantic similarity in feature space. A dual calibration strategy is then applied: standard isotonic regression calibrated confidence in putatively correct predictions, while underconfidence-regularized isotonic regression reduces confidence toward uniform distributions for putatively incorrect predictions, facilitating their identification for further investigations. A comprehensive evaluation is conducted using calibration metrics, uncertainty-aware performance measures, and empirical conformal coverage. Experiments on CIFAR-10 and CIFAR-100 with BiT and CoAtNet backbones show that the proposed method achieves lower confidently incorrect predictions, and competitive Expected Calibration Error compared with isotonic and focal-loss baselines. This work bridges calibration and uncertainty quantification through instance-level adaptivity, offering a practical post-hoc solution that requires no model retraining while improving both probability alignment and uncertainty-aware decision-making.
<div><strong>Authors:</strong> Hassan Gharoun, Mohammad Sadegh Khorshidi, Kasra Ranjbarigderi, Fang Chen, Amir H. Gandomi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an uncertainty-aware post-hoc calibration framework that first stratifies predictions into putatively correct and incorrect groups using proximity‑based conformal prediction, then applies a dual isotonic regression strategy—standard isotonic regression for the correct group and an underconfidence‑regularized isotonic regression for the incorrect group. This instance‑level adaptivity reduces confidently incorrect predictions while maintaining competitive Expected Calibration Error, demonstrated on CIFAR‑10/100 with BiT and CoAtNet backbones. The method requires no model retraining and bridges calibration with uncertainty‑aware decision‑making.", "summary_cn": "本文提出一种不确定性感知的后置校准框架，利用基于相似度的共形预测将预测划分为可能正确和可能错误两类，再对这两类分别使用标准等距回归和欠置信正则化等距回归进行双重校准，从而降低高置信错误预测并保持竞争性的期望校准误差（Expected Calibration Error）。在 CIFAR‑10/100 上使用 BiT 与 CoAtNet 骨干网络的实验表明，该方法无需重新训练模型即可提升概率对齐与不确定性感知决策性能。", "keywords": "post-hoc calibration, uncertainty quantification, isotonic regression, conformal prediction, confidently incorrect predictions, neural network reliability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"]}
]]></acme>

<pubDate>2025-10-19T23:55:36+00:00</pubDate>
</item>
<item>
<title>NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</title>
<link>https://papers.cool/arxiv/2510.17914</link>
<guid>https://papers.cool/arxiv/2510.17914</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> NeuCo-Bench is a new benchmark framework that evaluates lossy neural compression and representation learning for Earth Observation by using fixed-size, task‑agnostic embeddings. It provides an evaluation pipeline, a hidden‑task challenge leaderboard to reduce pretraining bias, and a scoring system balancing accuracy and stability, together with the SSL4EO‑S12‑downstream dataset. Initial results from a public challenge and ablations with foundation models demonstrate its potential as a community‑driven standard for neural embeddings in EO.<br /><strong>Summary (CN):</strong> NeuCo-Bench 是一个用于评估地球观测（EO）中有损神经压缩和表征学习的基准框架，采用固定大小、任务无关的嵌入向量。它提供了评估流水线、用于降低预训练偏差的隐藏任务挑战排行榜以及兼顾准确性和稳定性的评分系统，并发布了 SSL4EO‑S12‑downstream 数据集。通过在 2025 CVPR EARTHVISION 研讨会的公开挑战以及对最先进基础模型的消融实验，展示了该框架在 EO 及其他领域推动社区标准化评估的潜力。<br /><strong>Keywords:</strong> neural embeddings, Earth observation, benchmark, representation learning, compression, downstream tasks, SSL4EO, evaluation pipeline<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy) neural compression and representation learning in the context of Earth Observation (EO). Our approach builds on fixed-size embeddings that act as compact, task-agnostic representations applicable to a broad range of downstream tasks. NeuCo-Bench comprises three core components: (i) an evaluation pipeline built around reusable embeddings, (ii) a new challenge mode with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii) a scoring system that balances accuracy and stability. To support reproducibility, we release SSL4EO-S12-downstream, a curated multispectral, multitemporal EO dataset. We present initial results from a public challenge at the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art foundation models. NeuCo-Bench provides a first step towards community-driven, standardized evaluation of neural embeddings for EO and beyond.
<div><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "NeuCo-Bench is a new benchmark framework that evaluates lossy neural compression and representation learning for Earth Observation by using fixed-size, task‑agnostic embeddings. It provides an evaluation pipeline, a hidden‑task challenge leaderboard to reduce pretraining bias, and a scoring system balancing accuracy and stability, together with the SSL4EO‑S12‑downstream dataset. Initial results from a public challenge and ablations with foundation models demonstrate its potential as a community‑driven standard for neural embeddings in EO.", "summary_cn": "NeuCo-Bench 是一个用于评估地球观测（EO）中有损神经压缩和表征学习的基准框架，采用固定大小、任务无关的嵌入向量。它提供了评估流水线、用于降低预训练偏差的隐藏任务挑战排行榜以及兼顾准确性和稳定性的评分系统，并发布了 SSL4EO‑S12‑downstream 数据集。通过在 2025 CVPR EARTHVISION 研讨会的公开挑战以及对最先进基础模型的消融实验，展示了该框架在 EO 及其他领域推动社区标准化评估的潜力。", "keywords": "neural embeddings, Earth observation, benchmark, representation learning, compression, downstream tasks, SSL4EO, evaluation pipeline", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rikard Vinge", "Isabelle Wittmann", "Jannik Schneider", "Michael Marszalek", "Luis Gilch", "Thomas Brunschwiler", "Conrad M Albrecht"]}
]]></acme>

<pubDate>2025-10-19T23:47:33+00:00</pubDate>
</item>
<item>
<title>The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</title>
<link>https://papers.cool/arxiv/2510.17901</link>
<guid>https://papers.cool/arxiv/2510.17901</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Sherpa.ai Blind Vertical Federated Learning (SBVFL) paradigm, which decouples most node updates from the central server to drastically cut communication overhead in vertical federated learning. Experiments demonstrate that SBVFL achieves about a 99% reduction in communication rounds while retaining model accuracy and robustness across various sensitive domains such as healthcare and finance.<br /><strong>Summary (CN):</strong> 本文提出了 Sherpa.ai Blind Vertical Federated Learning (SBVFL) 框架，通过将大部分节点更新与中心服务器解耦，实现垂直联邦学习中通信开销的极大削减。实验表明，SBVFL 在保持模型准确性和鲁棒性的同时，可将通信次数降低约 99%，适用于医疗、金融等敏感领域。<br /><strong>Keywords:</strong> vertical federated learning, communication reduction, privacy, distributed training, SBVFL, federated learning, security, healthcare, finance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</div>
Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications. In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.
<div><strong>Authors:</strong> Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Sherpa.ai Blind Vertical Federated Learning (SBVFL) paradigm, which decouples most node updates from the central server to drastically cut communication overhead in vertical federated learning. Experiments demonstrate that SBVFL achieves about a 99% reduction in communication rounds while retaining model accuracy and robustness across various sensitive domains such as healthcare and finance.", "summary_cn": "本文提出了 Sherpa.ai Blind Vertical Federated Learning (SBVFL) 框架，通过将大部分节点更新与中心服务器解耦，实现垂直联邦学习中通信开销的极大削减。实验表明，SBVFL 在保持模型准确性和鲁棒性的同时，可将通信次数降低约 99%，适用于医疗、金融等敏感领域。", "keywords": "vertical federated learning, communication reduction, privacy, distributed training, SBVFL, federated learning, security, healthcare, finance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Alex Acero", "Daniel M. Jimenez-Gutierrez", "Dario Pighin", "Enrique Zuazua", "Joaquin Del Rio", "Xabi Uribe-Etxebarria"]}
]]></acme>

<pubDate>2025-10-19T10:27:07+00:00</pubDate>
</item>
<item>
<title>Automated Algorithm Design for Auto-Tuning Optimizers</title>
<link>https://papers.cool/arxiv/2510.17899</link>
<guid>https://papers.cool/arxiv/2510.17899</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a framework that uses large language models (LLMs) to automatically generate optimization algorithms tailored for auto-tuning tasks, leveraging problem descriptions and search‑space characteristics as prompts. Empirical evaluation on four real‑world auto‑tuning applications across six hardware platforms shows that LLM‑generated optimizers can achieve up to 72.4% improvement over state‑of‑the‑art human‑designed methods.<br /><strong>Summary (CN):</strong> 本文提出一种利用大语言模型（LLM）自动生成针对自动调优任务的优化算法的框架，使用问题描述和搜索空间特征作为提示词。对四个真实应用和六个平台的实验表明，LLM 生成的优化器在性能上可比现有人工设计的方法提升最高达 72.4%。<br /><strong>Keywords:</strong> auto-tuning, optimizer design, large language models, meta-optimization, performance tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</div>
Automatic performance tuning (auto-tuning) is essential for optimizing high-performance applications, where vast and irregular parameter spaces make manual exploration infeasible. Traditionally, auto-tuning relies on well-established optimization algorithms such as evolutionary algorithms, annealing methods, or surrogate model-based optimizers to efficiently find near-optimal configurations. However, designing effective optimizers remains challenging, as no single method performs best across all tuning tasks. In this work, we explore a new paradigm: using large language models (LLMs) to automatically generate optimization algorithms tailored to auto-tuning problems. We introduce a framework that prompts LLMs with problem descriptions and search-space characteristics results to produce specialized optimization strategies, which are iteratively examined and improved. These generated algorithms are evaluated on four real-world auto-tuning applications across six hardware platforms and compared against the state-of-the-art in optimization algorithms of two contemporary auto-tuning frameworks. The evaluation demonstrates that providing additional application- and search space-specific information in the generation stage results in an average performance improvement of 30.7\% and 14.6\%, respectively. In addition, our results show that LLM-generated optimizers can rival, and in various cases outperform, existing human-designed algorithms, with our best-performing generated optimization algorithms achieving, on average, 72.4\% improvement over state-of-the-art optimizers for auto-tuning.
<div><strong>Authors:</strong> Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a framework that uses large language models (LLMs) to automatically generate optimization algorithms tailored for auto-tuning tasks, leveraging problem descriptions and search‑space characteristics as prompts. Empirical evaluation on four real‑world auto‑tuning applications across six hardware platforms shows that LLM‑generated optimizers can achieve up to 72.4% improvement over state‑of‑the‑art human‑designed methods.", "summary_cn": "本文提出一种利用大语言模型（LLM）自动生成针对自动调优任务的优化算法的框架，使用问题描述和搜索空间特征作为提示词。对四个真实应用和六个平台的实验表明，LLM 生成的优化器在性能上可比现有人工设计的方法提升最高达 72.4%。", "keywords": "auto-tuning, optimizer design, large language models, meta-optimization, performance tuning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Floris-Jan Willemsen", "Niki van Stein", "Ben van Werkhoven"]}
]]></acme>

<pubDate>2025-10-19T09:38:15+00:00</pubDate>
</item>
<item>
<title>L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts</title>
<link>https://papers.cool/arxiv/2510.17898</link>
<guid>https://papers.cool/arxiv/2510.17898</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> L-MoE introduces a lightweight mixture-of-experts framework where each expert is a LoRA adapter, enabling end-to-end differentiable routing and joint optimization of expert parameters and gating. The gating network computes a weighted average of adapter weights per token, allowing dynamic skill composition while keeping inference cost constant. This design yields a parameter‑efficient, modular language model that can be trained from scratch on specialized tasks.<br /><strong>Summary (CN):</strong> L-MoE 提出一种轻量化的专家混合框架，将每个专家定义为 LoRA 适配器，实现了可微分的路由并联合优化专家参数与门控网络。门控网络为每个 token 计算适配器权重的加权平均，从而实现动态技能组合且推理成本保持恒定。该设计产生参数高效、模块化的语言模型，可从头在特定任务上端到端训练。<br /><strong>Keywords:</strong> Mixture of Experts, LoRA, low-rank adaptation, parameter-efficient fine-tuning, dynamic routing, modular language models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shihao Ji, Zihui Song</div>
The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In this work, we unify these two paradigms into a novel, end-to-end trainable framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines MoE experts not as dense feed-forward networks, but as a collection of task-specialized, low-rank adapters. A lightweight gating network, trained jointly with the experts, learns to dynamically compose these LoRA adapters by computing a weighted average of their parameters for each input token. This composition is fully differentiable, allowing gradients from a standard auto-regressive language modeling objective to flow back through the entire architecture, simultaneously refining both the expert adapters and the routing strategy. This approach creates a highly parameter-efficient MoE model that is modular by design, allows for dynamic skill composition, and is trainable from end-to-end. We present the formal mathematical framework for L-MoE, detailing the differentiable routing mechanism and the joint optimization objective, thereby providing a new path toward building more efficient, scalable, and specialized language models.
<div><strong>Authors:</strong> Shihao Ji, Zihui Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "L-MoE introduces a lightweight mixture-of-experts framework where each expert is a LoRA adapter, enabling end-to-end differentiable routing and joint optimization of expert parameters and gating. The gating network computes a weighted average of adapter weights per token, allowing dynamic skill composition while keeping inference cost constant. This design yields a parameter‑efficient, modular language model that can be trained from scratch on specialized tasks.", "summary_cn": "L-MoE 提出一种轻量化的专家混合框架，将每个专家定义为 LoRA 适配器，实现了可微分的路由并联合优化专家参数与门控网络。门控网络为每个 token 计算适配器权重的加权平均，从而实现动态技能组合且推理成本保持恒定。该设计产生参数高效、模块化的语言模型，可从头在特定任务上端到端训练。", "keywords": "Mixture of Experts, LoRA, low-rank adaptation, parameter-efficient fine-tuning, dynamic routing, modular language models", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shihao Ji", "Zihui Song"]}
]]></acme>

<pubDate>2025-10-19T08:44:25+00:00</pubDate>
</item>
<item>
<title>Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism</title>
<link>https://papers.cool/arxiv/2510.17896</link>
<guid>https://papers.cool/arxiv/2510.17896</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a unified benchmark for evaluating long-context attention mechanisms, integrating both kernel-level optimizations and distributed context‑parallel strategies. It assesses methods across attention mask patterns, sequence length, and multi‑GPU scale, providing reproducible performance comparisons up to 96 GPUs and highlighting trade‑offs for LLM training with extremely long contexts.<br /><strong>Summary (CN):</strong> 本文提出一个统一的基准，用于评估长上下文注意力机制，整合了核层优化和分布式上下文并行策略。评测重点包括注意力掩码模式、序列长度以及多 GPU（最高 96 卡）规模下的性能，对极长上下文的 LLM 训练提供可复现的比较并揭示各方法的权衡。<br /><strong>Keywords:</strong> long-context attention, benchmark, kernel optimization, context parallelism, distributed training, LLM efficiency, attention mask patterns, scalability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</div>
Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose a unified benchmark that integrates representative attention kernels and context parallel mechanisms with a modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights method-specific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training.
<div><strong>Authors:</strong> Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a unified benchmark for evaluating long-context attention mechanisms, integrating both kernel-level optimizations and distributed context‑parallel strategies. It assesses methods across attention mask patterns, sequence length, and multi‑GPU scale, providing reproducible performance comparisons up to 96 GPUs and highlighting trade‑offs for LLM training with extremely long contexts.", "summary_cn": "本文提出一个统一的基准，用于评估长上下文注意力机制，整合了核层优化和分布式上下文并行策略。评测重点包括注意力掩码模式、序列长度以及多 GPU（最高 96 卡）规模下的性能，对极长上下文的 LLM 训练提供可复现的比较并揭示各方法的权衡。", "keywords": "long-context attention, benchmark, kernel optimization, context parallelism, distributed training, LLM efficiency, attention mask patterns, scalability", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tao Bu", "Qiangang Wang", "Bowen Zeng", "Hanwen Sun", "Yunpeng Huang", "Chun Cao", "Jingwei Xu"]}
]]></acme>

<pubDate>2025-10-19T07:07:37+00:00</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17895</link>
<guid>https://papers.cool/arxiv/2510.17895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a federated unlearning framework for large language models that separates unlearning from retention using task-specific adapter modules and a hierarchical merging strategy, enabling scalable and privacy-preserving removal of undesired knowledge. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate that the method handles heterogeneous unlearning requests while preserving model utility compared to baselines.<br /><strong>Summary (CN):</strong> 本文提出了一种面向大型语言模型的联邦删除框架，通过任务特定的适配器学习将删除与保留解耦，并采用层次合并策略减轻冲突目标，实现可扩展且隐私保护的去除不良知识。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够处理多样化的删除请求，同时相较于基线保持模型效用。<br /><strong>Keywords:</strong> federated unlearning, large language models, privacy, adapter learning, hierarchical merging, decentralized learning, continual unlearning, model forgetting, performance retention<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
<div><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a federated unlearning framework for large language models that separates unlearning from retention using task-specific adapter modules and a hierarchical merging strategy, enabling scalable and privacy-preserving removal of undesired knowledge. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate that the method handles heterogeneous unlearning requests while preserving model utility compared to baselines.", "summary_cn": "本文提出了一种面向大型语言模型的联邦删除框架，通过任务特定的适配器学习将删除与保留解耦，并采用层次合并策略减轻冲突目标，实现可扩展且隐私保护的去除不良知识。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够处理多样化的删除请求，同时相较于基线保持模型效用。", "keywords": "federated unlearning, large language models, privacy, adapter learning, hierarchical merging, decentralized learning, continual unlearning, model forgetting, performance retention", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Yisheng Zhong", "Zhengbang Yang", "Zhuangdi Zhu"]}
]]></acme>

<pubDate>2025-10-19T04:24:51+00:00</pubDate>
</item>
<item>
<title>MIN-Merging: Merge the Important Neurons for Model Merging</title>
<link>https://papers.cool/arxiv/2510.17890</link>
<guid>https://papers.cool/arxiv/2510.17890</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MIN-Merging, a router-based framework that selectively merges the most important neurons from pre‑trained models to mitigate parameter conflicts during model merging. Experiments on both computer‑vision and natural‑language‑processing benchmarks demonstrate consistent in‑domain performance gains while preserving out‑of‑domain generalization. The results suggest that focusing on important neurons is an effective practical solution for improving merged model performance.<br /><strong>Summary (CN):</strong> 本文提出 MIN-Merging，一种基于路由器的框架，通过选择并合并最重要的神经元来降低模型合并过程中的参数冲突。在计算机视觉和自然语言处理基准上的实验表明，该方法在保持域外泛化能力的同时，实现了域内任务性能的持续提升。结果表明，聚焦重要神经元是提升合并模型表现的有效实用方案。<br /><strong>Keywords:</strong> model merging, important neuron selection, router-based framework, parameter conflict, in-domain performance, out-of-domain generalization, computer vision, natural language processing<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Yunfei Liang</div>
Recent advances in deep learning have led to a surge of open-source models across diverse domains. While model merging offers a promising way to combine their strengths, existing approaches often suffer from parameter conflicts that degrade performance on domain-specific tasks. We propose MIN-Merging, a router-based framework that selectively merges the most important neurons to reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent gains on in-domain tasks while retaining the generalization ability of pretrained models on out-of-domain tasks. These results highlight its effectiveness as a practical solution to the parameter conflict problem in model merging.
<div><strong>Authors:</strong> Yunfei Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MIN-Merging, a router-based framework that selectively merges the most important neurons from pre‑trained models to mitigate parameter conflicts during model merging. Experiments on both computer‑vision and natural‑language‑processing benchmarks demonstrate consistent in‑domain performance gains while preserving out‑of‑domain generalization. The results suggest that focusing on important neurons is an effective practical solution for improving merged model performance.", "summary_cn": "本文提出 MIN-Merging，一种基于路由器的框架，通过选择并合并最重要的神经元来降低模型合并过程中的参数冲突。在计算机视觉和自然语言处理基准上的实验表明，该方法在保持域外泛化能力的同时，实现了域内任务性能的持续提升。结果表明，聚焦重要神经元是提升合并模型表现的有效实用方案。", "keywords": "model merging, important neuron selection, router-based framework, parameter conflict, in-domain performance, out-of-domain generalization, computer vision, natural language processing", "scoring": {"interpretability": 5, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Yunfei Liang"]}
]]></acme>

<pubDate>2025-10-18T15:23:36+00:00</pubDate>
</item>
<item>
<title>Shock-Aware Physics-Guided Fusion-DeepONet Operator for Rarefied Micro-Nozzle Flows</title>
<link>https://papers.cool/arxiv/2510.17887</link>
<guid>https://papers.cool/arxiv/2510.17887</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a physics‑aware deep learning framework that combines a Fusion DeepONet operator architecture, a shock‑aligned coordinate system, and a two‑phase curriculum to build fast and accurate surrogate models for rarefied micro‑nozzle flows containing shocks. By first validating the method on the viscous Burgers equation, the authors demonstrate its ability to capture steep gradient dynamics and parameter dependencies efficiently.<br /><strong>Summary (CN):</strong> 本文提出了一种物理感知的深度学习框架，融合了 Fusion DeepONet 运算子结构、冲击对齐坐标系以及两阶段课程学习，以快速、精确地构建稀薄微喷嘴流动（含冲击） 的代理模型。作者首先在粘性 Burgers 方程上进行验证，展示了该方法在捕获陡峭梯度和参数依赖性方面的有效性。<br /><strong>Keywords:</strong> physics-informed neural networks, DeepONet, operator learning, shock-aware modeling, rarefied flow, micro-nozzle, surrogate modeling, curriculum learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ehsan Roohi, Amirmehran Mahdavi</div>
We present a comprehensive, physics aware deep learning framework for constructing fast and accurate surrogate models of rarefied, shock containing micro nozzle flows. The framework integrates three key components, a Fusion DeepONet operator learning architecture for capturing parameter dependencies, a physics-guided feature space that embeds a shock-aligned coordinate system, and a two-phase curriculum strategy emphasizing high-gradient regions. To demonstrate the generality and inductive bias of the proposed framework, we first validate it on the canonical viscous Burgers equation, which exhibits advective steepening and shock like gradients.
<div><strong>Authors:</strong> Ehsan Roohi, Amirmehran Mahdavi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a physics‑aware deep learning framework that combines a Fusion DeepONet operator architecture, a shock‑aligned coordinate system, and a two‑phase curriculum to build fast and accurate surrogate models for rarefied micro‑nozzle flows containing shocks. By first validating the method on the viscous Burgers equation, the authors demonstrate its ability to capture steep gradient dynamics and parameter dependencies efficiently.", "summary_cn": "本文提出了一种物理感知的深度学习框架，融合了 Fusion DeepONet 运算子结构、冲击对齐坐标系以及两阶段课程学习，以快速、精确地构建稀薄微喷嘴流动（含冲击） 的代理模型。作者首先在粘性 Burgers 方程上进行验证，展示了该方法在捕获陡峭梯度和参数依赖性方面的有效性。", "keywords": "physics-informed neural networks, DeepONet, operator learning, shock-aware modeling, rarefied flow, micro-nozzle, surrogate modeling, curriculum learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ehsan Roohi", "Amirmehran Mahdavi"]}
]]></acme>

<pubDate>2025-10-18T10:18:27+00:00</pubDate>
</item>
<item>
<title>CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings</title>
<link>https://papers.cool/arxiv/2510.17846</link>
<guid>https://papers.cool/arxiv/2510.17846</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes CARLE, a hybrid deep‑shallow framework that combines Res‑CNN, Res‑LSTM with multi‑head attention and a Random Forest Regressor to predict the Remaining Useful Life of rolling element bearings. It includes a preprocessing stage with Gaussian filtering and Continuous Wavelet Transform, and demonstrates robustness and generalization across bearing datasets through extensive ablation, noise, and cross‑domain experiments. Model transparency is examined using LIME and SHAP to provide interpretability insights.<br /><strong>Summary (CN):</strong> 本文提出了 CARLE，一种融合 Res‑CNN、Res‑LSTM 多头注意力和随机森林回归器的深浅结合框架，用于预测滚动轴承的剩余使用寿命。系统采用高斯滤波和连续小波变换进行预处理，并通过消融、噪声和跨域实验在 XJTU‑SY 与 PRONOSTIA 数据集上展示了鲁棒性和泛化能力。作者使用 LIME 和 SHAP 分析模型透明性，以提升解释性。<br /><strong>Keywords:</strong> remaining useful life, rolling element bearings, hybrid deep-shallow, Res-CNN, Res-LSTM, multi-head attention, random forest regressor, continuous wavelet transform, LIME, SHAP<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Waleed Razzaq, Yun-Bo Zhao</div>
Prognostic Health Management (PHM) systems monitor and predict equipment health. A key task is Remaining Useful Life (RUL) estimation, which predicts how long a component, such as a rolling element bearing, will operate before failure. Many RUL methods exist but often lack generalizability and robustness under changing operating conditions. This paper introduces CARLE, a hybrid AI framework that combines deep and shallow learning to address these challenges. CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual connections to capture spatial and temporal degradation patterns, and a Random Forest Regressor (RFR) for stable, accurate RUL prediction. A compact preprocessing pipeline applies Gaussian filtering for noise reduction and Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies measure each component's contribution, while noise and cross-domain experiments test robustness and generalization. Comparative results show CARLE outperforms several state-of-the-art methods, especially under dynamic conditions. Finally, we analyze model interpretability with LIME and SHAP to assess transparency and trustworthiness.
<div><strong>Authors:</strong> Waleed Razzaq, Yun-Bo Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes CARLE, a hybrid deep‑shallow framework that combines Res‑CNN, Res‑LSTM with multi‑head attention and a Random Forest Regressor to predict the Remaining Useful Life of rolling element bearings. It includes a preprocessing stage with Gaussian filtering and Continuous Wavelet Transform, and demonstrates robustness and generalization across bearing datasets through extensive ablation, noise, and cross‑domain experiments. Model transparency is examined using LIME and SHAP to provide interpretability insights.", "summary_cn": "本文提出了 CARLE，一种融合 Res‑CNN、Res‑LSTM 多头注意力和随机森林回归器的深浅结合框架，用于预测滚动轴承的剩余使用寿命。系统采用高斯滤波和连续小波变换进行预处理，并通过消融、噪声和跨域实验在 XJTU‑SY 与 PRONOSTIA 数据集上展示了鲁棒性和泛化能力。作者使用 LIME 和 SHAP 分析模型透明性，以提升解释性。", "keywords": "remaining useful life, rolling element bearings, hybrid deep-shallow, Res-CNN, Res-LSTM, multi-head attention, random forest regressor, continuous wavelet transform, LIME, SHAP", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Waleed Razzaq", "Yun-Bo Zhao"]}
]]></acme>

<pubDate>2025-10-10T21:43:26+00:00</pubDate>
</item>
<item>
<title>GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing</title>
<link>https://papers.cool/arxiv/2510.17843</link>
<guid>https://papers.cool/arxiv/2510.17843</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GRETEL, a goal-driven retrieval and execution-based trial framework that augments LLM tool selection by validating semantically retrieved candidates through sandboxed plan‑execute‑evaluate cycles, thereby bridging the semantic‑functional gap. Empirical results on the ToolBench benchmark show large gains in pass rate, recall, and NDCG compared to purely semantic retrieval methods. This demonstrates that execution‑grounded evidence yields more reliable tool choices for agentic systems.<br /><strong>Summary (CN):</strong> 本文提出 GRETEL——一种目标驱动的检索与执行式试验框架，通过在沙箱中对语义检索得到的候选工具进行计划‑执行‑评估循环，提供基于执行的证据，以弥合语义‑功能差距。 在 ToolBench 基准上的实验显示，与仅依赖语义相似度的检索相比，GRETEL 在通过率、召回率和 NDCG 等指标上均有显著提升。 该方法表明执行验证能够显著提升 LLM 代理系统的工具选择可靠性。<br /><strong>Keywords:</strong> tool selection, LLM agents, execution validation, semantic-functional gap, sandbox evaluation, ToolBench, retrieval, functional tools, agentic workflow, reliability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Zongze Wu, Yani Guo, Churong Liang, Runnan Li</div>
Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.
<div><strong>Authors:</strong> Zongze Wu, Yani Guo, Churong Liang, Runnan Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GRETEL, a goal-driven retrieval and execution-based trial framework that augments LLM tool selection by validating semantically retrieved candidates through sandboxed plan‑execute‑evaluate cycles, thereby bridging the semantic‑functional gap. Empirical results on the ToolBench benchmark show large gains in pass rate, recall, and NDCG compared to purely semantic retrieval methods. This demonstrates that execution‑grounded evidence yields more reliable tool choices for agentic systems.", "summary_cn": "本文提出 GRETEL——一种目标驱动的检索与执行式试验框架，通过在沙箱中对语义检索得到的候选工具进行计划‑执行‑评估循环，提供基于执行的证据，以弥合语义‑功能差距。 在 ToolBench 基准上的实验显示，与仅依赖语义相似度的检索相比，GRETEL 在通过率、召回率和 NDCG 等指标上均有显著提升。 该方法表明执行验证能够显著提升 LLM 代理系统的工具选择可靠性。", "keywords": "tool selection, LLM agents, execution validation, semantic-functional gap, sandbox evaluation, ToolBench, retrieval, functional tools, agentic workflow, reliability", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Zongze Wu", "Yani Guo", "Churong Liang", "Runnan Li"]}
]]></acme>

<pubDate>2025-10-10T00:12:51+00:00</pubDate>
</item>
<item>
<title>From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs</title>
<link>https://papers.cool/arxiv/2510.17817</link>
<guid>https://papers.cool/arxiv/2510.17817</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PRISM, a forecasting framework that combines a score-based diffusion preconditioner with a dynamic, correlation‑thresholded graph encoder and a physics‑regularized forecast head, and provides theoretical contraction and Lipschitz guarantees for long‑horizon stability. Empirical results on six benchmarks show state‑of‑the‑art performance with notable improvements in MSE and MAE.<br /><strong>Summary (CN):</strong> 本文提出 PRISM 框架，将基于分数的扩散预处理器 (score‑based diffusion preconditioner) 与动态相关阈值图编码器 (dynamic, correlation‑thresholded graph encoder) 以及带有通用物理惩罚的预测头相结合，并证明了在温和条件下的收敛性以及图块的 Lipschitz 上界，解释了模型的鲁棒性。实验在六个标准基准上实现了 SOTA，显著提升了 MSE 和 MAE。<br /><strong>Keywords:</strong> time-series forecasting, diffusion preconditioner, dynamic graph encoder, physics-informed regularization, Lipschitz bounds, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Hongwei Ma, Junbin Gao, Minh-ngoc Tran</div>
Long-horizon multivariate time-series forecasting is challenging because realistic predictions must (i) denoise heterogeneous signals, (ii) track time-varying cross-series dependencies, and (iii) remain stable and physically plausible over long rollout horizons. We present PRISM, which couples a score-based diffusion preconditioner with a dynamic, correlation-thresholded graph encoder and a forecast head regularized by generic physics penalties. We prove contraction of the induced horizon dynamics under mild conditions and derive Lipschitz bounds for graph blocks, explaining the model's robustness. On six standard benchmarks , PRISM achieves consistent SOTA with strong MSE and MAE gains.
<div><strong>Authors:</strong> Hongwei Ma, Junbin Gao, Minh-ngoc Tran</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PRISM, a forecasting framework that combines a score-based diffusion preconditioner with a dynamic, correlation‑thresholded graph encoder and a physics‑regularized forecast head, and provides theoretical contraction and Lipschitz guarantees for long‑horizon stability. Empirical results on six benchmarks show state‑of‑the‑art performance with notable improvements in MSE and MAE.", "summary_cn": "本文提出 PRISM 框架，将基于分数的扩散预处理器 (score‑based diffusion preconditioner) 与动态相关阈值图编码器 (dynamic, correlation‑thresholded graph encoder) 以及带有通用物理惩罚的预测头相结合，并证明了在温和条件下的收敛性以及图块的 Lipschitz 上界，解释了模型的鲁棒性。实验在六个标准基准上实现了 SOTA，显著提升了 MSE 和 MAE。", "keywords": "time-series forecasting, diffusion preconditioner, dynamic graph encoder, physics-informed regularization, Lipschitz bounds, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Hongwei Ma", "Junbin Gao", "Minh-ngoc Tran"]}
]]></acme>

<pubDate>2025-09-27T08:35:23+00:00</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18866</link>
<guid>https://papers.cool/arxiv/2510.18866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LightMem introduces a three-stage memory system for large language models, inspired by the Atkinson-Shiffrin human memory model, to efficiently store, retrieve, and consolidate historical interaction information. By using a sensory memory for rapid filtering, a topic-aware short-term memory for structured consolidation, and an offline-updated long-term memory, LightMem achieves up to 10.9% accuracy improvements while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate its superior performance‑efficiency trade‑off.<br /><strong>Summary (CN):</strong> LightMem 基于阿特金森‑舒弗林（Atkinson‑Shiffrin）的人类记忆模型，提出了一个由感官记忆、主题感知短期记忆和离线更新的长期记忆组成的三阶段记忆系统，用于大语言模型的历史交互信息的高效存储与检索。该系统通过快速过滤无关信息、主题化组织与摘要以及脱钩的离线合并，实现了准确率最高提升 10.9% 并显著降低 token 使用、API 调用次数和运行时间。 在 GPT 与 Qwen 骨干模型上的 LongMemEval 实验验证了其优越的性能‑效率平衡。<br /><strong>Keywords:</strong> memory-augmented generation, lightweight memory, LLM, efficient retrieval, long-term memory, Atkinson-Shiffrin model<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
<div><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LightMem introduces a three-stage memory system for large language models, inspired by the Atkinson-Shiffrin human memory model, to efficiently store, retrieve, and consolidate historical interaction information. By using a sensory memory for rapid filtering, a topic-aware short-term memory for structured consolidation, and an offline-updated long-term memory, LightMem achieves up to 10.9% accuracy improvements while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate its superior performance‑efficiency trade‑off.", "summary_cn": "LightMem 基于阿特金森‑舒弗林（Atkinson‑Shiffrin）的人类记忆模型，提出了一个由感官记忆、主题感知短期记忆和离线更新的长期记忆组成的三阶段记忆系统，用于大语言模型的历史交互信息的高效存储与检索。该系统通过快速过滤无关信息、主题化组织与摘要以及脱钩的离线合并，实现了准确率最高提升 10.9% 并显著降低 token 使用、API 调用次数和运行时间。 在 GPT 与 Qwen 骨干模型上的 LongMemEval 实验验证了其优越的性能‑效率平衡。", "keywords": "memory-augmented generation, lightweight memory, LLM, efficient retrieval, long-term memory, Atkinson-Shiffrin model", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:58:17+00:00</pubDate>
</item>
<item>
<title>Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study</title>
<link>https://papers.cool/arxiv/2510.18852</link>
<guid>https://papers.cool/arxiv/2510.18852</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Lyapunov-Aware Quantum-Inspired Reinforcement Learning (LQRL) framework that couples variational quantum circuits with Lyapunov stability constraints for continuous-time vehicle longitudinal control. In an adaptive cruise control simulation, the stability‑aware policy gradient ensures asymptotic convergence and bounded state evolution, demonstrating safety‑guaranteed quantum policy learning despite occasional transient overshoot. The results show the feasibility of embedding formal safety verification into quantum reinforcement learning for autonomous control.<br /><strong>Summary (CN):</strong> 本文提出了一种基于李雅普诺夫的量子强化学习（LQRL）框架，将变分量子电路与李雅普诺夫稳定性分析相结合，用于连续时间车辆纵向控制。通过在自适应巡航控制仿真中嵌入稳定性约束的策略梯度，展示了在保证有界状态演化的前提下实现安全且可解释的控制。实验表明该方法能够在激进加速下出现瞬时超调，但整体保持系统稳定，验证了在量子强化学习中融合安全保证的可行性。<br /><strong>Keywords:</strong> quantum reinforcement learning, Lyapunov stability, continuous-time control, autonomous vehicle, variational quantum circuits, safety-aware RL, policy gradient, quantum-inspired control, stability-constrained learning, adaptive cruise control<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Nutkritta Kraipatthanapong, Natthaphat Thathong, Pannita Suksawas, Thanunnut Klunklin, Kritin Vongthonglua, Krit Attahakul, Aueaphum Aueawatthanaphisut</div>
This paper presents a novel Lyapunov-Based Quantum Reinforcement Learning (LQRL) framework that integrates quantum policy optimization with Lyapunov stability analysis for continuous-time vehicle control. The proposed approach combines the representational power of variational quantum circuits (VQCs) with a stability-aware policy gradient mechanism to ensure asymptotic convergence and safe decision-making under dynamic environments. The vehicle longitudinal control problem was formulated as a continuous-state reinforcement learning task, where the quantum policy network generates control actions subject to Lyapunov stability constraints. Simulation experiments were conducted in a closed-loop adaptive cruise control scenario using a quantum-inspired policy trained under stability feedback. The results demonstrate that the LQRL framework successfully embeds Lyapunov stability verification into quantum policy learning, enabling interpretable and stability-aware control performance. Although transient overshoot and Lyapunov divergence were observed under aggressive acceleration, the system maintained bounded state evolution, validating the feasibility of integrating safety guarantees within quantum reinforcement learning architectures. The proposed framework provides a foundational step toward provably safe quantum control in autonomous systems and hybrid quantum-classical optimization domains.
<div><strong>Authors:</strong> Nutkritta Kraipatthanapong, Natthaphat Thathong, Pannita Suksawas, Thanunnut Klunklin, Kritin Vongthonglua, Krit Attahakul, Aueaphum Aueawatthanaphisut</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Lyapunov-Aware Quantum-Inspired Reinforcement Learning (LQRL) framework that couples variational quantum circuits with Lyapunov stability constraints for continuous-time vehicle longitudinal control. In an adaptive cruise control simulation, the stability‑aware policy gradient ensures asymptotic convergence and bounded state evolution, demonstrating safety‑guaranteed quantum policy learning despite occasional transient overshoot. The results show the feasibility of embedding formal safety verification into quantum reinforcement learning for autonomous control.", "summary_cn": "本文提出了一种基于李雅普诺夫的量子强化学习（LQRL）框架，将变分量子电路与李雅普诺夫稳定性分析相结合，用于连续时间车辆纵向控制。通过在自适应巡航控制仿真中嵌入稳定性约束的策略梯度，展示了在保证有界状态演化的前提下实现安全且可解释的控制。实验表明该方法能够在激进加速下出现瞬时超调，但整体保持系统稳定，验证了在量子强化学习中融合安全保证的可行性。", "keywords": "quantum reinforcement learning, Lyapunov stability, continuous-time control, autonomous vehicle, variational quantum circuits, safety-aware RL, policy gradient, quantum-inspired control, stability-constrained learning, adaptive cruise control", "scoring": {"interpretability": 5, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Nutkritta Kraipatthanapong", "Natthaphat Thathong", "Pannita Suksawas", "Thanunnut Klunklin", "Kritin Vongthonglua", "Krit Attahakul", "Aueaphum Aueawatthanaphisut"]}
]]></acme>

<pubDate>2025-10-21T17:44:45+00:00</pubDate>
</item>
<item>
<title>MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training</title>
<link>https://papers.cool/arxiv/2510.18830</link>
<guid>https://papers.cool/arxiv/2510.18830</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long context windows. Using MTraining, the authors expand Qwen2.5‑3B’s context from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining accuracy across tasks such as RULER, PG‑19, InfiniteBench, and Needle‑in‑a‑Haystack.<br /><strong>Summary (CN):</strong> 本文提出 MTraining，一种分布式训练框架，结合动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，实现对超长上下文的大型语言模型的高效训练。使用 MTraining，作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文长度从 32K 扩展至 512K token，训练吞吐量提升最多 6 倍，并在 RULER、PG‑19、InfiniteBench、Needle‑in‑a‑Haystack 等任务上保持模型准确性。<br /><strong>Keywords:</strong> dynamic sparse attention, distributed training, ultra-long context, sparse ring attention, hierarchical sparse ring, LLM efficiency, Qwen2.5, large-scale training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.
<div><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long context windows. Using MTraining, the authors expand Qwen2.5‑3B’s context from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining accuracy across tasks such as RULER, PG‑19, InfiniteBench, and Needle‑in‑a‑Haystack.", "summary_cn": "本文提出 MTraining，一种分布式训练框架，结合动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，实现对超长上下文的大型语言模型的高效训练。使用 MTraining，作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文长度从 32K 扩展至 512K token，训练吞吐量提升最多 6 倍，并在 RULER、PG‑19、InfiniteBench、Needle‑in‑a‑Haystack 等任务上保持模型准确性。", "keywords": "dynamic sparse attention, distributed training, ultra-long context, sparse ring attention, hierarchical sparse ring, LLM efficiency, Qwen2.5, large-scale training", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Li", "Chengruidong Zhang", "Huiqiang Jiang", "Yucheng Li", "Yuqing Yang", "Lili Qiu"]}
]]></acme>

<pubDate>2025-10-21T17:25:32+00:00</pubDate>
</item>
<item>
<title>SO(3)-invariant PCA with application to molecular data</title>
<link>https://papers.cool/arxiv/2510.18827</link>
<guid>https://papers.cool/arxiv/2510.18827</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a principled framework for SO(3)-invariant principal component analysis that handles 3D volumetric datasets with unknown orientations without explicit data augmentation, reducing computational complexity to the square root of the number of covariance entries. By exploiting algebraic structure, the method enables efficient dimensionality reduction and denoising for molecular data, and experimental validation on real molecular datasets demonstrates its effectiveness.<br /><strong>Summary (CN):</strong> 本文提出了一种 SO(3) 不变的主成分分析框架，可在不进行显式数据增强的情况下处理未知方向的三维体数据，将计算复杂度降低至协方差条目数量的平方根。通过利用代数结构，该方法实现了对分子数据的高效降维和去噪，并在真实分子数据集上验证了其有效性。<br /><strong>Keywords:</strong> SO(3)-invariant PCA, rotational invariance, volumetric data, molecular structures, dimensionality reduction, covariance, algebraic methods, 3D data analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Michael Fraiman, Paulina Hoyos, Tamir Bendory, Joe Kileel, Oscar Mickelin, Nir Sharon, Amit Singer</div>
Principal component analysis (PCA) is a fundamental technique for dimensionality reduction and denoising; however, its application to three-dimensional data with arbitrary orientations -- common in structural biology -- presents significant challenges. A naive approach requires augmenting the dataset with many rotated copies of each sample, incurring prohibitive computational costs. In this paper, we extend PCA to 3D volumetric datasets with unknown orientations by developing an efficient and principled framework for SO(3)-invariant PCA that implicitly accounts for all rotations without explicit data augmentation. By exploiting underlying algebraic structure, we demonstrate that the computation involves only the square root of the total number of covariance entries, resulting in a substantial reduction in complexity. We validate the method on real-world molecular datasets, demonstrating its effectiveness and opening up new possibilities for large-scale, high-dimensional reconstruction problems.
<div><strong>Authors:</strong> Michael Fraiman, Paulina Hoyos, Tamir Bendory, Joe Kileel, Oscar Mickelin, Nir Sharon, Amit Singer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a principled framework for SO(3)-invariant principal component analysis that handles 3D volumetric datasets with unknown orientations without explicit data augmentation, reducing computational complexity to the square root of the number of covariance entries. By exploiting algebraic structure, the method enables efficient dimensionality reduction and denoising for molecular data, and experimental validation on real molecular datasets demonstrates its effectiveness.", "summary_cn": "本文提出了一种 SO(3) 不变的主成分分析框架，可在不进行显式数据增强的情况下处理未知方向的三维体数据，将计算复杂度降低至协方差条目数量的平方根。通过利用代数结构，该方法实现了对分子数据的高效降维和去噪，并在真实分子数据集上验证了其有效性。", "keywords": "SO(3)-invariant PCA, rotational invariance, volumetric data, molecular structures, dimensionality reduction, covariance, algebraic methods, 3D data analysis", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Michael Fraiman", "Paulina Hoyos", "Tamir Bendory", "Joe Kileel", "Oscar Mickelin", "Nir Sharon", "Amit Singer"]}
]]></acme>

<pubDate>2025-10-21T17:23:17+00:00</pubDate>
</item>
<item>
<title>Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location</title>
<link>https://papers.cool/arxiv/2510.18803</link>
<guid>https://papers.cool/arxiv/2510.18803</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This study analyzes 18 years (2005-2022) of NSERC-funded research proposals using three topic‑modeling approaches—LDA, Structural Topic Modeling, and BERTopic—and introduces COFFEE, a novel algorithm that enables covariate effect estimation for BERTopic. BERTopic consistently yields more granular, coherent, and emergent topics, such as the rapid growth of artificial‑intelligence research, while the covariate analysis uncovers provincial specializations and stable gender‑based thematic patterns across disciplines. The results provide an empirical basis for designing more equitable and effective funding strategies.<br /><strong>Summary (CN):</strong> 本研究对 2005–2022 年 18 年间 NSERC 资助的研究提案进行分析，比较了 LDA、结构主题模型 (STM) 和 BERTopic 三种主题建模方法，并提出了 COFFEE——一种用于 BERTopic 的协变量效应估计新算法。结果显示，BERTopic 能持续发现更细粒度、连贯且新兴的主题（如人工智能的快速扩张），而协变量分析则揭示了各省研究专长以及跨学科的一致性别主题模式。该工作为制定更公平、有效的科研资助策略提供了实证依据。<br /><strong>Keywords:</strong> topic modeling, LDA, STM, BERTopic, COFFEE algorithm, gender analysis, geographic analysis, research funding, AI trends, NSERC<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shirin Tavakoli Kafiabad, Andrea Schiffauerova, Ashkan Ebadi</div>
Optimizing national scientific investment requires a clear understanding of evolving research trends and the demographic and geographical forces shaping them, particularly in light of commitments to equity, diversity, and inclusion. This study addresses this need by analyzing 18 years (2005-2022) of research proposals funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). We conducted a comprehensive comparative evaluation of three topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic Modelling (STM), and BERTopic. We also introduced a novel algorithm, named COFFEE, designed to enable robust covariate effect estimation for BERTopic. This advancement addresses a significant gap, as BERTopic lacks a native function for covariate analysis, unlike the probabilistic STM. Our findings highlight that while all models effectively delineate core scientific domains, BERTopic outperformed by consistently identifying more granular, coherent, and emergent themes, such as the rapid expansion of artificial intelligence. Additionally, the covariate analysis, powered by COFFEE, confirmed distinct provincial research specializations and revealed consistent gender-based thematic patterns across various scientific disciplines. These insights offer a robust empirical foundation for funding organizations to formulate more equitable and impactful funding strategies, thereby enhancing the effectiveness of the scientific ecosystem.
<div><strong>Authors:</strong> Shirin Tavakoli Kafiabad, Andrea Schiffauerova, Ashkan Ebadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This study analyzes 18 years (2005-2022) of NSERC-funded research proposals using three topic‑modeling approaches—LDA, Structural Topic Modeling, and BERTopic—and introduces COFFEE, a novel algorithm that enables covariate effect estimation for BERTopic. BERTopic consistently yields more granular, coherent, and emergent topics, such as the rapid growth of artificial‑intelligence research, while the covariate analysis uncovers provincial specializations and stable gender‑based thematic patterns across disciplines. The results provide an empirical basis for designing more equitable and effective funding strategies.", "summary_cn": "本研究对 2005–2022 年 18 年间 NSERC 资助的研究提案进行分析，比较了 LDA、结构主题模型 (STM) 和 BERTopic 三种主题建模方法，并提出了 COFFEE——一种用于 BERTopic 的协变量效应估计新算法。结果显示，BERTopic 能持续发现更细粒度、连贯且新兴的主题（如人工智能的快速扩张），而协变量分析则揭示了各省研究专长以及跨学科的一致性别主题模式。该工作为制定更公平、有效的科研资助策略提供了实证依据。", "keywords": "topic modeling, LDA, STM, BERTopic, COFFEE algorithm, gender analysis, geographic analysis, research funding, AI trends, NSERC", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shirin Tavakoli Kafiabad", "Andrea Schiffauerova", "Ashkan Ebadi"]}
]]></acme>

<pubDate>2025-10-21T16:58:00+00:00</pubDate>
</item>
<item>
<title>A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.18777</link>
<guid>https://papers.cool/arxiv/2510.18777</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a purely Frequentist treatment of Variational Inference (VI), showing how it emerges from the Expectation‑Maximization algorithm and how Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs) can be viewed as deep‑learning extensions of this framework. By framing VI as a scalable approximation to intractable E‑steps for maximum‑likelihood estimation, the authors bridge the conceptual gap between classical statistical inference and modern generative AI.<br /><strong>Summary (CN):</strong> 本文从纯频率主义视角阐述变分推断（VI），展示其如何源自期望最大化（EM）算法，并将变分自编码器（VAE）和去噪扩散模型（DDM）视为该框架的深度学习扩展。通过将 VI 视为对不可解 E 步的可扩展近似用于最大似然估计，作者搭建了经典统计推断与现代生成式 AI 之间的概念桥梁。<br /><strong>Keywords:</strong> variational inference, frequentist perspective, expectation-maximization, variational autoencoder, diffusion model, generative modeling, statistical learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yen-Chi Chen</div>
While Variational Inference (VI) is central to modern generative models like Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its pedagogical treatment is split across disciplines. In statistics, VI is typically framed as a Bayesian method for posterior approximation. In machine learning, however, VAEs and DDMs are developed from a Frequentist viewpoint, where VI is used to approximate a maximum likelihood estimator. This creates a barrier for statisticians, as the principles behind VAEs and DDMs are hard to contextualize without a corresponding Frequentist introduction to VI. This paper provides that introduction: we explain the theory for VI, VAEs, and DDMs from a purely Frequentist perspective, starting with the classical Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable solution for intractable E-steps and how VAEs and DDMs are natural, deep-learning-based extensions of this framework, thereby bridging the gap between classical statistical inference and modern generative AI.
<div><strong>Authors:</strong> Yen-Chi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a purely Frequentist treatment of Variational Inference (VI), showing how it emerges from the Expectation‑Maximization algorithm and how Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs) can be viewed as deep‑learning extensions of this framework. By framing VI as a scalable approximation to intractable E‑steps for maximum‑likelihood estimation, the authors bridge the conceptual gap between classical statistical inference and modern generative AI.", "summary_cn": "本文从纯频率主义视角阐述变分推断（VI），展示其如何源自期望最大化（EM）算法，并将变分自编码器（VAE）和去噪扩散模型（DDM）视为该框架的深度学习扩展。通过将 VI 视为对不可解 E 步的可扩展近似用于最大似然估计，作者搭建了经典统计推断与现代生成式 AI 之间的概念桥梁。", "keywords": "variational inference, frequentist perspective, expectation-maximization, variational autoencoder, diffusion model, generative modeling, statistical learning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yen-Chi Chen"]}
]]></acme>

<pubDate>2025-10-21T16:25:19+00:00</pubDate>
</item>
<item>
<title>Analyse comparative d'algorithmes de restauration en architecture dépliée pour des signaux chromatographiques parcimonieux</title>
<link>https://papers.cool/arxiv/2510.18760</link>
<guid>https://papers.cool/arxiv/2510.18760</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a comparative evaluation of three unfolded neural network architectures for restoring sparse chromatographic signals from degraded observations. Using parameterized chromatographic signal databases, the study assesses performance with metrics tailored to physico‑chemical peak characterization, highlighting the strengths of each approach.<br /><strong>Summary (CN):</strong> 本文比较了三种展开（unfolded）神经网络结构在稀疏色谱信号恢复中的表现。通过基于参数化色谱信号数据库的实验，使用针对物理化学峰特征的指标评估各方法的性能，揭示了它们的优势。<br /><strong>Keywords:</strong> unfolded neural networks, signal restoration, sparse chromatographic signals, physico-chemical peak metrics, comparative study<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mouna Gharbi, Silvia Villa, Emilie Chouzenoux, Jean-Christophe Pesquet, Laurent Duval</div>
Data restoration from degraded observations, of sparsity hypotheses, is an active field of study. Traditional iterative optimization methods are now complemented by deep learning techniques. The development of unfolded methods benefits from both families. We carry out a comparative study of three architectures on parameterized chromatographic signal databases, highlighting the performance of these approaches, especially when employing metrics adapted to physico-chemical peak signal characterization.
<div><strong>Authors:</strong> Mouna Gharbi, Silvia Villa, Emilie Chouzenoux, Jean-Christophe Pesquet, Laurent Duval</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a comparative evaluation of three unfolded neural network architectures for restoring sparse chromatographic signals from degraded observations. Using parameterized chromatographic signal databases, the study assesses performance with metrics tailored to physico‑chemical peak characterization, highlighting the strengths of each approach.", "summary_cn": "本文比较了三种展开（unfolded）神经网络结构在稀疏色谱信号恢复中的表现。通过基于参数化色谱信号数据库的实验，使用针对物理化学峰特征的指标评估各方法的性能，揭示了它们的优势。", "keywords": "unfolded neural networks, signal restoration, sparse chromatographic signals, physico-chemical peak metrics, comparative study", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mouna Gharbi", "Silvia Villa", "Emilie Chouzenoux", "Jean-Christophe Pesquet", "Laurent Duval"]}
]]></acme>

<pubDate>2025-10-01T20:27:01+00:00</pubDate>
</item>
<item>
<title>Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses Without Sacrificing Precision</title>
<link>https://papers.cool/arxiv/2510.18749</link>
<guid>https://papers.cool/arxiv/2510.18749</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces symbolic emulators that approximate hypergeometric functions for ΛCDM comoving distance and linear growth factor, achieving sub‑0.001% and 0.05% errors across relevant redshifts and Ωm values. Integrated into a Dark Energy Survey‑like 3×2pt analysis, these emulators yield cosmological constraints matching those from standard numerical methods while dramatically reducing computation time and memory usage. The work demonstrates that symbolic approximations can scale likelihood‑based inference without sacrificing precision.<br /><strong>Summary (CN):</strong> 本文提出了用于 ΛCDM 宇宙学模型的符号化仿真器，通过近似超几何函数实现对共动距离和线性增长因子的高精度（误差分别低于 0.001% 和 0.05%）并覆盖 Ωₘ∈[0.1,0.5] 的红移范围。将这些仿真器整合到类似暗能量巡天的 3×2pt 分析中，得到的宇宙学约束与传统数值方法一致，同时显著提升了计算速度和内存效率。该工作展示了符号近似在保持精度的前提下，可实现可扩展的似然推断。<br /><strong>Keywords:</strong> symbolic emulator, cosmology, ΛCDM, hypergeometric approximation, dark energy survey, 3x2pt analysis, fast inference<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Deaglan J. Bartlett, Shivam Pandey</div>
In cosmology, emulators play a crucial role by providing fast and accurate predictions of complex physical models, enabling efficient exploration of high-dimensional parameter spaces that would be computationally prohibitive with direct numerical simulations. Symbolic emulators have emerged as promising alternatives to numerical approaches, delivering comparable accuracy with significantly faster evaluation times. While previous symbolic emulators were limited to relatively narrow prior ranges, we expand these to cover the parameter space relevant for current cosmological analyses. We introduce approximations to hypergeometric functions used for the $\Lambda$CDM comoving distance and linear growth factor which are accurate to better than 0.001% and 0.05%, respectively, for all redshifts and for $\Omega_{\rm m} \in [0.1, 0.5]$. We show that integrating symbolic emulators into a Dark Energy Survey-like $3\times2$pt analysis produces cosmological constraints consistent with those obtained using standard numerical methods. Our symbolic emulators offer substantial improvements in speed and memory usage, demonstrating their practical potential for scalable, likelihood-based inference.
<div><strong>Authors:</strong> Deaglan J. Bartlett, Shivam Pandey</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces symbolic emulators that approximate hypergeometric functions for ΛCDM comoving distance and linear growth factor, achieving sub‑0.001% and 0.05% errors across relevant redshifts and Ωm values. Integrated into a Dark Energy Survey‑like 3×2pt analysis, these emulators yield cosmological constraints matching those from standard numerical methods while dramatically reducing computation time and memory usage. The work demonstrates that symbolic approximations can scale likelihood‑based inference without sacrificing precision.", "summary_cn": "本文提出了用于 ΛCDM 宇宙学模型的符号化仿真器，通过近似超几何函数实现对共动距离和线性增长因子的高精度（误差分别低于 0.001% 和 0.05%）并覆盖 Ωₘ∈[0.1,0.5] 的红移范围。将这些仿真器整合到类似暗能量巡天的 3×2pt 分析中，得到的宇宙学约束与传统数值方法一致，同时显著提升了计算速度和内存效率。该工作展示了符号近似在保持精度的前提下，可实现可扩展的似然推断。", "keywords": "symbolic emulator, cosmology, ΛCDM, hypergeometric approximation, dark energy survey, 3x2pt analysis, fast inference", "scoring": {"interpretability": 1, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Deaglan J. Bartlett", "Shivam Pandey"]}
]]></acme>

<pubDate>2025-10-21T15:57:23+00:00</pubDate>
</item>
<item>
<title>Diffusion Buffer for Online Generative Speech Enhancement</title>
<link>https://papers.cool/arxiv/2510.18744</link>
<guid>https://papers.cool/arxiv/2510.18744</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Diffusion Buffer, a diffusion‑based generative speech‑enhancement model that processes each incoming audio frame with a single neural‑network call, enabling online operation on consumer‑grade GPUs. By aligning physical time with diffusion timesteps and employing a specially designed 2‑D convolutional UNet and a data‑prediction loss, the approach reduces algorithmic latency from 320‑960 ms to 32‑176 ms while improving quality, and it outperforms predictive baselines on unseen noisy speech.<br /><strong>Summary (CN):</strong> 本文提出 Diffusion Buffer，一种基于扩散的生成式语音增强模型，能够在每个输入音频帧只调用一次神经网络，从而在消费级 GPU 上实现在线处理。该方法通过将物理时间与扩散时间步对齐，并使用专门设计的 2D 卷积 UNet 与数据预测损失，大幅将算法延迟从 320‑960 ms 降至 32‑176 ms，同时提升了性能，并在未见噪声语音上超过了预测式基线。<br /><strong>Keywords:</strong> diffusion, speech enhancement, online generative models, UNet, algorithmic latency, data prediction loss<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann</div>
Online Speech Enhancement was mainly reserved for predictive models. A key advantage of these models is that for an incoming signal frame from a stream of data, the model is called only once for enhancement. In contrast, generative Speech Enhancement models often require multiple calls, resulting in a computational complexity that is too high for many online speech enhancement applications. This work presents the Diffusion Buffer, a generative diffusion-based Speech Enhancement model which only requires one neural network call per incoming signal frame from a stream of data and performs enhancement in an online fashion on a consumer-grade GPU. The key idea of the Diffusion Buffer is to align physical time with Diffusion time-steps. The approach progressively denoises frames through physical time, where past frames have more noise removed. Consequently, an enhanced frame is output to the listener with a delay defined by the Diffusion Buffer, and the output frame has a corresponding look-ahead. In this work, we extend upon our previous work by carefully designing a 2D convolutional UNet architecture that specifically aligns with the Diffusion Buffer's look-ahead. We observe that the proposed UNet improves performance, particularly when the algorithmic latency is low. Moreover, we show that using a Data Prediction loss instead of Denoising Score Matching loss enables flexible control over the trade-off between algorithmic latency and quality during inference. The extended Diffusion Buffer equipped with a novel NN and loss function drastically reduces the algorithmic latency from 320 - 960 ms to 32 - 176 ms with an even increased performance. While it has been shown before that offline generative diffusion models outperform predictive approaches in unseen noisy speech data, we confirm that the online Diffusion Buffer also outperforms its predictive counterpart on unseen noisy speech data.
<div><strong>Authors:</strong> Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Diffusion Buffer, a diffusion‑based generative speech‑enhancement model that processes each incoming audio frame with a single neural‑network call, enabling online operation on consumer‑grade GPUs. By aligning physical time with diffusion timesteps and employing a specially designed 2‑D convolutional UNet and a data‑prediction loss, the approach reduces algorithmic latency from 320‑960 ms to 32‑176 ms while improving quality, and it outperforms predictive baselines on unseen noisy speech.", "summary_cn": "本文提出 Diffusion Buffer，一种基于扩散的生成式语音增强模型，能够在每个输入音频帧只调用一次神经网络，从而在消费级 GPU 上实现在线处理。该方法通过将物理时间与扩散时间步对齐，并使用专门设计的 2D 卷积 UNet 与数据预测损失，大幅将算法延迟从 320‑960 ms 降至 32‑176 ms，同时提升了性能，并在未见噪声语音上超过了预测式基线。", "keywords": "diffusion, speech enhancement, online generative models, UNet, algorithmic latency, data prediction loss", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bunlong Lay", "Rostislav Makarov", "Simon Welker", "Maris Hillemann", "Timo Gerkmann"]}
]]></acme>

<pubDate>2025-10-21T15:52:33+00:00</pubDate>
</item>
<item>
<title>Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</title>
<link>https://papers.cool/arxiv/2510.18731</link>
<guid>https://papers.cool/arxiv/2510.18731</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that rewards large language models for providing correct answers and for abstaining when a question is unsolvable within a multi-turn conversation. By progressively increasing dialogue difficulty through a competence‑gated curriculum, RLAAR stabilizes training and reduces the Lost-in-Conversation degradation, achieving higher accuracy and calibrated abstention rates on benchmark tasks.<br /><strong>Summary (CN):</strong> 本文提出了“可验证准确性与弃答奖励的课程强化学习”（RLAAR）框架，鼓励大语言模型在多轮对话中不仅给出正确答案，还在问题不可解时进行弃答。该方法通过能力门控的课程逐步提升对话难度，稳定训练并显著缓解对话中性能衰减（Lost-in-Conversation），在基准测试中提升了准确率并实现了更高的校准弃答率。<br /><strong>Keywords:</strong> curriculum learning, reinforcement learning with verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM safety, reliability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ming Li</div>
Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.
<div><strong>Authors:</strong> Ming Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that rewards large language models for providing correct answers and for abstaining when a question is unsolvable within a multi-turn conversation. By progressively increasing dialogue difficulty through a competence‑gated curriculum, RLAAR stabilizes training and reduces the Lost-in-Conversation degradation, achieving higher accuracy and calibrated abstention rates on benchmark tasks.", "summary_cn": "本文提出了“可验证准确性与弃答奖励的课程强化学习”（RLAAR）框架，鼓励大语言模型在多轮对话中不仅给出正确答案，还在问题不可解时进行弃答。该方法通过能力门控的课程逐步提升对话难度，稳定训练并显著缓解对话中性能衰减（Lost-in-Conversation），在基准测试中提升了准确率并实现了更高的校准弃答率。", "keywords": "curriculum learning, reinforcement learning with verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM safety, reliability", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ming Li"]}
]]></acme>

<pubDate>2025-10-21T15:32:26+00:00</pubDate>
</item>
<item>
<title>Adapting Language Balance in Code-Switching Speech</title>
<link>https://papers.cool/arxiv/2510.18724</link>
<guid>https://papers.cool/arxiv/2510.18724</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a differentiable surrogate that highlights code‑switching points by exploiting the difference between the embedded and main language embeddings, thereby reducing context bias during generation. Experiments on Arabic and Chinese‑English code‑switched speech show improved prediction of switching locations and lower substitution errors, indicating increased robustness of large foundational models to code‑switching scenarios.<br /><strong>Summary (CN):</strong> 本文提出一种可微分的代理方法，通过利用嵌入语言与主体语言之间的差异来突出代码切换点，从而降低生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验显示，该方法能够更准确地预测切换位置并减少替换错误，提升了大型基础模型在代码切换情形下的鲁棒性。<br /><strong>Keywords:</strong> code-switching, speech recognition, language balance, differentiable surrogate, robustness, Arabic, Chinese-English, embedding bias<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a differentiable surrogate that highlights code‑switching points by exploiting the difference between the embedded and main language embeddings, thereby reducing context bias during generation. Experiments on Arabic and Chinese‑English code‑switched speech show improved prediction of switching locations and lower substitution errors, indicating increased robustness of large foundational models to code‑switching scenarios.", "summary_cn": "本文提出一种可微分的代理方法，通过利用嵌入语言与主体语言之间的差异来突出代码切换点，从而降低生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验显示，该方法能够更准确地预测切换位置并减少替换错误，提升了大型基础模型在代码切换情形下的鲁棒性。", "keywords": "code-switching, speech recognition, language balance, differentiable surrogate, robustness, Arabic, Chinese-English, embedding bias", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:55+00:00</pubDate>
</item>
<item>
<title>Bayesian Low-Rank Factorization for Robust Model Adaptation</title>
<link>https://papers.cool/arxiv/2510.18723</link>
<guid>https://papers.cool/arxiv/2510.18723</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Bayesian low-rank factorized adapters for speech foundation models to enable robust domain adaptation, especially for code-switching scenarios, by placing near-zero priors on adaptation matrices to keep them sparse. Experiments on the Whisper model show minimal performance loss on the original tasks while achieving significant gains on new multilingual domains and reducing catastrophic forgetting compared to LoRA. The results suggest Bayesian adapters can fine‑tune large speech models without sacrificing their general capabilities.<br /><strong>Summary (CN):</strong> 本文提出了基于贝叶斯低秩因子化的适配器，用于在保持稀疏性的同时对语音基础模型进行稳健的领域适配，特别针对代码切换场景。对 Whisper 模型的实验表明，在保持原有任务性能基本不变的情况下，新领域表现显著提升，并相比 LoRA 大幅降低了对基础模型的灾难性遗忘。结果显示，贝叶斯适配器能够在不牺牲通用能力的前提下对大规模语音模型进行微调。<br /><strong>Keywords:</strong> Bayesian adaptation, low-rank factorization, adapters, speech foundation models, Whisper, code-switching, catastrophic forgetting, LoRA, robust fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Bayesian low-rank factorized adapters for speech foundation models to enable robust domain adaptation, especially for code-switching scenarios, by placing near-zero priors on adaptation matrices to keep them sparse. Experiments on the Whisper model show minimal performance loss on the original tasks while achieving significant gains on new multilingual domains and reducing catastrophic forgetting compared to LoRA. The results suggest Bayesian adapters can fine‑tune large speech models without sacrificing their general capabilities.", "summary_cn": "本文提出了基于贝叶斯低秩因子化的适配器，用于在保持稀疏性的同时对语音基础模型进行稳健的领域适配，特别针对代码切换场景。对 Whisper 模型的实验表明，在保持原有任务性能基本不变的情况下，新领域表现显著提升，并相比 LoRA 大幅降低了对基础模型的灾难性遗忘。结果显示，贝叶斯适配器能够在不牺牲通用能力的前提下对大规模语音模型进行微调。", "keywords": "Bayesian adaptation, low-rank factorization, adapters, speech foundation models, Whisper, code-switching, catastrophic forgetting, LoRA, robust fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:30+00:00</pubDate>
</item>
<item>
<title>Differentially Private E-Values</title>
<link>https://papers.cool/arxiv/2510.18654</link>
<guid>https://papers.cool/arxiv/2510.18654</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a general framework for converting non-private e-values into differentially private ones by using a novel biased multiplicative noise mechanism that preserves statistical validity. It proves that the private e-values retain strong power and are asymptotically as powerful as the original e-values, and validates the method on tasks such as online risk monitoring, private healthcare analysis, and conformal e-prediction.<br /><strong>Summary (CN):</strong> 本文提出了一种通用框架，将非私密 e 值通过新颖的带偏乘性噪声机制转化为差分隐私 e 值，从而保持统计有效性。作者证明了私密 e 值仍具有强大的统计功效，并在在线风险监控、私密医疗和符合性 e 预测等场景中验证了该方法的有效性。<br /><strong>Keywords:</strong> differential privacy, e-values, biased multiplicative noise, statistical inference, private hypothesis testing, online risk monitoring, conformal prediction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Daniel Csillag, Diego Mesquita</div>
E-values have gained prominence as flexible tools for statistical inference and risk control, enabling anytime- and post-hoc-valid procedures under minimal assumptions. However, many real-world applications fundamentally rely on sensitive data, which can be leaked through e-values. To ensure their safe release, we propose a general framework to transform non-private e-values into differentially private ones. Towards this end, we develop a novel biased multiplicative noise mechanism that ensures our e-values remain statistically valid. We show that our differentially private e-values attain strong statistical power, and are asymptotically as powerful as their non-private counterparts. Experiments across online risk monitoring, private healthcare, and conformal e-prediction demonstrate our approach's effectiveness and illustrate its broad applicability.
<div><strong>Authors:</strong> Daniel Csillag, Diego Mesquita</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a general framework for converting non-private e-values into differentially private ones by using a novel biased multiplicative noise mechanism that preserves statistical validity. It proves that the private e-values retain strong power and are asymptotically as powerful as the original e-values, and validates the method on tasks such as online risk monitoring, private healthcare analysis, and conformal e-prediction.", "summary_cn": "本文提出了一种通用框架，将非私密 e 值通过新颖的带偏乘性噪声机制转化为差分隐私 e 值，从而保持统计有效性。作者证明了私密 e 值仍具有强大的统计功效，并在在线风险监控、私密医疗和符合性 e 预测等场景中验证了该方法的有效性。", "keywords": "differential privacy, e-values, biased multiplicative noise, statistical inference, private hypothesis testing, online risk monitoring, conformal prediction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Daniel Csillag", "Diego Mesquita"]}
]]></acme>

<pubDate>2025-10-21T14:03:35+00:00</pubDate>
</item>
<item>
<title>Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression</title>
<link>https://papers.cool/arxiv/2510.18650</link>
<guid>https://papers.cool/arxiv/2510.18650</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that extends beyond first‑order linear quantization by using binary quadratic expressions while keeping a highly compact representation. Experiments on a matrix‑compression benchmark and post‑training quantization of Vision‑Transformer models show that BQQ achieves a better memory‑error trade‑off and improves PTQ performance, outperforming state‑of‑the‑art methods by up to 2.2% (calibration‑based) and 59.1% (data‑free) at an effective 2‑bit precision.<br /><strong>Summary (CN):</strong> 本文提出二元二次量化（Binary Quadratic Quantization，BQQ）方法，利用二元二次表达式在保持极度紧凑数据格式的同时，超越传统的一阶线性量化，实现对实值矩阵的高效压缩。通过矩阵压缩基准测试以及对预训练 Vision Transformer 模型的后训练量化实验，BQQ 展现出更优的记忆‑误差折中，并在等价 2 位量化下相较于最先进的 PTQ 方法分别提升 2.2%（校准）和 59.1%（无数据） 的性能，显示出二元二次表达式在高效矩阵近似与网络压缩中的惊人效果。<br /><strong>Keywords:</strong> binary quadratic quantization, matrix compression, post-training quantization, Vision Transformer, low-bit quantization, binary coding, neural network compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
This paper proposes a novel matrix quantization method, Binary Quadratic Quantization (BQQ). In contrast to conventional first-order quantization approaches, such as uniform quantization and binary coding quantization, that approximate real-valued matrices via linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. We validate our approach with two experiments: a matrix compression benchmark and post-training quantization (PTQ) on pretrained Vision Transformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error than conventional methods for compressing diverse matrix data. It also delivers strong PTQ performance, even though we neither target state-of-the-art PTQ accuracy under tight memory constraints nor rely on PTQ-specific binary matrix optimization. For example, our proposed method outperforms the state-of-the-art PTQ method by up to 2.2\% and 59.1% on the ImageNet dataset under the calibration-based and data-free scenarios, respectively, with quantization equivalent to 2 bits. These findings highlight the surprising effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression.
<div><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that extends beyond first‑order linear quantization by using binary quadratic expressions while keeping a highly compact representation. Experiments on a matrix‑compression benchmark and post‑training quantization of Vision‑Transformer models show that BQQ achieves a better memory‑error trade‑off and improves PTQ performance, outperforming state‑of‑the‑art methods by up to 2.2% (calibration‑based) and 59.1% (data‑free) at an effective 2‑bit precision.", "summary_cn": "本文提出二元二次量化（Binary Quadratic Quantization，BQQ）方法，利用二元二次表达式在保持极度紧凑数据格式的同时，超越传统的一阶线性量化，实现对实值矩阵的高效压缩。通过矩阵压缩基准测试以及对预训练 Vision Transformer 模型的后训练量化实验，BQQ 展现出更优的记忆‑误差折中，并在等价 2 位量化下相较于最先进的 PTQ 方法分别提升 2.2%（校准）和 59.1%（无数据） 的性能，显示出二元二次表达式在高效矩阵近似与网络压缩中的惊人效果。", "keywords": "binary quadratic quantization, matrix compression, post-training quantization, Vision Transformer, low-bit quantization, binary coding, neural network compression", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kyo Kuroki", "Yasuyuki Okoshi", "Thiem Van Chu", "Kazushi Kawamura", "Masato Motomura"]}
]]></acme>

<pubDate>2025-10-21T13:58:46+00:00</pubDate>
</item>
<item>
<title>ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
<link>https://papers.cool/arxiv/2510.18637</link>
<guid>https://papers.cool/arxiv/2510.18637</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ε‑Seg introduces a hierarchical variational autoencoder framework with center‑region masking, contrastive learning and a Gaussian‑mixture‑model prior to achieve semantic segmentation of electron‑microscopy images using extremely sparse labels. The method learns robust latent embeddings and directly predicts class labels with an MLP head, demonstrating competitive performance on dense EM and fluorescence microscopy datasets.<br /><strong>Summary (CN):</strong> ε‑Seg 利用层级变分自编码器、中心区域掩码、对比学习和高斯混合模型先验，在极少标注（0.05%）下实现电子显微镜图像的语义分割。该方法通过学习稳健的潜在嵌入并使用 MLP 头直接预测类别，在密集 EM 数据集和荧光显微数据上表现出竞争力。<br /><strong>Keywords:</strong> semantic segmentation, electron microscopy, hierarchical variational autoencoder, contrastive learning, sparse supervision<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.
<div><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ε‑Seg introduces a hierarchical variational autoencoder framework with center‑region masking, contrastive learning and a Gaussian‑mixture‑model prior to achieve semantic segmentation of electron‑microscopy images using extremely sparse labels. The method learns robust latent embeddings and directly predicts class labels with an MLP head, demonstrating competitive performance on dense EM and fluorescence microscopy datasets.", "summary_cn": "ε‑Seg 利用层级变分自编码器、中心区域掩码、对比学习和高斯混合模型先验，在极少标注（0.05%）下实现电子显微镜图像的语义分割。该方法通过学习稳健的潜在嵌入并使用 MLP 头直接预测类别，在密集 EM 数据集和荧光显微数据上表现出竞争力。", "keywords": "semantic segmentation, electron microscopy, hierarchical variational autoencoder, contrastive learning, sparse supervision", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Sheida Rahnamai Kordasiabi", "Damian Dalle Nogare", "Florian Jug"]}
]]></acme>

<pubDate>2025-10-21T13:41:07+00:00</pubDate>
</item>
<item>
<title>C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression</title>
<link>https://papers.cool/arxiv/2510.18636</link>
<guid>https://papers.cool/arxiv/2510.18636</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces C‑SWAP, an explainability‑aware one‑shot structured pruning framework that uses causal relationships between model predictions and internal structures to guide a progressive pruning process. By leveraging cause‑effect analysis, the method removes redundant components without fine‑tuning, achieving substantial model size reductions on CNN and vision‑transformer classifiers while preserving performance. Experiments show that C‑SWAP outperforms existing one‑shot pruning techniques in the trade‑off between compression and accuracy.<br /><strong>Summary (CN):</strong> 本文提出 C‑SWAP，一种基于可解释性的一次性结构化剪枝框架，利用模型预测与内部结构之间的因果关系（cause‑effect）进行渐进式剪枝。该方法在无需微调的情况下移除冗余结构，实现了对 CNN 和视觉 Transformer 分类模型的显著压缩，并保持了性能。实验表明，C‑SWAP 在压缩率与准确率的平衡上优于现有一次性剪枝方法。<br /><strong>Keywords:</strong> structured pruning, one-shot pruning, explainability, causal-aware pruning, model compression, CNN, vision transformer, post-training pruning, efficiency, neural network compression<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.
<div><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces C‑SWAP, an explainability‑aware one‑shot structured pruning framework that uses causal relationships between model predictions and internal structures to guide a progressive pruning process. By leveraging cause‑effect analysis, the method removes redundant components without fine‑tuning, achieving substantial model size reductions on CNN and vision‑transformer classifiers while preserving performance. Experiments show that C‑SWAP outperforms existing one‑shot pruning techniques in the trade‑off between compression and accuracy.", "summary_cn": "本文提出 C‑SWAP，一种基于可解释性的一次性结构化剪枝框架，利用模型预测与内部结构之间的因果关系（cause‑effect）进行渐进式剪枝。该方法在无需微调的情况下移除冗余结构，实现了对 CNN 和视觉 Transformer 分类模型的显著压缩，并保持了性能。实验表明，C‑SWAP 在压缩率与准确率的平衡上优于现有一次性剪枝方法。", "keywords": "structured pruning, one-shot pruning, explainability, causal-aware pruning, model compression, CNN, vision transformer, post-training pruning, efficiency, neural network compression", "scoring": {"interpretability": 6, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baptiste Bauvin", "Loïc Baret", "Ola Ahmad"]}
]]></acme>

<pubDate>2025-10-21T13:40:11+00:00</pubDate>
</item>
<item>
<title>A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents</title>
<link>https://papers.cool/arxiv/2510.18608</link>
<guid>https://papers.cool/arxiv/2510.18608</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes integrating continual learning and compositionality principles into foundation models to enable more flexible and efficient robotic agents that can adapt to dynamic real‑world environments without full retraining. It outlines a compositional paradigm that leverages the rich representations of large multimodal models and introduces methods for incremental skill composition and knowledge transfer in robotics tasks.<br /><strong>Summary (CN):</strong> 本文提出将持续学习和组合性原则引入基础模型，以实现能够在动态真实环境中无需整体重新训练即可灵活、高效适应的机器人代理。文中阐述了一种组合范式，利用大规模多模态模型的丰富表征，提出了增量技能组合和知识转移的技术方法。<br /><strong>Keywords:</strong> foundation models, continual learning, compositionality, robotic control, skill composition, knowledge transfer, adaptability, multimodal representations<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Luigi Quarantiello, Elia Piccoli, Jack Bell, Malio Li, Giacomo Carfì, Eric Nuertey Coleman, Gerlando Gramaglia, Lanpei Li, Mauro Madeddu, Irene Testa, Vincenzo Lomonaco</div>
The birth of Foundation Models brought unprecedented results in a wide range of tasks, from language to vision, to robotic control. These models are able to process huge quantities of data, and can extract and develop rich representations, which can be employed across different domains and modalities. However, they still have issues in adapting to dynamic, real-world scenarios without retraining the entire model from scratch. In this work, we propose the application of Continual Learning and Compositionality principles to foster the development of more flexible, efficient and smart AI solutions.
<div><strong>Authors:</strong> Luigi Quarantiello, Elia Piccoli, Jack Bell, Malio Li, Giacomo Carfì, Eric Nuertey Coleman, Gerlando Gramaglia, Lanpei Li, Mauro Madeddu, Irene Testa, Vincenzo Lomonaco</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes integrating continual learning and compositionality principles into foundation models to enable more flexible and efficient robotic agents that can adapt to dynamic real‑world environments without full retraining. It outlines a compositional paradigm that leverages the rich representations of large multimodal models and introduces methods for incremental skill composition and knowledge transfer in robotics tasks.", "summary_cn": "本文提出将持续学习和组合性原则引入基础模型，以实现能够在动态真实环境中无需整体重新训练即可灵活、高效适应的机器人代理。文中阐述了一种组合范式，利用大规模多模态模型的丰富表征，提出了增量技能组合和知识转移的技术方法。", "keywords": "foundation models, continual learning, compositionality, robotic control, skill composition, knowledge transfer, adaptability, multimodal representations", "scoring": {"interpretability": 3, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Luigi Quarantiello", "Elia Piccoli", "Jack Bell", "Malio Li", "Giacomo Carfì", "Eric Nuertey Coleman", "Gerlando Gramaglia", "Lanpei Li", "Mauro Madeddu", "Irene Testa", "Vincenzo Lomonaco"]}
]]></acme>

<pubDate>2025-10-21T13:06:52+00:00</pubDate>
</item>
<item>
<title>Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels</title>
<link>https://papers.cool/arxiv/2510.18604</link>
<guid>https://papers.cool/arxiv/2510.18604</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Channel-Aware Vector Quantization (CAVQ) within a joint source-channel coding framework (VQJSCC) that incorporates channel transition probabilities into the quantization process for discrete semantic communication over memoryless channels. By aligning easily confused symbols with semantically similar codewords and employing a multi-codebook alignment mechanism, the method mitigates the digital cliff effect and improves reconstruction quality across various modulation schemes. Experiments show that VQJSCC outperforms existing digital semantic communication baselines in robustness and efficiency.<br /><strong>Summary (CN):</strong> 本文提出在联合源信道编码框架（VQJSCC）中加入通道感知向量量化（CAVQ），将离散信道的转移概率纳入量化过程，实现语义特征与调制星座的直接映射。通过多码本对齐机制，将易混淆的符号与语义相似的码字对应，从而缓解数字悬崖效应，在多种调制方案下提升重建质量。实验表明 VQJSCC 在鲁棒性和效率方面均优于现有的数字语义通信基线。<br /><strong>Keywords:</strong> semantic communication, vector quantization, channel-aware quantization, joint source-channel coding, discrete memoryless channel, robustness, digital cliff effect, multi-codebook alignment, modulation schemes<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zian Meng, Qiang Li, Wenqian Tang, Mingdie Yan, Xiaohu Ge</div>
Deep learning-based semantic communication has largely relied on analog or semi-digital transmission, which limits compatibility with modern digital communication infrastructures. Recent studies have employed vector quantization (VQ) to enable discrete semantic transmission, yet existing methods neglect channel state information during codebook optimization, leading to suboptimal robustness. To bridge this gap, we propose a channel-aware vector quantization (CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed VQJSCC, established on a discrete memoryless channel. In this framework, semantic features are discretized and directly mapped to modulation constellation symbols, while CAVQ integrates channel transition probabilities into the quantization process, aligning easily confused symbols with semantically similar codewords. A multi-codebook alignment mechanism is further introduced to handle mismatches between codebook order and modulation order by decomposing the transmission stream into multiple independently optimized subchannels. Experimental results demonstrate that VQJSCC effectively mitigates the digital cliff effect, achieves superior reconstruction quality across various modulation schemes, and outperforms state-of-the-art digital semantic communication baselines in both robustness and efficiency.
<div><strong>Authors:</strong> Zian Meng, Qiang Li, Wenqian Tang, Mingdie Yan, Xiaohu Ge</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Channel-Aware Vector Quantization (CAVQ) within a joint source-channel coding framework (VQJSCC) that incorporates channel transition probabilities into the quantization process for discrete semantic communication over memoryless channels. By aligning easily confused symbols with semantically similar codewords and employing a multi-codebook alignment mechanism, the method mitigates the digital cliff effect and improves reconstruction quality across various modulation schemes. Experiments show that VQJSCC outperforms existing digital semantic communication baselines in robustness and efficiency.", "summary_cn": "本文提出在联合源信道编码框架（VQJSCC）中加入通道感知向量量化（CAVQ），将离散信道的转移概率纳入量化过程，实现语义特征与调制星座的直接映射。通过多码本对齐机制，将易混淆的符号与语义相似的码字对应，从而缓解数字悬崖效应，在多种调制方案下提升重建质量。实验表明 VQJSCC 在鲁棒性和效率方面均优于现有的数字语义通信基线。", "keywords": "semantic communication, vector quantization, channel-aware quantization, joint source-channel coding, discrete memoryless channel, robustness, digital cliff effect, multi-codebook alignment, modulation schemes", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zian Meng", "Qiang Li", "Wenqian Tang", "Mingdie Yan", "Xiaohu Ge"]}
]]></acme>

<pubDate>2025-10-21T13:02:35+00:00</pubDate>
</item>
<item>
<title>CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder</title>
<link>https://papers.cool/arxiv/2510.18583</link>
<guid>https://papers.cool/arxiv/2510.18583</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CovMatch is a multimodal dataset distillation framework that aligns the cross‑covariance of real and synthetic image‑text features while regularizing each modality’s feature distribution. By jointly optimizing both the vision and text encoders, it achieves stronger cross‑modal alignment and improves retrieval performance, outperforming prior methods on Flickr30K and COCO with only 500 synthetic pairs.<br /><strong>Summary (CN):</strong> CovMatch 是一种多模态数据蒸馏框架，通过对齐真实和合成图文特征的交叉协方差并对每个模态的特征分布进行正则化，实现对视觉编码器和文本编码器的联合优化，从而增强跨模态对齐并提升检索效果。实验表明，在 Flickr30K 与 COCO 数据集上，仅使用 500 对合成样本即可超越现有最先进方法，检索准确率提升最高 6.8%。<br /><strong>Keywords:</strong> multimodal dataset distillation, cross-covariance alignment, vision-language models, synthetic image-text pairs, contrastive learning, Flickr30K, COCO, retrieval accuracy<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yongmin Lee, Hye Won Chung</div>
Multimodal dataset distillation aims to synthesize a small set of image-text pairs that enables efficient training of large-scale vision-language models. While dataset distillation has shown promise in unimodal tasks, extending it to multimodal contrastive learning presents key challenges: learning cross-modal alignment and managing the high computational cost of large encoders. Prior approaches address scalability by freezing the text encoder and update only the image encoder and text projection layer. However, we find this severely limits semantic alignment and becomes a bottleneck for performance scaling. We propose CovMatch, a scalable dataset distillation framework that aligns the cross-covariance of real and synthetic features while regularizing feature distributions within each modality. Unlike prior approaches, CovMatch enables joint optimization of both encoders, leading to stronger cross-modal alignment and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms state-of-the-art multimodal distillation methods and achieves up to 6.8% absolute gains in retrieval accuracy using only 500 synthetic pairs.
<div><strong>Authors:</strong> Yongmin Lee, Hye Won Chung</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CovMatch is a multimodal dataset distillation framework that aligns the cross‑covariance of real and synthetic image‑text features while regularizing each modality’s feature distribution. By jointly optimizing both the vision and text encoders, it achieves stronger cross‑modal alignment and improves retrieval performance, outperforming prior methods on Flickr30K and COCO with only 500 synthetic pairs.", "summary_cn": "CovMatch 是一种多模态数据蒸馏框架，通过对齐真实和合成图文特征的交叉协方差并对每个模态的特征分布进行正则化，实现对视觉编码器和文本编码器的联合优化，从而增强跨模态对齐并提升检索效果。实验表明，在 Flickr30K 与 COCO 数据集上，仅使用 500 对合成样本即可超越现有最先进方法，检索准确率提升最高 6.8%。", "keywords": "multimodal dataset distillation, cross-covariance alignment, vision-language models, synthetic image-text pairs, contrastive learning, Flickr30K, COCO, retrieval accuracy", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yongmin Lee", "Hye Won Chung"]}
]]></acme>

<pubDate>2025-10-21T12:36:25+00:00</pubDate>
</item>
<item>
<title>A Multi-Evidence Framework Rescues Low- Power Prognostic Signals and Rejects Statistical Artifacts in Cancer Genomics</title>
<link>https://papers.cool/arxiv/2510.18571</link>
<guid>https://papers.cool/arxiv/2510.18571</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The authors present a five‑criteria computational framework that combines causal inference methods (inverse probability weighting, doubly robust estimation) with orthogonal biological validation to separate true prognostic signals from statistical artifacts in underpowered cancer genomics cohorts. Applied to TCGA breast‑cancer mortality data, the framework discards a false‑positive driver (RYR2) and highlights a biologically plausible candidate (KMT2C) despite marginal statistical significance, demonstrating how multi‑evidence integration can rescue low‑power signals.<br /><strong>Summary (CN):</strong> 作者提出了一种五准则计算框架，将因果推断（逆概率加权、双稳健估计）与生物学验证（表达、突变模式、文献）相结合，以在低功效的癌症基因组学数据中区分真实的预后信号和统计伪迹。在 TCGA 乳腺癌死亡率分析中，该框架排除了伪阳性的 RYR2 基因，同时尽管统计显著性边缘，却因强生物证据而突出 KMT2C 基因，展示了多证据整合在拯救低功效信号方面的有效性。<br /><strong>Keywords:</strong> causal inference, cancer genomics, prognostic biomarkers, low-power studies, multi-evidence framework, TCGA, KMT2C, RYR2, inverse probability weighting, doubly robust estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 2, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Gokturk Aytug Akarlar</div>
Motivation: Standard genome-wide association studies in cancer genomics rely on statistical significance with multiple testing correction, but systematically fail in underpowered cohorts. In TCGA breast cancer (n=967, 133 deaths), low event rates (13.8%) create severe power limitations, producing false negatives for known drivers and false positives for large passenger genes. Results: We developed a five-criteria computational framework integrating causal inference (inverse probability weighting, doubly robust estimation) with orthogonal biological validation (expression, mutation patterns, literature evidence). Applied to TCGA-BRCA mortality analysis, standard Cox+FDR detected zero genes at FDR<0.05, confirming complete failure in underpowered settings. Our framework correctly identified RYR2 - a cardiac gene with no cancer function - as a false positive despite nominal significance (p=0.024), while identifying KMT2C as a complex candidate requiring validation despite marginal significance (p=0.047, q=0.954). Power analysis revealed median power of 15.1% across genes, with KMT2C achieving only 29.8% power (HR=1.55), explaining borderline statistical significance despite strong biological evidence. The framework distinguished true signals from artifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations (passenger signature) with no hotspots, while KMT2C showed 6.7% silent mutations with 31.4% truncating variants (driver signature). This multi-evidence approach provides a template for analyzing underpowered cohorts, prioritizing biological interpretability over purely statistical significance. Availability: All code and analysis pipelines available at github.com/akarlaraytu/causal- inference-for-cancer-genomics
<div><strong>Authors:</strong> Gokturk Aytug Akarlar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The authors present a five‑criteria computational framework that combines causal inference methods (inverse probability weighting, doubly robust estimation) with orthogonal biological validation to separate true prognostic signals from statistical artifacts in underpowered cancer genomics cohorts. Applied to TCGA breast‑cancer mortality data, the framework discards a false‑positive driver (RYR2) and highlights a biologically plausible candidate (KMT2C) despite marginal statistical significance, demonstrating how multi‑evidence integration can rescue low‑power signals.", "summary_cn": "作者提出了一种五准则计算框架，将因果推断（逆概率加权、双稳健估计）与生物学验证（表达、突变模式、文献）相结合，以在低功效的癌症基因组学数据中区分真实的预后信号和统计伪迹。在 TCGA 乳腺癌死亡率分析中，该框架排除了伪阳性的 RYR2 基因，同时尽管统计显著性边缘，却因强生物证据而突出 KMT2C 基因，展示了多证据整合在拯救低功效信号方面的有效性。", "keywords": "causal inference, cancer genomics, prognostic biomarkers, low-power studies, multi-evidence framework, TCGA, KMT2C, RYR2, inverse probability weighting, doubly robust estimation", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gokturk Aytug Akarlar"]}
]]></acme>

<pubDate>2025-10-21T12:27:18+00:00</pubDate>
</item>
<item>
<title>Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data</title>
<link>https://papers.cool/arxiv/2510.18548</link>
<guid>https://papers.cool/arxiv/2510.18548</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an interval prediction method for Annual Average Daily Traffic (AADT) on minor roads by combining a Quantile Random Forest with Principal Component Analysis to quantify predictive uncertainty. Using data from over 2,000 roads in England and Wales, the approach achieves an 88.22% interval coverage probability and demonstrates improved accuracy and interpretability for transport planning.<br /><strong>Summary (CN):</strong> 该研究提出使用量化随机森林结合主成分分析，对英国和威尔士的次要道路年均日流量（AADT）进行区间预测，以量化不确定性并提供预测区间。实验在2000余条道路上验证，取得约88%的区间覆盖率和较窄的区间宽度，提升了交通规划的可靠性和可解释性。<br /><strong>Keywords:</strong> annual average daily traffic, quantile random forest, interval prediction, uncertainty quantification, principal component analysis, high-dimensional spatial data, traffic estimation<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Ying Yao, Daniel J. Graham</div>
Accurate annual average daily traffic (AADT) data are vital for transport planning and infrastructure management. However, automatic traffic detectors across national road networks often provide incomplete coverage, leading to underrepresentation of minor roads. While recent machine learning advances have improved AADT estimation at unmeasured locations, most models produce only point predictions and overlook estimation uncertainty. This study addresses that gap by introducing an interval prediction approach that explicitly quantifies predictive uncertainty. We integrate a Quantile Random Forest model with Principal Component Analysis to generate AADT prediction intervals, providing plausible traffic ranges bounded by estimated minima and maxima. Using data from over 2,000 minor roads in England and Wales, and evaluated with specialized interval metrics, the proposed method achieves an interval coverage probability of 88.22%, a normalized average width of 0.23, and a Winkler Score of 7,468.47. By combining machine learning with spatial and high-dimensional analysis, this framework enhances both the accuracy and interpretability of AADT estimation, supporting more robust and informed transport planning.
<div><strong>Authors:</strong> Ying Yao, Daniel J. Graham</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an interval prediction method for Annual Average Daily Traffic (AADT) on minor roads by combining a Quantile Random Forest with Principal Component Analysis to quantify predictive uncertainty. Using data from over 2,000 roads in England and Wales, the approach achieves an 88.22% interval coverage probability and demonstrates improved accuracy and interpretability for transport planning.", "summary_cn": "该研究提出使用量化随机森林结合主成分分析，对英国和威尔士的次要道路年均日流量（AADT）进行区间预测，以量化不确定性并提供预测区间。实验在2000余条道路上验证，取得约88%的区间覆盖率和较窄的区间宽度，提升了交通规划的可靠性和可解释性。", "keywords": "annual average daily traffic, quantile random forest, interval prediction, uncertainty quantification, principal component analysis, high-dimensional spatial data, traffic estimation", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Ying Yao", "Daniel J. Graham"]}
]]></acme>

<pubDate>2025-10-21T11:56:57+00:00</pubDate>
</item>
<item>
<title>Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18526</link>
<guid>https://papers.cool/arxiv/2510.18526</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces COUPLE, a framework that uses structural causal models and counterfactual reasoning to align large language models with pluralistic human values, capturing interdependencies and priority among value dimensions. By explicitly modeling causal relationships between high‑level values and model behavior, COUPLE enables fine‑grained control over value steering and improves interpretability of the alignment process. Experiments on two datasets show that COUPLE outperforms existing baselines across diverse value objectives.<br /><strong>Summary (CN):</strong> 本文提出 COUPLE 框架，利用结构因果模型和反事实推理将大型语言模型与多元人类价值观对齐，捕捉价值维度之间的相互依赖和优先级。通过显式建模高层价值与模型行为之间的因果关系，COUPLE 能实现细粒度的价值引导并提升对齐过程的可解释性。在两个不同价值体系的数据集上实验表明，COUPLE 在多种价值目标下均优于现有基线。<br /><strong>Keywords:</strong> pluralistic value alignment, counterfactual reasoning, structural causal model, LLM steerability, value prioritization, interpretability, alignment, causal modeling<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Hanze Guo, Jing Yao, Xiao Zhou, Xiaoyuan Yi, Xing Xie</div>
As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). In psychological and social value theories such as Schwartz's Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives.
<div><strong>Authors:</strong> Hanze Guo, Jing Yao, Xiao Zhou, Xiaoyuan Yi, Xing Xie</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces COUPLE, a framework that uses structural causal models and counterfactual reasoning to align large language models with pluralistic human values, capturing interdependencies and priority among value dimensions. By explicitly modeling causal relationships between high‑level values and model behavior, COUPLE enables fine‑grained control over value steering and improves interpretability of the alignment process. Experiments on two datasets show that COUPLE outperforms existing baselines across diverse value objectives.", "summary_cn": "本文提出 COUPLE 框架，利用结构因果模型和反事实推理将大型语言模型与多元人类价值观对齐，捕捉价值维度之间的相互依赖和优先级。通过显式建模高层价值与模型行为之间的因果关系，COUPLE 能实现细粒度的价值引导并提升对齐过程的可解释性。在两个不同价值体系的数据集上实验表明，COUPLE 在多种价值目标下均优于现有基线。", "keywords": "pluralistic value alignment, counterfactual reasoning, structural causal model, LLM steerability, value prioritization, interpretability, alignment, causal modeling", "scoring": {"interpretability": 7, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hanze Guo", "Jing Yao", "Xiao Zhou", "Xiaoyuan Yi", "Xing Xie"]}
]]></acme>

<pubDate>2025-10-21T11:12:45+00:00</pubDate>
</item>
<item>
<title>Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL</title>
<link>https://papers.cool/arxiv/2510.18516</link>
<guid>https://papers.cool/arxiv/2510.18516</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces POYO-SSL, a self‑supervised learning approach that first isolates statistically predictable neurons (using skewness and kurtosis) for pretraining and then fine‑tunes on the unpredictable population to decode dynamic visual experience from calcium imaging. On the Allen Brain Observatory dataset, POYO‑SSL achieves 12–13% relative improvement over training from scratch and demonstrates smooth, monotonic scaling with model size, unlike existing baselines that plateau or destabilize.<br /><strong>Summary (CN):</strong> 本文提出 POYO‑SSL，一种自监督学习方法，先使用偏度和峰度等高阶统计量挑选出统计上可预测的神经元进行预训练，再在不可预测的神经元上微调，以解码钙成像的动态视觉体验。 在 Allen 脑观测站数据集上，POYO‑SSL 相比从头训练提升约 12%‑13%，并随模型规模呈平滑单调的性能提升，而现有基线在扩大模型时出现平台期或不稳定。<br /><strong>Keywords:</strong> self-supervised learning, calcium imaging, neural decoding, POYO-SSL, predictability, Allen Brain Observatory, neural dynamics, foundation models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sangyoon Bae, Mehdi Azabou, Jiook Cha, Blake Richards</div>
Self-supervised learning (SSL) holds a great deal of promise for applications in neuroscience, due to the lack of large-scale, consistently labeled neural datasets. However, most neural datasets contain heterogeneous populations that mix stable, predictable cells with highly stochastic, stimulus-contingent ones, which has made it hard to identify consistent activity patterns during SSL. As a result, self-supervised pretraining has yet to show clear signs of benefits from scale on neural data. Here, we present a novel approach to self-supervised pretraining, POYO-SSL that exploits the heterogeneity of neural data to improve pre-training and achieve benefits of scale. Specifically, in POYO-SSL we pretrain only on predictable (statistically regular) neurons-identified on the pretraining split via simple higher-order statistics (skewness and kurtosis)-then we fine-tune on the unpredictable population for downstream tasks. On the Allen Brain Observatory dataset, this strategy yields approximately 12-13% relative gains over from-scratch training and exhibits smooth, monotonic scaling with model size. In contrast, existing state-of-the-art baselines plateau or destabilize as model size increases. By making predictability an explicit metric for crafting the data diet, POYO-SSL turns heterogeneity from a liability into an asset, providing a robust, biologically grounded recipe for scalable neural decoding and a path toward foundation models of neural dynamics.
<div><strong>Authors:</strong> Sangyoon Bae, Mehdi Azabou, Jiook Cha, Blake Richards</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces POYO-SSL, a self‑supervised learning approach that first isolates statistically predictable neurons (using skewness and kurtosis) for pretraining and then fine‑tunes on the unpredictable population to decode dynamic visual experience from calcium imaging. On the Allen Brain Observatory dataset, POYO‑SSL achieves 12–13% relative improvement over training from scratch and demonstrates smooth, monotonic scaling with model size, unlike existing baselines that plateau or destabilize.", "summary_cn": "本文提出 POYO‑SSL，一种自监督学习方法，先使用偏度和峰度等高阶统计量挑选出统计上可预测的神经元进行预训练，再在不可预测的神经元上微调，以解码钙成像的动态视觉体验。 在 Allen 脑观测站数据集上，POYO‑SSL 相比从头训练提升约 12%‑13%，并随模型规模呈平滑单调的性能提升，而现有基线在扩大模型时出现平台期或不稳定。", "keywords": "self-supervised learning, calcium imaging, neural decoding, POYO-SSL, predictability, Allen Brain Observatory, neural dynamics, foundation models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sangyoon Bae", "Mehdi Azabou", "Jiook Cha", "Blake Richards"]}
]]></acme>

<pubDate>2025-10-21T10:57:52+00:00</pubDate>
</item>
<item>
<title>Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18502</link>
<guid>https://papers.cool/arxiv/2510.18502</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a pipeline that combines a vision‑language model (VLM) with retrieval‑augmented generation (RAG) to achieve zero‑shot vehicle make and model recognition. The VLM extracts descriptive attributes from an image, retrieves matching textual entries from a database, and a language model uses the combined prompt to infer the vehicle identity, avoiding costly finetuning and enabling rapid updates for new models. Experiments show roughly a 20% improvement over a CLIP baseline, highlighting the potential of text‑based reasoning for scalable VMMR in smart‑city contexts.<br /><strong>Summary (CN):</strong> 本文提出一种将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 从图像中提取描述性属性，检索匹配的文本特征条目，再将其与描述一起构成提示，语言模型据此推断车辆的品牌和型号，从而避免大规模微调并能够通过新增文本快速适配新车型。实验表明该方法比 CLIP 基线提升约 20%，展示了基于文本推理的可扩展车辆识别在智慧城市中的潜力。<br /><strong>Keywords:</strong> vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, smart city<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.
<div><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a pipeline that combines a vision‑language model (VLM) with retrieval‑augmented generation (RAG) to achieve zero‑shot vehicle make and model recognition. The VLM extracts descriptive attributes from an image, retrieves matching textual entries from a database, and a language model uses the combined prompt to infer the vehicle identity, avoiding costly finetuning and enabling rapid updates for new models. Experiments show roughly a 20% improvement over a CLIP baseline, highlighting the potential of text‑based reasoning for scalable VMMR in smart‑city contexts.", "summary_cn": "本文提出一种将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 从图像中提取描述性属性，检索匹配的文本特征条目，再将其与描述一起构成提示，语言模型据此推断车辆的品牌和型号，从而避免大规模微调并能够通过新增文本快速适配新车型。实验表明该方法比 CLIP 基线提升约 20%，展示了基于文本推理的可扩展车辆识别在智慧城市中的潜力。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, smart city", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}
]]></acme>

<pubDate>2025-10-21T10:39:39+00:00</pubDate>
</item>
<item>
<title>Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.18457</link>
<guid>https://papers.cool/arxiv/2510.18457</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Foundation Model Variational Autoencoder (VFM-VAE) as a direct tokenizer for Latent Diffusion Models, addressing the robustness issues of distillation‑based approaches. By redesigning the decoder with Multi‑Scale Latent Fusion and Progressive Resolution Reconstruction, and proposing the SE‑CKNNA metric for representation dynamics, the method achieves rapid convergence and state‑of‑the‑art image quality (gFID 1.62). The work demonstrates that integrating VFMs without distillation yields superior tokenization and diffusion alignment.<br /><strong>Summary (CN):</strong> 本文提出 Vision Foundation Model 变分自编码器 (VFM‑VAE) 直接用作潜在扩散模型的视觉分词器，以解决蒸馏方法导致的对齐鲁棒性下降问题。通过采用多尺度潜在融合与渐进分辨率重构的解码器设计，并引入 SE‑CKNNA 指标分析表示动态，方法显著加速收敛并实现先进的图像质量（gFID 1.62）。实验表明，直接整合 VFM 能够提供更优的分词与扩散对齐效果。<br /><strong>Keywords:</strong> vision foundation models, latent diffusion, tokenizer, VFM-VAE, multi-scale latent fusion, SE-CKNNA, diffusion alignment, gFID<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.
<div><strong>Authors:</strong> Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Foundation Model Variational Autoencoder (VFM-VAE) as a direct tokenizer for Latent Diffusion Models, addressing the robustness issues of distillation‑based approaches. By redesigning the decoder with Multi‑Scale Latent Fusion and Progressive Resolution Reconstruction, and proposing the SE‑CKNNA metric for representation dynamics, the method achieves rapid convergence and state‑of‑the‑art image quality (gFID 1.62). The work demonstrates that integrating VFMs without distillation yields superior tokenization and diffusion alignment.", "summary_cn": "本文提出 Vision Foundation Model 变分自编码器 (VFM‑VAE) 直接用作潜在扩散模型的视觉分词器，以解决蒸馏方法导致的对齐鲁棒性下降问题。通过采用多尺度潜在融合与渐进分辨率重构的解码器设计，并引入 SE‑CKNNA 指标分析表示动态，方法显著加速收敛并实现先进的图像质量（gFID 1.62）。实验表明，直接整合 VFM 能够提供更优的分词与扩散对齐效果。", "keywords": "vision foundation models, latent diffusion, tokenizer, VFM-VAE, multi-scale latent fusion, SE-CKNNA, diffusion alignment, gFID", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Tianci Bi", "Xiaoyi Zhang", "Yan Lu", "Nanning Zheng"]}
]]></acme>

<pubDate>2025-10-21T09:30:45+00:00</pubDate>
</item>
<item>
<title>A machine learning approach to automation and uncertainty evaluation for self-validating thermocouples</title>
<link>https://papers.cool/arxiv/2510.18411</link>
<guid>https://papers.cool/arxiv/2510.18411</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a novel machine‑learning method to automatically detect the characteristic melting plateau of a phase‑change cell used in self‑validating thermocouples and to estimate the onset of melting with quantified uncertainty, eliminating manual analysis. Experiments on data from CCPI Europe achieve 100% plateau detection accuracy and a cross‑validated R² of 0.99 for calibration‑drift prediction.<br /><strong>Summary (CN):</strong> 本文提出一种新颖的机器学习方法，用于自动识别自校准热电偶中相变单元的熔融平台特征，并在检测到平台后量化熔融起始点及其不确定性，从而无需人工干预。基于 CCPI Europe 提供的测试数据，实验实现了 100% 的平台检测准确率，并在校准漂移预测上获得 0.99 的交叉验证 R²。<br /><strong>Keywords:</strong> thermocouple, self-validating, phase-change cell, melting plateau detection, machine learning, uncertainty quantification, calibration drift<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Samuel Bilson, Andrew Thompson, Declan Tucker, Jonathan Pearce</div>
Thermocouples are in widespread use in industry, but they are particularly susceptible to calibration drift in harsh environments. Self-validating thermocouples aim to address this issue by using a miniature phase-change cell (fixed-point) in close proximity to the measurement junction (tip) of the thermocouple. The fixed point is a crucible containing an ingot of metal with a known melting temperature. When the process temperature being monitored passes through the melting temperature of the ingot, the thermocouple output exhibits a "plateau" during melting. Since the melting temperature of the ingot is known, the thermocouple can be recalibrated in situ. Identifying the melting plateau to determine the onset of melting is reasonably well established but requires manual intervention involving zooming in on the region around the actual melting temperature, a process which can depend on the shape of the melting plateau. For the first time, we present a novel machine learning approach to recognize and identify the characteristic shape of the melting plateau and once identified, to quantity the point at which melting begins, along with its associated uncertainty. This removes the need for human intervention in locating and characterizing the melting point. Results from test data provided by CCPI Europe show 100% accuracy of melting plateau detection. They also show a cross-validated R2 of 0.99 on predictions of calibration drift.
<div><strong>Authors:</strong> Samuel Bilson, Andrew Thompson, Declan Tucker, Jonathan Pearce</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a novel machine‑learning method to automatically detect the characteristic melting plateau of a phase‑change cell used in self‑validating thermocouples and to estimate the onset of melting with quantified uncertainty, eliminating manual analysis. Experiments on data from CCPI Europe achieve 100% plateau detection accuracy and a cross‑validated R² of 0.99 for calibration‑drift prediction.", "summary_cn": "本文提出一种新颖的机器学习方法，用于自动识别自校准热电偶中相变单元的熔融平台特征，并在检测到平台后量化熔融起始点及其不确定性，从而无需人工干预。基于 CCPI Europe 提供的测试数据，实验实现了 100% 的平台检测准确率，并在校准漂移预测上获得 0.99 的交叉验证 R²。", "keywords": "thermocouple, self-validating, phase-change cell, melting plateau detection, machine learning, uncertainty quantification, calibration drift", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Samuel Bilson", "Andrew Thompson", "Declan Tucker", "Jonathan Pearce"]}
]]></acme>

<pubDate>2025-10-21T08:38:28+00:00</pubDate>
</item>
<item>
<title>S2AP: Score-space Sharpness Minimization for Adversarial Pruning</title>
<link>https://papers.cool/arxiv/2510.18381</link>
<guid>https://papers.cool/arxiv/2510.18381</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug‑in technique that minimizes sharpness in the importance‑score space during mask selection for adversarial pruning, thereby stabilizing the binary mask and improving the robustness of compressed models. By perturbing importance scores and minimizing the resulting robust loss, S2AP reduces sharp local minima that previously caused unstable mask choices. Extensive experiments across datasets, models, and sparsity levels show consistent robustness gains over existing adversarial pruning pipelines.<br /><strong>Summary (CN):</strong> 本文提出了“Score-space Sharpness-aware Adversarial Pruning (S2AP)”方法，通过在权重重要性分数空间加入扰动并最小化相应的鲁棒损失，以降低分数空间的尖锐性，实现对抗性剪枝过程中的掩码选择更稳定，从而提升模型压缩后的鲁棒性。实验在多个数据集、模型和稀疏度下验证了 S2AP 相比传统对抗性剪枝方法在稳健性上的显著提升。<br /><strong>Keywords:</strong> adversarial pruning, score-space sharpness, model compression, robustness, mask selection, importance scores, pruning stability, adversarial robustness, sparse networks, fine-tuning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
Adversarial pruning methods have emerged as a powerful tool for compressing neural networks while preserving robustness against adversarial attacks. These methods typically follow a three-step pipeline: (i) pretrain a robust model, (ii) select a binary mask for weight pruning, and (iii) finetune the pruned model. To select the binary mask, these methods minimize a robust loss by assigning an importance score to each weight, and then keep the weights with the highest scores. However, this score-space optimization can lead to sharp local minima in the robust loss landscape and, in turn, to an unstable mask selection, reducing the robustness of adversarial pruning methods. To overcome this issue, we propose a novel plug-in method for adversarial pruning, termed Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we introduce the concept of score-space sharpness minimization, which operates during the mask search by perturbing importance scores and minimizing the corresponding robust loss. Extensive experiments across various datasets, models, and sparsity levels demonstrate that S2AP effectively minimizes sharpness in score space, stabilizing the mask selection, and ultimately improving the robustness of adversarial pruning methods.
<div><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug‑in technique that minimizes sharpness in the importance‑score space during mask selection for adversarial pruning, thereby stabilizing the binary mask and improving the robustness of compressed models. By perturbing importance scores and minimizing the resulting robust loss, S2AP reduces sharp local minima that previously caused unstable mask choices. Extensive experiments across datasets, models, and sparsity levels show consistent robustness gains over existing adversarial pruning pipelines.", "summary_cn": "本文提出了“Score-space Sharpness-aware Adversarial Pruning (S2AP)”方法，通过在权重重要性分数空间加入扰动并最小化相应的鲁棒损失，以降低分数空间的尖锐性，实现对抗性剪枝过程中的掩码选择更稳定，从而提升模型压缩后的鲁棒性。实验在多个数据集、模型和稀疏度下验证了 S2AP 相比传统对抗性剪枝方法在稳健性上的显著提升。", "keywords": "adversarial pruning, score-space sharpness, model compression, robustness, mask selection, importance scores, pruning stability, adversarial robustness, sparse networks, fine-tuning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Giorgio Piras", "Qi Zhao", "Fabio Brau", "Maura Pintor", "Christian Wressnegger", "Battista Biggio"]}
]]></acme>

<pubDate>2025-10-21T07:55:31+00:00</pubDate>
</item>
<item>
<title>PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion</title>
<link>https://papers.cool/arxiv/2510.18348</link>
<guid>https://papers.cool/arxiv/2510.18348</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Phase-Guided Terrain Traversal (PGTT), a perception‑aware deep reinforcement learning method for legged robots that imposes gait structure via reward shaping rather than explicit gait priors. PGTT encodes per‑leg phase with a cubic Hermite spline to adapt swing height to local terrain statistics and adds a swing‑phase contact penalty, enabling morphology‑agnostic policies trained in simulation and transferred to real robots such as Unitree Go2 and ANYmal‑C. Experiments show PGTT achieves higher success rates under disturbances and obstacles while converging roughly twice as fast as strong baselines.<br /><strong>Summary (CN):</strong> 本文提出相位引导地形穿越（PGTT）方法，通过奖励塑形而非显式的步态先验来约束四足机器人的步态结构，实现感知驱动的深度强化学习控制。PGTT 使用三次Hermite样条对每条腿的相位进行编码，以适应局部地形高度统计并加入相位接触惩罚，使策略能够在关节空间直接操作，具备形态无关性，并在仿真和真实机器人（如Unitree Go2、ANYmal‑C）上验证。实验表明 PGTT 在推力扰动和离散障碍下的成功率显著提升，并且收敛速度约为基线的两倍。<br /><strong>Keywords:</strong> perceptive locomotion, reinforcement learning, phase-guided reward shaping, legged robots, terrain adaptation, morphology-agnostic control, heightmap, MuJoCo, real‑world transfer<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Alexandros Ntagkas, Chairi Kiourt, Konstantinos Chatzilygeroudis</div>
State-of-the-art perceptive Reinforcement Learning controllers for legged robots either (i) impose oscillator or IK-based gait priors that constrain the action space, add bias to the policy optimization and reduce adaptability across robot morphologies, or (ii) operate "blind", which struggle to anticipate hind-leg terrain, and are brittle to noise. In this paper, we propose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL approach that overcomes these limitations by enforcing gait structure purely through reward shaping, thereby reducing inductive bias in policy learning compared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase as a cubic Hermite spline that adapts swing height to local heightmap statistics and adds a swing- phase contact penalty, while the policy acts directly in joint space supporting morphology-agnostic deployment. Trained in MuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and domain randomization, PGTT achieves the highest success under push disturbances (median +7.5% vs. the next best method) and on discrete obstacles (+9%), with comparable velocity tracking, and converging to an effective policy roughly 2x faster than strong end-to-end baselines. We validate PGTT on a Unitree Go2 using a real-time LiDAR elevation-to-heightmap pipeline, and we report preliminary results on ANYmal-C obtained with the same hyperparameters. These findings indicate that terrain-adaptive, phase-guided reward shaping is a simple and general mechanism for robust perceptive locomotion across platforms.
<div><strong>Authors:</strong> Alexandros Ntagkas, Chairi Kiourt, Konstantinos Chatzilygeroudis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Phase-Guided Terrain Traversal (PGTT), a perception‑aware deep reinforcement learning method for legged robots that imposes gait structure via reward shaping rather than explicit gait priors. PGTT encodes per‑leg phase with a cubic Hermite spline to adapt swing height to local terrain statistics and adds a swing‑phase contact penalty, enabling morphology‑agnostic policies trained in simulation and transferred to real robots such as Unitree Go2 and ANYmal‑C. Experiments show PGTT achieves higher success rates under disturbances and obstacles while converging roughly twice as fast as strong baselines.", "summary_cn": "本文提出相位引导地形穿越（PGTT）方法，通过奖励塑形而非显式的步态先验来约束四足机器人的步态结构，实现感知驱动的深度强化学习控制。PGTT 使用三次Hermite样条对每条腿的相位进行编码，以适应局部地形高度统计并加入相位接触惩罚，使策略能够在关节空间直接操作，具备形态无关性，并在仿真和真实机器人（如Unitree Go2、ANYmal‑C）上验证。实验表明 PGTT 在推力扰动和离散障碍下的成功率显著提升，并且收敛速度约为基线的两倍。", "keywords": "perceptive locomotion, reinforcement learning, phase-guided reward shaping, legged robots, terrain adaptation, morphology-agnostic control, heightmap, MuJoCo, real‑world transfer", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Alexandros Ntagkas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"]}
]]></acme>

<pubDate>2025-10-21T07:00:18+00:00</pubDate>
</item>
<item>
<title>ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</title>
<link>https://papers.cool/arxiv/2510.18339</link>
<guid>https://papers.cool/arxiv/2510.18339</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how to adapt open-weight large language models to the electrocardiography domain by fine‑tuning on specialty literature and compares them against retrieval‑augmented generation (RAG) and a proprietary model (Claude Sonnet 3.7). Finetuned Llama 3.1 70B outperforms its base model on multiple-choice and automatic metrics, while Claude 3.7 and RAG are preferred by human experts for complex queries. The study highlights significant heterogeneity across evaluation methods and demonstrates that domain‑specific adaptation can achieve competitive performance while preserving privacy.<br /><strong>Summary (CN):</strong> 本文研究了如何通过在心电图专用文献上微调开源大语言模型来实现领域适配，并将其与检索增强生成 (RAG) 和商业模型 Claude Sonnet 3.7 进行比较。微调后的 Llama 3.1 70B 在多项选择题和自动指标上超越了基础模型，而在人类专家的复杂查询评估中 Claude 3.7 与 RAG 更受青睐。研究指出评估方法之间存在显著差异，并证明领域特化的微调和 RAG 可以在保持隐私的前提下实现与专有模型竞争的性能。<br /><strong>Keywords:</strong> electrocardiography, domain adaptation, large language models, fine-tuning, retrieval-augmented generation, medical AI, privacy, evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.
<div><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how to adapt open-weight large language models to the electrocardiography domain by fine‑tuning on specialty literature and compares them against retrieval‑augmented generation (RAG) and a proprietary model (Claude Sonnet 3.7). Finetuned Llama 3.1 70B outperforms its base model on multiple-choice and automatic metrics, while Claude 3.7 and RAG are preferred by human experts for complex queries. The study highlights significant heterogeneity across evaluation methods and demonstrates that domain‑specific adaptation can achieve competitive performance while preserving privacy.", "summary_cn": "本文研究了如何通过在心电图专用文献上微调开源大语言模型来实现领域适配，并将其与检索增强生成 (RAG) 和商业模型 Claude Sonnet 3.7 进行比较。微调后的 Llama 3.1 70B 在多项选择题和自动指标上超越了基础模型，而在人类专家的复杂查询评估中 Claude 3.7 与 RAG 更受青睐。研究指出评估方法之间存在显著差异，并证明领域特化的微调和 RAG 可以在保持隐私的前提下实现与专有模型竞争的性能。", "keywords": "electrocardiography, domain adaptation, large language models, fine-tuning, retrieval-augmented generation, medical AI, privacy, evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lara Ahrens", "Wilhelm Haverkamp", "Nils Strodthoff"]}
]]></acme>

<pubDate>2025-10-21T06:45:38+00:00</pubDate>
</item>
<item>
<title>Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning</title>
<link>https://papers.cool/arxiv/2510.18332</link>
<guid>https://papers.cool/arxiv/2510.18332</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines an "inhomogeneity parameter" that quantifies the extent to which a training set requires an inhomogeneous correlation structure in the target function. It shows how to compute this parameter for various datasets and proves that, within Gaussian‑process regression, a non‑zero inhomogeneity parameter forces the use of a non‑stationary kernel, affecting prediction quality and reliability on test inputs.<br /><strong>Summary (CN):</strong> 本文提出了“inhomogeneity parameter”（不均匀性参数），用于量化训练集是否需要在目标函数中呈现不均匀的相关结构，并提供了在不同数据集上计算该参数的方法。文中证明，在基于高斯过程的回归框架中，若不均匀性参数为非零，则必须使用非平稳核函数，这会影响模型在测试输入上的预测质量和可靠性。<br /><strong>Keywords:</strong> inhomogeneity parameter, non-stationary Gaussian process, dataset heterogeneity, supervised learning, prediction reliability, training set analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Gargi Roy, Dalia Chakrabarty</div>
We introduce parametrisation of that property of the available training dataset, that necessitates an inhomogeneous correlation structure for the function that is learnt as a model of the relationship between the pair of variables, observations of which comprise the considered training data. We refer to a parametrisation of this property of a given training set, as its ``inhomogeneity parameter''. It is easy to compute this parameter for small-to-large datasets, and we demonstrate such computation on multiple publicly-available datasets, while also demonstrating that conventional ``non-stationarity'' of data does not imply a non-zero inhomogeneity parameter of the dataset. We prove that - within the probabilistic Gaussian Process-based learning approach - a training set with a non-zero inhomogeneity parameter renders it imperative, that the process that is invoked to model the sought function, be non-stationary. Following the learning of a real-world multivariate function with such a Process, quality and reliability of predictions at test inputs, are demonstrated to be affected by the inhomogeneity parameter of the training data.
<div><strong>Authors:</strong> Gargi Roy, Dalia Chakrabarty</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines an \"inhomogeneity parameter\" that quantifies the extent to which a training set requires an inhomogeneous correlation structure in the target function. It shows how to compute this parameter for various datasets and proves that, within Gaussian‑process regression, a non‑zero inhomogeneity parameter forces the use of a non‑stationary kernel, affecting prediction quality and reliability on test inputs.", "summary_cn": "本文提出了“inhomogeneity parameter”（不均匀性参数），用于量化训练集是否需要在目标函数中呈现不均匀的相关结构，并提供了在不同数据集上计算该参数的方法。文中证明，在基于高斯过程的回归框架中，若不均匀性参数为非零，则必须使用非平稳核函数，这会影响模型在测试输入上的预测质量和可靠性。", "keywords": "inhomogeneity parameter, non-stationary Gaussian process, dataset heterogeneity, supervised learning, prediction reliability, training set analysis", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Gargi Roy", "Dalia Chakrabarty"]}
]]></acme>

<pubDate>2025-10-21T06:34:22+00:00</pubDate>
</item>
<item>
<title>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</title>
<link>https://papers.cool/arxiv/2510.18316</link>
<guid>https://papers.cool/arxiv/2510.18316</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MoMaGen, a constrained‑optimization framework that automatically generates large, diverse demonstration datasets for multi‑step bimanual mobile manipulation. By enforcing hard constraints such as reachability and soft constraints like camera visibility, MoMaGen enables training imitation‑learning policies from a single human demonstration and requires only a few real‑world fine‑tuning trajectories for deployment on physical robots. Experiments on four tasks show higher dataset diversity than prior static‑bimanual methods and successful transfer to real hardware.<br /><strong>Summary (CN):</strong> 本文提出 MoMaGen，一种基于约束优化的自动化演示数据生成框架，用于多步骤双臂移动操作任务。该框架在确保可达性（hard constraint）等硬约束的同时，平衡相机可视性（soft constraint）等软约束，从单一人为示例生成多样化数据，并只需少量真实演示即可微调出可在实体机器人上部署的模仿学习策略。实验在四个任务上展示了较以往静态双臂方法更高的数据多样性以及成功的实际机器人迁移。<br /><strong>Keywords:</strong> constrained optimization, data generation, imitation learning, bimanual manipulation, mobile robotics, demonstration synthesis, reachability constraints, camera visibility, robot learning, dataset diversity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
Imitation learning from large-scale, diverse human demonstrations has proven effective for training robots, but collecting such data is costly and time-consuming. This challenge is amplified for multi-step bimanual mobile manipulation, where humans must teleoperate both a mobile base and two high-degree-of-freedom arms. Prior automated data generation frameworks have addressed static bimanual manipulation by augmenting a few human demonstrations in simulation, but they fall short for mobile settings due to two key challenges: (1) determining base placement to ensure reachability, and (2) positioning the camera to provide sufficient visibility for visuomotor policies. To address these issues, we introduce MoMaGen, which formulates data generation as a constrained optimization problem that enforces hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility during navigation). This formulation generalizes prior approaches and provides a principled foundation for future methods. We evaluate MoMaGen on four multi-step bimanual mobile manipulation tasks and show that it generates significantly more diverse datasets than existing methods. Leveraging this diversity, MoMaGen can train successful imitation learning policies from a single source demonstration, and these policies can be fine-tuned with as few as 40 real-world demonstrations to achieve deployment on physical robotic hardware. More details are available at our project page: momagen.github.io.
<div><strong>Authors:</strong> Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MoMaGen, a constrained‑optimization framework that automatically generates large, diverse demonstration datasets for multi‑step bimanual mobile manipulation. By enforcing hard constraints such as reachability and soft constraints like camera visibility, MoMaGen enables training imitation‑learning policies from a single human demonstration and requires only a few real‑world fine‑tuning trajectories for deployment on physical robots. Experiments on four tasks show higher dataset diversity than prior static‑bimanual methods and successful transfer to real hardware.", "summary_cn": "本文提出 MoMaGen，一种基于约束优化的自动化演示数据生成框架，用于多步骤双臂移动操作任务。该框架在确保可达性（hard constraint）等硬约束的同时，平衡相机可视性（soft constraint）等软约束，从单一人为示例生成多样化数据，并只需少量真实演示即可微调出可在实体机器人上部署的模仿学习策略。实验在四个任务上展示了较以往静态双臂方法更高的数据多样性以及成功的实际机器人迁移。", "keywords": "constrained optimization, data generation, imitation learning, bimanual manipulation, mobile robotics, demonstration synthesis, reachability constraints, camera visibility, robot learning, dataset diversity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chengshu Li", "Mengdi Xu", "Arpit Bahety", "Hang Yin", "Yunfan Jiang", "Huang Huang", "Josiah Wong", "Sujay Garlanka", "Cem Gokmen", "Ruohan Zhang", "Weiyu Liu", "Jiajun Wu", "Roberto Martín-Martín", "Li Fei-Fei"]}
]]></acme>

<pubDate>2025-10-21T05:56:47+00:00</pubDate>
</item>
<item>
<title>A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces</title>
<link>https://papers.cool/arxiv/2510.18300</link>
<guid>https://papers.cool/arxiv/2510.18300</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a distributed, end‑to‑end framework that partitions large GPU execution traces and processes them in parallel using causal graph methods to expose performance variability and dependencies. Experimental evaluation shows a 67% scalability improvement when analyzing multiple traces simultaneously, demonstrating the system’s effectiveness for high‑performance computing environments.<br /><strong>Summary (CN):</strong> 本文提出了一种分布式端到端框架，通过对大型 GPU 执行轨迹进行划分并并行处理，利用因果图方法揭示性能波动和执行流之间的依赖关系。实验结果显示，在同时分析多个轨迹时，系统的可扩展性提升了 67%，验证了其在高性能计算环境中的有效性。<br /><strong>Keywords:</strong> causal modeling, GPU performance variability, distributed framework, parallel trace analysis, HPC, causal graphs, scalability, performance bottlenecks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard</div>
Large-scale GPU traces play a critical role in identifying performance bottlenecks within heterogeneous High-Performance Computing (HPC) architectures. However, the sheer volume and complexity of a single trace of data make performance analysis both computationally expensive and time-consuming. To address this challenge, we present an end-to-end parallel performance analysis framework designed to handle multiple large-scale GPU traces efficiently. Our proposed framework partitions and processes trace data concurrently and employs causal graph methods and parallel coordinating chart to expose performance variability and dependencies across execution flows. Experimental results demonstrate a 67% improvement in terms of scalability, highlighting the effectiveness of our pipeline for analyzing multiple traces independently.
<div><strong>Authors:</strong> Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a distributed, end‑to‑end framework that partitions large GPU execution traces and processes them in parallel using causal graph methods to expose performance variability and dependencies. Experimental evaluation shows a 67% scalability improvement when analyzing multiple traces simultaneously, demonstrating the system’s effectiveness for high‑performance computing environments.", "summary_cn": "本文提出了一种分布式端到端框架，通过对大型 GPU 执行轨迹进行划分并并行处理，利用因果图方法揭示性能波动和执行流之间的依赖关系。实验结果显示，在同时分析多个轨迹时，系统的可扩展性提升了 67%，验证了其在高性能计算环境中的有效性。", "keywords": "causal modeling, GPU performance variability, distributed framework, parallel trace analysis, HPC, causal graphs, scalability, performance bottlenecks", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"]}
]]></acme>

<pubDate>2025-10-21T05:11:29+00:00</pubDate>
</item>
<item>
<title>Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models</title>
<link>https://papers.cool/arxiv/2510.18287</link>
<guid>https://papers.cool/arxiv/2510.18287</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an efficient few-shot method for identity‑preserving attribute editing in 3D‑aware generative face models, requiring only ten or fewer labeled images to estimate latent space edit directions. By leveraging existing face datasets with masks and the 2D Attribute Style Manipulation (ASM) technique, the authors demonstrate linear and continuous edits across poses, including one‑shot stylization and face aging. Experimental results show that the estimated directions enable photorealistic, view‑consistent edits while maintaining the subject's identity.<br /><strong>Summary (CN):</strong> 本文提出一种高效的少样本方法，对 3D 感知生成模型中的人脸进行身份保持的属性编辑，仅需十张或更少的标记图像即可估计潜在空间的编辑方向。通过利用带有掩码的人脸数据集以及二维属性风格操作（ASM）技术，展示了跨姿态的线性连续编辑，包括一次性风格化和人脸老化。实验表明，该方法能够实现真实感、视角一致的编辑，同时保持主体身份。<br /><strong>Keywords:</strong> few-shot editing, identity preservation, 3D-aware generative models, latent space directions, facial attribute manipulation, style manifold, ASM, synthetic data<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other<br /><strong>Authors:</strong> Vishal Vinod</div>
Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving face aging. Code and results are available at: https://vishal-vinod.github.io/gmpi-edit/
<div><strong>Authors:</strong> Vishal Vinod</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an efficient few-shot method for identity‑preserving attribute editing in 3D‑aware generative face models, requiring only ten or fewer labeled images to estimate latent space edit directions. By leveraging existing face datasets with masks and the 2D Attribute Style Manipulation (ASM) technique, the authors demonstrate linear and continuous edits across poses, including one‑shot stylization and face aging. Experimental results show that the estimated directions enable photorealistic, view‑consistent edits while maintaining the subject's identity.", "summary_cn": "本文提出一种高效的少样本方法，对 3D 感知生成模型中的人脸进行身份保持的属性编辑，仅需十张或更少的标记图像即可估计潜在空间的编辑方向。通过利用带有掩码的人脸数据集以及二维属性风格操作（ASM）技术，展示了跨姿态的线性连续编辑，包括一次性风格化和人脸老化。实验表明，该方法能够实现真实感、视角一致的编辑，同时保持主体身份。", "keywords": "few-shot editing, identity preservation, 3D-aware generative models, latent space directions, facial attribute manipulation, style manifold, ASM, synthetic data", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}, "authors": ["Vishal Vinod"]}
]]></acme>

<pubDate>2025-10-21T04:27:46+00:00</pubDate>
</item>
<item>
<title>SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws</title>
<link>https://papers.cool/arxiv/2510.18266</link>
<guid>https://papers.cool/arxiv/2510.18266</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents SPIKE, a Stable Physics-Informed Kernel Evolution method that solves inviscid hyperbolic conservation laws by minimizing strong-form residuals while still capturing weak solutions with discontinuities. Using reproducing kernel representations and Tikhonov regularization, SPIKE smoothly transitions through shock formation without explicit shock detection or artificial viscosity, preserving conservation and satisfying Rankine‑Hugoniot conditions. Experiments on scalar and vector-valued conservation laws demonstrate the method’s effectiveness.<br /><strong>Summary (CN):</strong> 本文提出了 SPIKE（Stable Physics-Informed Kernel Evolution）方法，用于求解无粘性双曲守恒律，通过最小化强形式残差仍能捕获包含不连续性的弱解。该方法利用再现核（reproducing kernel）表示并通过 Tikhonov 正则化平滑地跨越冲击形成过程，无需显式冲击检测或人工粘性，自动保持守恒并满足 Rankine‑Hugoniot 条件。对标量及向量守恒律的数值实验验证了该方法的有效性。<br /><strong>Keywords:</strong> hyperbolic conservation laws, physics-informed kernel methods, shock capturing, Tikhonov regularization, Rankine-Hugoniot conditions, reproducing kernels, numerical PDEs<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hua Su, Lei Zhang, Jin Zhao</div>
We introduce the Stable Physics-Informed Kernel Evolution (SPIKE) method for numerical computation of inviscid hyperbolic conservation laws. SPIKE resolves a fundamental paradox: how strong-form residual minimization can capture weak solutions containing discontinuities. SPIKE employs reproducing kernel representations with regularized parameter evolution, where Tikhonov regularization provides a smooth transition mechanism through shock formation, allowing the dynamics to traverse shock singularities. This approach automatically maintains conservation, tracks characteristics, and captures shocks satisfying Rankine-Hugoniot conditions within a unified framework requiring no explicit shock detection or artificial viscosity. Numerical validation across scalar and vector-valued conservation laws confirms the method's effectiveness.
<div><strong>Authors:</strong> Hua Su, Lei Zhang, Jin Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents SPIKE, a Stable Physics-Informed Kernel Evolution method that solves inviscid hyperbolic conservation laws by minimizing strong-form residuals while still capturing weak solutions with discontinuities. Using reproducing kernel representations and Tikhonov regularization, SPIKE smoothly transitions through shock formation without explicit shock detection or artificial viscosity, preserving conservation and satisfying Rankine‑Hugoniot conditions. Experiments on scalar and vector-valued conservation laws demonstrate the method’s effectiveness.", "summary_cn": "本文提出了 SPIKE（Stable Physics-Informed Kernel Evolution）方法，用于求解无粘性双曲守恒律，通过最小化强形式残差仍能捕获包含不连续性的弱解。该方法利用再现核（reproducing kernel）表示并通过 Tikhonov 正则化平滑地跨越冲击形成过程，无需显式冲击检测或人工粘性，自动保持守恒并满足 Rankine‑Hugoniot 条件。对标量及向量守恒律的数值实验验证了该方法的有效性。", "keywords": "hyperbolic conservation laws, physics-informed kernel methods, shock capturing, Tikhonov regularization, Rankine-Hugoniot conditions, reproducing kernels, numerical PDEs", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hua Su", "Lei Zhang", "Jin Zhao"]}
]]></acme>

<pubDate>2025-10-21T03:34:49+00:00</pubDate>
</item>
<item>
<title>Learning under Quantization for High-Dimensional Linear Regression</title>
<link>https://papers.cool/arxiv/2510.18259</link>
<guid>https://papers.cool/arxiv/2510.18259</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides the first rigorous theoretical analysis of low‑bit quantization effects on high‑dimensional linear regression trained with stochastic gradient descent, deriving algorithm‑ and data‑dependent excess risk bounds for quantizing data, labels, parameters, activations, and gradients. It shows that multiplicative (input‑dependent) quantization avoids spectral distortion of the data, while additive (fixed‑step) quantization yields a beneficial batch‑size scaling effect, and quantitatively compares these regimes for common polynomial‑decay spectra. These results illuminate how practical quantization strategies influence learning dynamics under hardware constraints.<br /><strong>Summary (CN):</strong> 本文首次对低位量化在高维线性回归中的影响进行严格理论分析，针对 SGD 训练过程中的数据、标签、参数、激活和梯度量化给出算法与数据相关的过剩风险上界。研究表明，乘性（输入相关）量化可以消除数据谱的失真，而加性（固定步长）量化则会产生随批量大小提升的有益尺度效应，并对常见的多项式衰减谱进行定量比较。这些结果阐明了实际硬件约束下量化策略对学习动态的影响。<br /><strong>Keywords:</strong> quantization, high-dimensional linear regression, stochastic gradient descent, excess risk, additive quantization, multiplicative quantization, hardware constraints<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dechen Zhang, Junwei Su, Difan Zou</div>
The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simplest linear regression setting. We present the first systematic theoretical study of this fundamental question, analyzing finite-step stochastic gradient descent (SGD) for high-dimensional linear regression under a comprehensive range of quantization targets: data, labels, parameters, activations, and gradients. Our novel analytical framework establishes precise algorithm-dependent and data-dependent excess risk bounds that characterize how different quantization affects learning: parameter, activation, and gradient quantization amplify noise during training; data quantization distorts the data spectrum; and data and label quantization introduce additional approximation and quantized error. Crucially, we prove that for multiplicative quantization (with input-dependent quantization step), this spectral distortion can be eliminated, and for additive quantization (with constant quantization step), a beneficial scaling effect with batch size emerges. Furthermore, for common polynomial-decay data spectra, we quantitatively compare the risks of multiplicative and additive quantization, drawing a parallel to the comparison between FP and integer quantization methods. Our theory provides a powerful lens to characterize how quantization shapes the learning dynamics of optimization algorithms, paving the way to further explore learning theory under practical hardware constraints.
<div><strong>Authors:</strong> Dechen Zhang, Junwei Su, Difan Zou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides the first rigorous theoretical analysis of low‑bit quantization effects on high‑dimensional linear regression trained with stochastic gradient descent, deriving algorithm‑ and data‑dependent excess risk bounds for quantizing data, labels, parameters, activations, and gradients. It shows that multiplicative (input‑dependent) quantization avoids spectral distortion of the data, while additive (fixed‑step) quantization yields a beneficial batch‑size scaling effect, and quantitatively compares these regimes for common polynomial‑decay spectra. These results illuminate how practical quantization strategies influence learning dynamics under hardware constraints.", "summary_cn": "本文首次对低位量化在高维线性回归中的影响进行严格理论分析，针对 SGD 训练过程中的数据、标签、参数、激活和梯度量化给出算法与数据相关的过剩风险上界。研究表明，乘性（输入相关）量化可以消除数据谱的失真，而加性（固定步长）量化则会产生随批量大小提升的有益尺度效应，并对常见的多项式衰减谱进行定量比较。这些结果阐明了实际硬件约束下量化策略对学习动态的影响。", "keywords": "quantization, high-dimensional linear regression, stochastic gradient descent, excess risk, additive quantization, multiplicative quantization, hardware constraints", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dechen Zhang", "Junwei Su", "Difan Zou"]}
]]></acme>

<pubDate>2025-10-21T03:30:11+00:00</pubDate>
</item>
<item>
<title>Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning</title>
<link>https://papers.cool/arxiv/2510.18254</link>
<guid>https://papers.cool/arxiv/2510.18254</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether the "reflection" exhibited by frontier large language models functions like human reflective reasoning by testing them on an open‑ended, rule‑constrained task of generating scientific test items and revising them after self‑critique. Results show poor first‑pass performance and only modest gains after reflection, with models often repeating the same constraint violations, indicating that current reflective mechanisms lack goal‑driven error detection and principled repair. The authors conclude that reliable constraint adherence requires external structure rather than relying on internal reflection.<br /><strong>Summary (CN):</strong> 本文研究了前沿大语言模型的“反思”是否具有人类式的反思推理能力，采用一个开放式且受规则限制的任务：生成科学测试题并在自我批评后进行修正。实验发现模型首次生成表现差，反思后提升有限且常重复相同的约束违规，表明当前的反思机制缺乏目标驱动的错误检测和原则性修复。作者认为，要实现可靠的约束遵守仍需外部结构的强制，而不能仅依赖模型内部的反思。<br /><strong>Keywords:</strong> reflective reasoning, large language models, self-correction, open-ended tasks, constraint violation, mechanistic interpretability, AI safety, evaluation<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Sion Weatherhead, Flora Salim, Aaron Belbasis</div>
Humans do not just find mistakes after the fact -- we often catch them mid-stream because 'reflection' is tied to the goal and its constraints. Today's large language models produce reasoning tokens and 'reflective' text, but is it functionally equivalent with human reflective reasoning? Prior work on closed-ended tasks -- with clear, external 'correctness' signals -- can make 'reflection' look effective while masking limits in self-correction. We therefore test eight frontier models on a simple, real-world task that is open-ended yet rule-constrained, with auditable success criteria: to produce valid scientific test items, then revise after considering their own critique. First-pass performance is poor (often zero valid items out of 4 required; mean $\approx$ 1), and reflection yields only modest gains (also $\approx$ 1). Crucially, the second attempt frequently repeats the same violation of constraint, indicating 'corrective gains' arise largely from chance production of a valid item rather than error detection and principled, constraint-sensitive repair. Performance before and after reflection deteriorates as open-endedness increases, and models marketed for 'reasoning' show no advantage. Our results suggest that current LLM 'reflection' lacks functional evidence of the active, goal-driven monitoring that helps humans respect constraints even on a first pass. Until such mechanisms are instantiated in the model itself, reliable performance requires external structure that enforces constraints.
<div><strong>Authors:</strong> Sion Weatherhead, Flora Salim, Aaron Belbasis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether the \"reflection\" exhibited by frontier large language models functions like human reflective reasoning by testing them on an open‑ended, rule‑constrained task of generating scientific test items and revising them after self‑critique. Results show poor first‑pass performance and only modest gains after reflection, with models often repeating the same constraint violations, indicating that current reflective mechanisms lack goal‑driven error detection and principled repair. The authors conclude that reliable constraint adherence requires external structure rather than relying on internal reflection.", "summary_cn": "本文研究了前沿大语言模型的“反思”是否具有人类式的反思推理能力，采用一个开放式且受规则限制的任务：生成科学测试题并在自我批评后进行修正。实验发现模型首次生成表现差，反思后提升有限且常重复相同的约束违规，表明当前的反思机制缺乏目标驱动的错误检测和原则性修复。作者认为，要实现可靠的约束遵守仍需外部结构的强制，而不能仅依赖模型内部的反思。", "keywords": "reflective reasoning, large language models, self-correction, open-ended tasks, constraint violation, mechanistic interpretability, AI safety, evaluation", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"]}
]]></acme>

<pubDate>2025-10-21T03:24:21+00:00</pubDate>
</item>
<item>
<title>Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN</title>
<link>https://papers.cool/arxiv/2510.18252</link>
<guid>https://papers.cool/arxiv/2510.18252</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates different synthetic oversampling ratios using SMOTE, BorderlineSMOTE, and ADASYN on the Give Me Some Credit dataset, training XGBoost models and assessing performance via AUC and Gini. It finds that ADASYN with a 1x multiplication (doubling the minority class) yields the best results, corresponding to an optimal majority‑to‑minority ratio of about 6.6:1, while higher oversampling harms performance. This demonstrates a diminishing‑returns effect and provides a reproducible framework for determining optimal augmentation ratios in imbalanced credit scoring.<br /><strong>Summary (CN):</strong> 本文在 Give Me Some Credit 数据集上系统评估了 SMOTE、BorderlineSMOTE 和 ADASYN 等合成过采样比例，使用 XGBoost 进行建模并通过 AUC 和 Gini 指标评估性能。研究发现，ADASYN 的 1 倍倍率（即将少数类样本数量翻倍）实现了最佳效果，对应的多数类:少数类比例约为 6.6:1，且更高的过采样倍率会导致性能下降。该结果揭示了合成过样的递减收益现象，并为在不平衡信用评分场景中确定最佳数据增强比例提供了可复现的框架。<br /><strong>Keywords:</strong> credit scoring, class imbalance, data augmentation, ADASYN, SMOTE, XGBoost, oversampling ratio, bootstrap evaluation, AUC, Gini<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Luis H. Chia</div>
Credit scoring models face a critical challenge: severe class imbalance, with default rates typically below 10%, which hampers model learning and predictive performance. While synthetic data augmentation techniques such as SMOTE and ADASYN have been proposed to address this issue, the optimal augmentation ratio remains unclear, with practitioners often defaulting to full balancing (1:1 ratio) without empirical justification. This study systematically evaluates 10 data augmentation scenarios using the Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x, 3x). All models were trained using XGBoost and evaluated on a held-out test set of 29,173 real observations. Statistical significance was assessed using bootstrap testing with 1,000 iterations. Key findings reveal that ADASYN with 1x multiplication (doubling the minority class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of 0.3557, representing statistically significant improvements of +0.77% and +3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors (2x and 3x) resulted in performance degradation, with 3x showing a -0.48% decrease in AUC, suggesting a "law of diminishing returns" for synthetic oversampling. The optimal class imbalance ratio was found to be 6.6:1 (majority:minority), contradicting the common practice of balancing to 1:1. This work provides the first empirical evidence of an optimal "sweet spot" for data augmentation in credit scoring, with practical guidelines for industry practitioners and researchers working with imbalanced datasets. While demonstrated on a single representative dataset, the methodology provides a reproducible framework for determining optimal augmentation ratios in other imbalanced domains.
<div><strong>Authors:</strong> Luis H. Chia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates different synthetic oversampling ratios using SMOTE, BorderlineSMOTE, and ADASYN on the Give Me Some Credit dataset, training XGBoost models and assessing performance via AUC and Gini. It finds that ADASYN with a 1x multiplication (doubling the minority class) yields the best results, corresponding to an optimal majority‑to‑minority ratio of about 6.6:1, while higher oversampling harms performance. This demonstrates a diminishing‑returns effect and provides a reproducible framework for determining optimal augmentation ratios in imbalanced credit scoring.", "summary_cn": "本文在 Give Me Some Credit 数据集上系统评估了 SMOTE、BorderlineSMOTE 和 ADASYN 等合成过采样比例，使用 XGBoost 进行建模并通过 AUC 和 Gini 指标评估性能。研究发现，ADASYN 的 1 倍倍率（即将少数类样本数量翻倍）实现了最佳效果，对应的多数类:少数类比例约为 6.6:1，且更高的过采样倍率会导致性能下降。该结果揭示了合成过样的递减收益现象，并为在不平衡信用评分场景中确定最佳数据增强比例提供了可复现的框架。", "keywords": "credit scoring, class imbalance, data augmentation, ADASYN, SMOTE, XGBoost, oversampling ratio, bootstrap evaluation, AUC, Gini", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Luis H. Chia"]}
]]></acme>

<pubDate>2025-10-21T03:22:43+00:00</pubDate>
</item>
<item>
<title>LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling</title>
<link>https://papers.cool/arxiv/2510.18239</link>
<guid>https://papers.cool/arxiv/2510.18239</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces LIME, a novel recommendation architecture that decouples user‑candidate interactions via low‑rank link embeddings and employs a linear XOR‑based attention mechanism, reducing inference cost from quadratic to linear in sequence length and making it nearly independent of candidate set size. Experiments on public and industrial datasets demonstrate near‑state‑of‑the‑art accuracy with up to 10× speedup, and deployment on a large platform shows improved user engagement with minimal inference overhead.<br /><strong>Summary (CN):</strong> 本文提出 LIME，一种新颖的推荐系统架构，通过低秩“link embeddings”实现用户与候选项交互的预计算，并使用线性 XOR 注意力机制将对用户序列长度的复杂度从二次降至线性，使推理成本几乎与候选集规模无关。公共和工业数据集实验表明，在保持接近最先进准确率的同时，推理速度提升最高可达 10 倍；在大型推荐平台的实际部署也显示出用户互动提升且推理开销最小。<br /><strong>Keywords:</strong> recommendation systems, efficient inference, linear attention, XOR attention, link embeddings, large-scale recommendation, decoupled attention, scaling, user-item interaction, LIME<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yunjiang Jiang, Ayush Agarwal, Yang Liu, Bi Xue</div>
Scaling large recommendation systems requires advancing three major frontiers: processing longer user histories, expanding candidate sets, and increasing model capacity. While promising, transformers' computational cost scales quadratically with the user sequence length and linearly with the number of candidates. This trade-off makes it prohibitively expensive to expand candidate sets or increase sequence length at inference, despite the significant performance improvements. We introduce \textbf{LIME}, a novel architecture that resolves this trade-off. Through two key innovations, LIME fundamentally reduces computational complexity. First, low-rank ``link embeddings" enable pre-computation of attention weights by decoupling user and candidate interactions, making the inference cost nearly independent of candidate set size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the complexity with respect to user sequence length from quadratic ($O(N^2)$) to linear ($O(N)$). Experiments on public and industrial datasets show LIME achieves near-parity with state-of-the-art transformers but with a 10$\times$ inference speedup on large candidate sets or long sequence lengths. When tested on a major recommendation platform, LIME improved user engagement while maintaining minimal inference costs with respect to candidate set size and user history length, establishing a new paradigm for efficient and expressive recommendation systems.
<div><strong>Authors:</strong> Yunjiang Jiang, Ayush Agarwal, Yang Liu, Bi Xue</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces LIME, a novel recommendation architecture that decouples user‑candidate interactions via low‑rank link embeddings and employs a linear XOR‑based attention mechanism, reducing inference cost from quadratic to linear in sequence length and making it nearly independent of candidate set size. Experiments on public and industrial datasets demonstrate near‑state‑of‑the‑art accuracy with up to 10× speedup, and deployment on a large platform shows improved user engagement with minimal inference overhead.", "summary_cn": "本文提出 LIME，一种新颖的推荐系统架构，通过低秩“link embeddings”实现用户与候选项交互的预计算，并使用线性 XOR 注意力机制将对用户序列长度的复杂度从二次降至线性，使推理成本几乎与候选集规模无关。公共和工业数据集实验表明，在保持接近最先进准确率的同时，推理速度提升最高可达 10 倍；在大型推荐平台的实际部署也显示出用户互动提升且推理开销最小。", "keywords": "recommendation systems, efficient inference, linear attention, XOR attention, link embeddings, large-scale recommendation, decoupled attention, scaling, user-item interaction, LIME", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yunjiang Jiang", "Ayush Agarwal", "Yang Liu", "Bi Xue"]}
]]></acme>

<pubDate>2025-10-21T02:53:17+00:00</pubDate>
</item>
<item>
<title>The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective</title>
<link>https://papers.cool/arxiv/2510.18215</link>
<guid>https://papers.cool/arxiv/2510.18215</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies the bias-variance tradeoff among Sample Average Approximation (SAA), Estimate-Then-Optimize (ETO), and Integrated Estimation-Optimization (IEO) when the model is locally misspecified, i.e., nearly well‑specified. Using contiguity theory, it derives explicit formulas for decision bias and variance, revealing how the relative importance of bias and variance depends on the degree and direction of misspecification. These results provide a finer-grained understanding of when model‑based approaches outperform or underperform SAA under slight misspecification.<br /><strong>Summary (CN):</strong> 本文研究在局部失配（即模型几乎是良好指定）情形下，样本平均近似（SAA）、先估计再优化（ETO）以及集成估计‑优化（IEO）之间的偏差‑方差权衡。利用统计学中的相邻性理论，作者推导出决策偏差和方差的显式表达式，揭示偏差和方差的相对重要性随失配程度和方向的变化而变化。这些结果为在轻度失配情况下模型‑基方法相较于 SAA 的优劣提供了更细致的理解。<br /><strong>Keywords:</strong> bias-variance tradeoff, data-driven stochastic optimization, local misspecification, sample average approximation, estimate-then-optimize, integrated estimation-optimization, contiguity theory<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Haixiang Lan, Luofeng Liao, Adam N. Elmachtoub, Christian Kroer, Henry Lam, Haofeng Zhang</div>
Data-driven stochastic optimization is ubiquitous in machine learning and operational decision-making problems. Sample average approximation (SAA) and model-based approaches such as estimate-then-optimize (ETO) or integrated estimation-optimization (IEO) are all popular, with model-based approaches being able to circumvent some of the issues with SAA in complex context-dependent problems. Yet the relative performance of these methods is poorly understood, with most results confined to the dichotomous cases of the model-based approach being either well-specified or misspecified. We develop the first results that allow for a more granular analysis of the relative performance of these methods under a local misspecification setting, which models the scenario where the model-based approach is nearly well-specified. By leveraging tools from contiguity theory in statistics, we show that there is a bias-variance tradeoff between SAA, IEO, and ETO under local misspecification, and that the relative importance of the bias and the variance depends on the degree of local misspecification. Moreover, we derive explicit expressions for the decision bias, which allows us to characterize (un)impactful misspecification directions, and provide further geometric understanding of the variance.
<div><strong>Authors:</strong> Haixiang Lan, Luofeng Liao, Adam N. Elmachtoub, Christian Kroer, Henry Lam, Haofeng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies the bias-variance tradeoff among Sample Average Approximation (SAA), Estimate-Then-Optimize (ETO), and Integrated Estimation-Optimization (IEO) when the model is locally misspecified, i.e., nearly well‑specified. Using contiguity theory, it derives explicit formulas for decision bias and variance, revealing how the relative importance of bias and variance depends on the degree and direction of misspecification. These results provide a finer-grained understanding of when model‑based approaches outperform or underperform SAA under slight misspecification.", "summary_cn": "本文研究在局部失配（即模型几乎是良好指定）情形下，样本平均近似（SAA）、先估计再优化（ETO）以及集成估计‑优化（IEO）之间的偏差‑方差权衡。利用统计学中的相邻性理论，作者推导出决策偏差和方差的显式表达式，揭示偏差和方差的相对重要性随失配程度和方向的变化而变化。这些结果为在轻度失配情况下模型‑基方法相较于 SAA 的优劣提供了更细致的理解。", "keywords": "bias-variance tradeoff, data-driven stochastic optimization, local misspecification, sample average approximation, estimate-then-optimize, integrated estimation-optimization, contiguity theory", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Haixiang Lan", "Luofeng Liao", "Adam N. Elmachtoub", "Christian Kroer", "Henry Lam", "Haofeng Zhang"]}
]]></acme>

<pubDate>2025-10-21T01:35:50+00:00</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18214</link>
<guid>https://papers.cool/arxiv/2510.18214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Language Safety Understanding (VLSU), a framework that evaluates multimodal foundation models by probing fine‑grained safety severity and combinatorial image‑text interactions across 17 safety patterns, revealing that joint reasoning performance drops dramatically compared to unimodal signals. Benchmarking eleven state‑of‑the‑art models on 8,187 annotated samples shows systematic failures in compositional safety judgment and a trade‑off between over‑blocking borderline content and under‑refusing clearly unsafe content. VLSU thus highlights critical alignment gaps and provides a testbed for improving robust vision‑language safety.<br /><strong>Summary (CN):</strong> 本文提出了视觉语言安全理解（VLSU）框架，通过对 17 种安全模式的细粒度严重程度分类和图文组合分析，系统评估多模态基础模型的安全性，发现模型在需要联合图像‑文本推理时表现大幅下降。对 8,187 条人工标注样本的十余种最先进模型评估显示，尽管单模态安全信号辨识准确率超 90%，但在联合理解上准确率仅为 20%~55%，且在边缘案例的阻断与拒绝之间存在显著权衡。VLSU 揭示了关键的对齐缺口，并提供了提升稳健视觉‑语言安全的基准测试平台。<br /><strong>Keywords:</strong> multimodal safety, vision-language, compositional reasoning, benchmark, safety classification, joint understanding, alignment evaluation<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 9, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
<div><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Language Safety Understanding (VLSU), a framework that evaluates multimodal foundation models by probing fine‑grained safety severity and combinatorial image‑text interactions across 17 safety patterns, revealing that joint reasoning performance drops dramatically compared to unimodal signals. Benchmarking eleven state‑of‑the‑art models on 8,187 annotated samples shows systematic failures in compositional safety judgment and a trade‑off between over‑blocking borderline content and under‑refusing clearly unsafe content. VLSU thus highlights critical alignment gaps and provides a testbed for improving robust vision‑language safety.", "summary_cn": "本文提出了视觉语言安全理解（VLSU）框架，通过对 17 种安全模式的细粒度严重程度分类和图文组合分析，系统评估多模态基础模型的安全性，发现模型在需要联合图像‑文本推理时表现大幅下降。对 8,187 条人工标注样本的十余种最先进模型评估显示，尽管单模态安全信号辨识准确率超 90%，但在联合理解上准确率仅为 20%~55%，且在边缘案例的阻断与拒绝之间存在显著权衡。VLSU 揭示了关键的对齐缺口，并提供了提升稳健视觉‑语言安全的基准测试平台。", "keywords": "multimodal safety, vision-language, compositional reasoning, benchmark, safety classification, joint understanding, alignment evaluation", "scoring": {"interpretability": 4, "understanding": 7, "safety": 9, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}
]]></acme>

<pubDate>2025-10-21T01:30:31+00:00</pubDate>
</item>
<item>
<title>A Definition of AGI</title>
<link>https://papers.cool/arxiv/2510.18212</link>
<guid>https://papers.cool/arxiv/2510.18212</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a quantifiable framework for defining Artificial General Intelligence (AGI) by aligning it with the cognitive versatility of a well‑educated adult, using the Cattell‑Horn‑Carroll theory to break intelligence into ten core domains and adapting human psychometric tests for AI evaluation. Applying the framework to current models reveals a highly uneven cognitive profile, with strong performance in knowledge‑intensive domains but major deficits in fundamental abilities such as long‑term memory storage, and provides concrete AGI scores for systems like GPT‑4 and GPT‑5.<br /><strong>Summary (CN):</strong> 本文提出一个可量化的 AGI（人工通用智能）定义框架，将其与受过良好教育的成人的认知多样性和熟练程度相匹配，基于 Cattell‑Horn‑Carroll（CHC）理论将智力划分为十个核心认知领域，并将已有的人类心理测量工具适配用于 AI 系统评估。对现有模型的应用显示出极其“锯齿形”的认知画像——在知识密集型领域表现优异，却在长期记忆存储等基础认知机器方面存在关键缺失，并给出如 GPT‑4（27%）和 GPT‑5（58%）等系统的 AGI 分数。<br /><strong>Keywords:</strong> AGI definition, Cattell-Horn-Carroll, cognitive benchmarking, AI evaluation, psychometrics, intelligence measurement, capability gap<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua Bengio</div>
The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.
<div><strong>Authors:</strong> Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua Bengio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a quantifiable framework for defining Artificial General Intelligence (AGI) by aligning it with the cognitive versatility of a well‑educated adult, using the Cattell‑Horn‑Carroll theory to break intelligence into ten core domains and adapting human psychometric tests for AI evaluation. Applying the framework to current models reveals a highly uneven cognitive profile, with strong performance in knowledge‑intensive domains but major deficits in fundamental abilities such as long‑term memory storage, and provides concrete AGI scores for systems like GPT‑4 and GPT‑5.", "summary_cn": "本文提出一个可量化的 AGI（人工通用智能）定义框架，将其与受过良好教育的成人的认知多样性和熟练程度相匹配，基于 Cattell‑Horn‑Carroll（CHC）理论将智力划分为十个核心认知领域，并将已有的人类心理测量工具适配用于 AI 系统评估。对现有模型的应用显示出极其“锯齿形”的认知画像——在知识密集型领域表现优异，却在长期记忆存储等基础认知机器方面存在关键缺失，并给出如 GPT‑4（27%）和 GPT‑5（58%）等系统的 AGI 分数。", "keywords": "AGI definition, Cattell-Horn-Carroll, cognitive benchmarking, AI evaluation, psychometrics, intelligence measurement, capability gap", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dan Hendrycks", "Dawn Song", "Christian Szegedy", "Honglak Lee", "Yarin Gal", "Erik Brynjolfsson", "Sharon Li", "Andy Zou", "Lionel Levine", "Bo Han", "Jie Fu", "Ziwei Liu", "Jinwoo Shin", "Kimin Lee", "Mantas Mazeika", "Long Phan", "George Ingebretsen", "Adam Khoja", "Cihang Xie", "Olawale Salaudeen", "Matthias Hein", "Kevin Zhao", "Alexander Pan", "David Duvenaud", "Bo Li", "Steve Omohundro", "Gabriel Alfour", "Max Tegmark", "Kevin McGrew", "Gary Marcus", "Jaan Tallinn", "Eric Schmidt", "Yoshua Bengio"]}
]]></acme>

<pubDate>2025-10-21T01:28:35+00:00</pubDate>
</item>
<item>
<title>RESCUE: Retrieval Augmented Secure Code Generation</title>
<link>https://papers.cool/arxiv/2510.18204</link>
<guid>https://papers.cool/arxiv/2510.18204</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RESCUE, a retrieval‑augmented generation framework designed to improve the security of code produced by large language models. RESCUE builds a hybrid knowledge base by distilling security guidelines and slicing programs, then employs a hierarchical multi‑faceted retrieval that traverses this base and injects relevant security facts at each level. Experiments on four benchmarks show that RESCUE raises SecurePass@1 by about 4.8 points over existing methods, establishing a new state‑of‑the‑art for secure code generation.<br /><strong>Summary (CN):</strong> 本文提出 RESCUE，一种检索增强生成框架，旨在提升大语言模型生成代码的安全性。RESCUE 通过 LLM 辅助的聚类‑摘要蒸馏结合程序切片构建混合知识库，并采用层次化多层面检索，从上至下遍历知识库，在每一层整合关键的安全事实。实验在四个基准上表明，RESCUE 将 SecurePass@1 提升约 4.8 分，达到安全代码生成的最新水平。<br /><strong>Keywords:</strong> secure code generation, retrieval-augmented generation, knowledge base distillation, hierarchical retrieval, security guidelines, code vulnerability, LLM safety, SecurePass, program slicing, RAG<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Jiahao Shi, Tianyi Zhang</div>
Despite recent advances, Large Language Models (LLMs) still generate vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to enhance LLMs for secure code generation by incorporating external security knowledge. However, the conventional RAG design struggles with the noise of raw security-related documents, and existing retrieval methods overlook the significant security semantics implicitly embedded in task descriptions. To address these issues, we propose RESCUE, a new RAG framework for secure code generation with two key innovations. First, we propose a hybrid knowledge base construction method that combines LLM-assisted cluster-then-summarize distillation with program slicing, producing both high-level security guidelines and concise, security-focused code examples. Second, we design a hierarchical multi-faceted retrieval to traverse the constructed knowledge base from top to bottom and integrates multiple security-critical facts at each hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated RESCUE on four benchmarks and compared it with five state-of-the-art secure code generation methods on six LLMs. The results demonstrate that RESCUE improves the SecurePass@1 metric by an average of 4.8 points, establishing a new state-of-the-art performance for security. Furthermore, we performed in-depth analysis and ablation studies to rigorously validate the effectiveness of individual components in RESCUE.
<div><strong>Authors:</strong> Jiahao Shi, Tianyi Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RESCUE, a retrieval‑augmented generation framework designed to improve the security of code produced by large language models. RESCUE builds a hybrid knowledge base by distilling security guidelines and slicing programs, then employs a hierarchical multi‑faceted retrieval that traverses this base and injects relevant security facts at each level. Experiments on four benchmarks show that RESCUE raises SecurePass@1 by about 4.8 points over existing methods, establishing a new state‑of‑the‑art for secure code generation.", "summary_cn": "本文提出 RESCUE，一种检索增强生成框架，旨在提升大语言模型生成代码的安全性。RESCUE 通过 LLM 辅助的聚类‑摘要蒸馏结合程序切片构建混合知识库，并采用层次化多层面检索，从上至下遍历知识库，在每一层整合关键的安全事实。实验在四个基准上表明，RESCUE 将 SecurePass@1 提升约 4.8 分，达到安全代码生成的最新水平。", "keywords": "secure code generation, retrieval-augmented generation, knowledge base distillation, hierarchical retrieval, security guidelines, code vulnerability, LLM safety, SecurePass, program slicing, RAG", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Jiahao Shi", "Tianyi Zhang"]}
]]></acme>

<pubDate>2025-10-21T01:13:03+00:00</pubDate>
</item>
<item>
<title>FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo</title>
<link>https://papers.cool/arxiv/2510.18193</link>
<guid>https://papers.cool/arxiv/2510.18193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FST.ai 2.0, an ecosystem that combines graph‑convolutional‑network pose recognition, credal‑set epistemic uncertainty, and visual explainability overlays to assist referees, coaches, and athletes in real‑time Taekwondo competitions. Interactive dashboards enable transparent human‑AI collaboration for scoring, fairness monitoring, and Para‑Taekwondo classification, achieving an 85% reduction in decision‑review time and high referee trust. By integrating perception, uncertainty modeling, and governance‑aware design, the system aims to provide fair, accountable, and human‑aligned decision making in sports.<br /><strong>Summary (CN):</strong> 本文提出 FST.ai 2.0 系统，将基于图卷积网络的姿态动作识别、可信集合（credal set）不确定性建模以及可视化解释层叠加，实时支持跆拳道裁判、教练和运动员的决策。交互式仪表盘实现透明的人机协作，用于计分、公平性监测和残奥跆拳道分类，实现了 85% 的裁判复审时间降低并获得高信任度。通过感知、误差建模和治理层面的设计，系统旨在提供公平、可解释且与人类价值对齐的体育裁判方案。<br /><strong>Keywords:</strong> explainable AI, pose action recognition, graph convolutional networks, epistemic uncertainty, credal sets, fairness, sports officiating, human-AI collaboration, decision support<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athlete performance analysis, and Para-Taekwondo classification. Beyond automated scoring, FST.ai~2.0 incorporates modules for referee training, fairness monitoring, and policy-level analytics within the World Taekwondo ecosystem. Experimental validation on competition data demonstrates an {85\% reduction in decision review time} and {93\% referee trust} in AI-assisted decisions. The framework thus establishes a transparent and extensible pipeline for trustworthy, data-driven officiating and athlete assessment. By bridging real-time perception, explainable inference, and governance-aware design, FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned AI in sports.
<div><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FST.ai 2.0, an ecosystem that combines graph‑convolutional‑network pose recognition, credal‑set epistemic uncertainty, and visual explainability overlays to assist referees, coaches, and athletes in real‑time Taekwondo competitions. Interactive dashboards enable transparent human‑AI collaboration for scoring, fairness monitoring, and Para‑Taekwondo classification, achieving an 85% reduction in decision‑review time and high referee trust. By integrating perception, uncertainty modeling, and governance‑aware design, the system aims to provide fair, accountable, and human‑aligned decision making in sports.", "summary_cn": "本文提出 FST.ai 2.0 系统，将基于图卷积网络的姿态动作识别、可信集合（credal set）不确定性建模以及可视化解释层叠加，实时支持跆拳道裁判、教练和运动员的决策。交互式仪表盘实现透明的人机协作，用于计分、公平性监测和残奥跆拳道分类，实现了 85% 的裁判复审时间降低并获得高信任度。通过感知、误差建模和治理层面的设计，系统旨在提供公平、可解释且与人类价值对齐的体育裁判方案。", "keywords": "explainable AI, pose action recognition, graph convolutional networks, epistemic uncertainty, credal sets, fairness, sports officiating, human-AI collaboration, decision support", "scoring": {"interpretability": 7, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Keivan Shariatmadar", "Ahmad Osman", "Ramin Ray", "Usman Dildar", "Kisam Kim"]}
]]></acme>

<pubDate>2025-10-21T00:35:56+00:00</pubDate>
</item>
<item>
<title>Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network</title>
<link>https://papers.cool/arxiv/2510.18190</link>
<guid>https://papers.cool/arxiv/2510.18190</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a compact multi‑task multi‑scale neural network that jointly estimates piano dynamic levels, change points, beats, and downbeats from Bark‑scale specific loudness features, allowing 60‑second audio inputs with only 0.5 M parameters. Evaluated on the MazurkaBL dataset, it achieves state‑of‑the‑art performance across all four tasks, setting a new benchmark for piano dynamic estimation and enabling large‑scale, resource‑efficient analysis of musical expression.<br /><strong>Summary (CN):</strong> 本文提出一种紧凑的多任务多尺度神经网络，利用 Bark 频率尺度的特定响度特征，共享潜在表示以同时预测钢琴力度水平、变化点、拍子和下拍，实现 60 秒音频输入且仅需 0.5 M 参数。基于公开的 MazurkaBL 数据集评估后，模型在四项任务上均达到了最新的性能水平，树立了钢琴力度估计的新基准，并为大规模、资源高效的音乐表情分析提供了强大工具。<br /><strong>Keywords:</strong> piano dynamics, beat tracking, downbeat detection, multi-task learning, multi-scale network, Bark-scale loudness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhanhong He, Hanyu Meng, David Huang, Roberto Togneri</div>
Estimating piano dynamic from audio recordings is a fundamental challenge in computational music analysis. In this paper, we propose an efficient multi-task network that jointly predicts dynamic levels, change points, beats, and downbeats from a shared latent representation. These four targets form the metrical structure of dynamics in the music score. Inspired by recent vocal dynamic research, we use a multi-scale network as the backbone, which takes Bark-scale specific loudness as the input feature. Compared to log-Mel as input, this reduces model size from 14.7 M to 0.5 M, enabling long sequential input. We use a 60-second audio length in audio segmentation, which doubled the length of beat tracking commonly used. Evaluated on the public MazurkaBL dataset, our model achieves state-of-the-art results across all tasks. This work sets a new benchmark for piano dynamic estimation and delivers a powerful and compact tool, paving the way for large-scale, resource-efficient analysis of musical expression.
<div><strong>Authors:</strong> Zhanhong He, Hanyu Meng, David Huang, Roberto Togneri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a compact multi‑task multi‑scale neural network that jointly estimates piano dynamic levels, change points, beats, and downbeats from Bark‑scale specific loudness features, allowing 60‑second audio inputs with only 0.5 M parameters. Evaluated on the MazurkaBL dataset, it achieves state‑of‑the‑art performance across all four tasks, setting a new benchmark for piano dynamic estimation and enabling large‑scale, resource‑efficient analysis of musical expression.", "summary_cn": "本文提出一种紧凑的多任务多尺度神经网络，利用 Bark 频率尺度的特定响度特征，共享潜在表示以同时预测钢琴力度水平、变化点、拍子和下拍，实现 60 秒音频输入且仅需 0.5 M 参数。基于公开的 MazurkaBL 数据集评估后，模型在四项任务上均达到了最新的性能水平，树立了钢琴力度估计的新基准，并为大规模、资源高效的音乐表情分析提供了强大工具。", "keywords": "piano dynamics, beat tracking, downbeat detection, multi-task learning, multi-scale network, Bark-scale loudness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhanhong He", "Hanyu Meng", "David Huang", "Roberto Togneri"]}
]]></acme>

<pubDate>2025-10-21T00:32:13+00:00</pubDate>
</item>
<item>
<title>Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains</title>
<link>https://papers.cool/arxiv/2510.18176</link>
<guid>https://papers.cool/arxiv/2510.18176</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how post‑training with Reinforcement Learning with Verifiable Rewards (RLVR) affects the intermediate reasoning steps of a small LLM on math problems. By introducing a First‑Order Logic‑based trace‑coherence metric, they show that RLVR improves local coherence of reasoning traces, especially on cases where the base model fails, but this does not necessarily lead to logically valid or correct solutions. The work highlights the gap between improved trace coherence and final answer correctness, cautioning against conflating the two when claiming reasoning improvements.<br /><strong>Summary (CN):</strong> 本文研究了使用可验证奖励的强化学习（RLVR）进行后训练对小型语言模型在数学题目中间推理步骤的影响。通过引入基于一阶逻辑的“轨迹一致性”度量，作者发现 RLVR 能提升推理轨迹的局部一致性，尤其是在基础模型失败但 RL 模型成功的案例中，但这并不必然带来逻辑上有效或答案正确的解答。研究指出，提高局部一致性并不等同于获得有效的数学证明，提醒在声称推理能力提升时需谨慎区分两者。<br /><strong>Keywords:</strong> RLVR, trace coherence, reasoning traces, GSM8K, GRPO, LLM alignment, interpretability, math reasoning, token-level analysis<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Soumya Rani Samineni, Durgesh Kalwar, Vardaan Gangal, Siddhant Bhambri, Subbarao Kambhampati</div>
Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of Large Language Models (LLMs) has been shown to improve accuracy on reasoning tasks and continues to attract significant attention. Existing RLVR methods, however, typically treat all tokens uniformly without accounting for token-level advantages. These methods primarily evaluate performance based on final answer correctness or Pass@K accuracy, and yet make claims about RL post-training leading to improved reasoning traces. This motivates our investigation into the effect of RL post-training on intermediate tokens which are not directly incentivized. To study this, we design an experimental setup using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We introduce trace coherence, a First-Order Logic (FOL)-based measure to capture the consistency of reasoning steps by identifying errors in the traces. We distinguish between trace validity and trace coherence, noting that the former implies logical soundness while the latter measures local coherence via lack of errors. Our results show that RL post-training overall improves trace coherence with the most significant gains on problems where the base model fails but the RL model succeeds. Surprisingly, RL enhances local coherence without necessarily producing valid or correct solutions. This highlights a crucial distinction: improved local coherence in reasoning steps does not guarantee final answer correctness. We argue that claims of improved reasoning via RL must be examined with care, as these may be based on improved trace coherence, which may not translate into fully valid mathematical proofs.
<div><strong>Authors:</strong> Soumya Rani Samineni, Durgesh Kalwar, Vardaan Gangal, Siddhant Bhambri, Subbarao Kambhampati</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how post‑training with Reinforcement Learning with Verifiable Rewards (RLVR) affects the intermediate reasoning steps of a small LLM on math problems. By introducing a First‑Order Logic‑based trace‑coherence metric, they show that RLVR improves local coherence of reasoning traces, especially on cases where the base model fails, but this does not necessarily lead to logically valid or correct solutions. The work highlights the gap between improved trace coherence and final answer correctness, cautioning against conflating the two when claiming reasoning improvements.", "summary_cn": "本文研究了使用可验证奖励的强化学习（RLVR）进行后训练对小型语言模型在数学题目中间推理步骤的影响。通过引入基于一阶逻辑的“轨迹一致性”度量，作者发现 RLVR 能提升推理轨迹的局部一致性，尤其是在基础模型失败但 RL 模型成功的案例中，但这并不必然带来逻辑上有效或答案正确的解答。研究指出，提高局部一致性并不等同于获得有效的数学证明，提醒在声称推理能力提升时需谨慎区分两者。", "keywords": "RLVR, trace coherence, reasoning traces, GSM8K, GRPO, LLM alignment, interpretability, math reasoning, token-level analysis", "scoring": {"interpretability": 6, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Vardaan Gangal", "Siddhant Bhambri", "Subbarao Kambhampati"]}
]]></acme>

<pubDate>2025-10-20T23:58:31+00:00</pubDate>
</item>
<item>
<title>AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI</title>
<link>https://papers.cool/arxiv/2510.18170</link>
<guid>https://papers.cool/arxiv/2510.18170</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AgentChangeBench is a benchmark that evaluates how tool-augmented language model agents handle mid‑dialogue goal shifts across multiple enterprise domains, using four metrics: Task Success Rate, Tool Use Efficiency, Tool Call Redundancy Rate, and Goal‑Shift Recovery Time. The dataset contains 2,835 task sequences with five user personas designed to trigger realistic shifts, revealing that high static accuracy does not guarantee robustness under dynamic goals. Experiments on frontier models (e.g., GPT‑4o vs. Gemini) show large gaps in recovery performance and redundancy, highlighting the need for dedicated evaluation of goal‑shift resilience.<br /><strong>Summary (CN):</strong> AgentChangeBench 是一个评估工具增强语言模型代理在多轮对话中应对目标变更能力的基准，使用任务成功率、工具使用效率、工具调用冗余率和目标变更恢复时间四个指标。数据集包含 2,835 条任务序列和五种用户角色，旨在模拟真实企业场景中的目标切换，揭示仅凭静态准确率并不能保证在动态目标下的鲁棒性。对前沿模型（如 GPT‑4o 与 Gemini）的实验显示恢复率和冗余率存在显著差距，强调了专门评估目标变更适应性的必要性。<br /><strong>Keywords:</strong> goal shift, conversational AI, tool-augmented agents, robustness benchmark, task success rate, recovery time, enterprise domains, evaluation metrics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Manik Rana, Calissa Man, Anotida Expected Msiiwa, Jeffrey Paine, Kevin Zhu, Sunishchal Dev, Vasu Sharma, Ahan M R</div>
Goal changes are a defining feature of real world multi-turn interactions, yet current agent benchmarks primarily evaluate static objectives or one-shot tool use. We introduce AgentChangeBench, a benchmark explicitly designed to measure how tool augmented language model agents adapt to mid dialogue goal shifts across three enterprise domains. Our framework formalizes evaluation through four complementary metrics: Task Success Rate (TSR) for effectiveness, Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency. AgentChangeBench comprises 2,835 task sequences and five user personas, each designed to trigger realistic shift points in ongoing workflows. Using this setup, we evaluate several frontier models and uncover sharp contrasts obscured by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$ recovery on airline booking shifts while Gemini collapses to $48.6\%$, and retail tasks show near perfect parameter validity yet redundancy rates above $80\%$, revealing major inefficiencies. These findings demonstrate that high raw accuracy does not imply robustness under dynamic goals, and that explicit measurement of recovery time and redundancy is essential. AgentChangeBench establishes a reproducible testbed for diagnosing and improving agent resilience in realistic enterprise settings.
<div><strong>Authors:</strong> Manik Rana, Calissa Man, Anotida Expected Msiiwa, Jeffrey Paine, Kevin Zhu, Sunishchal Dev, Vasu Sharma, Ahan M R</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AgentChangeBench is a benchmark that evaluates how tool-augmented language model agents handle mid‑dialogue goal shifts across multiple enterprise domains, using four metrics: Task Success Rate, Tool Use Efficiency, Tool Call Redundancy Rate, and Goal‑Shift Recovery Time. The dataset contains 2,835 task sequences with five user personas designed to trigger realistic shifts, revealing that high static accuracy does not guarantee robustness under dynamic goals. Experiments on frontier models (e.g., GPT‑4o vs. Gemini) show large gaps in recovery performance and redundancy, highlighting the need for dedicated evaluation of goal‑shift resilience.", "summary_cn": "AgentChangeBench 是一个评估工具增强语言模型代理在多轮对话中应对目标变更能力的基准，使用任务成功率、工具使用效率、工具调用冗余率和目标变更恢复时间四个指标。数据集包含 2,835 条任务序列和五种用户角色，旨在模拟真实企业场景中的目标切换，揭示仅凭静态准确率并不能保证在动态目标下的鲁棒性。对前沿模型（如 GPT‑4o 与 Gemini）的实验显示恢复率和冗余率存在显著差距，强调了专门评估目标变更适应性的必要性。", "keywords": "goal shift, conversational AI, tool-augmented agents, robustness benchmark, task success rate, recovery time, enterprise domains, evaluation metrics", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Manik Rana", "Calissa Man", "Anotida Expected Msiiwa", "Jeffrey Paine", "Kevin Zhu", "Sunishchal Dev", "Vasu Sharma", "Ahan M R"]}
]]></acme>

<pubDate>2025-10-20T23:48:07+00:00</pubDate>
</item>
<item>
<title>Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model</title>
<link>https://papers.cool/arxiv/2510.18165</link>
<guid>https://papers.cool/arxiv/2510.18165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Saber introduces a training‑free sampling algorithm for diffusion language models that adaptively accelerates generation and incorporates a back‑tracking remasking step, aiming to improve inference speed and maintain output quality in code‑generation tasks. Experiments on standard benchmarks show modest gains in Pass@1 accuracy (≈1.9 % improvement) while achieving over‑two‑fold speedup compared to existing DLM sampling methods. The method focuses on sampling efficiency rather than model interpretability or safety.<br /><strong>Summary (CN):</strong> Saber 提出一种无需训练的采样算法，通过自适应加速和回溯重掩码来提升扩散语言模型在代码生成中的推理速度和输出质量。实验表明，在多个基准上 Pass@1 精度提升约 1.9%，推理速度提升约 251%。该工作主要关注采样效率，而非模型可解释性或安全性。<br /><strong>Keywords:</strong> diffusion language model, code generation, adaptive acceleration, backtracking, sampling algorithm, inference speedup, Pass@1 accuracy<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
Diffusion language models (DLMs) are emerging as a powerful and promising alternative to the dominant autoregressive paradigm, offering inherent advantages in parallel generation and bidirectional context modeling. However, the performance of DLMs on code generation tasks, which have stronger structural constraints, is significantly hampered by the critical trade-off between inference speed and output quality. We observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance. In this paper, we introduce efficient Sampling with Adaptive acceleration and Backtracking Enhanced Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to achieve better inference speed and output quality in code generation. Specifically, Saber is motivated by two key insights in the DLM generation process: 1) it can be adaptively accelerated as more of the code context is established; 2) it requires a backtracking mechanism to reverse the generated tokens. Extensive experiments on multiple mainstream code generation benchmarks show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over mainstream DLM sampling methods, meanwhile achieving an average 251.4% inference speedup. By leveraging the inherent advantages of DLMs, our work significantly narrows the performance gap with autoregressive models in code generation.
<div><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Saber introduces a training‑free sampling algorithm for diffusion language models that adaptively accelerates generation and incorporates a back‑tracking remasking step, aiming to improve inference speed and maintain output quality in code‑generation tasks. Experiments on standard benchmarks show modest gains in Pass@1 accuracy (≈1.9 % improvement) while achieving over‑two‑fold speedup compared to existing DLM sampling methods. The method focuses on sampling efficiency rather than model interpretability or safety.", "summary_cn": "Saber 提出一种无需训练的采样算法，通过自适应加速和回溯重掩码来提升扩散语言模型在代码生成中的推理速度和输出质量。实验表明，在多个基准上 Pass@1 精度提升约 1.9%，推理速度提升约 251%。该工作主要关注采样效率，而非模型可解释性或安全性。", "keywords": "diffusion language model, code generation, adaptive acceleration, backtracking, sampling algorithm, inference speedup, Pass@1 accuracy", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-20T23:38:12+00:00</pubDate>
</item>
<item>
<title>Beating the Winner's Curse via Inference-Aware Policy Optimization</title>
<link>https://papers.cool/arxiv/2510.18161</link>
<guid>https://papers.cool/arxiv/2510.18161</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces inference‑aware policy optimization, a method that jointly maximizes estimated objective value and the probability of achieving statistically significant improvement over an observational baseline, thereby mitigating the winner's curse in offline treatment‑selection problems. The authors derive the Pareto frontier of this trade‑off, propose an algorithm that uses counterfactual outcome predictors to estimate the frontier, and demonstrate its advantages in simulated experiments.<br /><strong>Summary (CN):</strong> 本文提出了推断感知的策略优化方法，在离线治疗决策问题中同时最大化预测目标值和超越观测基线并达到统计显著性的概率，以缓解胜者诅咒现象。作者推导了该两目标权衡的帕累托前沿，设计了利用反事实结果预测器估计前沿的算法，并模拟实验中展示了其效果。<br /><strong>Keywords:</strong> offline policy learning, winner's curse, inference-aware optimization, counterfactual prediction, Pareto frontier, policy evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Hamsa Bastani, Osbert Bastani, Bryce McLaughlin</div>
There has been a surge of recent interest in automatically learning policies to target treatment decisions based on rich individual covariates. A common approach is to train a machine learning model to predict counterfactual outcomes, and then select the policy that optimizes the predicted objective value. In addition, practitioners also want confidence that the learned policy has better performance than the incumbent policy according to downstream policy evaluation. However, due to the winner's curse-an issue where the policy optimization procedure exploits prediction errors rather than finding actual improvements-predicted performance improvements are often not substantiated by downstream policy optimization. To address this challenge, we propose a novel strategy called inference-aware policy optimization, which modifies policy optimization to account for how the policy will be evaluated downstream. Specifically, it optimizes not only for the estimated objective value, but also for the chances that the policy will be statistically significantly better than the observational policy used to collect data. We mathematically characterize the Pareto frontier of policies according to the tradeoff of these two goals. Based on our characterization, we design a policy optimization algorithm that uses machine learning to predict counterfactual outcomes, and then plugs in these predictions to estimate the Pareto frontier; then, the decision-maker can select the policy that optimizes their desired tradeoff, after which policy evaluation can be performed on the test set as usual. Finally, we perform simulations to illustrate the effectiveness of our methodology.
<div><strong>Authors:</strong> Hamsa Bastani, Osbert Bastani, Bryce McLaughlin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces inference‑aware policy optimization, a method that jointly maximizes estimated objective value and the probability of achieving statistically significant improvement over an observational baseline, thereby mitigating the winner's curse in offline treatment‑selection problems. The authors derive the Pareto frontier of this trade‑off, propose an algorithm that uses counterfactual outcome predictors to estimate the frontier, and demonstrate its advantages in simulated experiments.", "summary_cn": "本文提出了推断感知的策略优化方法，在离线治疗决策问题中同时最大化预测目标值和超越观测基线并达到统计显著性的概率，以缓解胜者诅咒现象。作者推导了该两目标权衡的帕累托前沿，设计了利用反事实结果预测器估计前沿的算法，并模拟实验中展示了其效果。", "keywords": "offline policy learning, winner's curse, inference-aware optimization, counterfactual prediction, Pareto frontier, policy evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Hamsa Bastani", "Osbert Bastani", "Bryce McLaughlin"]}
]]></acme>

<pubDate>2025-10-20T23:28:12+00:00</pubDate>
</item>
<item>
<title>Extracting Rule-based Descriptions of Attention Features in Transformers</title>
<link>https://papers.cool/arxiv/2510.18148</link>
<guid>https://papers.cool/arxiv/2510.18148</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method to automatically extract rule‑based descriptions for Sparse Autoencoder (SAE) features in transformer attention layers, using skip‑gram, absence, and counting rules to capture token pattern influences on output probabilities. Experiments on GPT‑2 small show that most features can be succinctly described with about 100 skip‑gram rules, and that absence and counting rules are prevalent even in early layers. This work provides a taxonomy of rule types and demonstrates their utility as an alternative to exemplar‑based interpretations.<br /><strong>Summary (CN):</strong> 本文提出一种自动提取 Transformer 注意力层中稀疏自动编码器（SAE）特征的规则化描述的方法，利用跳词规则、缺失规则和计数规则捕获输入词模式对输出概率的影响。对 GPT‑2 small 的实验表明，大多数特征可以用约 100 条跳词规则简洁描述，且缺失规则和计数规则在早期层就已广泛出现。该工作给出了规则类型的初步分类，并展示了其相对于基于示例的解释的优势。<br /><strong>Keywords:</strong> mechanistic interpretability, sparse autoencoder, attention features, rule-based description, skip-gram rules, absence rules, counting rules, GPT-2, transformer<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
Mechanistic interpretability strives to explain model behavior in terms of bottom-up primitives. The leading paradigm is to express hidden states as a sparse linear combination of basis vectors, called features. However, this only identifies which text sequences (exemplars) activate which features; the actual interpretation of features requires subjective inspection of these exemplars. This paper advocates for a different solution: rule-based descriptions that match token patterns in the input and correspondingly increase or decrease the likelihood of specific output tokens. Specifically, we extract rule-based descriptions of SAE features trained on the outputs of attention layers. While prior work treats the attention layers as an opaque box, we describe how it may naturally be expressed in terms of interactions between input and output features, of which we study three types: (1) skip-gram rules of the form "[Canadian city]... speaks --> English", (2) absence rules of the form "[Montreal]... speaks -/-> English," and (3) counting rules that toggle only when the count of a word exceeds a certain value or the count of another word. Absence and counting rules are not readily discovered by inspection of exemplars, where manual and automatic descriptions often identify misleading or incomplete explanations. We then describe a simple approach to extract these types of rules automatically from a transformer, and apply it to GPT-2 small. We find that a majority of features may be described well with around 100 skip-gram rules, though absence rules are abundant even as early as the first layer (in over a fourth of features). We also isolate a few examples of counting rules. This paper lays the groundwork for future research into rule-based descriptions of features by defining them, showing how they may be extracted, and providing a preliminary taxonomy of some of the behaviors they represent.
<div><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method to automatically extract rule‑based descriptions for Sparse Autoencoder (SAE) features in transformer attention layers, using skip‑gram, absence, and counting rules to capture token pattern influences on output probabilities. Experiments on GPT‑2 small show that most features can be succinctly described with about 100 skip‑gram rules, and that absence and counting rules are prevalent even in early layers. This work provides a taxonomy of rule types and demonstrates their utility as an alternative to exemplar‑based interpretations.", "summary_cn": "本文提出一种自动提取 Transformer 注意力层中稀疏自动编码器（SAE）特征的规则化描述的方法，利用跳词规则、缺失规则和计数规则捕获输入词模式对输出概率的影响。对 GPT‑2 small 的实验表明，大多数特征可以用约 100 条跳词规则简洁描述，且缺失规则和计数规则在早期层就已广泛出现。该工作给出了规则类型的初步分类，并展示了其相对于基于示例的解释的优势。", "keywords": "mechanistic interpretability, sparse autoencoder, attention features, rule-based description, skip-gram rules, absence rules, counting rules, GPT-2, transformer", "scoring": {"interpretability": 8, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Dan Friedman", "Adithya Bhaskar", "Alexander Wettig", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-20T22:52:40+00:00</pubDate>
</item>
<item>
<title>Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models</title>
<link>https://papers.cool/arxiv/2510.18143</link>
<guid>https://papers.cool/arxiv/2510.18143</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PaDA-Agent, an evaluation-driven data augmentation framework that discovers failure patterns from validation data and generates targeted augmentation samples to narrow the generalization gap of small language models during fine-tuning. Experiments on the Llama 3.2 1B Instruct model show notable performance gains over existing LLM-based augmentation methods.<br /><strong>Summary (CN):</strong> 本文提出了 PaDA-Agent，一种基于评估的数 据增强框架，通过从验证数据中发现失败模式并生成针对性的增强样本，以缩小小语言模型微调时的泛化差距。对 Llama 3.2 1B Instruct 模型的实验表明，其性能明显优于现有的基于大型语言模型的增强方法。<br /><strong>Keywords:</strong> data augmentation, small language models, evaluation-driven, pattern discovery, fine-tuning, generalization, PaDA-Agent, Llama 3.2, domain adaptation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Huan Song, Deeksha Razdan, Yiyue Qian, Arijit Ghosh Chowdhury, Parth Patwa, Aman Chadha, Shinan Zhang, Sharlina Keshava, Hannah Marlowe</div>
Small Language Models (SLMs) offer compelling advantages in deployment cost and latency, but their accuracy often lags behind larger models, particularly for complex domain-specific tasks. While supervised fine-tuning can help bridge this performance gap, it requires substantial manual effort in data preparation and iterative optimization. We present PaDA-Agent (Pattern-guided Data Augmentation Agent), an evaluation-driven approach that streamlines the data augmentation process for SLMs through coordinated operations. Unlike state-of-the-art approaches that focus on model training errors only and generating error-correcting samples, PaDA-Agent discovers failure patterns from the validation data via evaluations and drafts targeted data augmentation strategies aiming to directly reduce the generalization gap. Our experimental results demonstrate significant improvements over state-of-the-art LLM-based data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.
<div><strong>Authors:</strong> Huan Song, Deeksha Razdan, Yiyue Qian, Arijit Ghosh Chowdhury, Parth Patwa, Aman Chadha, Shinan Zhang, Sharlina Keshava, Hannah Marlowe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PaDA-Agent, an evaluation-driven data augmentation framework that discovers failure patterns from validation data and generates targeted augmentation samples to narrow the generalization gap of small language models during fine-tuning. Experiments on the Llama 3.2 1B Instruct model show notable performance gains over existing LLM-based augmentation methods.", "summary_cn": "本文提出了 PaDA-Agent，一种基于评估的数 据增强框架，通过从验证数据中发现失败模式并生成针对性的增强样本，以缩小小语言模型微调时的泛化差距。对 Llama 3.2 1B Instruct 模型的实验表明，其性能明显优于现有的基于大型语言模型的增强方法。", "keywords": "data augmentation, small language models, evaluation-driven, pattern discovery, fine-tuning, generalization, PaDA-Agent, Llama 3.2, domain adaptation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Huan Song", "Deeksha Razdan", "Yiyue Qian", "Arijit Ghosh Chowdhury", "Parth Patwa", "Aman Chadha", "Shinan Zhang", "Sharlina Keshava", "Hannah Marlowe"]}
]]></acme>

<pubDate>2025-10-20T22:36:46+00:00</pubDate>
</item>
<item>
<title>Generalization Below the Edge of Stability: The Role of Data Geometry</title>
<link>https://papers.cool/arxiv/2510.18120</link>
<guid>https://papers.cool/arxiv/2510.18120</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper theoretically investigates how the geometry of data influences generalization in overparameterized two-layer ReLU networks trained below the edge of stability. It derives bounds that adapt to intrinsic dimension for mixtures of low‑dimensional balls and shows deterioration of rates as probability mass concentrates near the unit sphere, linking shatterability to memorization versus representation learning. These results unify several empirical observations about geometry‑dependent generalization.<br /><strong>Summary (CN):</strong> 本文从理论上探讨数据几何如何影响在边界稳定性以下训练的过参数化两层 ReLU 网络的泛化能力。研究给出针对低维球混合分布的内在维度自适应的泛化界，并展示当概率质量集中于单位球面时，泛化速率会恶化，说明易被 ReLU 阈值“打碎”的数据倾向于记忆而非共享特征学习。这些结果统一了此前关于数据几何与泛化的若干实验观察。<br /><strong>Keywords:</strong> generalization, data geometry, edge of stability, overparameterized neural networks, two-layer ReLU, implicit bias, low-dimensional manifolds, spectral bounds<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tongtong Liang, Alexander Cloninger, Rahul Parhi, Yu-Xiang Wang</div>
Understanding generalization in overparameterized neural networks hinges on the interplay between the data geometry, neural architecture, and training dynamics. In this paper, we theoretically explore how data geometry controls this implicit bias. This paper presents theoretical results for overparameterized two-layer ReLU networks trained below the edge of stability. First, for data distributions supported on a mixture of low-dimensional balls, we derive generalization bounds that provably adapt to the intrinsic dimension. Second, for a family of isotropic distributions that vary in how strongly probability mass concentrates toward the unit sphere, we derive a spectrum of bounds showing that rates deteriorate as the mass concentrates toward the sphere. These results instantiate a unifying principle: When the data is harder to "shatter" with respect to the activation thresholds of the ReLU neurons, gradient descent tends to learn representations that capture shared patterns and thus finds solutions that generalize well. On the other hand, for data that is easily shattered (e.g., data supported on the sphere) gradient descent favors memorization. Our theoretical results consolidate disparate empirical findings that have appeared in the literature.
<div><strong>Authors:</strong> Tongtong Liang, Alexander Cloninger, Rahul Parhi, Yu-Xiang Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper theoretically investigates how the geometry of data influences generalization in overparameterized two-layer ReLU networks trained below the edge of stability. It derives bounds that adapt to intrinsic dimension for mixtures of low‑dimensional balls and shows deterioration of rates as probability mass concentrates near the unit sphere, linking shatterability to memorization versus representation learning. These results unify several empirical observations about geometry‑dependent generalization.", "summary_cn": "本文从理论上探讨数据几何如何影响在边界稳定性以下训练的过参数化两层 ReLU 网络的泛化能力。研究给出针对低维球混合分布的内在维度自适应的泛化界，并展示当概率质量集中于单位球面时，泛化速率会恶化，说明易被 ReLU 阈值“打碎”的数据倾向于记忆而非共享特征学习。这些结果统一了此前关于数据几何与泛化的若干实验观察。", "keywords": "generalization, data geometry, edge of stability, overparameterized neural networks, two-layer ReLU, implicit bias, low-dimensional manifolds, spectral bounds", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tongtong Liang", "Alexander Cloninger", "Rahul Parhi", "Yu-Xiang Wang"]}
]]></acme>

<pubDate>2025-10-20T21:40:36+00:00</pubDate>
</item>
<item>
<title>PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</title>
<link>https://papers.cool/arxiv/2510.18109</link>
<guid>https://papers.cool/arxiv/2510.18109</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> PrivaDE is a cryptographic protocol that enables model builders to evaluate the utility of candidate datasets without revealing proprietary model information, while ensuring data providers' privacy beyond the disclosed utility score. The design leverages blockchain's trustless environment, model distillation, model splitting, and cut-and-choose zero-knowledge proofs to achieve malicious-security guarantees and practical runtimes (under 15 minutes for models with millions of parameters). The paper also introduces a unified utility scoring function combining empirical loss, predictive entropy, and feature-space diversity, suitable for active-learning pipelines in decentralized machine learning ecosystems.<br /><strong>Summary (CN):</strong> PrivaDE 是一种加密协议，允许模型构建者在不泄露模型专有信息的情况下评估候选数据集的效用，同时确保数据提供者的隐私仅泄露效用评分。该方案利用区块链的去中心化特性、模型蒸馏、模型拆分以及挑选-证明零知识证明，实现了对恶意行为的安全保障，并将运行时间压缩至 15 分钟以内（即使模型参数达数百万）。文中还提出一种统一的效用评分函数，综合经验损失、预测熵和特征空间多样性，可无缝集成到去中心化机器学习的主动学习工作流中。<br /><strong>Keywords:</strong> privacy-preserving, data utility scoring, blockchain, zero-knowledge proofs, model distillation, active learning<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wan Ki Wong, Sahel Torkamani, Michele Ciampi, Rik Sarkar</div>
Evaluating the relevance of data is a critical task for model builders seeking to acquire datasets that enhance model performance. Ideally, such evaluation should allow the model builder to assess the utility of candidate data without exposing proprietary details of the model. At the same time, data providers must be assured that no information about their data - beyond the computed utility score - is disclosed to the model builder. In this paper, we present PrivaDE, a cryptographic protocol for privacy-preserving utility scoring and selection of data for machine learning. While prior works have proposed data evaluation protocols, our approach advances the state of the art through a practical, blockchain-centric design. Leveraging the trustless nature of blockchains, PrivaDE enforces malicious-security guarantees and ensures strong privacy protection for both models and datasets. To achieve efficiency, we integrate several techniques - including model distillation, model splitting, and cut-and-choose zero-knowledge proofs - bringing the runtime to a practical level. Furthermore, we propose a unified utility scoring function that combines empirical loss, predictive entropy, and feature-space diversity, and that can be seamlessly integrated into active-learning workflows. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters. Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.
<div><strong>Authors:</strong> Wan Ki Wong, Sahel Torkamani, Michele Ciampi, Rik Sarkar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "PrivaDE is a cryptographic protocol that enables model builders to evaluate the utility of candidate datasets without revealing proprietary model information, while ensuring data providers' privacy beyond the disclosed utility score. The design leverages blockchain's trustless environment, model distillation, model splitting, and cut-and-choose zero-knowledge proofs to achieve malicious-security guarantees and practical runtimes (under 15 minutes for models with millions of parameters). The paper also introduces a unified utility scoring function combining empirical loss, predictive entropy, and feature-space diversity, suitable for active-learning pipelines in decentralized machine learning ecosystems.", "summary_cn": "PrivaDE 是一种加密协议，允许模型构建者在不泄露模型专有信息的情况下评估候选数据集的效用，同时确保数据提供者的隐私仅泄露效用评分。该方案利用区块链的去中心化特性、模型蒸馏、模型拆分以及挑选-证明零知识证明，实现了对恶意行为的安全保障，并将运行时间压缩至 15 分钟以内（即使模型参数达数百万）。文中还提出一种统一的效用评分函数，综合经验损失、预测熵和特征空间多样性，可无缝集成到去中心化机器学习的主动学习工作流中。", "keywords": "privacy-preserving, data utility scoring, blockchain, zero-knowledge proofs, model distillation, active learning", "scoring": {"interpretability": 1, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wan Ki Wong", "Sahel Torkamani", "Michele Ciampi", "Rik Sarkar"]}
]]></acme>

<pubDate>2025-10-20T21:14:32+00:00</pubDate>
</item>
<item>
<title>From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs</title>
<link>https://papers.cool/arxiv/2510.18104</link>
<guid>https://papers.cool/arxiv/2510.18104</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a shift from narrow AutoRecSys tools to a fully autonomous Recommender‑Systems Research Lab (AutoRecLab) that automates the entire research pipeline, including problem ideation, experimental design, execution, result interpretation, and manuscript drafting. It outlines an agenda for the RecSys community to build open prototypes, establish benchmarks, create review venues for AI‑generated submissions, and define standards for attribution, reproducibility, ethics, governance, privacy, and fairness. The goal is to increase research throughput, uncover non‑obvious insights, and responsibly integrate automated research systems into the field.<br /><strong>Summary (CN):</strong> 本文提出将狭窄的 AutoRecSys 工具转变为完整的自主推荐系统研究实验室（AutoRecLab），实现从问题构思、实验设计与执行、结果解释到论文撰写的全流程自动化。文中提出社区应构建开源原型、设立评估基准、创建 AI 生成稿件的审稿渠道，并制定归属、可重复性以及伦理、治理、隐私和公平的标准，以提升研究产出效率并负责任地引入自动化研究系统。<br /><strong>Keywords:</strong> autonomous recommender systems, AutoRecLab, automated research, LLM-driven ideation, reproducibility, AI governance, recommendation systems, safety, fairness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - control<br /><strong>Authors:</strong> Joeran Beel, Bela Gipp, Tobias Vente, Moritz Baumgart, Philipp Meister</div>
Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.
<div><strong>Authors:</strong> Joeran Beel, Bela Gipp, Tobias Vente, Moritz Baumgart, Philipp Meister</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a shift from narrow AutoRecSys tools to a fully autonomous Recommender‑Systems Research Lab (AutoRecLab) that automates the entire research pipeline, including problem ideation, experimental design, execution, result interpretation, and manuscript drafting. It outlines an agenda for the RecSys community to build open prototypes, establish benchmarks, create review venues for AI‑generated submissions, and define standards for attribution, reproducibility, ethics, governance, privacy, and fairness. The goal is to increase research throughput, uncover non‑obvious insights, and responsibly integrate automated research systems into the field.", "summary_cn": "本文提出将狭窄的 AutoRecSys 工具转变为完整的自主推荐系统研究实验室（AutoRecLab），实现从问题构思、实验设计与执行、结果解释到论文撰写的全流程自动化。文中提出社区应构建开源原型、设立评估基准、创建 AI 生成稿件的审稿渠道，并制定归属、可重复性以及伦理、治理、隐私和公平的标准，以提升研究产出效率并负责任地引入自动化研究系统。", "keywords": "autonomous recommender systems, AutoRecLab, automated research, LLM-driven ideation, reproducibility, AI governance, recommendation systems, safety, fairness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Joeran Beel", "Bela Gipp", "Tobias Vente", "Moritz Baumgart", "Philipp Meister"]}
]]></acme>

<pubDate>2025-10-20T20:58:50+00:00</pubDate>
</item>
<item>
<title>Accelerating Vision Transformers with Adaptive Patch Sizes</title>
<link>https://papers.cool/arxiv/2510.18091</link>
<guid>https://papers.cool/arxiv/2510.18091</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Patch Transformers (APT), which allocate multiple patch sizes within a single image, using larger patches in homogeneous regions and smaller patches in complex areas to reduce token count. This approach yields up to 50% speedup in inference and training for large Vision Transformers while preserving downstream performance, and can be applied to already fine‑tuned models with rapid convergence.<br /><strong>Summary (CN):</strong> 本文提出自适应补丁 Transformer（APT），在同一图像中使用不同尺寸的补丁，在结构单一的区域使用较大补丁，在复杂区域使用较小补丁，从而降低 token 数量。该方法在大型 Vision Transformer 上实现了最高 50% 的推理和训练加速，且保持下游任务性能，并可快速适配已微调模型。<br /><strong>Keywords:</strong> adaptive patch, vision transformer, token reduction, efficient inference, dynamic patching, computer vision, ViT-L, ViT-H<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\% faster training and inference in visual QA, object detection, and semantic segmentation.
<div><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Patch Transformers (APT), which allocate multiple patch sizes within a single image, using larger patches in homogeneous regions and smaller patches in complex areas to reduce token count. This approach yields up to 50% speedup in inference and training for large Vision Transformers while preserving downstream performance, and can be applied to already fine‑tuned models with rapid convergence.", "summary_cn": "本文提出自适应补丁 Transformer（APT），在同一图像中使用不同尺寸的补丁，在结构单一的区域使用较大补丁，在复杂区域使用较小补丁，从而降低 token 数量。该方法在大型 Vision Transformer 上实现了最高 50% 的推理和训练加速，且保持下游任务性能，并可快速适配已微调模型。", "keywords": "adaptive patch, vision transformer, token reduction, efficient inference, dynamic patching, computer vision, ViT-L, ViT-H", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rohan Choudhury", "JungEun Kim", "Jinhyung Park", "Eunho Yang", "László A. Jeni", "Kris M. Kitani"]}
]]></acme>

<pubDate>2025-10-20T20:37:11+00:00</pubDate>
</item>
<item>
<title>Arbitrated Indirect Treatment Comparisons</title>
<link>https://papers.cool/arxiv/2510.18071</link>
<guid>https://papers.cool/arxiv/2510.18071</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a new class of methods called arbitrated indirect treatment comparisons to resolve the MAIC paradox, where different sponsors reach conflicting conclusions due to targeting different populations. By estimating treatment effects in a common overlap population, the approach aims to provide consistent estimates across studies using individual participant data and aggregate data. The methodology is positioned as a solution for health technology assessment contexts where indirect comparisons are common.<br /><strong>Summary (CN):</strong> 本文提出一种称为仲裁间接治疗比较的新方法，以解决 MAIC 悖论——即不同资助方因针对不同人群而对同一数据得出冲突结论的问题。该方法通过在公共的重叠人群中估计治疗效果，实现了使用个体参与者数据和汇总数据的研究之间的一致估计。此方法旨在用于健康技术评估中的间接比较情境。<br /><strong>Keywords:</strong> matching-adjusted indirect comparison, MAIC paradox, arbitrated indirect treatment comparison, overlap population, health technology assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yixin Fang, Weili He</div>
Matching-adjusted indirect comparison (MAIC) has been increasingly employed in health technology assessments (HTA). By reweighting subjects from a trial with individual participant data (IPD) to match the covariate summary statistics of another trial with only aggregate data (AgD), MAIC facilitates the estimation of a treatment effect defined with respect to the AgD trial population. This manuscript introduces a new class of methods, termed arbitrated indirect treatment comparisons, designed to address the ``MAIC paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox arises when different sponsors, analyzing the same data, reach conflicting conclusions regarding which treatment is more effective. The underlying issue is that each sponsor implicitly targets a different population. To resolve this inconsistency, the proposed methods focus on estimating treatment effects in a common target population, specifically chosen to be the overlap population.
<div><strong>Authors:</strong> Yixin Fang, Weili He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a new class of methods called arbitrated indirect treatment comparisons to resolve the MAIC paradox, where different sponsors reach conflicting conclusions due to targeting different populations. By estimating treatment effects in a common overlap population, the approach aims to provide consistent estimates across studies using individual participant data and aggregate data. The methodology is positioned as a solution for health technology assessment contexts where indirect comparisons are common.", "summary_cn": "本文提出一种称为仲裁间接治疗比较的新方法，以解决 MAIC 悖论——即不同资助方因针对不同人群而对同一数据得出冲突结论的问题。该方法通过在公共的重叠人群中估计治疗效果，实现了使用个体参与者数据和汇总数据的研究之间的一致估计。此方法旨在用于健康技术评估中的间接比较情境。", "keywords": "matching-adjusted indirect comparison, MAIC paradox, arbitrated indirect treatment comparison, overlap population, health technology assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yixin Fang", "Weili He"]}
]]></acme>

<pubDate>2025-10-20T20:07:47+00:00</pubDate>
</item>
<item>
<title>Fast Agnostic Learners in the Plane</title>
<link>https://papers.cool/arxiv/2510.18057</link>
<guid>https://papers.cool/arxiv/2510.18057</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the computational time complexity of agnostic learning for fundamental geometric concept classes in the plane, presenting proper agnostic learners for triangles, 4‑gons, 5‑gons, and convex sets with substantially improved polynomial dependence on \(\epsilon\) compared to prior algorithms. It also shows that agnostic learning of convex sets under arbitrary distributions is impossible due to infinite VC-dimension, and highlights that proper learners yield tolerant property testers with matching runtimes. The work raises the question of whether an inherent gap exists between sample and time complexity for agnostic learning of natural concept classes.<br /><strong>Summary (CN):</strong> 本文研究了平面几何概念类的无情学习（agnostic learning）的时间复杂度，提出了针对三角形、四边形、五边形以及凸集的正式无情学习算法，并显著改进了对 \(\epsilon\) 的多项式依赖，优于已有方法。文中还证明在一般分布下无情学习凸集是不可能的，因为该概念类的 VC 维度无限，并指出正式学习器可以产生具有匹配运行时间的容错属性测试器。作者进一步讨论了样本复杂度与时间复杂度之间是否必然存在差距的问题。<br /><strong>Keywords:</strong> agnostic learning, computational geometry, triangles, convex polygons, sample complexity, time complexity, proper learner, tolerant property testing<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Talya Eden, Ludmila Glinskih, Sofya Raskhodnikova</div>
We investigate the computational efficiency of agnostic learning for several fundamental geometric concept classes in the plane. While the sample complexity of agnostic learning is well understood, its time complexity has received much less attention. We study the class of triangles and, more generally, the class of convex polygons with $k$ vertices for small $k$, as well as the class of convex sets in a square. We present a proper agnostic learner for the class of triangles that has optimal sample complexity and runs in time $\tilde O({\epsilon^{-6}})$, improving on the algorithm of Dobkin and Gunopulos (COLT `95) that runs in time $\tilde O({\epsilon^{-10}})$. For 4-gons and 5-gons, we improve the running time from $O({\epsilon^{-12}})$, achieved by Fischer and Kwek (eCOLT `96), to $\tilde O({\epsilon^{-8}})$ and $\tilde O({\epsilon^{-10}})$, respectively. We also design a proper agnostic learner for convex sets under the uniform distribution over a square with running time $\tilde O({\epsilon^{-5}})$, improving on the previous $\tilde O(\epsilon^{-8})$ bound at the cost of slightly higher sample complexity. Notably, agnostic learning of convex sets in $[0,1]^2$ under general distributions is impossible because this concept class has infinite VC-dimension. Our agnostic learners use data structures and algorithms from computational geometry and their analysis relies on tools from geometry and probabilistic combinatorics. Because our learners are proper, they yield tolerant property testers with matching running times. Our results raise a fundamental question of whether a gap between the sample and time complexity is inherent for agnostic learning of these and other natural concept classes.
<div><strong>Authors:</strong> Talya Eden, Ludmila Glinskih, Sofya Raskhodnikova</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the computational time complexity of agnostic learning for fundamental geometric concept classes in the plane, presenting proper agnostic learners for triangles, 4‑gons, 5‑gons, and convex sets with substantially improved polynomial dependence on \\(\\epsilon\\) compared to prior algorithms. It also shows that agnostic learning of convex sets under arbitrary distributions is impossible due to infinite VC-dimension, and highlights that proper learners yield tolerant property testers with matching runtimes. The work raises the question of whether an inherent gap exists between sample and time complexity for agnostic learning of natural concept classes.", "summary_cn": "本文研究了平面几何概念类的无情学习（agnostic learning）的时间复杂度，提出了针对三角形、四边形、五边形以及凸集的正式无情学习算法，并显著改进了对 \\(\\epsilon\\) 的多项式依赖，优于已有方法。文中还证明在一般分布下无情学习凸集是不可能的，因为该概念类的 VC 维度无限，并指出正式学习器可以产生具有匹配运行时间的容错属性测试器。作者进一步讨论了样本复杂度与时间复杂度之间是否必然存在差距的问题。", "keywords": "agnostic learning, computational geometry, triangles, convex polygons, sample complexity, time complexity, proper learner, tolerant property testing", "scoring": {"interpretability": 1, "understanding": 4, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Talya Eden", "Ludmila Glinskih", "Sofya Raskhodnikova"]}
]]></acme>

<pubDate>2025-10-20T19:49:33+00:00</pubDate>
</item>
<item>
<title>TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation</title>
<link>https://papers.cool/arxiv/2510.18038</link>
<guid>https://papers.cool/arxiv/2510.18038</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents TriggerNet, an explainable AI framework that combines Grad-CAM, RISE, FullGrad, and TCAV to generate visual explanations for deep learning models used in plant classification and red palm mite disease detection. It evaluates multiple CNN and transformer architectures as well as classic ML classifiers, and employs Snorkel to create heuristic‑guided annotations for disease categories, reducing manual labeling effort. Experiments on a diverse RGB dataset of eleven plant species demonstrate the framework’s ability to compare model performance and provide interpretable insights into predictions.<br /><strong>Summary (CN):</strong> 本文提出 TriggerNet，一种可解释 AI 框架，融合 Grad-CAM、RISE、FullGrad 和 TCAV 等方法，为用于植物分类和红棕螨（red palm mite）病害检测的深度学习模型生成可视化解释。论文评估了多种 CNN、ViT 等模型及传统机器学习分类器，并使用 Snorkel 通过启发式规则进行标注，降低人工标注成本。实验在包含 11 种植物的 RGB 数据集上展示了该框架在模型比较和提供可解释预测方面的效果。<br /><strong>Keywords:</strong> explainable AI, visual explanations, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, Snorkel, weak supervision<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
The red palm mite infestation has become a serious concern, particularly in regions with extensive palm cultivation, leading to reduced productivity and economic losses. Accurate and early identification of mite-infested plants is critical for effective management. The current study focuses on evaluating and comparing the ML model for classifying the affected plants and detecting the infestation. TriggerNet is a novel interpretable AI framework that integrates Grad-CAM, RISE, FullGrad, and TCAV to generate novel visual explanations for deep learning models in plant classification and disease detection. This study applies TriggerNet to address red palm mite (Raoiella indica) infestation, a major threat to palm cultivation and agricultural productivity. A diverse set of RGB images across 11 plant species, Arecanut, Date Palm, Bird of Paradise, Coconut Palm, Ginger, Citrus Tree, Palm Oil, Orchid, Banana Palm, Avocado Tree, and Cast Iron Plant was utilized for training and evaluation. Advanced deep learning models like CNN, EfficientNet, MobileNet, ViT, ResNet50, and InceptionV3, alongside machine learning classifiers such as Random Forest, SVM, and KNN, were employed for plant classification. For disease classification, all plants were categorized into four classes: Healthy, Yellow Spots, Reddish Bronzing, and Silk Webbing. Snorkel was used to efficiently label these disease classes by leveraging heuristic rules and patterns, reducing manual annotation time and improving dataset reliability.
<div><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents TriggerNet, an explainable AI framework that combines Grad-CAM, RISE, FullGrad, and TCAV to generate visual explanations for deep learning models used in plant classification and red palm mite disease detection. It evaluates multiple CNN and transformer architectures as well as classic ML classifiers, and employs Snorkel to create heuristic‑guided annotations for disease categories, reducing manual labeling effort. Experiments on a diverse RGB dataset of eleven plant species demonstrate the framework’s ability to compare model performance and provide interpretable insights into predictions.", "summary_cn": "本文提出 TriggerNet，一种可解释 AI 框架，融合 Grad-CAM、RISE、FullGrad 和 TCAV 等方法，为用于植物分类和红棕螨（red palm mite）病害检测的深度学习模型生成可视化解释。论文评估了多种 CNN、ViT 等模型及传统机器学习分类器，并使用 Snorkel 通过启发式规则进行标注，降低人工标注成本。实验在包含 11 种植物的 RGB 数据集上展示了该框架在模型比较和提供可解释预测方面的效果。", "keywords": "explainable AI, visual explanations, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, Snorkel, weak supervision", "scoring": {"interpretability": 7, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Harshini Suresha", "Kavitha SH"]}
]]></acme>

<pubDate>2025-10-20T19:23:17+00:00</pubDate>
</item>
<item>
<title>Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware</title>
<link>https://papers.cool/arxiv/2510.18036</link>
<guid>https://papers.cool/arxiv/2510.18036</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a hardware‑aware multimodal emotion recognition system that combines acoustic and linguistic features using a late‑fusion architecture optimized for Edge TPU. By integrating a quantised transformer‑based acoustic model with frozen keyword embeddings from a DSResNet‑SE network, the design achieves real‑time inference within a 1.8 MB memory budget and 21–23 ms latency, improving macro F1 by 6.3% over unimodal baselines on edge‑captured IEMOCAP data.<br /><strong>Summary (CN):</strong> 本文提出了一种面向硬件的多模态情感识别系统，采用在 Edge TPU 上优化的后期融合架构，将声学特征与语言特征相结合。通过将量化的 Transformer（声学模型）与 DSResNet‑SE 网络的冻结关键词嵌入融合，实现了 1.8 MB 内存限制和 21–23 ms 延迟的实时推理，在边缘设备捕获的 IEMOCAP 数据上相较于单模态基线提升了 6.3% 的宏 F1。<br /><strong>Keywords:</strong> emotion recognition, multimodal fusion, late fusion, transformer, edge TPU, quantization, low-power hardware, acoustic-linguistic features<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Stavros Mitsis, Ermos Hadjikyriakos, Humaid Ibrahim, Savvas Neofytou, Shashwat Raman, James Myles, Eiman Kanjo</div>
Deploying emotion recognition systems in real-world environments where devices must be small, low-power, and private remains a significant challenge. This is especially relevant for applications such as tension monitoring, conflict de-escalation, and responsive wearables, where cloud-based solutions are impractical. Multimodal emotion recognition has advanced through deep learning, but most systems remain unsuitable for deployment on ultra-constrained edge devices. Prior work typically relies on powerful hardware, lacks real-time performance, or uses unimodal input. This paper addresses that gap by presenting a hardware-aware emotion recognition system that combines acoustic and linguistic features using a late-fusion architecture optimised for Edge TPU. The design integrates a quantised transformer-based acoustic model with frozen keyword embeddings from a DSResNet-SE network, enabling real-time inference within a 1.8MB memory budget and 21-23ms latency. The pipeline ensures spectrogram alignment between training and deployment using MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP samples captured through the Coral Dev Board Micro microphone shows a 6.3% macro F1 improvement over unimodal baselines. This work demonstrates that accurate, real-time multimodal emotion inference is achievable on microcontroller-class edge platforms through task-specific fusion and hardware-guided model design.
<div><strong>Authors:</strong> Stavros Mitsis, Ermos Hadjikyriakos, Humaid Ibrahim, Savvas Neofytou, Shashwat Raman, James Myles, Eiman Kanjo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a hardware‑aware multimodal emotion recognition system that combines acoustic and linguistic features using a late‑fusion architecture optimized for Edge TPU. By integrating a quantised transformer‑based acoustic model with frozen keyword embeddings from a DSResNet‑SE network, the design achieves real‑time inference within a 1.8 MB memory budget and 21–23 ms latency, improving macro F1 by 6.3% over unimodal baselines on edge‑captured IEMOCAP data.", "summary_cn": "本文提出了一种面向硬件的多模态情感识别系统，采用在 Edge TPU 上优化的后期融合架构，将声学特征与语言特征相结合。通过将量化的 Transformer（声学模型）与 DSResNet‑SE 网络的冻结关键词嵌入融合，实现了 1.8 MB 内存限制和 21–23 ms 延迟的实时推理，在边缘设备捕获的 IEMOCAP 数据上相较于单模态基线提升了 6.3% 的宏 F1。", "keywords": "emotion recognition, multimodal fusion, late fusion, transformer, edge TPU, quantization, low-power hardware, acoustic-linguistic features", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Stavros Mitsis", "Ermos Hadjikyriakos", "Humaid Ibrahim", "Savvas Neofytou", "Shashwat Raman", "James Myles", "Eiman Kanjo"]}
]]></acme>

<pubDate>2025-10-20T19:18:22+00:00</pubDate>
</item>
<item>
<title>From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18030</link>
<guid>https://papers.cool/arxiv/2510.18030</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper revisits global structured pruning for large language models, introducing GISP-Global Iterative Structured Pruning, a post‑training method that uses first‑order loss‑based importance scores aggregated at the structure level with block‑wise normalization. By iteratively pruning attention heads and MLP channels while supporting task‑specific objectives, GISP achieves higher sparsity without perplexity collapse and enables a "prune‑once, deploy‑many" workflow, showing consistent improvements on Llama, Mistral, and DeepSeek models across language modeling and downstream tasks.<br /><strong>Summary (CN):</strong> 本文重新审视了大语言模型的全局结构化剪枝，提出了 GISP‑Global 迭代结构化剪枝方法，该方法在后训练阶段使用基于模型整体损失的一阶重要性评分并在结构层面进行块归一化聚合，以逐层剪除注意力头和 MLP 通道。通过迭代剪枝并支持任务特定目标，GISP 在保持或提升稀疏度的同时避免困惑度崩溃，实现“一次剪枝，多次部署”，在 Llama、Mistral 与 DeepSeek 系列模型的语言建模及下游任务上均表现出显著提升。<br /><strong>Keywords:</strong> structured pruning, large language models, global importance weighting, iterative pruning, sparsity, task-aligned calibration, efficiency, Llama, Mistral<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a "prune-once, deploy-many" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.
<div><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper revisits global structured pruning for large language models, introducing GISP-Global Iterative Structured Pruning, a post‑training method that uses first‑order loss‑based importance scores aggregated at the structure level with block‑wise normalization. By iteratively pruning attention heads and MLP channels while supporting task‑specific objectives, GISP achieves higher sparsity without perplexity collapse and enables a \"prune‑once, deploy‑many\" workflow, showing consistent improvements on Llama, Mistral, and DeepSeek models across language modeling and downstream tasks.", "summary_cn": "本文重新审视了大语言模型的全局结构化剪枝，提出了 GISP‑Global 迭代结构化剪枝方法，该方法在后训练阶段使用基于模型整体损失的一阶重要性评分并在结构层面进行块归一化聚合，以逐层剪除注意力头和 MLP 通道。通过迭代剪枝并支持任务特定目标，GISP 在保持或提升稀疏度的同时避免困惑度崩溃，实现“一次剪枝，多次部署”，在 Llama、Mistral 与 DeepSeek 系列模型的语言建模及下游任务上均表现出显著提升。", "keywords": "structured pruning, large language models, global importance weighting, iterative pruning, sparsity, task-aligned calibration, efficiency, Llama, Mistral", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"]}
]]></acme>

<pubDate>2025-10-20T19:04:09+00:00</pubDate>
</item>
<item>
<title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
<link>https://papers.cool/arxiv/2510.18016</link>
<guid>https://papers.cool/arxiv/2510.18016</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ViBED-Net, a dual‑stream deep network that jointly processes facial crops and full video frames with EfficientNetV2 to extract spatial features, then models their temporal dynamics using LSTM and Transformer encoders for student engagement detection in e‑learning videos. Evaluated on the DAiSEE dataset, the LSTM variant achieves 73.43% accuracy, surpassing prior state‑of‑the‑art methods and demonstrating the benefit of combining face‑aware and scene‑aware cues. The approach is modular and applicable to broader contexts such as user experience research and content personalization.<br /><strong>Summary (CN):</strong> 本文提出 ViBED-Net，一种双流深度网络，分别对面部裁剪和完整视频帧使用 EfficientNetV2 提取空间特征，再通过 LSTM 与 Transformer 编码器建模其时序信息，以实现在线学习场景中的学生参与度检测。 在 DAiSEE 数据集上，LSTM 版本取得 73.43% 的准确率，优于现有最先进方法，展示了面部与全场景线索结合的优势。 该模型结构模块化，可推广至用户体验研究和内容个性化等应用。<br /><strong>Keywords:</strong> engagement detection, video analysis, facial expression, scene context, EfficientNetV2, LSTM, transformer, affective computing, e-learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas</div>
Engagement detection in online learning environments is vital for improving student outcomes and personalizing instruction. We present ViBED-Net (Video-Based Engagement Detection Network), a novel deep learning framework designed to assess student engagement from video data using a dual-stream architecture. ViBED-Net captures both facial expressions and full-scene context by processing facial crops and entire video frames through EfficientNetV2 for spatial feature extraction. These features are then analyzed over time using two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and Transformer encoders. Our model is evaluated on the DAiSEE dataset, a large-scale benchmark for affective state recognition in e-learning. To enhance performance on underrepresented engagement classes, we apply targeted data augmentation techniques. Among the tested variants, ViBED-Net with LSTM achieves 73.43\% accuracy, outperforming existing state-of-the-art approaches. ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal cues significantly improves engagement detection accuracy. Its modular design allows flexibility for application across education, user experience research, and content personalization. This work advances video-based affective computing by offering a scalable, high-performing solution for real-world engagement analysis. The source code for this project is available on https://github.com/prateek-gothwal/ViBED-Net .
<div><strong>Authors:</strong> Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ViBED-Net, a dual‑stream deep network that jointly processes facial crops and full video frames with EfficientNetV2 to extract spatial features, then models their temporal dynamics using LSTM and Transformer encoders for student engagement detection in e‑learning videos. Evaluated on the DAiSEE dataset, the LSTM variant achieves 73.43% accuracy, surpassing prior state‑of‑the‑art methods and demonstrating the benefit of combining face‑aware and scene‑aware cues. The approach is modular and applicable to broader contexts such as user experience research and content personalization.", "summary_cn": "本文提出 ViBED-Net，一种双流深度网络，分别对面部裁剪和完整视频帧使用 EfficientNetV2 提取空间特征，再通过 LSTM 与 Transformer 编码器建模其时序信息，以实现在线学习场景中的学生参与度检测。 在 DAiSEE 数据集上，LSTM 版本取得 73.43% 的准确率，优于现有最先进方法，展示了面部与全场景线索结合的优势。 该模型结构模块化，可推广至用户体验研究和内容个性化等应用。", "keywords": "engagement detection, video analysis, facial expression, scene context, EfficientNetV2, LSTM, transformer, affective computing, e-learning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Prateek Gothwal", "Deeptimaan Banerjee", "Ashis Kumer Biswas"]}
]]></acme>

<pubDate>2025-10-20T18:48:25+00:00</pubDate>
</item>
<item>
<title>SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone</title>
<link>https://papers.cool/arxiv/2510.17998</link>
<guid>https://papers.cool/arxiv/2510.17998</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SimBA, a three‑phase framework (stalk, prowl, pounce) that uses only raw evaluation scores to analyze large language model benchmarks, discover a small representative subset of datasets, and predict performance on unseen models. Applied to HELM, MMLU, and BigBenchLite, SimBA achieves >95% coverage with a tiny fraction of datasets and can preserve model rankings with near‑zero mean‑squared error predictions. The approach aims to aid model developers in efficient training decisions and help dataset creators assess novelty relative to existing benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了 SimBA，一个由三个阶段（stalk、prowl、pounce）组成的框架，仅利用原始评估分数对大规模语言模型基准进行分析、发现小规模代表性数据子集，并预测未见模型的性能。在 HELM、MMLU 和 BigBenchLite 基准上的实验表明，SimBA 能以极少的数据子集实现>95%的覆盖，并且在保持模型排名的同时，预测误差接近零。该方法旨在帮助模型开发者提高训练效率，并帮助数据集创建者评估新数据集相对于现有基准的差异性。<br /><strong>Keywords:</strong> benchmark analysis, performance matrices, representative subset, model selection, LM benchmarks, HELM, MMLU, BigBenchLite<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
Modern language models are evaluated on large benchmarks, which are difficult to make sense of, especially for model selection. Looking at the raw evaluation numbers themselves using a model-centric lens, we propose SimBA, a three phase framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk, where we conduct dataset & model comparisons, prowl, where we discover a representative subset, and pounce, where we use the representative subset to predict performance on a held-out set of models. Applying SimBA to three popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all three benchmarks, datasets and models relate strongly to one another (stalk). We develop an representative set discovery algorithm which covers a benchmark using raw evaluation scores alone. Using our algorithm, we find that with 6.25% (1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl). Additionally, using just these representative subsets, we can both preserve model ranks and predict performance on a held-out set of models with near zero mean-squared error (pounce). Taken together, SimBA can help model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark. Our code is open source, available at https://github.com/nishantsubramani/simba.
<div><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SimBA, a three‑phase framework (stalk, prowl, pounce) that uses only raw evaluation scores to analyze large language model benchmarks, discover a small representative subset of datasets, and predict performance on unseen models. Applied to HELM, MMLU, and BigBenchLite, SimBA achieves >95% coverage with a tiny fraction of datasets and can preserve model rankings with near‑zero mean‑squared error predictions. The approach aims to aid model developers in efficient training decisions and help dataset creators assess novelty relative to existing benchmarks.", "summary_cn": "本文提出了 SimBA，一个由三个阶段（stalk、prowl、pounce）组成的框架，仅利用原始评估分数对大规模语言模型基准进行分析、发现小规模代表性数据子集，并预测未见模型的性能。在 HELM、MMLU 和 BigBenchLite 基准上的实验表明，SimBA 能以极少的数据子集实现>95%的覆盖，并且在保持模型排名的同时，预测误差接近零。该方法旨在帮助模型开发者提高训练效率，并帮助数据集创建者评估新数据集相对于现有基准的差异性。", "keywords": "benchmark analysis, performance matrices, representative subset, model selection, LM benchmarks, HELM, MMLU, BigBenchLite", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Subramani", "Alfredo Gomez", "Mona Diab"]}
]]></acme>

<pubDate>2025-10-20T18:23:27+00:00</pubDate>
</item>
<item>
<title>QINNs: Quantum-Informed Neural Networks</title>
<link>https://papers.cool/arxiv/2510.17984</link>
<guid>https://papers.cool/arxiv/2510.17984</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Quantum-Informed Neural Networks (QINNs), a framework that incorporates quantum information concepts, specifically the Quantum Fisher Information Matrix (QFIM), into classical deep learning models for collider physics. Using jet tagging as an example, QFIM embeddings are incorporated into graph neural networks, improving expressivity and providing interpretable patterns that differentiate QCD and hadronic top jets. The approach demonstrates a practical, scalable way to embed quantum-inspired, physics‑grounded features into existing deep learning pipelines.<br /><strong>Summary (CN):</strong> 本文提出量子信息神经网络（QINNs）框架，将量子信息概念，尤其是量子费舍尔信息矩阵（Quantum Fisher Information Matrix, QFIM），引入纯经典的深度学习模型用于碰撞器数据分析。以 Jet 标记任务为案例，将 QFIM 作为轻量级嵌入加入图神经网络，提升模型的表达能力，并展示了 QCD 与强子顶喷流在 QFIM 表征上的可解释差异。该方法提供了一种实用、可扩展的方式，将量子启发的物理特征嵌入到已有的深度学习流程中。<br /><strong>Keywords:</strong> quantum-informed neural networks, quantum Fisher information matrix, graph neural networks, jet tagging, particle physics, feature embeddings, interpretability, collider data<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Aritra Bal, Markus Klute, Benedikt Maier, Melik Oughton, Eric Pezone, Michael Spannowsky</div>
Classical deep neural networks can learn rich multi-particle correlations in collider data, but their inductive biases are rarely anchored in physics structure. We propose quantum-informed neural networks (QINNs), a general framework that brings quantum information concepts and quantum observables into purely classical models. While the framework is broad, in this paper, we study one concrete realisation that encodes each particle as a qubit and uses the Quantum Fisher Information Matrix (QFIM) as a compact, basis-independent summary of particle correlations. Using jet tagging as a case study, QFIMs act as lightweight embeddings in graph neural networks, increasing model expressivity and plasticity. The QFIM reveals distinct patterns for QCD and hadronic top jets that align with physical expectations. Thus, QINNs offer a practical, interpretable, and scalable route to quantum-informed analyses, that is, tomography, of particle collisions, particularly by enhancing well-established deep learning approaches.
<div><strong>Authors:</strong> Aritra Bal, Markus Klute, Benedikt Maier, Melik Oughton, Eric Pezone, Michael Spannowsky</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Quantum-Informed Neural Networks (QINNs), a framework that incorporates quantum information concepts, specifically the Quantum Fisher Information Matrix (QFIM), into classical deep learning models for collider physics. Using jet tagging as an example, QFIM embeddings are incorporated into graph neural networks, improving expressivity and providing interpretable patterns that differentiate QCD and hadronic top jets. The approach demonstrates a practical, scalable way to embed quantum-inspired, physics‑grounded features into existing deep learning pipelines.", "summary_cn": "本文提出量子信息神经网络（QINNs）框架，将量子信息概念，尤其是量子费舍尔信息矩阵（Quantum Fisher Information Matrix, QFIM），引入纯经典的深度学习模型用于碰撞器数据分析。以 Jet 标记任务为案例，将 QFIM 作为轻量级嵌入加入图神经网络，提升模型的表达能力，并展示了 QCD 与强子顶喷流在 QFIM 表征上的可解释差异。该方法提供了一种实用、可扩展的方式，将量子启发的物理特征嵌入到已有的深度学习流程中。", "keywords": "quantum-informed neural networks, quantum Fisher information matrix, graph neural networks, jet tagging, particle physics, feature embeddings, interpretability, collider data", "scoring": {"interpretability": 5, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Aritra Bal", "Markus Klute", "Benedikt Maier", "Melik Oughton", "Eric Pezone", "Michael Spannowsky"]}
]]></acme>

<pubDate>2025-10-20T18:03:15+00:00</pubDate>
</item>
<item>
<title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title>
<link>https://papers.cool/arxiv/2510.17959</link>
<guid>https://papers.cool/arxiv/2510.17959</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a universal spectral tokenizer that learns self-supervised representations from heterogeneous astronomical spectra across different object types and resolutions, processing them on native wavelength grids. The resulting aligned, homogeneous embeddings can be efficiently adapted to a variety of downstream tasks, demonstrating that a single model can unify spectral data across domains. This approach is presented as a building block for foundation models in astronomy and potentially other scientific fields with sequential data.<br /><strong>Summary (CN):</strong> 本文提出了一种通用光谱分词器，能够在自监督方式下从不同天体类型和分辨率的异构天文光谱中学习表征，直接在原始波长网格上处理数据。得到的对齐且同质的嵌入可高效适配多种下游任务，展示单一模型能够统一跨域光谱数据。作者将该方法视为天文学基础模型的关键构件，并有望拓展至其他具有异构序列数据的科学领域（如气候与医疗）。<br /><strong>Keywords:</strong> spectral tokenization, self-supervised learning, heterogeneous spectra, foundation models, astronomy, representation learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho</div>
Sequential scientific data span many resolutions and domains, and unifying them into a common representation is a key step toward developing foundation models for the sciences. Astronomical spectra exemplify this challenge: massive surveys have collected millions of spectra across a wide range of wavelengths and resolutions, yet analyses remain fragmented across spectral domains (e.g., optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting the ability to pool information across datasets. We present a deep learning model that jointly learns from heterogeneous spectra in a self-supervised manner. Our universal spectral tokenizer processes spectra from a variety of object types and resolutions directly on their native wavelength grids, producing intrinsically aligned, homogeneous, and physically meaningful representations that can be efficiently adapted to achieve competitive performance across a range of downstream tasks. For the first time, we demonstrate that a single model can unify spectral data across resolutions and domains, suggesting that our model can serve as a powerful building block for foundation models in astronomy -- and potentially extend to other scientific domains with heterogeneous sequential data, such as climate and healthcare.
<div><strong>Authors:</strong> Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a universal spectral tokenizer that learns self-supervised representations from heterogeneous astronomical spectra across different object types and resolutions, processing them on native wavelength grids. The resulting aligned, homogeneous embeddings can be efficiently adapted to a variety of downstream tasks, demonstrating that a single model can unify spectral data across domains. This approach is presented as a building block for foundation models in astronomy and potentially other scientific fields with sequential data.", "summary_cn": "本文提出了一种通用光谱分词器，能够在自监督方式下从不同天体类型和分辨率的异构天文光谱中学习表征，直接在原始波长网格上处理数据。得到的对齐且同质的嵌入可高效适配多种下游任务，展示单一模型能够统一跨域光谱数据。作者将该方法视为天文学基础模型的关键构件，并有望拓展至其他具有异构序列数据的科学领域（如气候与医疗）。", "keywords": "spectral tokenization, self-supervised learning, heterogeneous spectra, foundation models, astronomy, representation learning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jeff Shen", "Francois Lanusse", "Liam Holden Parker", "Ollie Liu", "Tom Hehir", "Leopoldo Sarra", "Lucas Meyer", "Micah Bowles", "Sebastian Wagner-Carena", "Sebastian Wagner-Carena", "Helen Qu", "Siavash Golkar", "Alberto Bietti", "Hatim Bourfoune", "Nathan Cassereau", "Pierre Cornette", "Keiya Hirashima", "Geraud Krawezik", "Ruben Ohana", "Nicholas Lourie", "Michael McCabe", "Rudy Morel", "Payel Mukhopadhyay", "Mariel Pettee", "Bruno Régaldo-Saint Blancard", "Kyunghyun Cho", "Miles Cranmer", "Shirley Ho"]}
]]></acme>

<pubDate>2025-10-20T18:00:00+00:00</pubDate>
</item>
<item>
<title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>
<link>https://papers.cool/arxiv/2510.17947</link>
<guid>https://papers.cool/arxiv/2510.17947</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PLAGUE, a plug‑and‑play framework that structures multi‑turn jailbreak attacks on large language models into three phases—Primer, Planner, and Finisher—drawing on lifelong‑learning agents to systematically explore attack families. Experiments show the red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack success by over 30% and reaching 81.4% on OpenAI o3 and 67.3% on Claude Opus 4.1. The work provides tools and insights into plan initialization, context optimization, and lifelong learning for comprehensive evaluation of model vulnerabilities.<br /><strong>Summary (CN):</strong> 本文提出 PLAGUE 框架，将针对大语言模型的多轮 jailbreak 攻击拆解为 Primer、Planner 和 Finisher 三个阶段，借鉴终身学习代理以系统化探索攻击族。实验显示，使用 PLAGUE 构建的红队代理在攻击成功率上提升超过 30%，在 OpenAI o3 上达到 81.4%、Claude Opus 4.1 上达到 67.3%。该工作提供了关于计划初始化、上下文优化和终身学习的工具与洞见，用于全面评估模型脆弱性。<br /><strong>Keywords:</strong> jailbreak, multi-turn attacks, lifelong learning, red-teaming, LLM safety, prompt engineering, adversarial attacks, vulnerability evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.
<div><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PLAGUE, a plug‑and‑play framework that structures multi‑turn jailbreak attacks on large language models into three phases—Primer, Planner, and Finisher—drawing on lifelong‑learning agents to systematically explore attack families. Experiments show the red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack success by over 30% and reaching 81.4% on OpenAI o3 and 67.3% on Claude Opus 4.1. The work provides tools and insights into plan initialization, context optimization, and lifelong learning for comprehensive evaluation of model vulnerabilities.", "summary_cn": "本文提出 PLAGUE 框架，将针对大语言模型的多轮 jailbreak 攻击拆解为 Primer、Planner 和 Finisher 三个阶段，借鉴终身学习代理以系统化探索攻击族。实验显示，使用 PLAGUE 构建的红队代理在攻击成功率上提升超过 30%，在 OpenAI o3 上达到 81.4%、Claude Opus 4.1 上达到 67.3%。该工作提供了关于计划初始化、上下文优化和终身学习的工具与洞见，用于全面评估模型脆弱性。", "keywords": "jailbreak, multi-turn attacks, lifelong learning, red-teaming, LLM safety, prompt engineering, adversarial attacks, vulnerability evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"]}
]]></acme>

<pubDate>2025-10-20T17:37:03+00:00</pubDate>
</item>
<item>
<title>XDXD: End-to-end crystal structure determination with low resolution X-ray diffraction</title>
<link>https://papers.cool/arxiv/2510.17936</link>
<guid>https://papers.cool/arxiv/2510.17936</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> XDXD is an end-to-end diffusion-based generative deep‑learning framework that directly predicts complete atomic models from low‑resolution single‑crystal X‑ray diffraction data, bypassing manual electron‑density interpretation. Evaluated on 24,000 experimental structures, it achieves a 70.4 % match rate at 2.0 Å resolution with RMSE below 0.05, and demonstrates its potential on small peptide cases.<br /><strong>Summary (CN):</strong> XDXD 是一种基于扩散的端到端深度学习框架，能够直接从低分辨率单晶 X 射线衍射数据生成完整的原子模型，省去人工电子密度图的解释。在 24,000 个实验结构的基准测试中，该模型在 2.0 Å 分辨率下实现 70.4% 的匹配率，RMSE 低于 0.05，并在小肽案例中展示了潜力。<br /><strong>Keywords:</strong> crystal structure determination, X-ray diffraction, diffusion generative model, deep learning, low-resolution, atomic model prediction, protein crystallography<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiale Zhao, Cong Liu, Yuxuan Zhang, Chengyue Gong, Zhenyi Zhang, Shifeng Jin, Zhenyu Liu</div>
Determining crystal structures from X-ray diffraction data is fundamental across diverse scientific fields, yet remains a significant challenge when data is limited to low resolution. While recent deep learning models have made breakthroughs in solving the crystallographic phase problem, the resulting low-resolution electron density maps are often ambiguous and difficult to interpret. To overcome this critical bottleneck, we introduce XDXD, to our knowledge, the first end-to-end deep learning framework to determine a complete atomic model directly from low-resolution single-crystal X-ray diffraction data. Our diffusion-based generative model bypasses the need for manual map interpretation, producing chemically plausible crystal structures conditioned on the diffraction pattern. We demonstrate that XDXD achieves a 70.4\% match rate for structures with data limited to 2.0~\AA{} resolution, with a root-mean-square error (RMSE) below 0.05. Evaluated on a benchmark of 24,000 experimental structures, our model proves to be robust and accurate. Furthermore, a case study on small peptides highlights the model's potential for extension to more complex systems, paving the way for automated structure solution in previously intractable cases.
<div><strong>Authors:</strong> Jiale Zhao, Cong Liu, Yuxuan Zhang, Chengyue Gong, Zhenyi Zhang, Shifeng Jin, Zhenyu Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "XDXD is an end-to-end diffusion-based generative deep‑learning framework that directly predicts complete atomic models from low‑resolution single‑crystal X‑ray diffraction data, bypassing manual electron‑density interpretation. Evaluated on 24,000 experimental structures, it achieves a 70.4 % match rate at 2.0 Å resolution with RMSE below 0.05, and demonstrates its potential on small peptide cases.", "summary_cn": "XDXD 是一种基于扩散的端到端深度学习框架，能够直接从低分辨率单晶 X 射线衍射数据生成完整的原子模型，省去人工电子密度图的解释。在 24,000 个实验结构的基准测试中，该模型在 2.0 Å 分辨率下实现 70.4% 的匹配率，RMSE 低于 0.05，并在小肽案例中展示了潜力。", "keywords": "crystal structure determination, X-ray diffraction, diffusion generative model, deep learning, low-resolution, atomic model prediction, protein crystallography", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiale Zhao", "Cong Liu", "Yuxuan Zhang", "Chengyue Gong", "Zhenyi Zhang", "Shifeng Jin", "Zhenyu Liu"]}
]]></acme>

<pubDate>2025-10-20T15:50:21+00:00</pubDate>
</item>
<item>
<title>Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy</title>
<link>https://papers.cool/arxiv/2510.17916</link>
<guid>https://papers.cool/arxiv/2510.17916</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a constructive proof that the Free Energy Principle can be implemented through exact local credit assignment by decomposing gradient computation hierarchically into spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map. Empirical results show the TFM predicts oracle gradients with 0.9693 Pearson correlation and yields emergent properties such as high retention after interference, autonomous recovery from structural damage, self‑organized criticality, and sample‑efficient reinforcement learning without replay buffers. The work bridges concepts from dissipative structures, free‑energy minimization, and attractor dynamics, suggesting that biologically plausible local rules can realize hierarchical inference over network topology.<br /><strong>Summary (CN):</strong> 本文提供了一个构造性证明，表明自由能原理（Free Energy Principle）可以通过精确的局部信用分配实现，其方式是将梯度计算层次化分解：空间信用通过反馈对齐（feedback alignment）实现，时间信用通过资格迹（eligibility traces）实现，结构信用通过营养场映射（Trophic Field Map）实现。实验结果显示，营养场映射对梯度的预测相关系数为0.9693，并且出现了如干扰后高保留率、结构损坏后自主恢复、自组织临界性以及无需重放缓冲区的样本高效连续控制强化学习等涌现能力。该工作将耗散结构、自由能最小化和吸引子动力学等概念统一起来，表明生物学上可行的局部规则能够实现对网络拓扑的层次推断。<br /><strong>Keywords:</strong> free energy principle, hierarchical gradient decomposition, feedback alignment, eligibility traces, trophic field map, local credit assignment, self-organized criticality, sample-efficient reinforcement learning, non-equilibrium steady state<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Michael James McCulloch</div>
The Free Energy Principle (FEP) states that self-organizing systems must minimize variational free energy to persist, but the path from principle to implementable algorithm has remained unclear. We present a constructive proof that the FEP can be realized through exact local credit assignment. The system decomposes gradient computation hierarchically: spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map (TFM) that estimates expected gradient magnitude for each connection block. We prove these mechanisms are exact at their respective levels and validate the central claim empirically: the TFM achieves 0.9693 Pearson correlation with oracle gradients. This exactness produces emergent capabilities including 98.6% retention after task interference, autonomous recovery from 75% structural damage, self-organized criticality (spectral radius p ~= 1.0$), and sample-efficient reinforcement learning on continuous control tasks without replay buffers. The architecture unifies Prigogine's dissipative structures, Friston's free energy minimization, and Hopfield's attractor dynamics, demonstrating that exact hierarchical inference over network topology can be implemented with local, biologically plausible rules.
<div><strong>Authors:</strong> Michael James McCulloch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a constructive proof that the Free Energy Principle can be implemented through exact local credit assignment by decomposing gradient computation hierarchically into spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map. Empirical results show the TFM predicts oracle gradients with 0.9693 Pearson correlation and yields emergent properties such as high retention after interference, autonomous recovery from structural damage, self‑organized criticality, and sample‑efficient reinforcement learning without replay buffers. The work bridges concepts from dissipative structures, free‑energy minimization, and attractor dynamics, suggesting that biologically plausible local rules can realize hierarchical inference over network topology.", "summary_cn": "本文提供了一个构造性证明，表明自由能原理（Free Energy Principle）可以通过精确的局部信用分配实现，其方式是将梯度计算层次化分解：空间信用通过反馈对齐（feedback alignment）实现，时间信用通过资格迹（eligibility traces）实现，结构信用通过营养场映射（Trophic Field Map）实现。实验结果显示，营养场映射对梯度的预测相关系数为0.9693，并且出现了如干扰后高保留率、结构损坏后自主恢复、自组织临界性以及无需重放缓冲区的样本高效连续控制强化学习等涌现能力。该工作将耗散结构、自由能最小化和吸引子动力学等概念统一起来，表明生物学上可行的局部规则能够实现对网络拓扑的层次推断。", "keywords": "free energy principle, hierarchical gradient decomposition, feedback alignment, eligibility traces, trophic field map, local credit assignment, self-organized criticality, sample-efficient reinforcement learning, non-equilibrium steady state", "scoring": {"interpretability": 5, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Michael James McCulloch"]}
]]></acme>

<pubDate>2025-10-20T00:19:32+00:00</pubDate>
</item>
<item>
<title>Learning Time-Varying Graphs from Incomplete Graph Signals</title>
<link>https://papers.cool/arxiv/2510.17903</link>
<guid>https://papers.cool/arxiv/2510.17903</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified non-convex optimization framework that simultaneously infers a sequence of time-varying graph Laplacians and imputes missing entries in partially observed graph signals. By incorporating a fused-lasso regularizer to enforce temporal smoothness and solving the problem with an efficient ADMM algorithm, the method achieves theoretical convergence guarantees and non-asymptotic error bounds, demonstrating superior performance over existing baselines in experiments.<br /><strong>Summary (CN):</strong> 本文提出一种统一的非凸优化框架，能够同步推断随时间变化的图拉普拉斯矩阵并填补部分观测图信号中的缺失数据。通过在拉普拉斯序列上加入 fused-lasso 类型的正则化实现时间平滑，并利用 ADMM 设计算法得到闭式解，提供收敛至驻点的理论保证以及非渐近误差界，实验表明在收敛速度和图学习、信号恢复精度上均显著优于现有基线。<br /><strong>Keywords:</strong> time-varying graph learning, graph signal processing, missing data imputation, fused lasso, ADMM, non-convex optimization, Laplacian estimation, statistical guarantees<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chuansen Peng, Xiaojing Shen</div>
This paper tackles the challenging problem of jointly inferring time-varying network topologies and imputing missing data from partially observed graph signals. We propose a unified non-convex optimization framework to simultaneously recover a sequence of graph Laplacian matrices while reconstructing the unobserved signal entries. Unlike conventional decoupled methods, our integrated approach facilitates a bidirectional flow of information between the graph and signal domains, yielding superior robustness, particularly in high missing-data regimes. To capture realistic network dynamics, we introduce a fused-lasso type regularizer on the sequence of Laplacians. This penalty promotes temporal smoothness by penalizing large successive changes, thereby preventing spurious variations induced by noise while still permitting gradual topological evolution. For solving the joint optimization problem, we develop an efficient Alternating Direction Method of Multipliers (ADMM) algorithm, which leverages the problem's structure to yield closed-form solutions for both the graph and signal subproblems. This design ensures scalability to large-scale networks and long time horizons. On the theoretical front, despite the inherent non-convexity, we establish a convergence guarantee, proving that the proposed ADMM scheme converges to a stationary point. Furthermore, we derive non-asymptotic statistical guarantees, providing high-probability error bounds for the graph estimator as a function of sample size, signal smoothness, and the intrinsic temporal variability of the graph. Extensive numerical experiments validate the approach, demonstrating that it significantly outperforms state-of-the-art baselines in both convergence speed and the joint accuracy of graph learning and signal recovery.
<div><strong>Authors:</strong> Chuansen Peng, Xiaojing Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified non-convex optimization framework that simultaneously infers a sequence of time-varying graph Laplacians and imputes missing entries in partially observed graph signals. By incorporating a fused-lasso regularizer to enforce temporal smoothness and solving the problem with an efficient ADMM algorithm, the method achieves theoretical convergence guarantees and non-asymptotic error bounds, demonstrating superior performance over existing baselines in experiments.", "summary_cn": "本文提出一种统一的非凸优化框架，能够同步推断随时间变化的图拉普拉斯矩阵并填补部分观测图信号中的缺失数据。通过在拉普拉斯序列上加入 fused-lasso 类型的正则化实现时间平滑，并利用 ADMM 设计算法得到闭式解，提供收敛至驻点的理论保证以及非渐近误差界，实验表明在收敛速度和图学习、信号恢复精度上均显著优于现有基线。", "keywords": "time-varying graph learning, graph signal processing, missing data imputation, fused lasso, ADMM, non-convex optimization, Laplacian estimation, statistical guarantees", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chuansen Peng", "Xiaojing Shen"]}
]]></acme>

<pubDate>2025-10-19T11:12:13+00:00</pubDate>
</item>
<item>
<title>TritonRL: Training LLMs to Think and Code Triton Without Cheating</title>
<link>https://papers.cool/arxiv/2510.17891</link>
<guid>https://papers.cool/arxiv/2510.17891</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces TritonRL, a domain‑specialized large language model for generating Triton kernels, trained via supervised fine‑tuning on curated datasets and further improved with a reinforcement‑learning framework that employs robust, verifiable rewards and hierarchical reward decomposition to mitigate reward hacking. Experiments on KernelBench show that TritonRL achieves state‑of‑the‑art correctness and speedup compared to existing Triton‑specific models, demonstrating the effectiveness of the RL‑based training paradigm for high‑performance kernel synthesis.<br /><strong>Summary (CN):</strong> 本文提出 TritonRL——一种面向 Triton 内核生成的专用大型语言模型，通过在精选数据集上进行监督微调后，再利用包含可验证奖励和层级奖励分解的强化学习框架进行进一步优化，以防止奖励黑客行为。实验在 KernelBench 上表明，TritonRL 在正确性和加速方面均超过现有的 Triton 专用模型，验证了基于强化学习的训练范式在高性能内核合成中的有效性。<br /><strong>Keywords:</strong> Triton, LLM, code generation, reinforcement learning, reward hacking, hierarchical rewards, kernel synthesis, verification, domain-specific language<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiin Woo, Shaowei Zhu, Allen Nie, Zhen Jia, Yida Wang, Youngsuk Park</div>
With the rapid evolution of large language models (LLMs), the demand for automated, high-performance system kernels has emerged as a key enabler for accelerating development and deployment. We introduce TritonRL, a domain-specialized LLM for Triton kernel generation, trained with a novel training framework that enables robust and automated kernel synthesis. Unlike general-purpose programming languages, Triton kernel generation faces unique challenges due to data scarcity and incomplete evaluation criteria, vulnerable to reward hacking. Our approach addresses these challenges end-to-end by distilling Triton-specific knowledge through supervised fine-tuning on curated datasets, and further improving code quality via reinforcement learning (RL) with robust, verifiable rewards and hierarchical reward assignment. Our RL framework robustly detects reward hacking and guides both reasoning traces and code tokens through fine-grained verification and hierarchical reward decomposition, enabling the model to generate high-quality Triton kernels that can truly replace existing modules. With robust and fine-grained evaluation, our experiments on KernelBench demonstrate that TritonRL achieves state-of-the-art correctness and speedup, surpassing all other Triton-specific models and underscoring the effectiveness of our RL-based training paradigm.
<div><strong>Authors:</strong> Jiin Woo, Shaowei Zhu, Allen Nie, Zhen Jia, Yida Wang, Youngsuk Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces TritonRL, a domain‑specialized large language model for generating Triton kernels, trained via supervised fine‑tuning on curated datasets and further improved with a reinforcement‑learning framework that employs robust, verifiable rewards and hierarchical reward decomposition to mitigate reward hacking. Experiments on KernelBench show that TritonRL achieves state‑of‑the‑art correctness and speedup compared to existing Triton‑specific models, demonstrating the effectiveness of the RL‑based training paradigm for high‑performance kernel synthesis.", "summary_cn": "本文提出 TritonRL——一种面向 Triton 内核生成的专用大型语言模型，通过在精选数据集上进行监督微调后，再利用包含可验证奖励和层级奖励分解的强化学习框架进行进一步优化，以防止奖励黑客行为。实验在 KernelBench 上表明，TritonRL 在正确性和加速方面均超过现有的 Triton 专用模型，验证了基于强化学习的训练范式在高性能内核合成中的有效性。", "keywords": "Triton, LLM, code generation, reinforcement learning, reward hacking, hierarchical rewards, kernel synthesis, verification, domain-specific language", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiin Woo", "Shaowei Zhu", "Allen Nie", "Zhen Jia", "Yida Wang", "Youngsuk Park"]}
]]></acme>

<pubDate>2025-10-18T21:36:10+00:00</pubDate>
</item>
<item>
<title>Graphical model for tensor factorization by sparse sampling</title>
<link>https://papers.cool/arxiv/2510.17886</link>
<guid>https://papers.cool/arxiv/2510.17886</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a graphical model for tensor factorization based on sparse measurements arranged according to a graph, targeting scenarios with substantial missing data such as recommendation systems. It develops message‑passing algorithms and a replica theory that become exact in a dense‑limit regime, and evaluates performance in a Bayes‑optimal teacher‑student setting.<br /><strong>Summary (CN):</strong> 本文提出一种基于稀疏采样的张量分解图模型，利用随机图结构处理数据缺失严重的场景（如推荐系统）。作者构建了消息传递算法并在密集极限下推导了精确的复制理论，在贝叶斯最优的师生设定中评估了统计推断性能。<br /><strong>Keywords:</strong> tensor factorization, sparse sampling, graphical model, message passing, replica theory, Bayesian inference, recommendation systems<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Angelo Giorgio, Riki Nagasawa, Shuta Yokoi, Tomoyuki Obuchi, Hajime Yoshino</div>
We consider tensor factorizations based on sparse measurements of the tensor components. The measurements are designed in a way that the underlying graph of interactions is a random graph. The setup will be useful in cases where a substantial amount of data is missing, as in recommendation systems heavily used in social network services. In order to obtain theoretical insights on the setup, we consider statistical inference of the tensor factorization in a high dimensional limit, which we call as dense limit, where the graphs are large and dense but not fully connected. We build message-passing algorithms and test them in a Bayes optimal teacher-student setting. We also develop a replica theory, which becomes exact in the dense limit,to examine the performance of statistical inference.
<div><strong>Authors:</strong> Angelo Giorgio, Riki Nagasawa, Shuta Yokoi, Tomoyuki Obuchi, Hajime Yoshino</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a graphical model for tensor factorization based on sparse measurements arranged according to a graph, targeting scenarios with substantial missing data such as recommendation systems. It develops message‑passing algorithms and a replica theory that become exact in a dense‑limit regime, and evaluates performance in a Bayes‑optimal teacher‑student setting.", "summary_cn": "本文提出一种基于稀疏采样的张量分解图模型，利用随机图结构处理数据缺失严重的场景（如推荐系统）。作者构建了消息传递算法并在密集极限下推导了精确的复制理论，在贝叶斯最优的师生设定中评估了统计推断性能。", "keywords": "tensor factorization, sparse sampling, graphical model, message passing, replica theory, Bayesian inference, recommendation systems", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Angelo Giorgio", "Riki Nagasawa", "Shuta Yokoi", "Tomoyuki Obuchi", "Hajime Yoshino"]}
]]></acme>

<pubDate>2025-10-18T06:06:21+00:00</pubDate>
</item>
<item>
<title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title>
<link>https://papers.cool/arxiv/2510.17884</link>
<guid>https://papers.cool/arxiv/2510.17884</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper empirically evaluates several open‑source large language models on the task of password guessing from synthetic user profiles and finds that they achieve less than 1.5% Hit@10, far behind traditional rule‑based methods. Detailed analysis attributes the poor performance to limited domain adaptation and memorization capabilities of current LLMs when not fine‑tuned on leaked password data. The results highlight a key limitation of LLMs for adversarial cybersecurity applications.<br /><strong>Summary (CN):</strong> 本文对多种开源大语言模型在基于合成用户信息进行密码猜测的任务上进行实证评估，发现其 Hit@10 低于 1.5%，远逊于传统规则式方法。通过分析指出，当前 LLM 在缺乏泄露密码数据微调的情况下，缺乏足够的领域适应和记忆能力，导致在密码推断上表现不佳。研究揭示了 LLM 在对抗性网络安全场景中的关键局限性。<br /><strong>Keywords:</strong> LLM, password cracking, cybersecurity, empirical study, adversarial misuse, language models, security, generative reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other<br /><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</div>
The remarkable capabilities of Large Language Models (LLMs) in natural language understanding and generation have sparked interest in their potential for cybersecurity applications, including password guessing. In this study, we conduct an empirical investigation into the efficacy of pre-trained LLMs for password cracking using synthetic user profiles. Specifically, we evaluate the performance of state-of-the-art open-source LLMs such as TinyLLaMA, Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords based on structured user attributes (e.g., name, birthdate, hobbies). Our results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext and SHA-256 hash comparisons, reveal consistently poor performance, with all models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional rule-based and combinator-based cracking methods demonstrate significantly higher success rates. Through detailed analysis and visualization, we identify key limitations in the generative reasoning of LLMs when applied to the domain-specific task of password guessing. Our findings suggest that, despite their linguistic prowess, current LLMs lack the domain adaptation and memorization capabilities required for effective password inference, especially in the absence of supervised fine-tuning on leaked password datasets. This study provides critical insights into the limitations of LLMs in adversarial contexts and lays the groundwork for future efforts in secure, privacy-preserving, and robust password modeling.
<div><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper empirically evaluates several open‑source large language models on the task of password guessing from synthetic user profiles and finds that they achieve less than 1.5% Hit@10, far behind traditional rule‑based methods. Detailed analysis attributes the poor performance to limited domain adaptation and memorization capabilities of current LLMs when not fine‑tuned on leaked password data. The results highlight a key limitation of LLMs for adversarial cybersecurity applications.", "summary_cn": "本文对多种开源大语言模型在基于合成用户信息进行密码猜测的任务上进行实证评估，发现其 Hit@10 低于 1.5%，远逊于传统规则式方法。通过分析指出，当前 LLM 在缺乏泄露密码数据微调的情况下，缺乏足够的领域适应和记忆能力，导致在密码推断上表现不佳。研究揭示了 LLM 在对抗性网络安全场景中的关键局限性。", "keywords": "LLM, password cracking, cybersecurity, empirical study, adversarial misuse, language models, security, generative reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}, "authors": ["Mohammad Abdul Rehman", "Syed Imad Ali Shah", "Abbas Anwar", "Noor Islam"]}
]]></acme>

<pubDate>2025-10-18T02:15:28+00:00</pubDate>
</item>
<item>
<title>From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15</title>
<link>https://papers.cool/arxiv/2510.17883</link>
<guid>https://papers.cool/arxiv/2510.17883</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates a prompt‑only approach using zero‑ and few‑shot large language models (LLMs) for network intrusion detection on the UNSW‑NB15 dataset, converting network flows into compact textual records enriched with lightweight boolean flags and constraining responses with a grammar. Experiments compare zero‑shot, instruction‑guided, and few‑shot prompting against strong tabular and neural baselines, showing that instruction‑guided prompts with calibrated scoring improve macro‑F1 scores but still lag behind traditional models as data size grows. Contributions include a flow‑to‑text protocol, a calibration method for decision thresholds, and a reproducibility bundle of prompts and evaluation scripts.<br /><strong>Summary (CN):</strong> 本文评估了在 UNSW‑NB15 数据集上使用零样本和少样本大语言模型（LLM）进行网络入侵检测的纯提示方法，将网络流转换为紧凑的文本记录并添加轻量布尔特征，同时通过语法约束保证输出结构化。实验比较了零样本、指令引导和少样本提示与强大的表格及神经基线，发现指令引导加校准的提示显著提升了 macro‑F1，但在数据规模扩大时仍不及传统模型。主要贡献包括流转文本协议、阈值校准方法以及包含提示、语法和评估脚本的可复现代码包。<br /><strong>Keywords:</strong> network intrusion detection, large language models, -shot, few-shot, prompt engineering, calibration, grammar-constrained, cybersecurity, UNSW-NB15<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas n=Anwar, Noor Islam</div>
Large Language Models (LLMs) can reason over natural-language inputs, but their role in intrusion detection without fine-tuning remains uncertain. This study evaluates a prompt-only approach on UNSW-NB15 by converting each network flow to a compact textual record and augmenting it with lightweight, domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer anomalies, rare service/state, short bursts). To reduce output drift and support measurement, the model is constrained to produce structured, grammar-valid responses, and a single decision threshold is calibrated on a small development split. We compare zero-shot, instruction-guided, and few-shot prompting to strong tabular and neural baselines under identical splits, reporting accuracy, precision, recall, F1, and macro scores. Empirically, unguided prompting is unreliable, while instructions plus flags substantially improve detection quality; adding calibrated scoring further stabilizes results. On a balanced subset of two hundred flows, a 7B instruction-tuned model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot cues and calibration attains F1 near 0.68 on one thousand examples. As the evaluation set grows to two thousand flows, decision quality decreases, revealing sensitivity to coverage and prompting. Tabular baselines remain more stable and faster, yet the prompt-only pipeline requires no gradient training, produces readable artifacts, and adapts easily through instructions and flags. Contributions include a flow-to-text protocol with interpretable cues, a calibration method for thresholding, a systematic baseline comparison, and a reproducibility bundle with prompts, grammar, metrics, and figures.
<div><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas n=Anwar, Noor Islam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates a prompt‑only approach using zero‑ and few‑shot large language models (LLMs) for network intrusion detection on the UNSW‑NB15 dataset, converting network flows into compact textual records enriched with lightweight boolean flags and constraining responses with a grammar. Experiments compare zero‑shot, instruction‑guided, and few‑shot prompting against strong tabular and neural baselines, showing that instruction‑guided prompts with calibrated scoring improve macro‑F1 scores but still lag behind traditional models as data size grows. Contributions include a flow‑to‑text protocol, a calibration method for decision thresholds, and a reproducibility bundle of prompts and evaluation scripts.", "summary_cn": "本文评估了在 UNSW‑NB15 数据集上使用零样本和少样本大语言模型（LLM）进行网络入侵检测的纯提示方法，将网络流转换为紧凑的文本记录并添加轻量布尔特征，同时通过语法约束保证输出结构化。实验比较了零样本、指令引导和少样本提示与强大的表格及神经基线，发现指令引导加校准的提示显著提升了 macro‑F1，但在数据规模扩大时仍不及传统模型。主要贡献包括流转文本协议、阈值校准方法以及包含提示、语法和评估脚本的可复现代码包。", "keywords": "network intrusion detection, large language models,-shot, few-shot, prompt engineering, calibration, grammar-constrained, cybersecurity, UNSW-NB15", "scoring": {"interpretability": 4, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Mohammad Abdul Rehman", "Syed Imad Ali Shah", "Abbas n=Anwar", "Noor Islam"]}
]]></acme>

<pubDate>2025-10-18T02:11:50+00:00</pubDate>
</item>
<item>
<title>Decoding Listeners Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer</title>
<link>https://papers.cool/arxiv/2510.17879</link>
<guid>https://papers.cool/arxiv/2510.17879</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a lightweight spiking transformer built on spiking neural networks for person identification from EEG signals, achieving 100% classification accuracy on the EEG-Music Emotion Recognition Challenge dataset while using less than 10% of the energy consumption of conventional deep neural networks. The approach leverages the temporal processing capabilities of SNNs to provide an efficient solution for security and personalized brain‑computer interface applications. Source code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出一种基于脉冲神经网络（SNN）和轻量级脉冲Transformer的EEG个人身份识别方法，在EEG‑Music情感识别挑战数据集上实现了100%分类准确率，同时能耗仅为传统深度网络的10%以下。该方法利用SNN处理EEG信号的时间复杂性，为安全认证和个性化脑机接口提供了高效的解决方案。代码已公开发布。<br /><strong>Keywords:</strong> EEG, person identification, spiking neural network, spiking transformer, energy-efficient BCI, low-power AI, temporal modeling, brain-computer interface, classification accuracy, lightweight architecture<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zheyuan Lin, Siqi Cai, Haizhou Li</div>
EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs. The source code is available at https://github.com/PatrickZLin/Decode-ListenerIdentity.
<div><strong>Authors:</strong> Zheyuan Lin, Siqi Cai, Haizhou Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a lightweight spiking transformer built on spiking neural networks for person identification from EEG signals, achieving 100% classification accuracy on the EEG-Music Emotion Recognition Challenge dataset while using less than 10% of the energy consumption of conventional deep neural networks. The approach leverages the temporal processing capabilities of SNNs to provide an efficient solution for security and personalized brain‑computer interface applications. Source code is released publicly.", "summary_cn": "本文提出一种基于脉冲神经网络（SNN）和轻量级脉冲Transformer的EEG个人身份识别方法，在EEG‑Music情感识别挑战数据集上实现了100%分类准确率，同时能耗仅为传统深度网络的10%以下。该方法利用SNN处理EEG信号的时间复杂性，为安全认证和个性化脑机接口提供了高效的解决方案。代码已公开发布。", "keywords": "EEG, person identification, spiking neural network, spiking transformer, energy-efficient BCI, low-power AI, temporal modeling, brain-computer interface, classification accuracy, lightweight architecture", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zheyuan Lin", "Siqi Cai", "Haizhou Li"]}
]]></acme>

<pubDate>2025-10-17T08:20:01+00:00</pubDate>
</item>
<item>
<title>Three-dimensional inversion of gravity data using implicit neural representations</title>
<link>https://papers.cool/arxiv/2510.17876</link>
<guid>https://papers.cool/arxiv/2510.17876</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a scientific‑machine‑learning method for three‑dimensional gravity inversion that models subsurface density as a continuous field using an implicit neural representation (INR). By training a deep network with a physics‑based forward‑model loss and positional encoding, the approach reconstructs detailed geological structures without explicit meshes, regularisation, or depth weighting, demonstrating efficacy on synthetic Gaussian random fields and a dipping‑block model. The authors argue that INR provides a scalable and flexible inversion framework that could extend to other geophysical or joint‑physics problems.<br /><strong>Summary (CN):</strong> 本文提出一种基于科学机器学习的三维重力反演方法，使用隐式神经表示（INR）将地下密度建模为连续场。通过物理正演损失和位置编码直接训练深度网络，实现了无需预定义网格、正则化或深度加权的高分辨率地质结构重建，并在高斯随机场和倾斜块体模型上展示了效果。作者认为该框架具有可扩展性，可推广至其他地球物理或多物理联合反演。<br /><strong>Keywords:</strong> implicit neural representation, gravity inversion, continuous density field, physics-informed neural network, positional encoding, geophysical inversion, synthetic experiments<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Pankaj K Mishra, Sanni Laaksonen, Jochen Kamm, Anand Singh</div>
Inversion of gravity data is an important method for investigating subsurface density variations relevant to diverse applications including mineral exploration, geothermal assessment, carbon storage, natural hydrogen, groundwater resources, and tectonic evolution. Here we present a scientific machine-learning approach for three-dimensional gravity inversion that represents subsurface density as a continuous field using an implicit neural representation (INR). The method trains a deep neural network directly through a physics-based forward-model loss, mapping spatial coordinates to a continuous density field without predefined meshes or discretisation. Positional encoding enhances the network's capacity to capture sharp contrasts and short-wavelength features that conventional coordinate-based networks tend to oversmooth due to spectral bias. We demonstrate the approach on synthetic examples including Gaussian random fields, representing realistic geological complexity, and a dipping block model to assess recovery of blocky structures. The INR framework reconstructs detailed structure and geologically plausible boundaries without explicit regularisation or depth weighting, while significantly reducing the number of inversion parameters. These results highlight the potential of implicit representations to enable scalable, flexible, and interpretable large-scale geophysical inversion. This framework could generalise to other geophysical methods and for joint/multiphysics inversion.
<div><strong>Authors:</strong> Pankaj K Mishra, Sanni Laaksonen, Jochen Kamm, Anand Singh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a scientific‑machine‑learning method for three‑dimensional gravity inversion that models subsurface density as a continuous field using an implicit neural representation (INR). By training a deep network with a physics‑based forward‑model loss and positional encoding, the approach reconstructs detailed geological structures without explicit meshes, regularisation, or depth weighting, demonstrating efficacy on synthetic Gaussian random fields and a dipping‑block model. The authors argue that INR provides a scalable and flexible inversion framework that could extend to other geophysical or joint‑physics problems.", "summary_cn": "本文提出一种基于科学机器学习的三维重力反演方法，使用隐式神经表示（INR）将地下密度建模为连续场。通过物理正演损失和位置编码直接训练深度网络，实现了无需预定义网格、正则化或深度加权的高分辨率地质结构重建，并在高斯随机场和倾斜块体模型上展示了效果。作者认为该框架具有可扩展性，可推广至其他地球物理或多物理联合反演。", "keywords": "implicit neural representation, gravity inversion, continuous density field, physics-informed neural network, positional encoding, geophysical inversion, synthetic experiments", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Pankaj K Mishra", "Sanni Laaksonen", "Jochen Kamm", "Anand Singh"]}
]]></acme>

<pubDate>2025-10-17T03:55:08+00:00</pubDate>
</item>
<item>
<title>Mixed Monotonicity Reachability Analysis of Neural ODE: A Trade-Off Between Tightness and Efficiency</title>
<link>https://papers.cool/arxiv/2510.17859</link>
<guid>https://papers.cool/arxiv/2510.17859</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an interval‑based reachability analysis for neural ordinary differential equations that leverages continuous‑time mixed‑monotonicity embeddings to obtain sound over‑approximations of reachable sets. Implemented in the TIRA toolbox, the method offers a trade‑off between tightness and computational efficiency, outperforming zonotope and star‑set approaches on high‑dimensional, real‑time safety‑critical examples. The approach is demonstrated on two neural ODE models—a spiral system and a fixed‑point attractor—highlighting its scalability for formal verification.<br /><strong>Summary (CN):</strong> 本文提出了一种基于区间的神经常微分方程式（neural ODE）可达性分析方法，利用连续时间混合单调性嵌入来计算可达集合的可信上界。该方法在 TIRA 工具箱中实现，在紧致性与计算效率之间实现权衡，并在高维、实时安全关键场景中相较于 zonotope 与星形集方法表现更优。作者在螺旋系统和固定点吸引子两种神经 ODE 示例上展示了该方法的可扩展性，为形式化验证提供了轻量化手段。<br /><strong>Keywords:</strong> neural ODE, mixed monotonicity, reachability analysis, interval methods, formal verification, safety-critical, TIRA, over-approximation, dynamical systems, real-time verification<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Abdelrahman Sayed Sayed, Pierre-Jean Meyer, Mohamed Ghazel</div>
Neural ordinary differential equations (neural ODE) are powerful continuous-time machine learning models for depicting the behavior of complex dynamical systems, but their verification remains challenging due to limited reachability analysis tools adapted to them. We propose a novel interval-based reachability method that leverages continuous-time mixed monotonicity techniques for dynamical systems to compute an over-approximation for the neural ODE reachable sets. By exploiting the geometric structure of full initial sets and their boundaries via the homeomorphism property, our approach ensures efficient bound propagation. By embedding neural ODE dynamics into a mixed monotone system, our interval-based reachability approach, implemented in TIRA with single-step, incremental, and boundary-based approaches, provides sound and computationally efficient over-approximations compared with CORA's zonotopes and NNV2.0 star set representations, while trading tightness for efficiency. This trade-off makes our method particularly suited for high-dimensional, real-time, and safety-critical applications. Applying mixed monotonicity to neural ODE reachability analysis paves the way for lightweight formal analysis by leveraging the symmetric structure of monotone embeddings and the geometric simplicity of interval boxes, opening new avenues for scalable verification aligned with the symmetry and geometry of neural representations. This novel approach is illustrated on two numerical examples of a spiral system and a fixed-point attractor system modeled as a neural ODE.
<div><strong>Authors:</strong> Abdelrahman Sayed Sayed, Pierre-Jean Meyer, Mohamed Ghazel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an interval‑based reachability analysis for neural ordinary differential equations that leverages continuous‑time mixed‑monotonicity embeddings to obtain sound over‑approximations of reachable sets. Implemented in the TIRA toolbox, the method offers a trade‑off between tightness and computational efficiency, outperforming zonotope and star‑set approaches on high‑dimensional, real‑time safety‑critical examples. The approach is demonstrated on two neural ODE models—a spiral system and a fixed‑point attractor—highlighting its scalability for formal verification.", "summary_cn": "本文提出了一种基于区间的神经常微分方程式（neural ODE）可达性分析方法，利用连续时间混合单调性嵌入来计算可达集合的可信上界。该方法在 TIRA 工具箱中实现，在紧致性与计算效率之间实现权衡，并在高维、实时安全关键场景中相较于 zonotope 与星形集方法表现更优。作者在螺旋系统和固定点吸引子两种神经 ODE 示例上展示了该方法的可扩展性，为形式化验证提供了轻量化手段。", "keywords": "neural ODE, mixed monotonicity, reachability analysis, interval methods, formal verification, safety-critical, TIRA, over-approximation, dynamical systems, real-time verification", "scoring": {"interpretability": 3, "understanding": 5, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Abdelrahman Sayed Sayed", "Pierre-Jean Meyer", "Mohamed Ghazel"]}
]]></acme>

<pubDate>2025-10-15T10:25:32+00:00</pubDate>
</item>
<item>
<title>Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch</title>
<link>https://papers.cool/arxiv/2510.17858</link>
<guid>https://papers.cool/arxiv/2510.17858</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an ultra-efficient post‑training technique that shortcuts large pre‑trained flow‑matching diffusion models into few‑step samplers by using velocity‑field self‑distillation, eliminating the need for step‑size embeddings and enabling rapid training (e.g., a 3‑step Flux model in less than one A100 day). It also shows that this mechanism can be integrated during pretraining to produce models that inherently learn efficient flows, and presents the first few‑shot distillation method for billion‑parameter diffusion models with state‑of‑the‑art performance at minimal cost.<br /><strong>Summary (CN):</strong> 本文提出一种超高效的后训练方法，通过对速度场进行自蒸馏，将大规模预训练流匹配扩散模型转化为少步采样器，避免了对步长嵌入的需求，实现了快速训练（例如，3 步 Flux 模型在不到一天的 A100 GPU 上完成）。该机制还能在预训练阶段嵌入，使模型本身学习高效的少步流，并首次实现了对数十亿参数扩散模型的少样本蒸馏，在几乎无成本的情况下达到最先进的性能。<br /><strong>Keywords:</strong> flow matching, diffusion models, self-distillation, velocity field, few-step sampler, shortcutting, efficient sampling, few-shot distillation, large-scale diffusion, Flux<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</div>
We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation. While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch$\unicode{x2013}$a process nearly as costly as pretraining itself. Our key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding. Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux less than one A100 day. Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality. This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.
<div><strong>Authors:</strong> Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an ultra-efficient post‑training technique that shortcuts large pre‑trained flow‑matching diffusion models into few‑step samplers by using velocity‑field self‑distillation, eliminating the need for step‑size embeddings and enabling rapid training (e.g., a 3‑step Flux model in less than one A100 day). It also shows that this mechanism can be integrated during pretraining to produce models that inherently learn efficient flows, and presents the first few‑shot distillation method for billion‑parameter diffusion models with state‑of‑the‑art performance at minimal cost.", "summary_cn": "本文提出一种超高效的后训练方法，通过对速度场进行自蒸馏，将大规模预训练流匹配扩散模型转化为少步采样器，避免了对步长嵌入的需求，实现了快速训练（例如，3 步 Flux 模型在不到一天的 A100 GPU 上完成）。该机制还能在预训练阶段嵌入，使模型本身学习高效的少步流，并首次实现了对数十亿参数扩散模型的少样本蒸馏，在几乎无成本的情况下达到最先进的性能。", "keywords": "flow matching, diffusion models, self-distillation, velocity field, few-step sampler, shortcutting, efficient sampling, few-shot distillation, large-scale diffusion, Flux", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xu Cai", "Yang Wu", "Qianli Chen", "Haoran Wu", "Lichuan Xiang", "Hongkai Wen"]}
]]></acme>

<pubDate>2025-10-15T09:19:05+00:00</pubDate>
</item>
<item>
<title>Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</title>
<link>https://papers.cool/arxiv/2510.17854</link>
<guid>https://papers.cool/arxiv/2510.17854</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an embedding-based framework that uses image vectors and vector similarity to differentiate AI-generated images from human-created ones, hypothesizing that AI images cluster together in embedding space. Experiments across five benchmark embedding models show the method is robust to moderate perturbations and can efficiently verify image provenance, with a blockchain component for immutable tracking.<br /><strong>Summary (CN):</strong> 本文提出了一种基于图像嵌入向量和向量相似度的检测框架，用于区分 AI 生成的图像和人类创作的图像，假设 AI 图像在嵌入空间中会形成聚类。通过对五种基准嵌入模型的实验验证了该方法对中等程度扰动的鲁棒性，并结合区块链实现了图像来源的不可篡改追踪。<br /><strong>Keywords:</strong> AI-generated image detection, vector similarity, image embeddings, blockchain provenance, deepfake detection, generative AI, content authentication<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Jitendra Sharma, Arthur Carvalho, Suman Bhunia</div>
Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.
<div><strong>Authors:</strong> Jitendra Sharma, Arthur Carvalho, Suman Bhunia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an embedding-based framework that uses image vectors and vector similarity to differentiate AI-generated images from human-created ones, hypothesizing that AI images cluster together in embedding space. Experiments across five benchmark embedding models show the method is robust to moderate perturbations and can efficiently verify image provenance, with a blockchain component for immutable tracking.", "summary_cn": "本文提出了一种基于图像嵌入向量和向量相似度的检测框架，用于区分 AI 生成的图像和人类创作的图像，假设 AI 图像在嵌入空间中会形成聚类。通过对五种基准嵌入模型的实验验证了该方法对中等程度扰动的鲁棒性，并结合区块链实现了图像来源的不可篡改追踪。", "keywords": "AI-generated image detection, vector similarity, image embeddings, blockchain provenance, deepfake detection, generative AI, content authentication", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Jitendra Sharma", "Arthur Carvalho", "Suman Bhunia"]}
]]></acme>

<pubDate>2025-10-15T00:49:56+00:00</pubDate>
</item>
<item>
<title>Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis</title>
<link>https://papers.cool/arxiv/2510.17852</link>
<guid>https://papers.cool/arxiv/2510.17852</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a framework for migrating large-scale atmospheric and oceanic AI models, such as FourCastNet and AI‑GOMS, from PyTorch to MindSpore and optimizing them for Chinese domestic chips. It details software‑hardware adaptation, memory optimization, and parallelism techniques, and evaluates training speed, inference speed, accuracy, and energy efficiency compared to GPU implementations. Experimental results show that the migration preserves model accuracy while significantly improving operational efficiency on Chinese hardware.<br /><strong>Summary (CN):</strong> 本文提出了一个迁移大型大气和海洋 AI 模型（如 FourCastNet 和 AI‑GOMS）从 PyTorch 到 MindSpore 并针对国产芯片进行优化的框架。文章阐述了软件‑硬件适配、内存优化和并行化技术，并在训练速度、推理速度、模型精度和能效等指标上与 GPU 实现进行对比评估。实验结果表明，迁移过程保持了原有模型精度，同时在国产芯片上显著提升了运行效率。<br /><strong>Keywords:</strong> atmospheric AI, oceanic AI, model migration, MindSpore, Chinese chips, performance optimization, FourCastNet, AI-GOMS<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</div>
With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.
<div><strong>Authors:</strong> Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a framework for migrating large-scale atmospheric and oceanic AI models, such as FourCastNet and AI‑GOMS, from PyTorch to MindSpore and optimizing them for Chinese domestic chips. It details software‑hardware adaptation, memory optimization, and parallelism techniques, and evaluates training speed, inference speed, accuracy, and energy efficiency compared to GPU implementations. Experimental results show that the migration preserves model accuracy while significantly improving operational efficiency on Chinese hardware.", "summary_cn": "本文提出了一个迁移大型大气和海洋 AI 模型（如 FourCastNet 和 AI‑GOMS）从 PyTorch 到 MindSpore 并针对国产芯片进行优化的框架。文章阐述了软件‑硬件适配、内存优化和并行化技术，并在训练速度、推理速度、模型精度和能效等指标上与 GPU 实现进行对比评估。实验结果表明，迁移过程保持了原有模型精度，同时在国产芯片上显著提升了运行效率。", "keywords": "atmospheric AI, oceanic AI, model migration, MindSpore, Chinese chips, performance optimization, FourCastNet, AI-GOMS", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"]}
]]></acme>

<pubDate>2025-10-14T02:41:56+00:00</pubDate>
</item>
<item>
<title>Neural networks for neurocomputing circuits: a computational study of tolerance to noise and activation function non-uniformity when machine learning materials properties</title>
<link>https://papers.cool/arxiv/2510.17849</link>
<guid>https://papers.cool/arxiv/2510.17849</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a computational study on how circuit noise and non-uniform neuron activation functions affect the performance of neural networks implemented in analog neurocomputing circuits, focusing on materials informatics tasks. Results show that networks have generally low noise tolerance, with single‑hidden‑layer and over‑parameterized models being somewhat more robust, and that retraining with the actual activation shapes can mitigate the impact of activation function inhomogeneity.<br /><strong>Summary (CN):</strong> 本文对模拟神经计算电路中电路噪声和神经元激活函数非均匀性对神经网络性能的影响进行计算研究，主要针对材料信息学任务。结果表明，网络整体对噪声容忍度较低，但单隐藏层及参数过大模型具有一定的噪声鲁棒性，且通过使用实际激活函数形状重新训练可以减轻激活函数不均匀性的影响。<br /><strong>Keywords:</strong> analog neurocomputing, circuit noise, activation function non-uniformity, neural network robustness, materials informatics, noise tolerance, over-parameterization, retraining, hardware AI, computational study<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ye min Thant, Methawee Nukunudompanich, Chu-Chen Chueh, Manabu Ihara, Sergei Manzhos</div>
Dedicated analog neurocomputing circuits are promising for high-throughput, low power consumption applications of machine learning (ML) and for applications where implementing a digital computer is unwieldy (remote locations; small, mobile, and autonomous devices, extreme conditions, etc.). Neural networks (NN) implemented in such circuits, however, must contend with circuit noise and the non-uniform shapes of the neuron activation function (NAF) due to the dispersion of performance characteristics of circuit elements (such as transistors or diodes implementing the neurons). We present a computational study of the impact of circuit noise and NAF inhomogeneity in function of NN architecture and training regimes. We focus on one application that requires high-throughput ML: materials informatics, using as representative problem ML of formation energies vs. lowest-energy isomer of peri-condensed hydrocarbons, formation energies and band gaps of double perovskites, and zero point vibrational energies of molecules from QM9 dataset. We show that NNs generally possess low noise tolerance with the model accuracy rapidly degrading with noise level. Single-hidden layer NNs, and NNs with larger-than-optimal sizes are somewhat more noise-tolerant. Models that show less overfitting (not necessarily the lowest test set error) are more noise-tolerant. Importantly, we demonstrate that the effect of activation function inhomogeneity can be palliated by retraining the NN using practically realized shapes of NAFs.
<div><strong>Authors:</strong> Ye min Thant, Methawee Nukunudompanich, Chu-Chen Chueh, Manabu Ihara, Sergei Manzhos</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a computational study on how circuit noise and non-uniform neuron activation functions affect the performance of neural networks implemented in analog neurocomputing circuits, focusing on materials informatics tasks. Results show that networks have generally low noise tolerance, with single‑hidden‑layer and over‑parameterized models being somewhat more robust, and that retraining with the actual activation shapes can mitigate the impact of activation function inhomogeneity.", "summary_cn": "本文对模拟神经计算电路中电路噪声和神经元激活函数非均匀性对神经网络性能的影响进行计算研究，主要针对材料信息学任务。结果表明，网络整体对噪声容忍度较低，但单隐藏层及参数过大模型具有一定的噪声鲁棒性，且通过使用实际激活函数形状重新训练可以减轻激活函数不均匀性的影响。", "keywords": "analog neurocomputing, circuit noise, activation function non-uniformity, neural network robustness, materials informatics, noise tolerance, over-parameterization, retraining, hardware AI, computational study", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ye min Thant", "Methawee Nukunudompanich", "Chu-Chen Chueh", "Manabu Ihara", "Sergei Manzhos"]}
]]></acme>

<pubDate>2025-10-13T08:27:08+00:00</pubDate>
</item>
<item>
<title>MAT-Agent: Adaptive Multi-Agent Training Optimization</title>
<link>https://papers.cool/arxiv/2510.17845</link>
<guid>https://papers.cool/arxiv/2510.17845</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MAT-Agent, a multi-agent framework that treats the training of multi-label image classifiers as a collaborative, real-time optimization problem. Autonomous agents dynamically adjust data augmentation, optimizers, learning rates, and loss functions using non-stationary multi-armed bandit algorithms, achieving higher mAP and better rare-class performance on Pascal VOC, COCO, and VG-256 compared to static baselines.<br /><strong>Summary (CN):</strong> 本文提出 MAT-Agent 框架，将多标签图像分类的训练视为协同实时优化过程。多个自主智能体利用非平稳多臂赌博机算法动态调节数据增强、优化器、学习率和损失函数，在 Pascal VOC、COCO 和 VG-256 数据集上实现了更高的 mAP 和稀有类别性能，优于传统静态配置方法。<br /><strong>Keywords:</strong> multi-agent training, adaptive optimization, multi-label classification, non-stationary bandits, dynamic augmentation, mixed-precision training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.
<div><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MAT-Agent, a multi-agent framework that treats the training of multi-label image classifiers as a collaborative, real-time optimization problem. Autonomous agents dynamically adjust data augmentation, optimizers, learning rates, and loss functions using non-stationary multi-armed bandit algorithms, achieving higher mAP and better rare-class performance on Pascal VOC, COCO, and VG-256 compared to static baselines.", "summary_cn": "本文提出 MAT-Agent 框架，将多标签图像分类的训练视为协同实时优化过程。多个自主智能体利用非平稳多臂赌博机算法动态调节数据增强、优化器、学习率和损失函数，在 Pascal VOC、COCO 和 VG-256 数据集上实现了更高的 mAP 和稀有类别性能，优于传统静态配置方法。", "keywords": "multi-agent training, adaptive optimization, multi-label classification, non-stationary bandits, dynamic augmentation, mixed-precision training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Ningyuan Liu", "Keze Wang"]}
]]></acme>

<pubDate>2025-10-10T19:41:50+00:00</pubDate>
</item>
<item>
<title>Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks</title>
<link>https://papers.cool/arxiv/2510.17832</link>
<guid>https://papers.cool/arxiv/2510.17832</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a methodology that employs Diffusion Probabilistic Models to generate synthetic EEG signals for motor imagery brain-computer interface tasks. Experimental results show that the generated data achieve classification accuracies above 95% and exhibit low mean squared error and high correlation with real recordings, indicating their potential to augment limited EEG datasets. This approach aims to alleviate data scarcity and improve model performance in EEG‑based BCI applications.<br /><strong>Summary (CN):</strong> 本文提出使用扩散概率模型（Diffusion Probabilistic Models）生成用于运动意象任务的合成脑电（EEG）信号的方法。实验表明，合成数据在分类任务中准确率超过 95%，且均方误差低、与真实信号高度相关，展示了其在补充有限 EEG 数据集、提升 BCI 模型性能方面的潜力。<br /><strong>Keywords:</strong> synthetic EEG, diffusion probabilistic models, motor imagery, brain-computer interface, data augmentation, generative modeling, EEG classification<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Henrique de Lima Alexandre, Clodoaldo Aparecido de Moraes Lima</div>
Electroencephalography (EEG) is a widely used, non-invasive method for capturing brain activity, and is particularly relevant for applications in Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data remains a major challenge due to sensor costs, acquisition time, and inter-subject variability. To address these limitations, this study proposes a methodology for generating synthetic EEG signals associated with motor imagery brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves preprocessing real EEG data, training a diffusion model to reconstruct EEG channels from noise, and evaluating the quality of the generated signals through both signal-level and task-level metrics. For validation, we employed classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks (CNN), and U-Net to compare the performance of synthetic data against real data in classification tasks. The generated data achieved classification accuracies above 95%, with low mean squared error and high correlation with real signals. Our results demonstrate that synthetic EEG signals produced by diffusion models can effectively complement datasets, improving classification performance in EEG-based BCIs and addressing data scarcity.
<div><strong>Authors:</strong> Henrique de Lima Alexandre, Clodoaldo Aparecido de Moraes Lima</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a methodology that employs Diffusion Probabilistic Models to generate synthetic EEG signals for motor imagery brain-computer interface tasks. Experimental results show that the generated data achieve classification accuracies above 95% and exhibit low mean squared error and high correlation with real recordings, indicating their potential to augment limited EEG datasets. This approach aims to alleviate data scarcity and improve model performance in EEG‑based BCI applications.", "summary_cn": "本文提出使用扩散概率模型（Diffusion Probabilistic Models）生成用于运动意象任务的合成脑电（EEG）信号的方法。实验表明，合成数据在分类任务中准确率超过 95%，且均方误差低、与真实信号高度相关，展示了其在补充有限 EEG 数据集、提升 BCI 模型性能方面的潜力。", "keywords": "synthetic EEG, diffusion probabilistic models, motor imagery, brain-computer interface, data augmentation, generative modeling, EEG classification", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Henrique de Lima Alexandre", "Clodoaldo Aparecido de Moraes Lima"]}
]]></acme>

<pubDate>2025-10-03T02:02:05+00:00</pubDate>
</item>
<item>
<title>Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming</title>
<link>https://papers.cool/arxiv/2510.17823</link>
<guid>https://papers.cool/arxiv/2510.17823</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a robust adaptive beamforming method that mitigates steering vector estimation errors and improves covariance matrix reconstruction. It adaptively estimates interfering source directions, uses a preprocessing‑based spatial sampling strategy to construct the interference‑plus‑noise covariance matrix, and forms the signal covariance for the sector of interest to compute the steering vector via the power method. Simulations demonstrate superior performance and lower computational cost compared to existing techniques.<br /><strong>Summary (CN):</strong> 本文提出了一种鲁棒自适应波束形成方法，用于缓解波束指向矢量估计误差并改进协方差矩阵重建。该方法自适应估计干扰源方向，利用基于预处理的空间采样策略构建干扰加噪声协方差矩阵，并在感兴趣信号方向上形成信号协方差矩阵，进而通过幂法计算波束指向矢量。仿真结果显示该方法在性能和计算成本上优于现有技术。<br /><strong>Keywords:</strong> robust adaptive beamforming, steering vector mismatch, covariance matrix reconstruction, preprocessing-based spatial sampling, interference-plus-noise covariance, general linear combination, power method<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Saeed Mohammadzadeh, Rodrigo C. de Lamare, Yuriy Zakharov</div>
This work proposes an efficient, robust adaptive beamforming technique to deal with steering vector (SV) estimation mismatches and data covariance matrix reconstruction problems. In particular, the direction-of-arrival(DoA) of interfering sources is estimated with available snapshots in which the angular sectors of the interfering signals are computed adaptively. Then, we utilize the well-known general linear combination algorithm to reconstruct the interference-plus-noise covariance (IPNC) matrix using preprocessing-based spatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be replaced by the sample covariance matrix (SCM) in the shrinkage method. A power spectrum sampling strategy is then devised based on a preprocessing matrix computed with the estimated angular sectors' information. Moreover, the covariance matrix for the signal is formed for the angular sector of the signal-of-interest (SOI), which allows for calculating an SV for the SOI using the power method. An analysis of the array beampattern in the proposed PPBSS technique is carried out, and a study of the computational cost of competing approaches is conducted. Simulation results show the proposed method's effectiveness compared to existing approaches.
<div><strong>Authors:</strong> Saeed Mohammadzadeh, Rodrigo C. de Lamare, Yuriy Zakharov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a robust adaptive beamforming method that mitigates steering vector estimation errors and improves covariance matrix reconstruction. It adaptively estimates interfering source directions, uses a preprocessing‑based spatial sampling strategy to construct the interference‑plus‑noise covariance matrix, and forms the signal covariance for the sector of interest to compute the steering vector via the power method. Simulations demonstrate superior performance and lower computational cost compared to existing techniques.", "summary_cn": "本文提出了一种鲁棒自适应波束形成方法，用于缓解波束指向矢量估计误差并改进协方差矩阵重建。该方法自适应估计干扰源方向，利用基于预处理的空间采样策略构建干扰加噪声协方差矩阵，并在感兴趣信号方向上形成信号协方差矩阵，进而通过幂法计算波束指向矢量。仿真结果显示该方法在性能和计算成本上优于现有技术。", "keywords": "robust adaptive beamforming, steering vector mismatch, covariance matrix reconstruction, preprocessing-based spatial sampling, interference-plus-noise covariance, general linear combination, power method", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Saeed Mohammadzadeh", "Rodrigo C. de Lamare", "Yuriy Zakharov"]}
]]></acme>

<pubDate>2025-09-30T17:46:44+00:00</pubDate>
</item>
<item>
<title>CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms</title>
<link>https://papers.cool/arxiv/2510.17821</link>
<guid>https://papers.cool/arxiv/2510.17821</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CLARAE is a one‑dimensional encoder‑decoder designed for intracardiac atrial electrograms that preserves waveform morphology while compressing signals into a 64‑dimensional latent space. It delivers high‑fidelity reconstruction and robust denoising across a wide range of signal‑to‑noise ratios, and its latent embeddings enable rhythm classification with F1 scores above 0.97 and clear clustering by rhythm type. An interactive web application is provided for real‑time exploration of the pre‑trained models.<br /><strong>Summary (CN):</strong> CLARAE 是一种一维 (autoencoder) 编码‑解码网络，专为心内房性电图设计，能够在保留波形形态的同时将信号压缩至 64 维的 latent 空间。该模型在广泛的信噪比范围内实现高保真重建和稳健去噪，并且其嵌入在节律分类任务中获得超过 0.97 的 F1 分数，且不同节律在 latent 空间中呈明显聚类。作者还提供交互式网页应用，方便实时探索预训练模型。<br /><strong>Keywords:</strong> autoencoder, denoising, intracardiac electrogram, rhythm classification, latent embeddings, signal reconstruction, medical signal processing<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Long Lin, Pablo Peiro-Corbacho, Pablo Ávila, Alejandro Carta-Bergaz, Ángel Arenal, Gonzalo R. Ríos-Muñoz, Carlos Sevilla-Salcedo</div>
Intracavitary atrial electrograms (EGMs) provide high-resolution insights into cardiac electrophysiology but are often contaminated by noise and remain high-dimensional, limiting real-time analysis. We introduce CLARAE (CLArity-preserving Reconstruction AutoEncoder), a one-dimensional encoder--decoder designed for atrial EGMs, which achieves both high-fidelity reconstruction and a compact 64-dimensional latent representation. CLARAE is designed to preserve waveform morphology, mitigate reconstruction artifacts, and produce interpretable embeddings through three principles: downsampling with pooling, a hybrid interpolation--convolution upsampling path, and a bounded latent space. We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29 patients across three rhythm types (AF, SR300, SR600). Performance was benchmarked against six state-of-the-art autoencoders using reconstruction metrics, rhythm classification, and robustness across signal-to-noise ratios from -5 to 15 dB. In downstream rhythm classification, CLARAE achieved F1-scores above 0.97 for all rhythm types, and its latent space showed clear clustering by rhythm. In denoising tasks, it consistently ranked among the top performers for both unipolar and bipolar signals. In order to promote reproducibility and enhance accessibility, we offer an interactive web-based application. This platform enables users to explore pre-trained CLARAE models, visualize the reconstructions, and compute metrics in real time. Overall, CLARAE combines robust denoising with compact, discriminative representations, offering a practical foundation for clinical workflows such as rhythm discrimination, signal quality assessment, and real-time mapping.
<div><strong>Authors:</strong> Long Lin, Pablo Peiro-Corbacho, Pablo Ávila, Alejandro Carta-Bergaz, Ángel Arenal, Gonzalo R. Ríos-Muñoz, Carlos Sevilla-Salcedo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CLARAE is a one‑dimensional encoder‑decoder designed for intracardiac atrial electrograms that preserves waveform morphology while compressing signals into a 64‑dimensional latent space. It delivers high‑fidelity reconstruction and robust denoising across a wide range of signal‑to‑noise ratios, and its latent embeddings enable rhythm classification with F1 scores above 0.97 and clear clustering by rhythm type. An interactive web application is provided for real‑time exploration of the pre‑trained models.", "summary_cn": "CLARAE 是一种一维 (autoencoder) 编码‑解码网络，专为心内房性电图设计，能够在保留波形形态的同时将信号压缩至 64 维的 latent 空间。该模型在广泛的信噪比范围内实现高保真重建和稳健去噪，并且其嵌入在节律分类任务中获得超过 0.97 的 F1 分数，且不同节律在 latent 空间中呈明显聚类。作者还提供交互式网页应用，方便实时探索预训练模型。", "keywords": "autoencoder, denoising, intracardiac electrogram, rhythm classification, latent embeddings, signal reconstruction, medical signal processing", "scoring": {"interpretability": 5, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Long Lin", "Pablo Peiro-Corbacho", "Pablo Ávila", "Alejandro Carta-Bergaz", "Ángel Arenal", "Gonzalo R. Ríos-Muñoz", "Carlos Sevilla-Salcedo"]}
]]></acme>

<pubDate>2025-09-28T20:39:21+00:00</pubDate>
</item>
<item>
<title>Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach</title>
<link>https://papers.cool/arxiv/2510.17818</link>
<guid>https://papers.cool/arxiv/2510.17818</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a joint optimization framework for gridless two‑dimensional direction‑of‑arrival (DOA) estimation using a uniform circular array (UCA) from a single snapshot. By simultaneously estimating a manifold transformation matrix and the azimuth‑elevation source pairs, the method is solved with an inexact Augmented Lagrangian Method (iALM), avoiding costly semidefinite programming. Simulations show the approach achieves robust, high‑resolution DOA estimates in the challenging single‑snapshot scenario.<br /><strong>Summary (CN):</strong> 本文提出一种联合优化框架，用于在仅有单次快照的情况下对均匀圆形阵列（UCA）进行无格二维方向到达（DOA）估计。通过同时估计流形变换矩阵和方位‑俯仰源对，并采用不完全增强拉格朗日方法（iALM）求解，完全规避了半正定规划的高计算成本。仿真结果表明该方法在单快照条件下能够实现稳健且高分辨率的DOA估计。<br /><strong>Keywords:</strong> gridless DOA estimation, uniform circular array, single snapshot, augmented Lagrangian, iALM, joint optimization, signal processing<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 3, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Salar Nouri</div>
This paper tackles the challenging problem of gridless two-dimensional (2D) direction-of-arrival (DOA) estimation for a uniform circular array (UCA) from a single snapshot of data. Conventional gridless methods often fail in this scenario due to prohibitive computational costs or a lack of robustness. We propose a novel framework that overcomes these limitations by jointly estimating a manifold transformation matrix and the source azimuth-elevation pairs within a single, unified optimization problem. This problem is solved efficiently using an inexact Augmented Lagrangian Method (iALM), which completely circumvents the need for semidefinite programming. By unifying the objectives of data fidelity and transformation robustness, our approach is uniquely suited for the demanding single-snapshot case. Simulation results confirm that the proposed iALM framework provides robust and high-resolution, gridless 2D-DOA estimates, establishing its efficacy for challenging array signal processing applications.
<div><strong>Authors:</strong> Salar Nouri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a joint optimization framework for gridless two‑dimensional direction‑of‑arrival (DOA) estimation using a uniform circular array (UCA) from a single snapshot. By simultaneously estimating a manifold transformation matrix and the azimuth‑elevation source pairs, the method is solved with an inexact Augmented Lagrangian Method (iALM), avoiding costly semidefinite programming. Simulations show the approach achieves robust, high‑resolution DOA estimates in the challenging single‑snapshot scenario.", "summary_cn": "本文提出一种联合优化框架，用于在仅有单次快照的情况下对均匀圆形阵列（UCA）进行无格二维方向到达（DOA）估计。通过同时估计流形变换矩阵和方位‑俯仰源对，并采用不完全增强拉格朗日方法（iALM）求解，完全规避了半正定规划的高计算成本。仿真结果表明该方法在单快照条件下能够实现稳健且高分辨率的DOA估计。", "keywords": "gridless DOA estimation, uniform circular array, single snapshot, augmented Lagrangian, iALM, joint optimization, signal processing", "scoring": {"interpretability": 1, "understanding": 3, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Salar Nouri"]}
]]></acme>

<pubDate>2025-09-27T08:36:53+00:00</pubDate>
</item>
<item>
<title>Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification</title>
<link>https://papers.cool/arxiv/2510.17810</link>
<guid>https://papers.cool/arxiv/2510.17810</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper applies nonlinear time‑series analysis to lead‑II ECG recordings from the PTB‑XL dataset, extracting complexity measures and cross‑channel metrics (Spearman correlation and mutual information). Significant differences in these measures are reported between healthy and diseased subjects across five diagnostic super‑classes, and incorporating them into machine‑learning classifiers raises the AUC from 0.86 to 0.90.<br /><strong>Summary (CN):</strong> 本文对 PTB‑XL 数据库的 II 导联心电图使用非线性时间序列分析，提取复杂度度量和跨导联指标（Spearman 相关和互信息）。结果显示健康与疾病患者在几乎所有度量上存在显著差异，跨五类诊断超级类别均 $p<.001$。将这些复杂度特征加入机器学习模型后，AUC 从 0.86 提升至 0.90。<br /><strong>Keywords:</strong> ECG, nonlinear dynamics, complexity, time series analysis, machine learning, PTB-XL, classification, mutual information, Spearman correlation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Camilo Quiceno Quintero, Sandip Varkey George</div>
The complex dynamics of the heart are reflected in its electrical activity, captured through electrocardiograms (ECGs). In this study we use nonlinear time series analysis to understand how ECG complexity varies with cardiac pathology. Using the large PTB-XL dataset, we extracted nonlinear measures from lead II ECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations and mutual information. Significant differences between diseased and healthy individuals were found in almost all measures between healthy and diseased classes, and between 5 diagnostic superclasses ($p<.001$). Moreover, incorporating these complexity quantifiers into machine learning models substantially improved classification accuracy measured using area under the ROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90 (including cross-time series metrics).
<div><strong>Authors:</strong> Camilo Quiceno Quintero, Sandip Varkey George</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper applies nonlinear time‑series analysis to lead‑II ECG recordings from the PTB‑XL dataset, extracting complexity measures and cross‑channel metrics (Spearman correlation and mutual information). Significant differences in these measures are reported between healthy and diseased subjects across five diagnostic super‑classes, and incorporating them into machine‑learning classifiers raises the AUC from 0.86 to 0.90.", "summary_cn": "本文对 PTB‑XL 数据库的 II 导联心电图使用非线性时间序列分析，提取复杂度度量和跨导联指标（Spearman 相关和互信息）。结果显示健康与疾病患者在几乎所有度量上存在显著差异，跨五类诊断超级类别均 $p<.001$。将这些复杂度特征加入机器学习模型后，AUC 从 0.86 提升至 0.90。", "keywords": "ECG, nonlinear dynamics, complexity, time series analysis, machine learning, PTB-XL, classification, mutual information, Spearman correlation", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Camilo Quiceno Quintero", "Sandip Varkey George"]}
]]></acme>

<pubDate>2025-09-24T11:02:36+00:00</pubDate>
</item>
<item>
<title>In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning</title>
<link>https://papers.cool/arxiv/2510.17809</link>
<guid>https://papers.cool/arxiv/2510.17809</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a data‑driven framework for real‑time monitoring of gear power honing by analyzing vibration signals with time‑frequency methods and machine learning. Three subspace learning techniques—PCA, PCA+LDA, and regularized uncorrelated multilinear discriminant analysis (R‑UMLDA)—are compared for feature extraction, and the resulting features are classified with an SVM into four gear‑quality categories, achieving up to 100% accuracy on industrial data. The approach yields interpretable spectral features linked to process dynamics, facilitating integration into predictive‑maintenance systems.<br /><strong>Summary (CN):</strong> 本文提出一种基于振动信号时频分析和机器学习的实时齿轮精整监控框架。比较了三种子空间学习方法（PCA、PCA+LDA、正则化非相关多线性判别分析 R‑UMLDA）用于特征提取，并用 SVM 将特征分类为四个齿轮质量等级，在工业实验数据上实现最高 100% 的准确率。该方法提供可解释的谱特征，能够反映加工过程动态，便于集成到预测性维护系统中。<br /><strong>Keywords:</strong> vibration analysis, gear power honing, machine learning, PCA, LDA, R-UMLDA, SVM, in-process monitoring, predictive maintenance<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Massimo Capurso, Luciano Afferrante</div>
In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH) requirements demand high-precision finishing operations such as power honing. Conventional quality control strategies rely on post-process inspections and Statistical Process Control (SPC), which fail to capture transient machining anomalies and cannot ensure real-time defect detection. This study proposes a novel, data-driven framework for in-process monitoring of gear power honing using vibration signal analysis and machine learning. Our proposed methodology involves continuous data acquisition via accelerometers, followed by time-frequency signal analysis. We investigate and compare the efficacy of three subspace learning methods for features extraction: (1) Principal Component Analysis (PCA) for dimensionality reduction; (2) a two-stage framework combining PCA with Linear Discriminant Analysis (LDA) for enhanced class separation; and (3) Uncorrelated Multilinear Discriminant Analysis with Regularization (R-UMLDA), adapted for tensor data, which enforces feature decorrelation and includes regularization for small sample sizes. These extracted features are then fed into a Support Vector Machine (SVM) classifier to predict four distinct gear quality categories, established through rigorous geometrical inspections and test bench results of assembled gearboxes. The models are trained and validated on an experimental dataset collected in an industrial context during gear power-honing operations, with gears classified into four different quality categories. The proposed framework achieves high classification accuracy (up to 100%) in an industrial setting. The approach offers interpretable spectral features that correlate with process dynamics, enabling practical integration into real-time monitoring and predictive maintenance systems.
<div><strong>Authors:</strong> Massimo Capurso, Luciano Afferrante</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a data‑driven framework for real‑time monitoring of gear power honing by analyzing vibration signals with time‑frequency methods and machine learning. Three subspace learning techniques—PCA, PCA+LDA, and regularized uncorrelated multilinear discriminant analysis (R‑UMLDA)—are compared for feature extraction, and the resulting features are classified with an SVM into four gear‑quality categories, achieving up to 100% accuracy on industrial data. The approach yields interpretable spectral features linked to process dynamics, facilitating integration into predictive‑maintenance systems.", "summary_cn": "本文提出一种基于振动信号时频分析和机器学习的实时齿轮精整监控框架。比较了三种子空间学习方法（PCA、PCA+LDA、正则化非相关多线性判别分析 R‑UMLDA）用于特征提取，并用 SVM 将特征分类为四个齿轮质量等级，在工业实验数据上实现最高 100% 的准确率。该方法提供可解释的谱特征，能够反映加工过程动态，便于集成到预测性维护系统中。", "keywords": "vibration analysis, gear power honing, machine learning, PCA, LDA, R-UMLDA, SVM, in-process monitoring, predictive maintenance", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Massimo Capurso", "Luciano Afferrante"]}
]]></acme>

<pubDate>2025-09-24T08:32:28+00:00</pubDate>
</item>
</channel>
</rss>