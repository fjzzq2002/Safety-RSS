{"timestamp": "2025-10-22T03:22:24.636807", "feed": "arxiv_cscv", "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention", "link": "https://papers.cool/arxiv/2510.18775", "analysis": {"summary": "UltraGen introduces a hierarchical dual‑branch attention architecture that decomposes full attention into a local branch for detailed regional content and a global branch for semantic consistency, enabling native high‑resolution (1080p‑4K) video generation with diffusion transformers. The paper presents a spatially compressed global modeling strategy and a hierarchical cross‑window local attention mechanism, allowing efficient scaling of pre‑trained low‑resolution models to high resolutions while outperforming existing methods and two‑stage super‑resolution pipelines. Extensive experiments demonstrate superior qualitative and quantitative results across resolutions.", "summary_cn": "UltraGen 提出了一种层级双分支注意力架构，将完整注意力分解为局部分支（负责高保真区域内容）和全局分支（确保语义一致性），从而实现原生高分辨率（1080p‑4K）视频生成（使用 diffusion transformer）。论文又引入空间压缩全局部建模策略和层级交叉窗口局部注意力机制，使得预训练的低分辨率模型能够高效扩展至高分辨率，并在质量上超越现有方法及两阶段超分辨率流水线。大量实验显示在定性和定量评估上均取得显著提升。", "keywords": "high-resolution video generation,diffusion models,hierarchical attention,global-local attention,spatial compression,cross-window attention,video synthesis,Super-resolution", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Teng Hu", "Jiangning Zhang", "Zihan Su", "Ran Yi"]}, "usage": {"completion_tokens": 586, "prompt_tokens": 3456, "total_tokens": 4042, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 307, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00087, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0005184, "upstream_inference_completions_cost": 0.0003516}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:24.850938", "feed": "arxiv_cscv", "title": "Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression", "link": "https://papers.cool/arxiv/2510.18650", "analysis": {"summary": "The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that uses binary quadratic expressions instead of traditional linear binary bases. Experiments on a matrix compression benchmark and post‑training quantization of Vision Transformer models show that BQQ achieves better memory‑efficiency versus reconstruction error trade‑offs and improves PTQ performance, surpassing state‑of‑the‑art methods by up to 2.2% (calibration) and 59.1% (data‑free) at 2‑bit quantization.", "summary_cn": "本文提出二次二进制量化 (Binary Quadratic Quantization, BQQ) 方法，用二进制二次表达式替代传统的线性二进制基进行矩阵压缩。实验在矩阵压缩基准和 Vision Transformer 的后训练量化任务上显示，BQQ 在内存效率与重构误差之间取得更佳平衡，并在 2 位量化下分别比最先进的校准式和无数据式 PTQ 方法（PTQ）提升 2.2% 和 59.1%。", "keywords": "binary quadratic quantization, matrix compression, post-training quantization, vision transformer, low-bit quantization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kyo Kuroki", "Yasuyuki Okoshi", "Thiem Van Chu", "Kazushi Kawamura", "Masato Motomura"]}, "usage": {"completion_tokens": 553, "prompt_tokens": 3434, "total_tokens": 3987, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 255, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0008469, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0005151, "upstream_inference_completions_cost": 0.0003318}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:26.039130", "feed": "arxiv_cscv", "title": "Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting", "link": "https://papers.cool/arxiv/2510.18739", "analysis": {"summary": "The paper introduces ColIAGS, an enhanced 3D Gaussian Splatting framework for colonoscopy that explicitly models illumination attenuation using improved appearance and geometry modeling techniques, enabling more accurate novel view synthesis and geometric reconstruction despite dynamic lighting conditions.", "summary_cn": "本文提出了 ColIAGS，一种针对结肠镜检查的改进版 3D 高斯点绘框架，通过改进的外观建模和几何建模显式考虑光照衰减，使得在光源和摄像头动态变化的情况下仍能实现高质量的新视角合成和几何重建。", "keywords": "3D Gaussian Splatting, colonoscopy, illumination attenuation, view synthesis, geometry modeling, medical imaging", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Wang", "Ying Zhou", "Haoyu Zhao", "Rui Wang", "Qiang Hu", "Xing Zhang", "Qiang Li", "Zhiwei Wang"]}, "usage": {"completion_tokens": 472, "prompt_tokens": 3505, "total_tokens": 3977, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 283, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00080895, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00052575, "upstream_inference_completions_cost": 0.0002832}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:26.747242", "feed": "arxiv_cscv", "title": "ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation", "link": "https://papers.cool/arxiv/2510.18341", "analysis": {"summary": "The paper introduces ViSE, a four‑stage pipeline for realistic vision‑only street‑view extrapolation aimed at closed‑loop autonomous‑driving simulation. It combines data‑driven pseudo‑LiDAR initialization, a dimension‑reduced 2D‑SDF road surface model, generative priors for pseudo ground‑truth views, and an adaptation network to remove temporal artifacts, achieving state‑of‑the‑art performance on the RealADSim‑NVS benchmark.", "summary_cn": "本文提出 ViSE 系统，采用四阶段管线实现仅视觉的街景外推，用于自动驾驶闭环仿真。方法包括数据驱动的伪 LiDAR 初始化、用于道路表面的 2D‑SDF 几何先验、生成式伪真值视图以及去除时间伪影的自适应网络，在 RealADSim‑NVS 基准上取得领先.", "keywords": "vision-only extrapolation, novel view synthesis, pseudo-LiDAR, 2D-SDF, generative prior, autonomous driving simulation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kaiyuan Tan", "Yingying Shen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye"]}, "usage": {"completion_tokens": 570, "prompt_tokens": 3401, "total_tokens": 3971, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 368, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00085215, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00051015, "upstream_inference_completions_cost": 0.000342}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:29.434770", "feed": "arxiv_cscv", "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting", "link": "https://papers.cool/arxiv/2510.18714", "analysis": {"summary": "PLANA3R proposes a pose‑free, feed‑forward framework for metric planar 3D reconstruction of indoor scenes using only unposed two‑view images. It leverages Vision Transformers to predict sparse planar primitives, estimate relative camera poses, and supervise geometry through planar splatting with high‑resolution depth and normal maps, training without explicit plane annotations. Experiments show strong metric accuracy, generalization to out‑of‑domain environments, and accurate plane segmentation.", "summary_cn": "PLANA3R 提出一种无需相机姿态、端到端的度量平面 3D 重建框架，仅使用未配准的两幅图像。该方法利用 Vision Transformer 预测稀疏平面原语、估计相对相机位姿，并通过平面投影将高分辨率深度和法线图的梯度传播到几何学习，实现无需显式平面标注的训练。实验表明该模型在度量精度、跨域泛化以及平面分割方面表现出色。", "keywords": "planar 3D reconstruction, metric reconstruction, vision transformers, planar splatting, zero-shot, indoor scenes, depth estimation, pose estimation, plane segmentation", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Changkun Liu", "Bin Tan", "Zeran Ke", "Shangzhan Zhang", "Jiachen Liu", "Ming Qian", "Nan Xue", "Yujun Shen", "Tristan Braud"]}, "usage": {"completion_tokens": 519, "prompt_tokens": 3444, "total_tokens": 3963, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 256, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00034536, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013776, "upstream_inference_completions_cost": 0.0002076}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:29.641591", "feed": "arxiv_cscv", "title": "IF-VidCap: Can Video Caption Models Follow Instructions?", "link": "https://papers.cool/arxiv/2510.18726", "analysis": {"summary": "The paper introduces IF‑VidCap, a new benchmark designed to evaluate how well video captioning models can follow user instructions, assessing both format and content correctness across 1,400 curated samples. Extensive experiments on over 20 state‑of‑the‑art models show that while proprietary systems still lead, open‑source models are rapidly closing the gap, and specialized dense‑captioning models lag behind general‑purpose MLLMs on complex instructions.", "summary_cn": "本文提出 IF‑VidCap 基准，用于评估视频字幕模型的指令遵循能力，包含 1,400 条高质量样本，并从格式正确性和内容正确性两维度进行评价。对 20 多种主流模型的全面实验表明，尽管专有模型仍占优势，但开源模型正在快速逼近；而针对密集字幕的专门模型在复杂指令下表现不如通用多模态大语言模型（MLLM）。", "keywords": "controllable video captioning, instruction-following, benchmark, multimodal large language models, video caption evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shihao Li", "Yuanxing Zhang", "Jiangtao Wu", "Zhide Lei", "Yiwen He", "Runzhe Wen", "Chenxi Liao", "Chengkang Jiang", "An Ping", "Shuo Gao", "Suhan Wang", "Zhaozhou Bian", "Zijun Zhou", "Jingyi Xie", "Jiayi Zhou", "Jing Wang", "Yifan Yao", "Weihao Xie", "Yingshui Tan", "Yanghai Wang", "Qianqian Xie", "Zhaoxiang Zhang", "Jiaheng Liu"]}, "usage": {"completion_tokens": 548, "prompt_tokens": 3392, "total_tokens": 3940, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 321, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00035488, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013568, "upstream_inference_completions_cost": 0.0002192}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:30.008571", "feed": "arxiv_cscv", "title": "CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder", "link": "https://papers.cool/arxiv/2510.18583", "analysis": {"summary": "CovMatch introduces a multimodal dataset distillation framework that aligns the cross‑covariance of real and synthetic image‑text features while regularizing each modality's feature distribution, enabling joint optimization of both vision and text encoders. This approach yields stronger cross‑modal alignment and achieves up to 6.8% absolute improvement in retrieval accuracy on Flickr30K and COCO using only 500 synthetic pairs. The method addresses scalability challenges of multimodal contrastive learning by moving beyond frozen text encoders used in prior work.", "summary_cn": "CovMatch 提出一种多模态数据集蒸馏框架，通过对齐真实与合成图像‑文本特征的交叉协方差并对每个模态的特征分布进行正则化，实现视觉编码器和可训练文本编码器的联合优化。该方法显著提升跨模态对齐，在 Flickr30K 与 COCO 数据集上仅使用 500 对合成样本即可实现检索准确率最高提升 6.8%。相较于以往冻结文本编码器的做法，CovMatch 解决了多模态对比学习的可扩展性问题。", "keywords": "multimodal dataset distillation, cross-covariance alignment, vision-language models, synthetic image-text pairs, retrieval accuracy, Flickr30K, COCO, trainable text encoder, contrastive learning, scalability", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yongmin Lee", "Hye Won Chung"]}, "usage": {"completion_tokens": 690, "prompt_tokens": 3403, "total_tokens": 4093, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 369, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00041212, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013612, "upstream_inference_completions_cost": 0.000276}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:30.014996", "feed": "arxiv_cscv", "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos", "link": "https://papers.cool/arxiv/2510.18489", "analysis": {"summary": "Mono4DGS-HDR is the first system that reconstructs renderable four‑dimensional high‑dynamic‑range (HDR) scenes from unposed monocular low‑dynamic‑range (LDR) videos captured with alternating exposures, using a two‑stage Gaussian splatting optimization. The first stage learns an HDR Gaussian representation in an orthographic frame without camera poses, while the second stage refines world‑space Gaussians and estimates camera poses, aided by a temporal luminance regularization for consistency. Experiments on a newly built benchmark show substantial improvements over adapted state‑of‑the‑art methods in both quality and speed.", "summary_cn": "Mono4DGS-HDR 是首个从交替曝光的单目 LDR 视频中重建可渲染的四维 HDR 场景的系统，采用两阶段 Gaussian Splatting 优化。第一阶段在正交相机坐标系中学习 HDR 高斯表示，无需相机位姿；第二阶段将高斯转换到世界坐标并联合优化位姿，同时使用时序亮度正则化提升外观一致性。实验在新建的 HDR 视频重建基准上显示出在渲染质量和速度上显著优于现有方法。", "keywords": "HDR video reconstruction, Gaussian splatting, monocular video, alternating exposure, temporal luminance regularization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinfeng Liu", "Lingtong Kong", "Mi Zhou", "Jinwen Chen", "Dan Xu"]}, "usage": {"completion_tokens": 583, "prompt_tokens": 3401, "total_tokens": 3984, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 314, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00036924, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013604, "upstream_inference_completions_cost": 0.0002332}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:30.327308", "feed": "arxiv_cscv", "title": "Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models", "link": "https://papers.cool/arxiv/2510.18457", "analysis": {"summary": "The paper introduces Vision Foundation Model Variational Autoencoder (VFM‑VAE) as a direct visual tokenizer for latent diffusion models, avoiding distillation and using a decoder with multi‑scale latent fusion and progressive resolution reconstruction to preserve semantic fidelity while achieving high‑quality pixel reconstruction. It also proposes the SE‑CKNNA metric to analyze representation dynamics during diffusion training and a joint tokenizer‑diffusion alignment strategy that accelerates convergence, achieving gFID 2.20 in 80 epochs and 1.62 after 640 epochs.", "summary_cn": "本文提出 Vision Foundation Model 变分自编码器 (VFM‑VAE) 作为潜在扩散模型的直接视觉分词器，摆脱蒸馏，并通过多尺度潜在融合与渐进分辨率重建解码器在保持语义聚焦的同时实现高质量像素重建。文中还引入 SE‑CKNNA 指标用于分析扩散训练期间的表征动力学，并提出联合分词器‑扩散对齐策略，大幅加速收敛，使模型在 80 轮后达到 gFID 2.20，640 轮后达到 1.62。", "keywords": "vision foundation models, latent diffusion, tokenizer, VFM‑VAE, multi-scale latent fusion, SE‑CKNNA, gFID, diffusion alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tianci Bi", "Xiaoyi Zhang", "Yan Lu", "Nanning Zheng"]}, "usage": {"completion_tokens": 911, "prompt_tokens": 3492, "total_tokens": 4403, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 673, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0010704, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0005238, "upstream_inference_completions_cost": 0.0005466}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:31.123827", "feed": "arxiv_cscv", "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation", "link": "https://papers.cool/arxiv/2510.18716", "analysis": {"summary": "The paper introduces a KV cache compression framework for autoregressive image generation that decouples attention heads into spatial-locality and semantic-sink types, maintaining a short recent token window for the former and a compact set of highly‑attended tokens for the latter. This approach yields a 5× reduction in memory usage and a 6.6× speedup in throughput with minimal loss of visual quality, enabling efficient generation on limited hardware.", "summary_cn": "本文提出了一种针对自回归图像生成的 KV 缓存压缩框架，将注意力头解耦为空间局部头和语义汇聚头。对空间局部头保留最近的短窗口 token，对语义汇聚头则保留少量高度被关注的 token。实验表明，该方法在几乎不影响图像质量的情况下实现了 5 倍内存缩减和 6.6 倍吞吐提升，使资源受限设备上进行高效原生自回归图像生成成为可能。", "keywords": "autoregressive image generation, KV cache compression, spatial locality, semantic sink, token pruning, memory reduction, speedup, efficient inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Siyong Jian", "Huan Wang"]}, "usage": {"completion_tokens": 627, "prompt_tokens": 3410, "total_tokens": 4037, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 374, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003872, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001364, "upstream_inference_completions_cost": 0.0002508}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:31.371143", "feed": "arxiv_cscv", "title": "Entropy-Enhanced Conformal Features from Ricci Flow for Robust Alzheimer's Disease Classification", "link": "https://papers.cool/arxiv/2510.18396", "analysis": {"summary": "This paper proposes a novel method that uses Ricci flow to compute area distortion and conformal factors on cortical surface meshes, then applies Shannon entropy to the resulting geometric features to create compact descriptors for classifying Alzheimer's disease from MRI scans. The approach achieves high accuracy (≈98.6%) with several classifiers, notably MLP and Logistic Regression, and demonstrates statistical significance over other methods.", "summary_cn": "本文采用 Ricci 流算法对皮层表面网格进行面积畸变和共形因子计算，并结合 Shannon 熵对几何特征进行压缩，进而用于 Alzheimer's AD 患者的 MRI 诊断分类，表现出约98.6%的高准确率。", "keywords": "Alzheimer's disease, cortical morphometry, Ricci flow, conformal features, Shannon entropy, MRI classification, robust machine learning", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["F. Ahmadi", "B. Bidabad", "H. Nasiri"]}, "usage": {"completion_tokens": 223, "prompt_tokens": 3526, "total_tokens": 3749, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00027665, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001763, "upstream_inference_completions_cost": 0.00010035}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:32.503670", "feed": "arxiv_cscv", "title": "DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution", "link": "https://papers.cool/arxiv/2510.18851", "analysis": {"summary": "DP²O‑SR introduces a Direct Perceptual Preference Optimization framework for real‑world image super‑resolution that leverages pretrained text‑to‑image diffusion models. By constructing a hybrid reward from full‑reference and no‑reference IQA models and forming multiple preference pairs with hierarchical weighting, the method improves perceptual quality without human annotations and adapts to model capacity.", "summary_cn": "DP²O‑SR 提出一种直接感知偏好优化框架，用于实际图像超分辨率，利用预训练的文本到图像扩散模型。通过结合全参考与无参考图像质量评估模型构建混合奖励，并从同一模型的输出构造多对偏好样本，采用分层加权方式，实现无需人工标注的感知质量提升，并根据模型容量自适应优化。", "keywords": "real-world super-resolution, diffusion models, perceptual preference optimization, image quality assessment, hierarchical preference weighting, generative models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rongyuan Wu", "Lingchen Sun", "Zhengqiang Zhang", "Shihao Wang", "Tianhe Wu", "Qiaosi Yi", "Shuai Li", "Lei Zhang"]}, "usage": {"completion_tokens": 600, "prompt_tokens": 3506, "total_tokens": 4106, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 400, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0004453, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001753, "upstream_inference_completions_cost": 0.00027}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:34.836431", "feed": "arxiv_cscv", "title": "Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning for Historical Writer Identification", "link": "https://papers.cool/arxiv/2510.18671", "analysis": {"summary": "The paper evaluates how different pre‑processing methods, backbone architectures, and post‑processing strategies affect end‑to‑end deep learning models for historical writer identification, highlighting challenges such as weak low‑level feature capture and sensitivity to content noise. Experiments reveal that most configurations perform poorly in realistic, document‑level and zero‑shot settings, though one simpler setup reaches performance comparable to state‑of‑the‑art systems.", "summary_cn": "本文评估了不同的预处理方法、主干网络和后处理策略对历史手写作者识别端到端深度学习模型的影响，指出了低层视觉特征捕获不足和对内容噪声高度敏感等挑战。实验表明，大多数配置在真实文档级和零样本情境下表现不佳，但一种更简洁的设置能够达到与最先进系统相当的性能。", "keywords": "historical writer identification, deep learning, end-to-end pipeline, feature aggregation, patch sampling, handwriting recognition", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hanif Rasyidi", "Moshiur Farazi"]}, "usage": {"completion_tokens": 474, "prompt_tokens": 3456, "total_tokens": 3930, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 216, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00053476, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029776, "upstream_inference_completions_cost": 0.000237}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:35.818029", "feed": "arxiv_cscv", "title": "Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model", "link": "https://papers.cool/arxiv/2510.18573", "analysis": {"summary": "Kaleido is a subject-to-video (S2V) generation framework that synthesizes videos consistent with multiple reference images of target subjects. The paper addresses limitations of existing S2V models by introducing a data construction pipeline with low‑quality filtering and diverse synthesis, as well as a Reference Rotary Positional Encoding (R‑RoPE) to integrate multiple references more reliably, achieving superior consistency, fidelity, and generalization across benchmarks.", "summary_cn": "Kaleido 是一种面向主体‑到‑视频（S2V）的生成框架，能够基于多个目标主体的参考图像合成主体一致的视频。论文通过构建数据管线（包括低质量样本过滤和多样化合成）以及引入参考旋转位置编码（R‑RoPE）来更稳健地融合多图像，从而在一致性、保真度和泛化能力上显著超越现有方法。", "keywords": "subject-to-video, multi-subject video generation, reference video synthesis, Reference Rotary Positional Encoding, data pipeline, video consistency, generative models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenxing Zhang", "Jiayan Teng", "Zhuoyi Yang", "Tiankun Cao", "Cheng Wang", "Xiaotao Gu", "Jie Tang", "Dan Guo", "Meng Wang"]}, "usage": {"completion_tokens": 578, "prompt_tokens": 3430, "total_tokens": 4008, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 316, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058286, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029386, "upstream_inference_completions_cost": 0.000289}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:36.249473", "feed": "arxiv_cscv", "title": "Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models", "link": "https://papers.cool/arxiv/2510.18287", "analysis": {"summary": "The paper proposes an efficient few-shot method for identity-preserving attribute editing in 3D-aware generative models. By estimating latent space directions from ten or fewer labelled images per attribute, it achieves photorealistic, view-consistent edits such as illumination changes, eyeglass addition, aging, and hairstyle modifications while maintaining the subject's identity. Experiments also explore the linearity of edits and continuous style manipulation using the Attribute Style Manipulation (ASM) technique.", "summary_cn": "本文提出一种高效的少样本方法，用于在 3D 感知生成模型中实现保持身份的人脸属性编辑。通过仅使用十张或更少的带标签图像来估计潜在空间方向，实现光照、眼镜、老化、发型等属性的真实感、跨视角一致的编辑，并保持原始身份。实验还考察了编辑的线性可叠加性，并使用属性风格操作 (ASM) 技术研究连续风格流形。", "keywords": "few-shot editing, 3D-aware generative models, identity preservation, latent space directions, attribute editing, style manipulation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vishal Vinod"]}, "usage": {"completion_tokens": 629, "prompt_tokens": 3559, "total_tokens": 4188, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 359, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00039396, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00014236, "upstream_inference_completions_cost": 0.0002516}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:36.435185", "feed": "arxiv_cscv", "title": "AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering", "link": "https://papers.cool/arxiv/2510.18346", "analysis": {"summary": "AV-Master introduces a dual-path framework for audio-visual question answering that dynamically adapts temporal sampling and modality preference to focus on question-relevant segments, using a dynamic adaptive focus sampling mechanism and a preference-aware strategy. A dual-path contrastive loss further enforces consistency across temporal and modality dimensions, yielding state-of-the-art performance on multiple benchmarks, especially for complex reasoning tasks.", "summary_cn": "AV-Master 提出了一个双路径框架用于音视听问答，通过动态自适应聚焦采样机制在时间维度上聚焦与问题相关的音视频片段，并在模态维度上采用偏好感知策略独立建模每种模态的贡献，从而实现关键特征的选择激活。双路径对比损失进一步强化时间和模态维度间的一致性和互补性，使模型在多个大型基准上，尤其是复杂推理任务中显著超越现有方法。", "keywords": "audio-visual question answering, dynamic adaptive focus sampling, modality preference awareness, dual-path contrastive loss, cross-modal representation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiayu Zhang", "Qilang Ye", "Shuo Ye", "Xun Lin", "Zihan Song", "Zitong Yu"]}, "usage": {"completion_tokens": 579, "prompt_tokens": 3434, "total_tokens": 4013, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 302, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058396, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029446, "upstream_inference_completions_cost": 0.0002895}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:36.729134", "feed": "arxiv_cscv", "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction", "link": "https://papers.cool/arxiv/2510.18773", "analysis": {"summary": "The paper presents a fine‑tuned geospatial foundation model to predict and simulate urban heat island (UHI) patterns, using limited ground‑truth temperature data to benchmark performance. It demonstrates the model's ability to forecast land‑surface temperatures under future climate scenarios and visualizes mitigation impacts via an inpainting simulation. The results suggest that foundation models can aid climate‑resilient planning in data‑scarce urban areas.", "summary_cn": "本文利用微调后的地理空间基础模型对城市热岛（UHI）进行检测与模拟，采用有限的地面温度真实数据进行基准评估。研究展示了模型在未来气候情景下预测地表温度的能力，并通过修补（inpainting）模拟展示了缓解措施的效果。结果表明，基础模型可在数据稀缺的城市地区支持气候韧性规划。", "keywords": "urban heat island, geospatial foundation model, microclimate prediction, land surface temperature, climate mitigation, fine-tuning, inpainting simulation", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jannis Fleckenstein", "David Kreismann", "Tamara Rosemary Govindasamy", "Thomas Brunschwiler", "Etienne Vos", "Mattia Rigotti"]}, "usage": {"completion_tokens": 581, "prompt_tokens": 3380, "total_tokens": 3961, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 315, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057686, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028636, "upstream_inference_completions_cost": 0.0002905}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:36.767520", "feed": "arxiv_cscv", "title": "Learning Human-Object Interaction as Groups", "link": "https://papers.cool/arxiv/2510.18357", "analysis": {"summary": "The paper introduces GroupHOI, a framework for Human-Object Interaction detection that groups humans and objects based on geometric proximity and semantic similarity, using a learnable proximity estimator and self‑attention within each group to aggregate contextual cues. It enhances a transformer‑based interaction decoder with local HO‑pair features and demonstrates state‑of‑the‑art performance on HICO‑DET, V‑COCO, and the challenging Nonverbal Interaction Detection task. By shifting from pairwise to group‑wise relation modeling, the work addresses higher‑order interactions in real‑world scenes.", "summary_cn": "本文提出 GroupHOI 框架，用于人‑物交互检测，通过基于空间特征的可学习接近度估计器将人和物体按几何接近度与语义相似性分组，并在每个组内使用自注意力聚合上下文信息。该方法在 Transformer 交互解码器中加入局部 HO‑pair 特征，显著提升了 HICO‑DET、V‑COCO 以及更具挑战性的非语言交互检测（NVI‑DET）任务的性能。通过从点对关系转向组级建模，论文解决了真实场景中更高阶的交互行为。", "keywords": "human-object interaction, group modeling, transformer, proximity estimator, HOI detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiajun Hong", "Jianan Wei", "Wenguan Wang"]}, "usage": {"completion_tokens": 574, "prompt_tokens": 3443, "total_tokens": 4017, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 235, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058281, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029581, "upstream_inference_completions_cost": 0.000287}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.219389", "feed": "arxiv_cscv", "title": "TreeFedDG: Alleviating Global Drift in Federated Domain Generalization for Medical Image Segmentation", "link": "https://papers.cool/arxiv/2510.18268", "analysis": {"summary": "The paper identifies the Global Drift problem in federated domain generalization for medical image segmentation and proposes TreeFedDG, a tree‑structured hierarchical aggregation framework combined with a parameter‑difference style mixing (FedStyle) and progressive personalized fusion to mitigate drift and improve cross‑domain performance. During inference, feature similarity guides the selection of the most relevant model chain from the tree for ensemble decision making.", "summary_cn": "本文指出在联邦域泛化医学图像分割任务中出现的全局漂移（Global Drift）问题，并提出 TreeFedDG——基于树形拓扑的层次参数聚合框架，结合基于参数差异的风格混合（FedStyle）与渐进式个性化融合，以抑制全局模型偏移并提升跨域分割性能。推理阶段利用特征相似度检索最相关的模型链进行集成决策。", "keywords": "federated learning, domain generalization, medical image segmentation, global drift, tree topology, FedStyle, hierarchical aggregation, personalized fusion, cross-domain robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yucheng Song", "Chenxi Li", "Haokang Ding", "Zhining Liao", "Zhifang Liao"]}, "usage": {"completion_tokens": 908, "prompt_tokens": 3489, "total_tokens": 4397, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 722, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3136}, "cost": 0.00106815, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00052335, "upstream_inference_completions_cost": 0.0005448}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.322157", "feed": "arxiv_cscv", "title": "ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters", "link": "https://papers.cool/arxiv/2510.18431", "analysis": {"summary": "ScaleNet proposes an efficient method for scaling pretrained vision transformers by inserting additional layers that share weights with the original model and adding small parallel adapter modules as adjustment parameters. This allows depth expansion with negligible parameter increase and faster training, achieving a 7.42% accuracy gain on ImageNet-1K with only a third of the epochs compared to training from scratch. The approach also shows promise for downstream tasks such as object detection.", "summary_cn": "ScaleNet 提出了一种高效的 Vision Transformer（ViT）扩展方法，通过在预训练模型中插入共享权重的额外层，并为每层添加小规模并行适配器（adapter）作为调整参数，实现了增量参数的深度扩展。该方法在保持参数几乎不增的情况下，仅使用三分之一的训练轮数即可在 ImageNet-1K 上实现 7.42% 的准确率提升，并在目标检测等下游任务中展现出潜力。", "keywords": "ScaleNet, vision transformer scaling, incremental parameters, weight sharing, adapter modules, efficient training", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhiwei Hao", "Jianyuan Guo", "Li Shen", "Kai Han", "Yehui Tang", "Han Hu", "Yunhe Wang"]}, "usage": {"completion_tokens": 576, "prompt_tokens": 3466, "total_tokens": 4042, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 322, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058726, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029926, "upstream_inference_completions_cost": 0.000288}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.336955", "feed": "arxiv_cscv", "title": "Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling", "link": "https://papers.cool/arxiv/2510.18405", "analysis": {"summary": "The paper proposes an automated system for analyzing cricket videos that combines YOLOv8-based pitch and ball detection with OCR-driven scorecard extraction to identify wicket‑taking deliveries. It includes extensive image preprocessing and achieves high detection performance (99.5% mAP50 for pitch, 99.18% mAP50 for ball) and uses trajectory modeling to reveal batting weaknesses, aiming to support coaching and strategic decisions.", "summary_cn": "本文提出一种自动化的板球视频分析系统，结合基于 YOLOv8 的投球区和球体检测以及 OCR 驱动的记分卡提取，以识别夺门（wicket）投球。系统通过灰度变换、幂变换和形态学操作等预处理，实现了高检测精度（投球区 99.5% mAP50，球体 99.18% mAP50），并利用轨迹建模提供击球弱点分析，以辅助教练和战术决策。", "keywords": "cricket video analysis, YOLOv8, OCR, trajectory modeling, deep learning, object detection, sports analytics, ball detection, pitch detection", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mst Jannatun Ferdous", "Masum Billah", "Joy Karmoker", "Mohd Ruhul Ameen", "Akif Islam", "Md. Omar Faruqe"]}, "usage": {"completion_tokens": 577, "prompt_tokens": 3404, "total_tokens": 3981, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 267, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057846, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028996, "upstream_inference_completions_cost": 0.0002885}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.371703", "feed": "arxiv_cscv", "title": "SAM 2++: Tracking Anything at Any Granularity", "link": "https://papers.cool/arxiv/2510.18822", "analysis": {"summary": "SAM 2++ proposes a unified video tracking architecture that can handle targets at any granularity—masks, boxes, or points—by using task-specific prompts and a shared decoder. It introduces a task-adaptive memory mechanism to maintain consistent tracking across granularities and provides a large, multi-granularity dataset (Tracking-Any-Granularity) for training and evaluation. Experiments show state-of-the-art performance on diverse benchmarks, demonstrating the model's versatility and robustness across tracking tasks.", "summary_cn": "SAM 2++ 提出了一种统一的视频跟踪框架，能够通过任务特定的提示和通用解码器在掩码、边框或点等任意粒度上进行目标跟踪。该模型引入任务自适应记忆机制，以在不同粒度之间保持一致的匹配，并提供了大规模的多粒度数据集（Tracking-Any-Granularity）用于训练和评估。实验表明，其在多个基准上实现了最新的性能，展示了在多任务跟踪中的通用性和鲁棒性。", "keywords": "video tracking, unified model, multi-granularity, mask tracking, box tracking, point tracking, task-adaptive memory, prompt embeddings, dataset, multi-task benchmarking", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiaming Zhang", "Cheng Liang", "Yichun Yang", "Chenkai Zeng", "Yutao Cui", "Xinwen Zhang", "Xin Zhou", "Kai Ma", "Gangshan Wu", "Limin Wang"]}, "usage": {"completion_tokens": 613, "prompt_tokens": 3442, "total_tokens": 4055, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 305, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060216, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029566, "upstream_inference_completions_cost": 0.0003065}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.871495", "feed": "arxiv_cscv", "title": "Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving", "link": "https://papers.cool/arxiv/2510.18552", "analysis": {"summary": "The paper introduces Occluded nuScenes, an extension of the nuScenes benchmark that provides controlled, parameterised occlusions for camera, radar, and LiDAR sensors. It releases four camera occlusion types and scripted degradations for radar and LiDAR, enabling reproducible evaluation of perception and fusion models under partial sensor failures. The dataset aims to facilitate research on robust sensor fusion and safety‑critical perception in automated driving.", "summary_cn": "本文推出了 Occluded nuScenes 数据集，这是 nuScenes 基准的扩展，提供对相机、雷达和 LiDAR 传感器的受控、可参数化的遮挡。数据集发布了四种相机遮挡类型，并提供雷达和 LiDAR 的遮挡脚本，以实现对部分传感器失效情况下感知与融合模型的可重复评估。该资源旨在推动自动驾驶中鲁棒传感器融合和安全关键感知的研究。", "keywords": "occlusion dataset, sensor fusion, perception robustness, autonomous driving, multi-sensor, nuScenes, radar occlusion, LiDAR occlusion, safety-critical perception", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Sanjay Kumar", "Tim Brophy", "Reenu Mohandas", "Eoin Martino Grua", "Ganesh Sistu", "Valentina Donzella", "Ciaran Eising"]}, "usage": {"completion_tokens": 660, "prompt_tokens": 3422, "total_tokens": 4082, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 398, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062266, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.00033}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.967868", "feed": "arxiv_cscv", "title": "Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents", "link": "https://papers.cool/arxiv/2510.18703", "analysis": {"summary": "The paper introduces Vision-Centric Contrastive Learning (VC2L), a framework that treats text, images, and their combinations as rendered images processed by a single vision transformer, eliminating OCR and tokenization. By using snippet-level contrastive objectives on consecutive multimodal segments, VC2L captures cross‑modal relationships in web documents and achieves competitive performance on new retrieval benchmarks and existing datasets. The work demonstrates that unified vision‑centric approaches can effectively learn representations from complex web‑based multimodal data.", "summary_cn": "本文提出了 Vision‑Centric Contrastive Learning（VC2L）框架，将文本、图像及其组合渲染为图像，由单一视觉 Transformer 处理，从而无需 OCR、分词或模态融合。通过对连续多模态片段的片段级对比学习，VC2L 捕获网页文档中的跨模态关系，并在新建检索基准及现有数据集上取得竞争或更佳的表现。该工作展示了统一的视觉中心方法在复杂网页多模态数据上学习表示的潜力。", "keywords": "vision transformer, contrastive learning, multimodal web documents, pixel-space encoding, cross-modal retrieval, snippet-level contrastive, VC2L", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yiqi Lin", "Alex Jinpeng Wang", "Linjie Li", "Zhengyuan Yang", "Mike Zheng Shou"]}, "usage": {"completion_tokens": 602, "prompt_tokens": 3516, "total_tokens": 4118, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 300, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060776, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030676, "upstream_inference_completions_cost": 0.000301}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:37.992777", "feed": "arxiv_cscv", "title": "S2AP: Score-space Sharpness Minimization for Adversarial Pruning", "link": "https://papers.cool/arxiv/2510.18381", "analysis": {"summary": "The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug‑in technique that reduces sharpness in the importance‑score landscape during mask selection, thereby stabilizing binary pruning masks and improving the adversarial robustness of compressed neural networks. Experiments on multiple datasets, architectures, and sparsity levels show that S2AP consistently enhances robustness compared to existing adversarial pruning pipelines.", "summary_cn": "本文提出了“Score-space Sharpness-aware Adversarial Pruning (S2AP)” 方法，通过在掩码搜索阶段对重要性分数进行扰动并最小化对应的鲁棒损失，降低分数空间的尖锐度，从而稳定二值化剪枝掩码并提升压缩后模型的对抗鲁棒性。大量实验在不同数据集、模型和稀疏度下验证了 S2AP 相较于传统对抗剪枝方法在鲁棒性方面的显著提升。", "keywords": "adversarial pruning, score-space sharpness, robustness, model compression, mask selection, sharpness minimization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Giorgio Piras", "Qi Zhao", "Fabio Brau", "Maura Pintor", "Christian Wressnegger", "Battista Biggio"]}, "usage": {"completion_tokens": 605, "prompt_tokens": 3435, "total_tokens": 4040, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 341, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059711, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029461, "upstream_inference_completions_cost": 0.0003025}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.244369", "feed": "arxiv_cscv", "title": "Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization", "link": "https://papers.cool/arxiv/2510.18267", "analysis": {"summary": "The paper proposes a two-stage network for 3D human mesh recovery that first extracts global and local latent information from image features in a hybrid latent frequency domain, and then uses low-dimensional mesh-pose interaction with parallel optimization to refine pose and shape. This approach aims to reduce computational cost while improving reconstruction accuracy compared to state-of-the-art methods.", "summary_cn": "本文提出了一种两阶段网络用于 3D 人体网格恢复。第一阶段从图像特征的低频和高频成分中提取全局（如整体形状对齐）和局部（如纹理细节）潜在信息，形成混合潜在频域特征；第二阶段在该特征的帮助下，通过降维并行优化的低维网格姿态交互模块，提升 3D 姿态与形状的估计，并显著降低计算成本。", "keywords": "human mesh recovery, latent information, low-dimensional learning, 3D pose estimation, parallel optimization, attention mechanisms, shape alignment, texture detail, computer vision", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Sheng Yang"]}, "usage": {"completion_tokens": 637, "prompt_tokens": 3470, "total_tokens": 4107, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 406, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003936, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001388, "upstream_inference_completions_cost": 0.0002548}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.349746", "feed": "arxiv_cscv", "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation", "link": "https://papers.cool/arxiv/2510.18701", "analysis": {"summary": "The paper introduces UniGenBench++, a comprehensive benchmark for assessing the semantic consistency of text-to-image generation models across diverse real-world scenarios and multiple languages. It offers 600 hierarchical prompts in English and Chinese, evaluates models on 10 primary and 27 sub‑dimensions, and provides a trained offline evaluation model for efficient assessment. Benchmark results reveal strengths and weaknesses of various open‑ and closed‑source T2I systems.", "summary_cn": "本文提出 UniGenBench++，一个用于评估文本到图像生成模型语义一致性的综合基准，覆盖多种真实场景并提供中英双语提示。基准包含 600 条层级化提示，涵盖 10 个主要及 27 个子评估维度，并配备离线评估模型，以便高效衡量模型表现。实验展示了不同开源和闭源 T2I 模型的优势与不足。", "keywords": "text-to-image generation, semantic evaluation, multilingual benchmark, prompt diversity, robustness, evaluation metrics, UniGenBench++", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Jiazi Bu", "Yujie Zhou", "Yi Xin", "Junjun He", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"]}, "usage": {"completion_tokens": 625, "prompt_tokens": 3511, "total_tokens": 4136, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 417, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061851, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030601, "upstream_inference_completions_cost": 0.0003125}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.396493", "feed": "arxiv_cscv", "title": "A Geometric Approach to Steerable Convolutions", "link": "https://papers.cool/arxiv/2510.18813", "analysis": {"summary": "The paper presents an intuitive geometric derivation of steerable convolutional neural networks in any dimension, explaining the emergence of the Clebsch–Gordan decomposition and spherical harmonic bases from pattern‑matching principles. It proposes a novel construction of steerable layers using interpolation kernels, which improves implementation efficiency and provides greater robustness to noisy data.", "summary_cn": "本文提供了一种直观的几何推导方法，解释了在任意维度下可转向卷积神经网络中为何会出现 Clebsch–Gordan 分解和球面调和基函数（spherical harmonic），并基于模式匹配原理阐释其原理。作者进一步提出使用插值核（interpolation kernels）构建可转向卷积层的创新方案，提升了实现效率并增强了对噪声数据的鲁棒性。", "keywords": "steerable convolutions, geometric derivation, Clebsch–Gordan decomposition, spherical harmonics, interpolation kernels, equivariant neural networks, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Soumyabrata Kundu", "Risi Kondor"]}, "usage": {"completion_tokens": 652, "prompt_tokens": 3294, "total_tokens": 3946, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 439, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059946, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027346, "upstream_inference_completions_cost": 0.000326}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.753971", "feed": "arxiv_cscv", "title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework", "link": "https://papers.cool/arxiv/2510.18825", "analysis": {"summary": "The paper proposes a unified hierarchical mask framework that shows how graph transformer architectures can be expressed as attention mask constructions, introducing design principles linking receptive field size and label consistency to classification accuracy. Building on this framework, the authors develop M3Dphormer, a Mixture-of-Experts graph transformer with multi-level masking and a dual attention computation that adapts between dense and sparse modes, achieving state-of-the-art results on several benchmarks.", "summary_cn": "本文提出统一的层次掩码框架，展示图 Transformer 架构可以等价地视为注意力掩码的构造，并提出有效掩码应兼顾足够的感受野和标签一致性这一设计原则。在此基础上，作者构建 M3Dphormer——一种基于专家混合的图 Transformer，采用多层掩码和双模式注意力计算，根据局部掩码稀疏度在密集和稀疏模式间切换，在多个基准上实现了最新性能。", "keywords": "graph transformers, hierarchical masks, attention mask, mixture-of-experts, dual attention, receptive field, label consistency, graph representation learning", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yujie Xing", "Xiao Wang", "Bin Wu", "Hai Huang", "Chuan Shi"]}, "usage": {"completion_tokens": 700, "prompt_tokens": 3475, "total_tokens": 4175, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 481, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065061, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030061, "upstream_inference_completions_cost": 0.00035}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.888297", "feed": "arxiv_cscv", "title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery", "link": "https://papers.cool/arxiv/2510.18740", "analysis": {"summary": "The paper introduces SEAL, a Semantic-Aware Hierarchical Learning framework for Generalized Category Discovery, which leverages naturally occurring hierarchical structures to improve classification of both known and unknown classes. It proposes a hierarchical semantic-guided soft contrastive learning loss that generates soft negatives based on hierarchical similarity, and a Cross-Granularity Consistency module to align predictions across different granularity levels, achieving state-of-the-art results on several fine-grained and coarse-grained benchmarks.", "summary_cn": "本文提出 SEAL（语义感知层次学习）框架用于广义类别发现（Generalized Category Discovery），利用自然存在的层次结构提升对已知和未知类别的分类效果。框架中设计了层次语义引导的软对比学习损失（Hierarchical Semantic-Guided Soft Contrastive Learning），通过层次相似性生成软负例，并引入跨粒度一致性（Cross-Granularity Consistency）模块对不同粒度的预测进行对齐，在多个细粒度和粗粒度基准上实现了最先进的性能。", "keywords": "Generalized Category Discovery, hierarchical learning, semantic-aware, contrastive learning, soft negatives, cross-granularity consistency, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenqi He", "Yuanpei Liu", "Kai Han"]}, "usage": {"completion_tokens": 700, "prompt_tokens": 3414, "total_tokens": 4114, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 475, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064146, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029146, "upstream_inference_completions_cost": 0.00035}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:38.943073", "feed": "arxiv_cscv", "title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning", "link": "https://papers.cool/arxiv/2510.18837", "analysis": {"summary": "The paper introduces FedDEAP, an adaptive federated prompt tuning framework that enhances CLIP's zero-shot classification across multiple client domains by disentangling semantic and domain-specific features, employing a dual-prompt design (global semantic prompt and local domain prompt), and aligning transformed visual and textual representations. Experiments on four datasets demonstrate improved generalization in federated image recognition under domain shift and label heterogeneity.", "summary_cn": "本文提出 FedDEAP——一种自适应的联邦提示调优框架，通过语义与领域特征解耦、全局语义提示与本地领域提示的双提示设计，以及在两种变换下对齐视觉与文本表示，提升 CLIP 在多域联邦图像分类中的零样本泛化能力。实验在四个数据集上显示，该方法在面对域迁移和标签异构时显著提升了全局模型的泛化表现。", "keywords": "federated learning, CLIP, prompt tuning, domain adaptation, dual prompt, multi-domain, vision-language, label heterogeneity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yubin Zheng", "Pak-Hei Yeung", "Jing Xia", "Tianjie Ju", "Peng Tang", "Weidong Qiu", "Jagath C. Rajapakse"]}, "usage": {"completion_tokens": 646, "prompt_tokens": 3463, "total_tokens": 4109, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 393, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062181, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029881, "upstream_inference_completions_cost": 0.000323}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.028613", "feed": "arxiv_cscv", "title": "LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance", "link": "https://papers.cool/arxiv/2510.18446", "analysis": {"summary": "The paper presents LAND, a latent diffusion model that synthesizes high‑resolution 3D chest CT volumes (256³ at 1 mm isotropic) conditioned on anatomical masks of lungs and nodules. By incorporating global lung structure alongside nodule masks, the method produces anatomically plausible scans and can generate diverse examples with or without nodules, enabling low‑cost data creation for AI training. Experiments show that conditioning only on nodule masks leads to unrealistic anatomy, emphasizing the need for comprehensive anatomical guidance.", "summary_cn": "本文提出了 LAND，一种基于潜在扩散的模型，可在肺部和结节的三维解剖掩码条件下生成高分辨率 3D 胸部 CT（256³，1 mm 等距）体积图像。通过结合全局肺部结构与结节掩码，模型能够生成解剖上合理且多样化的扫描，支持生成有或无结节的 CT，帮助 AI 模型或医护人员的训练。实验表明，仅使用结节掩码会产生解剖错误的结果，凸显完整解剖引导的重要性。", "keywords": "latent diffusion, 3D chest CT synthesis, anatomical guidance, lung nodule generation, synthetic medical imaging, volumetric diffusion, mask-conditioned generation, high-resolution CT, data augmentation, AI training", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Anna Oliveras", "Roger Marí", "Rafael Redondo", "Oriol Guardià", "Ana Tost", "Bhalaji Nagarajan", "Carolina Migliorelli", "Vicent Ribas", "Petia Radeva"]}, "usage": {"completion_tokens": 719, "prompt_tokens": 3344, "total_tokens": 4063, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 395, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064046, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028096, "upstream_inference_completions_cost": 0.0003595}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.066260", "feed": "arxiv_cscv", "title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection", "link": "https://papers.cool/arxiv/2510.18819", "analysis": {"summary": "The paper proposes a teacher–student hybrid AI framework that combines supervised and self-supervised heads to detect tuberculosis, COVID-19, and associated symptoms from chest X‑rays. It achieves 98.85% accuracy for disease classification and a macro‑F1 of 90.09% for multilabel symptom detection, and provides explainability assessments showing predictions are based on relevant anatomical features.", "summary_cn": "本文提出了一种教师‑学生混合 AI 框架，融合监督头和自监督头，对胸部 X 光图像进行结核病、COVID‑19 以及相关症状的检测。该模型在疾病分类上达到 98.85% 的准确率，在多标签症状检测上实现 90.09% 的宏观 F1 分数，并通过可解释性评估表明预测依据的是相关的解剖特征。", "keywords": "tuberculosis detection, chest X-ray, teacher-student framework, hybrid AI, explainable AI, symptom detection, multi-label classification, self-supervised learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Neel Patel", "Alexander Wong", "Ashkan Ebadi"]}, "usage": {"completion_tokens": 693, "prompt_tokens": 3370, "total_tokens": 4063, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 483, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063136, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028486, "upstream_inference_completions_cost": 0.0003465}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.204463", "feed": "arxiv_cscv", "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder", "link": "https://papers.cool/arxiv/2510.18795", "analysis": {"summary": "ProCLIP introduces a curriculum learning framework that progressively aligns CLIP's image encoder with a large language model (LLM) based text embedder, addressing CLIP's limitations on token length and multilingual support. The method first distills knowledge from CLIP's original text encoder into the LLM embedder, then fine‑tunes the image encoder using contrastive learning with self‑distillation regularization and additional alignment losses. Experiments demonstrate that the progressive alignment preserves CLIP's pretrained vision-language knowledge while enabling long‑text and multilingual capabilities.", "summary_cn": "ProCLIP 提出一种基于 curriculum learning 的渐进式对齐框架，将 CLIP 的图像编码器与大型语言模型（LLM）文本嵌入器对齐，以克服 CLIP 在 77 token 限制和多语言支持方面的不足。该方法首先将 CLIP 原始文本编码器的知识蒸馏到 LLM 嵌入器中建立初始对齐，然后通过图像‑文本对比学习、使用自蒸馏正则化以及实例语义对齐损失和嵌入结构对齐损失，实现对图像编码器的进一步对齐。实验表明该渐进式对齐能够保留 CLIP 预训练的视觉‑语言知识，同时提升对长文本和多语言的处理能力。", "keywords": "vision-language, CLIP, LLM embedder, curriculum learning, contrastive learning, knowledge distillation, multilingual, fine-grained semantics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaoxing Hu", "Kaicheng Yang", "Ziyong Feng", "Qi Ming", "Zonghao Guo", "Xiang An", "Ziyong Feng", "Junchi Yan", "Xue Yang"]}, "usage": {"completion_tokens": 669, "prompt_tokens": 3516, "total_tokens": 4185, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 310, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064126, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030676, "upstream_inference_completions_cost": 0.0003345}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.509163", "feed": "arxiv_cscv", "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "link": "https://papers.cool/arxiv/2510.18632", "analysis": {"summary": "The paper introduces 3DThinker, a framework that enables vision-language models to perform geometric imagination and spatial reasoning from limited 2D views without any explicit 3D supervision. It aligns the model's internal 3D latent representation with that of a 3D foundation model during supervised training, then refines the reasoning trajectory using outcome-based optimization. Experiments on several benchmarks demonstrate consistent performance gains over strong baselines, showcasing a new way to integrate 3D representations into multimodal reasoning.", "summary_cn": "本文提出 3DThinker 框架，使视觉语言模型能够在仅有有限 2D 视角且无显式 3D 标注的情况下进行几何想象和空间推理。通过在监督阶段将模型生成的 3D 潜在空间与 3D 基础模型对齐，随后仅依据结果信号优化整个推理过程，从而提升内部 3D 思维能力。大量基准实验显示该方法相较于强基线有持续的性能提升，为将 3D 表征统一到多模态推理中提供了新视角。", "keywords": "3D reasoning, geometric imagination, vision-language models, multimodal reasoning, latent alignment, spatial understanding, 3D foundation model, limited view, mental imagery", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhangquan Chen", "Manyuan Zhang", "Xinlei Yu", "Xufang Luo", "Mingze Sun", "Zihao Pan", "Yan Feng", "Peng Pei", "Xunliang Cai", "Ruqi Huang"]}, "usage": {"completion_tokens": 716, "prompt_tokens": 3475, "total_tokens": 4191, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 424, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065861, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030061, "upstream_inference_completions_cost": 0.000358}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.544191", "feed": "arxiv_cscv", "title": "See the Text: From Tokenization to Visual Reading", "link": "https://papers.cool/arxiv/2510.18840", "analysis": {"summary": "The paper proposes SeeTok, a vision-centric tokenization approach that renders text as images and feeds them to pretrained multimodal large language models, leveraging OCR and text‑vision alignment capabilities. Experiments across three language tasks show that SeeTok matches or outperforms traditional subword tokenizers while using 4.43× fewer tokens and reducing FLOPs by 70.5%, also improving cross‑lingual generalization and robustness to typographic noise. The work argues for shifting from symbolic tokenization toward human‑like visual reading for more natural language modeling.", "summary_cn": "本文提出 SeeTok，一种视觉化的文本标记方法，将文字渲染为图像后交由预训练的多模态大语言模型处理，利用 OCR 和文本‑视觉对齐能力。实验在三项语言任务上显示，SeeTok 在匹配或超越传统子词标记器的同时，仅使用 4.43 倍更少的 token，计算量降低 70.5%，并提升跨语言泛化能力和对排版噪声的鲁棒性。该工作主张从符号化标记转向类似人类视觉阅读的自然语言模型。", "keywords": "visual tokenization, multimodal LLM, OCR, cross-lingual, token efficiency, visual-text, SeeTok", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"]}, "usage": {"completion_tokens": 672, "prompt_tokens": 3434, "total_tokens": 4106, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 365, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063046, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029446, "upstream_inference_completions_cost": 0.000336}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.631261", "feed": "arxiv_cscv", "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization", "link": "https://papers.cool/arxiv/2510.18433", "analysis": {"summary": "The paper presents ImageGem, a large-scale in-the-wild dataset containing fine-grained user preference annotations for generative image models, including 57K users, 242K customized LoRAs, 3M prompts, and 5M generated images. Using these annotations, the authors train improved preference alignment models and evaluate personalized image retrieval and diffusion model recommendation, also introducing an end-to-end framework for editing diffusion models in latent weight space to match individual preferences. The results demonstrate that ImageGem enables a new paradigm for generative model personalization.", "summary_cn": "本文推出 ImageGem 数据集，收集了 57 万用户的真实交互数据，包含 242K 定制 LoRA、300 万文本提示和 500 万生成图像，以细粒度用户偏好标注为核心。基于这些标注，作者训练了更好的偏好对齐模型，并在个性化图像检索、生成模型推荐以及在潜在权重空间编辑扩散模型以匹配用户偏好方面进行实验，展示了该数据集为生成模型个性化提供了全新范式。", "keywords": "generative model personalization, preference alignment, LoRA, diffusion models, image retrieval, vision-language, fine-grained user preferences, dataset", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Yuanhe Guo", "Linxi Xie", "Zhuoran Chen", "Kangrui Yu", "Ryan Po", "Guandao Yang", "Gordon Wetztein", "Hongyi Wen"]}, "usage": {"completion_tokens": 705, "prompt_tokens": 3385, "total_tokens": 4090, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 412, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063961, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028711, "upstream_inference_completions_cost": 0.0003525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.636923", "feed": "arxiv_cscv", "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression", "link": "https://papers.cool/arxiv/2510.18636", "analysis": {"summary": "The paper proposes C-SWAP, a one-shot structured pruning framework that incorporates explainability by using causal relationships between model predictions and structural components to guide pruning. By progressively removing structures deemed non-essential, the method achieves substantial model size reductions on CNN and vision transformer baselines without fine‑tuning, outperforming existing one-shot approaches. Experimental results demonstrate minimal performance loss while offering a superior trade‑off between compression and accuracy.", "summary_cn": "本文提出 C-SWAP，一种在一次性结构化剪枝过程中引入可解释性的框架，通过利用模型预测与结构之间的因果关系（cause‑effect relations）指导剪枝，逐步移除对性能影响不大的网络结构。该方法在卷积神经网络和视觉变换器（vision transformer）基准上实现了显著的模型体积缩减且无需微调，性能下降极小，整体压缩效果优于现有一次性剪枝方法。", "keywords": "structured pruning, explainability, causal pruning, one-shot pruning, model compression, CNN, vision transformer", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baptiste Bauvin", "Loïc Baret", "Ola Ahmad"]}, "usage": {"completion_tokens": 687, "prompt_tokens": 3480, "total_tokens": 4167, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 465, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064486, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030136, "upstream_inference_completions_cost": 0.0003435}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.642450", "feed": "arxiv_cscv", "title": "ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data", "link": "https://papers.cool/arxiv/2510.18637", "analysis": {"summary": "The paper introduces ε-Seg, a sparsely supervised semantic segmentation method for electron microscopy and fluorescence microscopy images that leverages hierarchical variational autoencoders, center-region masking, contrastive learning, and a Gaussian mixture model prior to learn robust latent embeddings and directly predict class labels. By requiring only a tiny fraction of pixel-level annotations (as low as 0.05%), ε-Seg achieves competitive segmentation performance on dense biological imaging datasets. Experiments demonstrate its effectiveness compared to baseline approaches across multiple microscopy modalities.", "summary_cn": "本文提出 ε-Seg，一种针对电子显微镜和荧光显微镜图像的稀疏监督语义分割方法，使用层次变分自编码器 (HVAE)、中心区域遮罩、对比学习和高斯混合模型先验来学习鲁棒的潜在嵌入，并直接预测类别标签。该方法仅需极少的像素标注（低至 0.05%），即可在密集的生物成像数据集上实现竞争性的分割效果。实验表明 ε-Seg 相较于基线在多种显微镜模态上表现出色。", "keywords": "sparse supervision, semantic segmentation, hierarchical variational autoencoder, contrastive learning, Gaussian mixture model, microscopy imaging, EM data, label-efficient learning", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sheida Rahnamai Kordasiabi", "Damian Dalle Nogare", "Florian Jug"]}, "usage": {"completion_tokens": 705, "prompt_tokens": 3465, "total_tokens": 4170, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 416, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065161, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029911, "upstream_inference_completions_cost": 0.0003525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.830166", "feed": "arxiv_cscv", "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation", "link": "https://papers.cool/arxiv/2510.18692", "analysis": {"summary": "The paper proposes Mixture-of-Groups Attention (MoGA), a learnable token routing mechanism that provides precise sparse attention without blockwise estimation, enabling efficient long-range interactions in Diffusion Transformers for video generation. Using MoGA, the authors build a model that can generate minute‑level, multi‑shot videos at 480p/24 fps with a context length of about 580k frames, demonstrating strong performance on multiple video generation benchmarks.", "summary_cn": "本文提出 Mixture-of-Groups Attention（MoGA），一种可学习的 token 路由器，实现了无需块级估计的精确稀疏注意力，从而在 Diffusion Transformer 中支持高效的长程交互。基于 MoGA，作者构建了能够端到端生成分钟级、多镜头、480p/24 fps 视频（上下文长度约 58 万帧）的模型，并在多项视频生成任务上验证了其有效性。", "keywords": "Mixture-of-Groups Attention, sparse attention, diffusion transformer, long video generation, token routing, FlashAttention, sequence parallelism, efficient attention, video diffusion models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Weinan Jia", "Yuning Lu", "Mengqi Huang", "Hualiang Wang", "Binyuan Huang", "Nan Chen", "Mu Liu", "Jidong Jiang", "Zhendong Mao"]}, "usage": {"completion_tokens": 695, "prompt_tokens": 3387, "total_tokens": 4082, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 434, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063491, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028741, "upstream_inference_completions_cost": 0.0003475}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:39.865309", "feed": "arxiv_cscv", "title": "Image augmentation with invertible networks in interactive satellite image change detection", "link": "https://papers.cool/arxiv/2510.18660", "analysis": {"summary": "The paper introduces an interactive satellite image change detection framework that uses active learning with a question‑answer loop to query a user for labels on a small display set. A novel invertible network maps these displays to a latent space where linear augmentations are applied, then maps the augmented data back for retraining the change detector in successive iterations. Experiments show that this approach outperforms existing methods on benchmark datasets.", "summary_cn": "本文提出了一种交互式卫星图像变化检测框架，采用主动学习的问答循环让用户为少量显示集标注标签。利用新颖的可逆网络将显示图像映射到潜在空间，在该空间进行线性增强后再映射回原始空间，用于后续迭代中重新训练检测模型。实验表明，该方法在基准数据集上优于现有技术。", "keywords": "active learning, satellite image change detection, invertible networks, data augmentation, interactive learning, remote sensing, latent space transformation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hichem Sahbi"]}, "usage": {"completion_tokens": 764, "prompt_tokens": 3346, "total_tokens": 4110, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 578, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066326, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028126, "upstream_inference_completions_cost": 0.000382}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:40.158094", "feed": "arxiv_cscv", "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs", "link": "https://papers.cool/arxiv/2510.18876", "analysis": {"summary": "The paper introduces Grasp Any Region (GAR), a region-level multimodal LLM architecture that uses a RoI‑aligned feature replay technique to incorporate necessary global context and model interactions among multiple prompts, enabling precise perception and advanced compositional reasoning for any image region. GAR also provides a new benchmark, GAR‑Bench, to evaluate single‑region comprehension as well as multi‑region interaction and reasoning. Experiments demonstrate that GAR‑1B attains state‑of‑the‑art captioning and VQA performance, and the zero‑shot GAR‑8B surpasses specialized video models on video‑referential benchmarks.", "summary_cn": "本文提出了 Grasp Any Region (GAR) 框架，这是一种基于 RoI 对齐特征回放的多模态大语言模型，用于在理解任意图像区域时引入全局上下文并建模多个提示之间的交互，从而实现精确感知和高级组合推理。GAR 同时提供了新基准 GAR‑Bench，用于评估单区域理解以及多区域交互与推理能力。实验表明，GAR‑1B 在字幕生成和视觉问答任务上达到业界领先水平，零样本的 GAR‑8B 在视频指代基准上亦超越专用视频模型。", "keywords": "multimodal LLM, region-level visual understanding, RoI-aligned feature replay, compositional reasoning, GAR-Bench, fine-grained perception, visual grounding, zero-shot transfer, VQA", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}, "usage": {"completion_tokens": 1092, "prompt_tokens": 3506, "total_tokens": 4598, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 834, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0006667, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001753, "upstream_inference_completions_cost": 0.0004914}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:40.582006", "feed": "arxiv_cscv", "title": "A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition", "link": "https://papers.cool/arxiv/2510.18705", "analysis": {"summary": "The paper introduces the Explicit Motion Information Mining (EMIM) module that integrates cost‑volume‑style affinity matrices into video transformers to better capture motion. EMIM samples key tokens from neighboring regions in the next frame, using the resulting matrix for both appearance aggregation and motion feature extraction, leading to state‑of‑the‑art results on motion‑sensitive benchmarks such as Something‑Something V1/V2.", "summary_cn": "本文提出显式运动信息挖掘 (EMIM) 模块，将传统动作识别中的代价体（cost volume）式亲和矩阵融合进视频 Transformer，以更好地捕获运动信息。EMIM 在下一帧的相邻区域中采样关键 token，构建亲和矩阵用于外观特征聚合并转化为运动特征，在 Something‑Something V1/V2 等运动敏感数据集上实现了最先进的性能。", "keywords": "Explicit Motion Information Mining, transformer, cost volume, action recognition, motion modeling, self-attention, affinity matrix, video understanding, Something-Something", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Peiqin Zhuang", "Lei Bai", "Yichao Wu", "Ding Liang", "Luping Zhou", "Yali Wang", "Wanli Ouyang"]}, "usage": {"completion_tokens": 739, "prompt_tokens": 3433, "total_tokens": 4172, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 558, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066381, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029431, "upstream_inference_completions_cost": 0.0003695}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:40.653387", "feed": "arxiv_cscv", "title": "Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding", "link": "https://papers.cool/arxiv/2510.18321", "analysis": {"summary": "The paper introduces Adaptive Token Ensemble Decoding (ATED), a training-free inference framework that reduces object hallucinations in large vision-language models by aggregating predictions from multiple models with uncertainty-based token-level weighting and diverse decoding paths. Experiments show that ATED markedly lowers hallucination rates on benchmark datasets without sacrificing fluency or relevance, offering a scalable and model-agnostic way to improve LVLM robustness for high‑stakes applications.", "summary_cn": "本文提出了一种无需训练的自适应令牌集成解码方法（ATED），通过对多个大规模视觉语言模型的预测进行基于不确定性的令牌级加权并结合多样的解码路径，显著降低了对象幻觉的产生。实验表明，在标准幻觉检测基准上，ATED 在不损失流畅性和相关性的前提下，大幅降低了幻觉率，为高风险场景中的 LVLM 稳健性提供了可扩展、模型无关的解决方案。", "keywords": "hallucination mitigation, vision-language models, adaptive token ensemble, uncertainty weighting, multimodal robustness, ensemble decoding, LVLM safety", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jinlin Li", "Yuran Wang", "Yifei Yuan", "Xiao Zhou", "Yingying Zhang", "Xixian Yong", "Yefeng Zheng", "Xian Wu"]}, "usage": {"completion_tokens": 694, "prompt_tokens": 4039, "total_tokens": 4733, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 359, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00037545, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020195, "upstream_inference_completions_cost": 0.0001735}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:40.827872", "feed": "arxiv_cscv", "title": "Accelerating Vision Transformers with Adaptive Patch Sizes", "link": "https://papers.cool/arxiv/2510.18091", "analysis": {"summary": "The paper introduces Adaptive Patch Transformers (APT), which employ multiple patch sizes within a single image to allocate larger patches to homogeneous regions and smaller patches to complex regions. This adaptive tokenization reduces the total number of tokens, yielding 40%‑50% speedups for large ViT models while preserving downstream performance, and can be applied to already fine‑tuned models with rapid convergence.", "summary_cn": "本文提出了自适应切片视觉变换器（Adaptive Patch Transformers, APT），在同一图像中使用不同尺度的切片：在内容均匀的区域使用更大的切片，在复杂区域使用更小的切片。该方法通过减少 token 数量，使得 ViT‑L 和 ViT‑H 的推理及训练吞吐量分别提升约 40%‑50%，且对下游任务性能影响不大，可直接应用于已微调的 ViT 并在约 1 个 epoch 内收敛。", "keywords": "vision transformer, adaptive patch size, dynamic token allocation, efficient inference, model acceleration, computer vision, dense prediction", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rohan Choudhury", "JungEun Kim", "Jinhyung Park", "Eunho Yang", "László A. Jeni", "Kris M. Kitani"]}, "usage": {"completion_tokens": 666, "prompt_tokens": 3364, "total_tokens": 4030, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 421, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0009042, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0005046, "upstream_inference_completions_cost": 0.0003996}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:41.110469", "feed": "arxiv_cscv", "title": "Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation", "link": "https://papers.cool/arxiv/2510.18502", "analysis": {"summary": "The paper proposes a zero‑shot vehicle make and model recognition pipeline that combines a vision‑language model with retrieval‑augmented generation. The VLM describes the vehicle image in text, retrieves matching entries from a database of vehicle attributes, and prompts a language model to infer the exact make and model, achieving about a 20% improvement over a CLIP baseline without any image‑specific finetuning.", "summary_cn": "本文提出一种零样本车辆品牌车型识别流水线，将视觉语言模型生成的图像描述与基于文本的检索增强生成相结合。系统从车辆属性数据库中检索相关条目，并将其与描述一起提示语言模型输出具体品牌和车型，实现相较于 CLIP 基线约 20% 的性能提升，无需对图像进行专门微调。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, smart city", "scoring": {"interpretability": 3, "understanding": 4, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}, "usage": {"completion_tokens": 786, "prompt_tokens": 3392, "total_tokens": 4178, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 647, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068116, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.000393}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:42.194194", "feed": "arxiv_cscv", "title": "RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation", "link": "https://papers.cool/arxiv/2510.18521", "analysis": {"summary": "RayPose reformulates template-based 6D object pose estimation as a ray alignment problem and trains a diffusion transformer to align a query image with multiple posed template views. The method reparameterizes rotation using object-centered camera rays and extends scale‑invariant translation estimation to dense translation offsets, employing a coarse‑to‑fine training strategy that samples narrowed template sets. Experiments on several benchmarks demonstrate competitive performance on unseen object pose estimation.", "summary_cn": "RayPose 将基于模板的 6D 目标姿态估计重新表述为光线对齐问题，并利用扩散 Transformer 将查询图像与多个已定位的模板视图对齐。该方法使用以对象为中心的相机光线对旋转进行重新参数化，并通过稠密平移偏移扩展尺度不变的平移估计，同时采用从粗到细的训练策略对模板采样进行收窄。实验在多个基准数据集上显示了在未见对象姿态估计方面的竞争性表现。", "keywords": "6D object pose estimation, diffusion transformer, ray bundling, template views, unseen objects, geometric priors", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junwen Huang", "Shishir Reddy Vutukur", "Peter KT Yu", "Nassir Navab", "Slobodan Ilic", "Benjamin Busam"]}, "usage": {"completion_tokens": 541, "prompt_tokens": 3396, "total_tokens": 3937, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 250, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00055926, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028876, "upstream_inference_completions_cost": 0.0002705}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:42.289540", "feed": "arxiv_cscv", "title": "Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection", "link": "https://papers.cool/arxiv/2510.18437", "analysis": {"summary": "The paper introduces RISE, a retrieval self-augmented framework for unsupervised camouflaged object detection that leverages the entire training dataset to build prototype libraries and generate pseudo-labels via clustering-then-retrieval and multi-view KNN retrieval. By constructing environment and object prototypes without any ground-truth annotations, RISE produces high-quality pseudo-masks that improve detection performance over existing unsupervised and prompt-based methods.", "summary_cn": "本文提出 RISE，一种检索自增强的无监督伪装目标检测框架，利用整个训练数据集构建环境和目标原型库，并通过聚类后检索及多视角 KNN 检索生成伪标签。该方法在无需任何标注的情况下产生高质量伪掩码，在实验中超越了现有的无监督和基于提示的方法。", "keywords": "camouflaged object detection, unsupervised learning, prototype library, KNN retrieval, clustering, multi-view retrieval, pseudo-labels", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ji Du", "Xin Wang", "Fangwei Hao", "Mingyang Yu", "Chunyuan Chen", "Jiesheng Wu", "Bin Wang", "Jing Xu", "Ping Li"]}, "usage": {"completion_tokens": 596, "prompt_tokens": 4110, "total_tokens": 4706, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 291, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003545, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002055, "upstream_inference_completions_cost": 0.000149}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:42.625146", "feed": "arxiv_cscv", "title": "Bayesian Fully-Connected Tensor Network for Hyperspectral-Multispectral Image Fusion", "link": "https://papers.cool/arxiv/2510.18400", "analysis": {"summary": "The paper introduces Bayesian Fully-Connected Tensor Network (BFCTN) for hyperspectral‑multispectral image fusion. By imposing a hierarchical sparse prior within a probabilistic framework and employing variational Bayesian inference combined with EM, the method preserves spatial‑spectral structures, reduces manual tuning, and improves robustness to noise. Experiments show state‑of‑the‑art accuracy and practical applicability.", "summary_cn": "本文提出了用于高光谱‑多光谱图像融合的贝叶斯全连接张量网络（BFCTN）。通过在概率框架中引入层次稀疏先验，并结合变分贝叶斯推断和 EM 算法，能够保留空间‑光谱结构、降低手动参数调节并提升对噪声的鲁棒性。实验表明该方法达到了先进的融合精度并具备实际应用潜力。", "keywords": "hyperspectral image fusion, multispectral fusion, Bayesian tensor network, Fully-Connected Tensor Network, variational Bayesian inference, EM algorithm, tensor decomposition, sparse prior", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Linsong Shan", "Zecan Yang", "Laurence T. Yang", "Changlong Li", "Honglu Zhao", "Xin Nie"]}, "usage": {"completion_tokens": 839, "prompt_tokens": 3469, "total_tokens": 4308, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 656, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071921, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029971, "upstream_inference_completions_cost": 0.0004195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:42.644304", "feed": "arxiv_cscv", "title": "Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection", "link": "https://papers.cool/arxiv/2510.18781", "analysis": {"summary": "The paper proposes a \"Rebellious Student\" framework that jointly enhances spectral and spatial background features for hyperspectral anomaly detection. The spatial branch is deliberately trained to diverge from a spectral teacher using decorrelation losses, enabling complementary feature learning without per-scene retraining. Experiments on the HAD100 benchmark demonstrate significant performance gains over existing methods with minimal computational overhead.", "summary_cn": "本文提出“Rebellious Student”（叛逆学生）框架，通过有意让空间分支与光谱教师分支背离并使用去相关损失，学习互补的空间特征，从而在高光谱异常检测中增强光谱和空间背景特征。该方法无需针对每个场景重新训练或调参，可与常规检测器结合实现参数自由的异常检测。实验在 HAD100 基准上显示出相较于多个基线的显著提升且计算开销极低。", "keywords": "hyperspectral anomaly detection, spectral-spatial feature enhancement, rebellious student, teacher-student paradigm, reverse distillation, decorrelation loss, background modeling", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Wenping Jin", "Yuyang Tang", "Li Zhu", "Fei Guo"]}, "usage": {"completion_tokens": 881, "prompt_tokens": 3447, "total_tokens": 4328, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 728, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073691, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029641, "upstream_inference_completions_cost": 0.0004405}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:43.225289", "feed": "arxiv_cscv", "title": "FeatureFool: Zero-Query Fooling of Video Models via Feature Map", "link": "https://papers.cool/arxiv/2510.18362", "analysis": {"summary": "The paper introduces FeatureFool, a zero‑query black‑box adversarial attack for video models that leverages extracted feature maps to shift the feature space of clean videos without any queries to the target model. Experiments demonstrate over 70% success against traditional video classifiers and effective transfer to Video‑LLMs while maintaining high visual quality. This approach reveals a new vulnerability in video‑based AI systems.", "summary_cn": "本文提出 FeatureFool，一种零查询（zero‑query）黑盒对抗攻击，利用深度网络提取的特征图（feature map）直接改变干净视频的特征空间，无需向目标模型发送查询。实验显示对传统视频分类器的攻击成功率超过 70%，且可迁移至 Video‑LLM，同时保持高视觉质量。该方法揭示了视频模型在安全性方面的新脆弱性。", "keywords": "zero-query attack, video adversarial attack, feature map, black-box attack, video classifiers, Video-LLM, robustness, adversarial perturbation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Duoxun Tang", "Xi Xiao", "Guangwu Hu", "Kangkang Sun", "Xiao Yang", "Dongyang Chen", "Qing Li", "Yongjie Yin", "Jiyao Wang"]}, "usage": {"completion_tokens": 868, "prompt_tokens": 3436, "total_tokens": 4304, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 665, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00072876, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029476, "upstream_inference_completions_cost": 0.000434}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:43.380386", "feed": "arxiv_cscv", "title": "GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization", "link": "https://papers.cool/arxiv/2510.18539", "analysis": {"summary": "The paper introduces GBlobs, a local point cloud feature descriptor designed to mitigate the geometric shortcut that causes LiDAR detectors to over‑rely on absolute object positions. By using GBlobs as inputs, the method forces networks to learn object‑centric representations, substantially improving generalization across varied sensor placements and achieving top performance in the RoboSense 2025 Track 3 challenge.", "summary_cn": "本文提出 GBlobs，一种局部点云特征描述子，用于消除 LiDAR 检测器对绝对位置的几何捷径依赖。通过将 GBlobs 作为网络输入，模型被迫学习以物体为中心的表示，从而在不同传感器摆放情况下显著提升泛化能力，并在 RoboSense 2025 第三赛道中取得最佳成绩。", "keywords": "GBlobs, LiDAR, 3D object detection, sensor placement, geometric shortcut, local point cloud descriptor, generalization, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Dušan Malić", "Christian Fruhwirth-Reisinger", "Alexander Prutsch", "Wei Lin", "Samuel Schulter", "Horst Possegger"]}, "usage": {"completion_tokens": 689, "prompt_tokens": 4005, "total_tokens": 4694, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 388, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003725, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020025, "upstream_inference_completions_cost": 0.00017225}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:43.820935", "feed": "arxiv_cscv", "title": "Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback", "link": "https://papers.cool/arxiv/2510.18353", "analysis": {"summary": "The paper proposes Diffusion Denoising Ranking Optimization (Diffusion-DRO), a preference learning framework for text-to-image diffusion models that casts human feedback as a ranking problem, removing the need for a reward model and integrating offline expert demonstrations with online policy-generated negative samples. This denoising-based objective simplifies training and mitigates the non-linear probability estimation issues of prior DPO methods. Experiments demonstrate improved generation quality across diverse and unseen prompts, outperforming state-of-the-art baselines in quantitative metrics and user studies.", "summary_cn": "本文提出 Diffusion Denoising Ranking Optimization（Diffusion-DRO），一种将人类偏好视为排序问题的扩散模型偏好学习框架，摆脱了奖励模型的依赖，并将离线专家示范与在线策略生成的负样本结合。该去噪式目标简化了训练并解决了之前 DPO 方法中的非线性概率估计问题。实验显示在各种挑战性和未见提示上，Diffusion-DRO 能提升生成质量，超越了最新基准的量化指标和用户研究结果。", "keywords": "diffusion models, preference learning, ranking optimization, inverse reinforcement learning, DPO, alignment, offline demonstrations, denoising, human feedback, generative AI", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yi-Lun Wu", "Bo-Kai Ruan", "Chiang Tseng", "Hong-Han Shuai"]}, "usage": {"completion_tokens": 778, "prompt_tokens": 4044, "total_tokens": 4822, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 417, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003967, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002022, "upstream_inference_completions_cost": 0.0001945}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:43.914109", "feed": "arxiv_cscv", "title": "GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation", "link": "https://papers.cool/arxiv/2510.18291", "analysis": {"summary": "The paper proposes GeoDiff, a training-free framework that augments pretrained diffusion-based monocular depth estimation models with stereo vision geometry to resolve scale ambiguity and produce metric depth. By treating depth estimation as an inverse problem and leveraging latent diffusion conditioned on RGB images together with stereo-derived scale and shift constraints, the method achieves state-of-the-art performance on diverse indoor and outdoor scenes without retraining.", "summary_cn": "本文提出 GeoDiff 框架，在预训练的基于扩散的单目深度估计模型上加入立体视觉几何约束，以解决尺度歧义并实现度量深度估计。通过将深度估计重新表述为逆问题，并结合以 RGB 图像为条件的潜在扩散模型与立体产生的尺度和位移约束，该方法在无需重新训练的情况下，在多种室内外场景中达到或超过最新水平。", "keywords": "metric depth estimation, diffusion models, latent diffusion, stereo guidance, geometry constraints, monocular depth, inverse problem", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tuan Pham", "Thanh-Tung Le", "Xiaohui Xie", "Stephan Mandt"]}, "usage": {"completion_tokens": 598, "prompt_tokens": 3351, "total_tokens": 3949, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 348, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028201, "upstream_inference_completions_cost": 0.000299}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:44.710028", "feed": "arxiv_cscv", "title": "GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data", "link": "https://papers.cool/arxiv/2510.18345", "analysis": {"summary": "The paper introduces GPTFace, a generative pre‑training framework that jointly learns facial visual and linguistic representations from large‑scale weakly paired web images and text using span masking and image‑text matching objectives. The pretrained model can be used for various downstream facial tasks such as attribute classification, expression recognition, and diverse face‑editing operations, achieving performance comparable to state‑of‑the‑art methods.", "summary_cn": "本文提出 GPTFace，一种生成式预训练框架，通过跨度掩码和图文匹配任务，从大规模弱相关的网络人脸图像与文本中学习面部视觉‑语言表征。该模型在属性分类、表情识别以及多种人脸编辑任务上表现与最先进方法相当。", "keywords": "facial representation, multimodal pretraining, span masking, image-text matching, generative transformer, face editing", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yudong Li", "Hao Li", "Xianxu Hou", "Linlin Shen"]}, "usage": {"completion_tokens": 877, "prompt_tokens": 3425, "total_tokens": 4302, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 776, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073161, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029311, "upstream_inference_completions_cost": 0.0004385}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:45.551785", "feed": "arxiv_cscv", "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence", "link": "https://papers.cool/arxiv/2510.18873", "analysis": {"summary": "DSI-Bench introduces a benchmark for evaluating dynamic spatial intelligence, comprising nearly 1,000 videos and 1,700 annotated questions covering nine distinct motion patterns of observers and objects. Experiments on 14 vision‑language and expert models reveal systematic shortcomings such as conflating observer and object motion, semantic biases, and inaccurate relative relationship inference. The benchmark highlights current limitations and guides future development of models with robust dynamic spatial reasoning.", "summary_cn": "DSI-Bench 提出了用于评估动态空间智能的基准，包含约 1,000 条视频和 1,700 条人工标注的问题，覆盖观察者和对象的九种独立运动模式。对 14 种视觉‑语言模型和专业模型的实验显示，它们常将观察者与对象的运动混淆、存在语义偏差，并且在动态场景中难以准确推断相对关系。该基准揭示了现有模型的局限，为未来具备稳健动态空间推理能力的模型研发提供指引。", "keywords": "dynamic spatial intelligence, benchmark, vision-language models, motion reasoning, spatial-temporal reasoning, dataset, observer motion, object motion, model evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziang Zhang", "Zehan Wang", "Guanghao Zhang", "Weilong Dai", "Yan Xia", "Ziang Yan", "Minjie Hong", "Zhou Zhao"]}, "usage": {"completion_tokens": 1000, "prompt_tokens": 3372, "total_tokens": 4372, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 872, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078516, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028516, "upstream_inference_completions_cost": 0.0005}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:45.977772", "feed": "arxiv_cscv", "title": "Investigating Demographic Bias in Brain MRI Segmentation: A Comparative Study of Deep-Learning and Non-Deep-Learning Methods", "link": "https://papers.cool/arxiv/2510.17999", "analysis": {"summary": "The paper evaluates demographic bias in brain MRI segmentation by comparing three deep‑learning models (UNesT, nnU‑Net, CoTr) and a traditional atlas‑based method (ANTs) on left and right nucleus accumbens across four race‑sex subgroups. It quantifies fairness in segmentation performance and examines how race and sex affect segmentation accuracy and derived volumes, finding that race‑matched training improves some models while nnU‑Net remains robust, and that sex effects persist while race effects largely disappear.", "summary_cn": "本文评估了脑部 MRI 分割中的人口统计偏差，比较了三种深度学习模型（UNesT、nnU‑Net、CoTr）和一种传统的基于图谱的方法（ANTs）在四个人口子组（黑人女性、黑人男性、白人女性、白人男性）对左右伏隔核的分割表现。通过公平性指标量化分割性能，并分析种族和性别对分割准确性及体积的影响，发现种族匹配训练可提升部分模型的表现，而 nnU‑Net 在不同人口组间保持稳健；性别效应在手工和偏差模型中均可观察到，而种族效应在大多数模型中消失。", "keywords": "ographic bias, medical image segmentation, MRI, fairness, UNesT, nnU-Net, CoTr, ANTs, nucleus accumbens, linear mixed models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Ghazal Danaee", "Marc Niethammer", "Jarrett Rushmore", "Sylvain Bouix"]}, "usage": {"completion_tokens": 784, "prompt_tokens": 3542, "total_tokens": 4326, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 443, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0010017, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0005313, "upstream_inference_completions_cost": 0.0004704}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:47.605965", "feed": "arxiv_cscv", "title": "HouseTour: A Virtual Real Estate A(I)gent", "link": "https://papers.cool/arxiv/2510.18054", "analysis": {"summary": "HouseTour presents a pipeline that generates spatially-aware 3D camera trajectories and natural language real‑estate descriptions from collections of images, using a diffusion process constrained by known camera poses and 3D Gaussian splatting for novel view synthesis. The authors also release the HouseTour dataset containing over 1,200 house‑tour videos with poses, 3D reconstructions, and professional descriptions, and introduce a joint metric to evaluate end‑to‑end performance.", "summary_cn": "HouseTour 提出了一套从房屋图片集合生成空间感知的 3D 相机轨迹以及自然语言房产描述的系统，采用受相机位姿约束的扩散过程并使用 3D 高斯 splatting 合成新视角视频。论文同时发布了包含 1200 多段房屋巡游视频、相机位姿、3D 重建和专业描述的 HouseTour 数据集，并引入联合评估指标来衡量端到端表现。", "keywords": "3D trajectory generation, diffusion model, Gaussian splatting, vision-language model, real estate video, dataset, spatial reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"]}, "usage": {"completion_tokens": 538, "prompt_tokens": 3390, "total_tokens": 3928, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 257, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0004116, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001695, "upstream_inference_completions_cost": 0.0002421}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:47.824912", "feed": "arxiv_cscv", "title": "TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation", "link": "https://papers.cool/arxiv/2510.18038", "analysis": {"summary": "The paper introduces TriggerNet, an explainable AI framework that combines Grad‑CAM, RISE, FullGrad and TCAV to produce visual explanations for deep‑learning plant classification and disease detection models. It evaluates a range of CNN, EfficientNet, MobileNet, ViT, ResNet‑50, InceptionV3 and classic classifiers on a multi‑species RGB dataset for red‑palm‑mite infestation, and uses Snorkel’s heuristic‑guided labeling to annotate disease categories efficiently.", "summary_cn": "本文提出了 TriggerNet 可解释人工智能框架，将 Grad‑CAM、RISE、FullGrad 和 TCAV 融合生成深度学习植物分类和病害检测模型的可视化解释。针对红棕螨侵害，作者在包含 11 种植物的 RGB 数据集上评估了 CNN、EfficientNet、MobileNet、ViT、ResNet‑50、InceptionV3 等模型以及传统分类器，并使用 Snorkel 的启发式规则进行病害类别的高效标注。", "keywords": "explainable AI, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, Snorkel, multi-model comparison", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Harshini Suresha", "Kavitha SH"]}, "usage": {"completion_tokens": 655, "prompt_tokens": 3494, "total_tokens": 4149, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 418, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00040176, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013976, "upstream_inference_completions_cost": 0.000262}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:47.910111", "feed": "arxiv_cscv", "title": "OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion", "link": "https://papers.cool/arxiv/2510.18253", "analysis": {"summary": "OpenInsGaussian introduces an open-vocabulary instance segmentation framework for 3D Gaussian representations that leverages context-aware feature extraction and attention-driven cross‑view fusion to improve semantic consistency and detail retention across multiple views. Extensive experiments show state‑of‑the‑art performance on benchmark datasets, highlighting its robustness for autonomous driving, robotics, and AR applications.", "summary_cn": "OpenInsGaussian 提出了一种基于 open‑vocabulary 的 3D Gaussian 实例分割框架，通过上下文感知特征提取和注意力驱动的跨视融合来提升多视角语义一致性和细节保留。大量实验表明其在基准数据集上取得了最先进的性能，展示了在自动驾驶、机器人和增强现实等场景中的鲁棒性。", "keywords": "open-vocabulary, instance segmentation, Gaussian splatting, context-aware fusion, attention-driven aggregation, 3D scene understanding", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tianyu Huang", "Runnan Chen", "Dongting Hu", "Fengming Huang", "Mingming Gong", "Tongliang Liu"]}, "usage": {"completion_tokens": 559, "prompt_tokens": 3444, "total_tokens": 4003, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 327, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057546, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029596, "upstream_inference_completions_cost": 0.0002795}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:48.329532", "feed": "arxiv_cscv", "title": "Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach", "link": "https://papers.cool/arxiv/2510.17873", "analysis": {"summary": "The paper audits five popular gender classification datasets, revealing widespread intersectional underrepresentation, and demonstrates that even classifiers trained on the most balanced datasets still exhibit significant bias across gender and race. It introduces BalancedFace, a new dataset that equalizes demographic subgroups across 189 intersections, and shows that training on this data substantially reduces fairness gaps while maintaining overall accuracy. The work highlights the effectiveness of data-centric interventions for fair gender classification.", "summary_cn": "本文审计了五个常用的性别分类数据集，发现它们普遍存在交叉维度的代表性不足，即使在最均衡的数据集上训练的分类器仍显示出显著的性别和种族偏差。文章推出了 BalancedFace 数据集，针对年龄、种族和性别的 189 种交叉子群实现了均衡，并证明在该数据上训练的模型在降低公平性差距方面表现出色，同时整体准确率几乎不受影响。此研究强调了以数据为中心的干预在实现公平性方面的巨大价值。", "keywords": "gender classification, bias mitigation, data-centric, BalancedFace, FairFace, intersectional fairness, demographic imbalance, MobileNetV2", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "alignment"}, "authors": ["Tadesse K Bahiru", "Natnael Tilahun Sinshaw", "Teshager Hailemariam Moges", "Dheeraj Kumar Singh"]}, "usage": {"completion_tokens": 837, "prompt_tokens": 4064, "total_tokens": 4901, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 485, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00041245, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002032, "upstream_inference_completions_cost": 0.00020925}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:48.441569", "feed": "arxiv_cscv", "title": "ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues", "link": "https://papers.cool/arxiv/2510.18016", "analysis": {"summary": "ViBED-Net is a video‑based engagement detection network that uses a dual‑stream architecture to process facial crops and full‑scene video frames with EfficientNetV2 for spatial features, then models temporal dynamics with LSTM or Transformer encoders, achieving 73.43% accuracy on the DAiSEE dataset and outperforming prior methods.", "summary_cn": "本文提出ViBED-Net，一个基于视频的学生参与度检测网络，采用双流架构同时捕获面部表情和全场景信息，通过EfficientNetV2提取空间特征，再利用LSTM或Transformer进行时序建模，在DAiSEE数据集上实现73.43%的准确率，显示面部和场景线索的结合显著提升了参与度检测效果。", "keywords": "engagement detection, affective computing, video analysis, dual‑stream network, EfficientNetV2, LSTM, Transformer, spatiotemporal cues, DAiSEE dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Prateek Gothwal", "Deeptimaan Banerjee", "Ashis Kumer Biswas"]}, "usage": {"completion_tokens": 686, "prompt_tokens": 3462, "total_tokens": 4148, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 525, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00041288, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013848, "upstream_inference_completions_cost": 0.0002744}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:49.107214", "feed": "arxiv_cscv", "title": "DeepSeek-OCR: Contexts Optical Compression", "link": "https://papers.cool/arxiv/2510.18234", "analysis": {"summary": "The paper introduces DeepSeek-OCR, a system that compresses long textual contexts into a compact 2D optical representation using a DeepEncoder and a large MoE decoder (DeepSeek3B-MoE-A570M). Experiments show that with compression ratios below 10x the OCR accuracy remains around 97%, while even at 20x the accuracy stays near 60%, demonstrating promise for applications like historical document digitization and memory forgetting mechanisms in LLMs. The method outperforms prior OCR systems on benchmarks while using far fewer vision tokens, and can generate large-scale training data efficiently.", "summary_cn": "本文提出 DeepSeek-OCR 系统，通过 DeepEncoder 将长文本上下文压缩为二维光学映射，并使用 DeepSeek3B-MoE-A570M（MoE 解码器）进行 OCR 解码。实验表明，在压缩比例低 10 倍时，OCR 精度约为 97%，即使在 20 倍压缩下仍保持约 60% 的准确率，展示了其在历史长文本压缩和 LLM 的记忆遗忘机制等研究方向的潜力。该方法在 OmniDocBench 基准上使用更少的视觉 token 超越了现有 OCR 系统，并能以单卡 A100‑40G 的速度每日生成超过 20 万页的训练数据。", "keywords": "optical compression, OCR, vision tokens, MoE decoder, long-context compression, DeepEncoder", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoran Wei", "Yaofeng Sun", "Yukun Li"]}, "usage": {"completion_tokens": 598, "prompt_tokens": 3486, "total_tokens": 4084, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 232, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060126, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030226, "upstream_inference_completions_cost": 0.000299}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:49.386484", "feed": "arxiv_cscv", "title": "VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis", "link": "https://papers.cool/arxiv/2510.18187", "analysis": {"summary": "VelocityNet proposes a dual‑pipeline system that combines head detection with dense optical‑flow analysis to compute individual pedestrian velocities in crowded scenes, then clusters these velocities into semantic motion categories and applies a percentile‑based scoring to flag deviations as anomalies. Experiments show it can operate in real time and detect varied abnormal motion patterns despite occlusions and density changes.", "summary_cn": "VelocityNet 提出一种双管道方案，将人头检测与密集光流相结合，提取每个人的运动速度，再通过层次聚类将速度划分为停顿、缓慢、正常和快速等语义类别，并使用基于百分位的异常评分衡量与学习的正常模式的偏差。实验表明该方法在高密度拥挤环境中能够实时检测多样的异常运动。", "keywords": "crowd anomaly detection, person-specific velocity, optical flow, hierarchical clustering, real-time monitoring", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Fatima AlGhamdi", "Omar Alharbi", "Abdullah Aldwyish", "Raied Aljadaany", "Muhammad Kamran J Khan", "Huda Alamri"]}, "usage": {"completion_tokens": 238, "prompt_tokens": 3321, "total_tokens": 3559, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00027315, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00016605, "upstream_inference_completions_cost": 0.0001071}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:49.581975", "feed": "arxiv_cscv", "title": "Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches", "link": "https://papers.cool/arxiv/2510.18668", "analysis": {"summary": "The paper proposes a tiny multi‑modal convolutional neural network that early‑fuses synchronized ECG and PCG recordings to perform binary cardiovascular classification on resource‑constrained edge devices. Trained on the PhysioNet 2016 challenge data, the model achieves competitive accuracy while reducing memory and compute requirements by three orders of magnitude, and its energy consumption is demonstrated on a microcontroller‑based sensor patch.", "summary_cn": "本文提出了一种用于心血管二分类的轻量级多模态卷积神经网络，通过对同步的 ECG 与 PCG 信号进行早期融合，在资源受限的医学边缘设备上运行。模型在 PhysioNet 2016 挑战数据上训练，保持竞争精度的同时将内存和计算成本降低了三个数量级，并在微控制器传感器贴片上验证了其能源消耗优势。", "keywords": "multi-modal, tiny CNN, ECG, PCG, edge devices, medical monitoring, early fusion, binary classification, energy-efficient inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mustafa Fuad Rifet Ibrahim", "Tunc Alkanat", "Maurice Meijer", "Felix Manthey", "Alexander Schlaefer", "Peer Stelldinger"]}, "usage": {"completion_tokens": 587, "prompt_tokens": 3434, "total_tokens": 4021, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 393, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00037216, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013736, "upstream_inference_completions_cost": 0.0002348}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:50.033011", "feed": "arxiv_cscv", "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent", "link": "https://papers.cool/arxiv/2510.18596", "analysis": {"summary": "CUARewardBench introduces the first comprehensive benchmark for assessing reward models on computer‑using agents (CUAs), covering both outcome reward models (ORM) and process reward models (PRM) across trajectory‑level and step‑level evaluation. The benchmark provides a diverse, expertly annotated dataset spanning ten software categories and seven agent architectures, and presents extensive analyses that reveal current VLM‑based reward models’ limitations in visual reasoning and knowledge. It also proposes the Unanimous Prompt Ensemble (UPE), which improves reward model reliability through strict unanimous voting and strategic prompt‑template configurations, achieving substantially higher precision and NPV than single models or traditional ensembles.", "summary_cn": "CUARewardBench 提出了首个用于评估计算机使用代理（CUA）奖励模型的全面基准，涵盖结果奖励模型（ORM）和过程奖励模型（PRM），并对轨迹级和步骤级进行评估。该基准提供了覆盖十类软件、七种代理架构的多样化、专家标注数据集，并通过大量实验揭示当前基于视觉语言模型的奖励模型在视觉推理和知识方面的局限。文中进一步提出一致性提示集成（UPE）方法，通过严格的一致投票和提示模板配置显著提升奖励模型的可靠性，精度和负预测值均大幅超越单模型和传统集成。", "keywords": "computer-using agents, reward models, benchmark, trajectory evaluation, vision-language models, ensemble, outcome reward, process reward, alignment, evaluation metrics", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Haojia Lin", "Xiaoyu Tan", "Yulei Qin", "Zihan Xu", "Yuchen Shi", "Zongyi Li", "Gang Li", "Shaofei Cai", "Siqi Cai", "Chaoyou Fu", "Ke Li", "Xing Sun"]}, "usage": {"completion_tokens": 685, "prompt_tokens": 3545, "total_tokens": 4230, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 402, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0004158, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001418, "upstream_inference_completions_cost": 0.000274}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:50.086187", "feed": "arxiv_cscv", "title": "RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology", "link": "https://papers.cool/arxiv/2510.18188", "analysis": {"summary": "The paper introduces RadDiagSeg-D, a new dataset that unifies abnormality detection, diagnosis, and multi-target segmentation for radiology, and presents RadDiagSeg-M, a vision‑language model that can jointly generate diagnostic text and corresponding segmentation masks. Experiments demonstrate that RadDiagSeg-M achieves strong performance across all components of this combined task, providing a competitive baseline for future research.", "summary_cn": "本文推出了 RadDiagSeg-D 数据集，将异常检测、诊断和多目标分割统一为放射学中的层次任务，并提出了能够同步生成诊断文本和对应分割掩码的视觉语言模型 RadDiagSeg-M。实验表明，该模型在文本与掩码联合生成的各子任务上表现优异，为后续研究提供了稳健的基准。", "keywords": "vision-language model, radiology, joint diagnosis, multi-target segmentation, medical imaging, dataset, abnormality detection, text-and-mask generation, RadDiagSeg-M", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chengrun Li", "Corentin Royer", "Haozhe Luo", "Bastian Wittmann", "Xia Li", "Ibrahim Hamamci", "Sezgin Er", "Anjany Sekuboyina", "Bjoern Menze"]}, "usage": {"completion_tokens": 554, "prompt_tokens": 3403, "total_tokens": 3957, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 310, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056681, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028981, "upstream_inference_completions_cost": 0.000277}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:50.095705", "feed": "arxiv_cscv", "title": "The Impact of Image Resolution on Biomedical Multimodal Large Language Models", "link": "https://papers.cool/arxiv/2510.18304", "analysis": {"summary": "The paper studies how image resolution influences the performance of biomedical multimodal large language models (MLLMs). It shows that training and inference on native-resolution images markedly improve task performance, while mismatched resolutions cause severe degradation, and proposes mixed-resolution training as a compromise between computational cost and accuracy.", "summary_cn": "本文研究了图像分辨率对生物医学多模态大语言模型（MLLM）性能的影响。研究表明，在原始分辨率上进行训练和推理能够显著提升任务表现，而训练‑推理分辨率不匹配会导致性能大幅下降，并提出混合分辨率训练以在计算资源和性能之间取得平衡。", "keywords": "biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, robustness, performance scaling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"]}, "usage": {"completion_tokens": 638, "prompt_tokens": 3964, "total_tokens": 4602, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 368, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003577, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001982, "upstream_inference_completions_cost": 0.0001595}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:50.386195", "feed": "arxiv_cscv", "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation", "link": "https://papers.cool/arxiv/2510.17864", "analysis": {"summary": "InsideOut extends 3D Gaussian splatting to jointly model high-fidelity RGB surface appearance and subsurface X-ray structures, enabling comprehensive 3D object representation useful for medical diagnostics, cultural heritage restoration, and manufacturing. The paper introduces a new paired RGB‑X‑ray dataset, a hierarchical fitting pipeline that aligns RGB and X-ray radiative Gaussian splats, and an X-ray reference loss that preserves internal consistency. Experiments demonstrate that the method bridges modality gaps and improves visualization, simulation, and non‑destructive testing capabilities.", "summary_cn": "InsideOut 将三维高斯点云扩展至同时建模高分辨率 RGB 表面细节和内部 X 光结构，从而实现对三维对象的完整表征，适用于医学诊断、文化遗产修复和制造业等领域。论文提供了新的 RGB 与 X 光配对数据集，提出了层次化拟合流程以对齐两种辐射高斯点，并设计了 X 光参考损失以保证内部结构的一致性。实验表明该方法成功弥合了两种模态的差异，提升了可视化、仿真和无损检测能力。", "keywords": "3D Gaussian splatting, RGB-X-ray fusion, radiative imaging, multimodal representation, subsurface visualization, medical imaging, cultural heritage, non-destructive testing", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jungmin Lee", "Seonghyuk Hong", "Juyong Lee", "Jaeyoon Lee", "Jongwon Choi"]}, "usage": {"completion_tokens": 653, "prompt_tokens": 3960, "total_tokens": 4613, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 272, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00036125, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.000198, "upstream_inference_completions_cost": 0.00016325}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:50.457467", "feed": "arxiv_cscv", "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection", "link": "https://papers.cool/arxiv/2510.18034", "analysis": {"summary": "The paper presents SAVANT, a structured reasoning framework that leverages vision-language models to detect semantic anomalies in autonomous driving scenes through a two-phase pipeline of scene description extraction and multi-modal evaluation across four semantic layers. SAVANT achieves up to 93.8% accuracy and 90.8% recall using a fine-tuned open-source 7B model, outperforming baseline approaches and enabling cost-effective local deployment. The framework also automatically labels thousands of real-world images, addressing data scarcity for anomaly detection.", "summary_cn": "本文提出了 SAVANT 框架，通过分层场景描述提取和多模态评估，将视觉语言模型的推理转化为结构化分析，以检测自动驾驶中的语义异常。该方法在四个语义层面（街道、基础设施、可移动物体、环境）上实现了最高 93.8% 的准确率和 90.8% 的召回率，显著优于无结构基线，并支持本地部署的开源 7B 模型。SAVANT 还能自动标注数千张真实图像，缓解异常检测的数据稀缺问题。", "keywords": "anomaly detection, autonomous driving, vision-language model, structured reasoning, semantic layers, open-source model, VLM prompting, out-of-distribution, safety monitoring, multi-modal evaluation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"]}, "usage": {"completion_tokens": 740, "prompt_tokens": 4079, "total_tokens": 4819, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 361, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00038895, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020395, "upstream_inference_completions_cost": 0.000185}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:51.024629", "feed": "arxiv_cscv", "title": "Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset", "link": "https://papers.cool/arxiv/2510.18172", "analysis": {"summary": "The paper presents LunarStereo, a novel photorealistic stereo dataset of the Moon created via ray tracing of high‑resolution topography and reflectance models, covering diverse altitudes, lighting, and viewing angles near the lunar South Pole. Using this dataset, the authors fine‑tune the MASt3R model for lunar 3D surface reconstruction and relative pose estimation, demonstrating significant performance gains over zero‑shot baselines on both synthetic and real lunar data.", "summary_cn": "本文推出 LunarStereo 数据集，通过光线追踪技术基于高分辨率月球地形和反射模型生成逼真的月球立体图像，涵盖不同高度、光照以及南极附近的视角。作者在此数据集上对 MASt3R 模型进行微调，实现月球三维表面重建和相对姿态估计，并在合成和真实月球数据上显著超越零样本基线。", "keywords": "stereo vision, lunar surface reconstruction, photorealistic dataset, MASt3R, pose estimation, deep learning, planetary imagery, cross-scale generalization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Clementine Grethen", "Simone Gasparini", "Geraldine Morin", "Jeremy Lebreton", "Lucas Marti", "Manuel Sanchez-Gestido"]}, "usage": {"completion_tokens": 600, "prompt_tokens": 4049, "total_tokens": 4649, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 266, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00035245, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020245, "upstream_inference_completions_cost": 0.00015}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:52.290135", "feed": "arxiv_cscv", "title": "Big Data, Tiny Targets: An Exploratory Study in Machine Learning-enhanced Detection of Microplastic from Filters", "link": "https://papers.cool/arxiv/2510.18089", "analysis": {"summary": "The paper explores the use of machine learning, specifically YOLO object detection models, to identify and quantify microplastic particles and fibres in scanning electron microscopy (SEM) images of filtered samples. It evaluates model performance, emphasizes the importance of preprocessing, and highlights challenges such as limited labeled data for reliable training.", "summary_cn": "本文研究了利用机器学习（YOLO 目标检测模型）在扫描电子显微镜（SEM）图像中检测并量化过滤样本中的微塑料颗粒和纤维的方法。文中评估了模型效果，强调了预处理的重要性，并指出了如专家标注数据不足等训练挑战。", "keywords": "microplastics, SEM imaging, object detection, YOLO, machine learning, environmental monitoring", "scoring": {"interpretability": 2, "understanding": 3, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Paul-Tiberiu Miclea", "Martin Sboron", "Hardik Vaghasiya", "Hoang Thinh Nguyen", "Meet Gadara", "Thomas Schmid"]}, "usage": {"completion_tokens": 571, "prompt_tokens": 3410, "total_tokens": 3981, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 378, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057636, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029086, "upstream_inference_completions_cost": 0.0002855}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:52.367221", "feed": "arxiv_cscv", "title": "EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation", "link": "https://papers.cool/arxiv/2510.18213", "analysis": {"summary": "The paper proposes EMA-SAM, a lightweight extension of Segment Anything Model 2 that adds a confidence-weighted exponential moving average pointer to maintain a stable latent prototype of papillary thyroid microcarcinoma lesions across ultrasound video frames. This approach improves temporal coherence and segmentation performance on a curated PTMC-RFA dataset and external video benchmarks while adding negligible computational overhead. The method demonstrates real-time applicability for interventional ultrasound with consistent Dice score gains over the baseline SAM-2.", "summary_cn": "本文提出 EMA‑SAM，这是一种在 Segment Anything Model 2 基础上加入置信度加权指数移动平均指针的轻量化扩展，用于在超声视频帧间保持甲状腺微癌病灶的稳定潜在原型，实现时间连贯性。该方法在自建 PTMC‑RFA 数据集及外部视频基准上提升了分割性能，且计算开销极小，保持约 30 FPS 的实时推理。", "keywords": "exponential moving average, SAM-2, ultrasound segmentation, PTMC, temporal coherence, medical video segmentation, real-time inference", "scoring": {"interpretability": 3, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Maryam Dialameh", "Hossein Rajabzadeh", "Jung Suk Sim", "Hyock Ju Kwon"]}, "usage": {"completion_tokens": 650, "prompt_tokens": 3533, "total_tokens": 4183, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 391, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063431, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030931, "upstream_inference_completions_cost": 0.000325}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:52.428911", "feed": "arxiv_cscv", "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications", "link": "https://papers.cool/arxiv/2510.18101", "analysis": {"summary": "This tutorial provides a concise overview of the 3D Gaussian Splatting (3DGS) pipeline, describing its splatting formulation and addressing limitations such as high memory usage, baked lighting, and limited secondary-ray support. It also surveys applications of 3DGS in surface reconstruction, avatar modeling, animation, and content generation, highlighting its real-time rendering capabilities and suitability for feed‑forward pipelines.", "summary_cn": "本文综述了 3D 高斯点绘（3DGS）管线，包括其体素渲染公式、内存占用大、光照直接烘焙以及次级光线支持有限等局限，并探讨了针对这些问题的改进。随后展示了 3DGS 在表面重建、化身建模、动画和内容生成等应用中的高效实时渲染优势，强调其在前向渲染流水线中的适用性。", "keywords": "3D Gaussian Splatting, volume rendering, novel view synthesis, real-time rendering, surface reconstruction, avatar modeling, animation, content generation, memory efficiency, graphics pipeline", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vitor Pereira Matias", "Daniel Perazzo", "Vinicius Silva", "Alberto Raposo", "Luiz Velho", "Afonso Paiva", "Tiago Novello"]}, "usage": {"completion_tokens": 606, "prompt_tokens": 3375, "total_tokens": 3981, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 314, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058861, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028561, "upstream_inference_completions_cost": 0.000303}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:52.615063", "feed": "arxiv_cscv", "title": "Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net", "link": "https://papers.cool/arxiv/2510.18326", "analysis": {"summary": "The paper proposes the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net) for few-shot image classification, especially targeting benchmark and disaster datasets. By linearly combining the Bhattacharyya coefficient and Hellinger distance, the method aggregates feature probability distributions to build robust class prototypes and introduces a distribution-based contrastive loss alongside cross-entropy. Experiments on four few-shot benchmarks and two disaster image collections show that ATTBHFA-Net outperforms existing approaches.", "summary_cn": "本文提出基于注意力的 Bhattacharyya-Hellinger 特征聚合网络 (ATTBHFA-Net) 用于少样本图像分类，特别针对基准数据集和灾害图像。该方法通过线性结合 Bhattacharyya 系数和 Hellinger 距离来比较并聚合特征概率分布，以构建稳健的原型，并引入基于分布的对比损失与交叉熵共同训练。实验在四个少样本基准和两个灾害图像数据集上验证了其相较于现有方法的显著提升。", "keywords": "few-shot learning, disaster imagery, Bhattacharyya coefficient, Hellinger distance, feature aggregation, attention network, contrastive loss, computer vision, prototype formation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-aplicable", "primary_focus": "other"}, "authors": ["Gao Yu Lee", "Tanmoy Dam", "Md Meftahul Ferdaus", "Daniel Puiu Poenar", "Vu Duong"]}, "usage": {"completion_tokens": 1167, "prompt_tokens": 4128, "total_tokens": 5295, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 794, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00049815, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002064, "upstream_inference_completions_cost": 0.00029175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:52.986174", "feed": "arxiv_cscv", "title": "DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices", "link": "https://papers.cool/arxiv/2510.18513", "analysis": {"summary": "The paper introduces DWaste, a computer‑vision platform for real‑time waste sorting on resource‑constrained smartphones and edge devices, emphasizing low latency, small model size, and reduced carbon emissions. It benchmarks several image classification and object detection models, showing trade‑offs between accuracy and resource consumption, and demonstrates that quantization can further improve efficiency.", "summary_cn": "本文提出 DWaste 系统，在资源受限的手机和边缘设备上实现实时垃圾分类，重点降低延迟、模型体积和碳排放。通过对多种分类和检测模型的基准测试，展示了准确率与资源消耗之间的权衡，并说明量化技术可显著提升效率。", "keywords": "waste sorting, computer vision, edge devices, model quantization, EfficientNetV2, YOLO, green AI, low-power inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Suman Kunwar"]}, "usage": {"completion_tokens": 593, "prompt_tokens": 4070, "total_tokens": 4663, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 317, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00035175, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002035, "upstream_inference_completions_cost": 0.00014825}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:53.700095", "feed": "arxiv_cscv", "title": "UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding", "link": "https://papers.cool/arxiv/2510.18262", "analysis": {"summary": "The paper introduces UWBench, a large-scale vision-language benchmark specifically designed for underwater understanding, containing over 15,000 high‑resolution images with detailed object referring expressions and 125,000 question‑answer pairs covering ecological reasoning. It defines three evaluation tasks—image captioning, visual grounding, and visual question answering—and shows that current state‑of‑the‑art VLMs struggle with the unique challenges of underwater imagery. The benchmark aims to advance research in marine science applications and autonomous underwater exploration.", "summary_cn": "本文提出了 UWBench，一个面向水下场景的规模化视觉‑语言基准，收录 15,000 多张高分辨率水下图像，配有人类验证的 15,281 条目标指涉描述和 124,983 条问答对，涵盖生态关系推理等多种能力。该基准设定了图像描述、视觉定位和视觉问答三项任务，并展示了现有最先进 VLM 在光照衰减、颜色失真和浊度等水下独特挑战下的性能不足。该资源旨在推动海洋科学、生态监测和水下自主探索等领域的视觉‑语言研究。", "keywords": "underwater vision-language, benchmark, VLM, marine ecology, visual grounding, image captioning, visual question answering, dataset, underwater imaging, multimodal reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Da Zhang", "Chenggang Rong", "Bingyu Li", "Feiyu Wang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"]}, "usage": {"completion_tokens": 828, "prompt_tokens": 3473, "total_tokens": 4301, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 511, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071431, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030031, "upstream_inference_completions_cost": 0.000414}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:53.906523", "feed": "arxiv_cscv", "title": "3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement", "link": "https://papers.cool/arxiv/2510.17875", "analysis": {"summary": "The paper proposes a 3D weakly supervised semantic segmentation approach that refines pseudo‑labels using a class‑aware module followed by a geometry‑aware module and a self‑training label‑update strategy. The method improves pseudo‑label quality and coverage, achieving state‑of‑the‑art results on ScanNet and S3DIS benchmarks. Experiments also demonstrate strong generalization in unsupervised settings.", "summary_cn": "本文提出了一种 3D 弱监督语义分割方法，通过类感知标签细化模块和几何感知标签细化模块以及自训练标签更新策略来优化伪标签。该方法提升了伪标签的质量和覆盖范围，在 ScanNet 和 S3DIS 基准上实现了最新水平的性能，并在无监督设置下表现出良好的泛化能力。", "keywords": "weakly supervised semantic segmentation, 3D geometry, pseudo-label refinement, class-aware guidance, self-training, ScanNet, S3DIS", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaoxu Xu", "Xuexun Liu", "Jinlong Li", "Yitian Yuan", "Qiudan Zhang", "Lin Ma", "Nicu Sebe", "Xu Wang"]}, "usage": {"completion_tokens": 708, "prompt_tokens": 3521, "total_tokens": 4229, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 492, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00049465, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00017605, "upstream_inference_completions_cost": 0.0003186}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:54.709937", "feed": "arxiv_cscv", "title": "BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining", "link": "https://papers.cool/arxiv/2510.18244", "analysis": {"summary": "BlendCLIP proposes a multimodal pretraining framework that combines synthetic CAD data with real-world LiDAR scans via a curriculum-based data mixing strategy to bridge the synthetic-to-real domain gap for zero-shot 3D object classification. By mining object-level triplets (point cloud, image, text) from real driving data and progressively adapting the model, the method achieves large accuracy gains on benchmarks such as nuScenes with only a small fraction of real samples. The approach demonstrates that targeted domain adaptation can replace extensive real-world annotation for robust open-vocabulary 3D perception.", "summary_cn": "BlendCLIP 提出了一种多模态预训练框架，通过课程式数据混合策略，将合成 CAD 数据与真实 LiDAR 扫描相结合，以缩小合成到真实的域差距，实现零样本 3D 物体分类。该方法从真实驾驶数据中挖掘点云、图像和文本三元组，并逐步适应模型，在仅使用极少真实样本的情况下显著提升了 nuScenes 等基准的准确率。研究表明，有针对性的域适应能够取代大规模真实标注，实现鲁棒的开放词汇 3D 感知。", "keywords": "zero-shot 3D classification, multimodal pretraining, domain adaptation, synthetic-to-real, curriculum mixing, LiDAR, CLIP, open-vocabulary perception", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ajinkya Khoche", "Gergő László Nagy", "Maciej Wozniak", "Thomas Gustafsson", "Patric Jensfelt"]}, "usage": {"completion_tokens": 742, "prompt_tokens": 3534, "total_tokens": 4276, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 423, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068046, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030946, "upstream_inference_completions_cost": 0.000371}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:54.930984", "feed": "arxiv_cscv", "title": "VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety", "link": "https://papers.cool/arxiv/2510.18214", "analysis": {"summary": "The paper introduces Vision Language Safety Understanding (VU), a framework that evaluates multimodal foundation models through fine‑grained severity categories and combinatorial analysis of 17 safety patterns, constructing a benchmark of 8,187 image‑text samples across 15 harm types. Experiments on eleven state‑of‑the‑art models reveal severe joint image‑text reasoning failures: while unimodal safety signals are classified with >90% accuracy, joint understanding drops to 20‑55%, and a large share of errors occur despite correct unimodal predictions. The study also quantifies the trade‑off between over‑blocking borderline content and under‑refusing unsafe content, highlighting critical alignment gaps in current models.", "summary_cn": "本文提出了“视觉语言安全理解（VLSU）”框架，通过细粒度的严重程度分类和 17 种安全模式的组合分析，对多模态基础模型进行系统评估，并构建了包含 8,187 条跨 15 类危害的图文样本基准。对十一种先进模型的实验表明，虽然单模态安全信号的准确率超过 90%，但在需要联合图文推理时准确率骤降至 20%‑55%，且约 34% 的错误发生在单模态已正确分类的情况下，显示出模型缺乏组合推理能力。研究还揭示了在降低对边缘内容的过度拦截与减少对危险内容的低拒绝率之间的权衡，突显当前模型在对齐方面的显著缺口。", "keywords": "multimodal safety, vision-language models, compositional reasoning, safety benchmark, alignment, joint understanding, severity classification, over-blocking, under-refusal", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}, "usage": {"completion_tokens": 779, "prompt_tokens": 3525, "total_tokens": 4304, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 421, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069761, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030811, "upstream_inference_completions_cost": 0.0003895}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:55.347855", "feed": "arxiv_cscv", "title": "Chimera: Compositional Image Generation using Part-based Concepting", "link": "https://papers.cool/arxiv/2510.18083", "analysis": {"summary": "The paper introduces Chimera, a personalized image generation model that can compose novel objects by combining specified parts from multiple source images based on textual instructions, without requiring explicit masks or annotations. It builds a dataset of semantic atoms (part, subject pairs), trains a diffusion prior with part-conditional guidance, and proposes the PartEval metric to evaluate part fidelity and compositional accuracy, showing significant improvements over baselines.", "summary_cn": "本文提出 Chimera，一种个性化图像生成模型，能够根据文本指令将多个源图像的指定部件组合成新对象，无需用户提供掩码或标注。作者构建了语义原子（部件、主题）数据集，训练了带部件条件引导的扩散先验模型，并推出 PartEval 指标评估部件保真度和组合准确性，实验表明相较基线在部件对齐和视觉质量上均有显著提升。", "keywords": "compositional image generation, diffusion prior, part-conditional guidance, semantic atoms, PartEval, personalized generative models", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shivam Singh", "Yiming Chen", "Agneet Chatterjee", "Amit Raj", "James Hays", "Yezhou Yang", "Chitra Baral"]}, "usage": {"completion_tokens": 534, "prompt_tokens": 4021, "total_tokens": 4555, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 211, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00033455, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020105, "upstream_inference_completions_cost": 0.0001335}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:55.476709", "feed": "arxiv_cscv", "title": "MAT-Agent: Adaptive Multi-Agent Training Optimization", "link": "https://papers.cool/arxiv/2510.17845", "analysis": {"summary": "MAT-Agent introduces a collaborative multi-agent framework that dynamically adjusts data augmentation, optimizers, learning rates, and loss functions during multi-label image classification training, using non‑stationary multi‑armed bandit algorithms to balance exploration and exploitation. The system is evaluated on Pascal VOC, COCO, and VG‑256, achieving higher mAP and F1 scores than several baselines while showing faster convergence and better cross‑domain generalization.", "summary_cn": "MAT-Agent 提出一种协作式多智能体框架，在多标签图像分类训练过程中动态调节数据增强、优化器、学习率和损失函数，采用非平稳多臂赌博机算法实现探索与利用的平衡。实验证明该方法在 Pascal VOC、COCO 和 VG‑256 数据集上相较于多种基线实现了更高的 mAP 与 F1 分数，并加快了收敛速度，提升了跨域泛化能力。", "keywords": "multi-agent training, multi-label classification, non-stationary multi-armed bandit, adaptive optimization, data augmentation, mAP", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Ningyuan Liu", "Keze Wang"]}, "usage": {"completion_tokens": 521, "prompt_tokens": 3484, "total_tokens": 4005, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 249, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056246, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030196, "upstream_inference_completions_cost": 0.0002605}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:55.601918", "feed": "arxiv_cscv", "title": "Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery", "link": "https://papers.cool/arxiv/2510.18256", "analysis": {"summary": "The paper introduces a hyperbolic space learning framework that leverages temporal motion priors to improve video‑based 3D human mesh recovery. By extracting motion features from pose and image sequences and optimizing mesh representations in hyperbolic space with a dedicated loss, the method captures the hierarchical structure of the human body and yields smoother, more accurate meshes compared to Euclidean baselines.", "summary_cn": "本文提出一种利用时间运动先验的双曲空间学习框架，以提升基于视频的 3D 人体网格恢复。该方法从姿态序列和图像特征序列中提取运动特征，在双曲空间中对网格进行优化，并引入双曲网格优化损失，从而更好地捕捉人体的层次结构，实现更平滑、更加精确的网格重建。", "keywords": "hyperbolic learning, temporal motion prior, 3D human mesh recovery, video-based reconstruction, hierarchical representation, non-Euclidean space, mesh optimization loss, pose estimation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Weibin Qiu", "Zhaocheng Jin", "Sheng Yang"]}, "usage": {"completion_tokens": 822, "prompt_tokens": 3478, "total_tokens": 4300, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 654, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071206, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030106, "upstream_inference_completions_cost": 0.000411}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:55.776354", "feed": "arxiv_cscv", "title": "GAN-based Content-Conditioned Generation of Handwritten Musical Symbols", "link": "https://papers.cool/arxiv/2510.17869", "analysis": {"summary": "The paper presents a symbol-level Generative Adversarial Network that creates realistic handwritten musical symbols, which are assembled into full scores using the Smashcima engraving software; visual fidelity evaluations show the generated symbols are highly realistic, addressing data scarcity in Optical Music Recognition.", "summary_cn": "该论文提出基于 GAN 的符号级手写音乐符号生成方法，利用 Smashcima 将生成的符号组装成完整乐谱，并通过视觉保真度评估证明合成符号高度逼真，可缓解光学乐谱识别中的标注数据稀缺问题。", "keywords": "GAN, synthetic data, optical music recognition, handwritten musical symbols, data augmentation, generative adversarial network, OMR", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gerard Asbert", "Pau Torras", "Lei Kang", "Alicia Fornés", "Josep Lladós"]}, "usage": {"completion_tokens": 655, "prompt_tokens": 3325, "total_tokens": 3980, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 509, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060561, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027811, "upstream_inference_completions_cost": 0.0003275}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:56.123249", "feed": "arxiv_cscv", "title": "CMIS-Net: A Cascaded Multi-Scale Individual Standardization Network for Backchannel Agreement Estimation", "link": "https://papers.cool/arxiv/2510.17855", "analysis": {"summary": "The paper introduces CMIS-Net, a cascaded multi‑scale individual standardization network that normalizes backchannel cues by removing person‑specific neutral baselines at both frame and sequence levels, enabling more accurate agreement detection. An implicit data augmentation module is also proposed to mitigate training data bias and improve generalization. Experiments show state‑of‑the‑art performance on backchannel agreement estimation tasks.", "summary_cn": "本文提出了 CMIS-Net，一种级联多尺度个体标准化网络，通过在帧层和序列层去除个人的中性基线，对回声信号进行标准化，从而更准确地检测同意行为。同时引入了隐式数据增强模块，以缓解训练数据分布偏差并提升模型的泛化能力。实验结果表明该方法在回声同意估计任务上达到了最新的性能水平。", "keywords": "backchannel, agreement detection, multi-scale modeling, individual normalization, cascaded network, data augmentation, conversational AI, speech emotion recognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuxuan Huang", "Kangzhong Wang", "Eugene Yujun Fu", "Grace Ngai", "Peter H. F. Ng"]}, "usage": {"completion_tokens": 623, "prompt_tokens": 3472, "total_tokens": 4095, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 398, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061166, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030016, "upstream_inference_completions_cost": 0.0003115}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:56.438203", "feed": "arxiv_cscv", "title": "World-in-World: World Models in a Closed-Loop World", "link": "https://papers.cool/arxiv/2510.18135", "analysis": {"summary": "The paper introduces World-in-World, a closed-loop benchmarking platform that evaluates generative world models based on their utility for embodied decision‑making rather than visual fidelity alone. It presents four environments, a unified online planning interface, and the first scaling law for world models in embodied settings, uncovering that controllability, post‑training data scaling, and inference‑time compute are more critical than visual quality for task success.", "summary_cn": "本文推出了 World-in-World 平台，在闭环环境中评估生成式世界模型对具身智能体决策的实用性，而不仅仅是视觉质量。通过四个环境、统一的在线规划接口以及首个针对具身设置的规模律研究，发现可控性、后训练数据扩展和推理计算资源比视觉逼真度更能提升任务成功率。", "keywords": "world models, closed-loop benchmark, embodied agents, planning, scaling laws, generative models, task success, inference compute", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Jiahan Zhang", "Muqing Jiang", "Nanru Dai", "Taiming Lu", "Arda Uzunoglu", "Shunchi Zhang", "Yana Wei", "Jiahao Wang", "Vishal M. Patel", "Paul Pu Liang", "Daniel Khashabi", "Cheng Peng", "Rama Chellappa", "Tianmin Shu", "Alan Yuille", "Yilun Du", "Jieneng Chen"]}, "usage": {"completion_tokens": 734, "prompt_tokens": 3443, "total_tokens": 4177, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 550, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066281, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029581, "upstream_inference_completions_cost": 0.000367}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:56.840990", "feed": "arxiv_cscv", "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "link": "https://papers.cool/arxiv/2510.18866", "analysis": {"summary": "The paper proposes LightMem, a lightweight memory-augmented generation system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson‑Shiffrin model. LightMem filters irrelevant data, groups content by topic, and performs offline consolidation, achieving higher accuracy while drastically reducing token usage, API calls, and runtime compared to existing memory systems.", "summary_cn": "本文提出 LightMem，一种面向大语言模型的轻量级记忆增强生成系统，受 Atkinson‑Shiffrin 人类记忆模型启发，将记忆划分为感官记忆、短期记忆和长期记忆三个阶段。感官记忆通过轻量压缩快速过滤无关信息并按主题分组，短期记忆对主题组进行整理与摘要，长期记忆在离线“睡眠”阶段进行整合，从而在提升准确率的同时将 token 使用量降低至原来的 1/117，API 调用降低至 1/159，运行时间提升超过 12 倍。", "keywords": "memory-augmented generation, lightweight memory, LLM, hierarchical memory, efficiency, Atkinson-Shiffrin model, token reduction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}, "usage": {"completion_tokens": 650, "prompt_tokens": 3445, "total_tokens": 4095, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 346, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00046475, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00017225, "upstream_inference_completions_cost": 0.0002925}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:57.545240", "feed": "arxiv_cscv", "title": "Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach", "link": "https://papers.cool/arxiv/2510.17854", "analysis": {"summary": "The paper proposes a framework that uses image embeddings and vector similarity to differentiate AI‑generated images from human‑created ones, based on the hypothesis that AI‑generated images cluster together in embedding space. Experiments across five benchmark embedding models show the method is robust to moderate perturbations and can reliably trace image provenance, with a blockchain component for tamper‑evident recording. The approach aims to provide an efficient, generalizable tool for authenticating digital visual content.", "summary_cn": "本文提出一种利用图像嵌入和向量相似度区分 AI 生成图像与人工创作图像的框架，核心假设是 AI 生成的图像在嵌入空间中会聚成相似簇。通过在五种基准嵌入模型上进行实验，证明该方法对中等程度的扰动具有鲁棒性，并可保持对原图的相似匹配，同时结合区块链实现防篡改的溯源记录。该方案旨在提供一种高效且通用的数字图像真实性验证工具。", "keywords": "AI-generated image detection, embedding similarity, provenance, blockchain, deepfake detection, vector similarity, image embeddings, authenticity verification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Jitendra Sharma", "Arthur Carvalho", "Suman Bhunia"]}, "usage": {"completion_tokens": 654, "prompt_tokens": 3438, "total_tokens": 4092, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 365, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062206, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029506, "upstream_inference_completions_cost": 0.000327}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:57.870149", "feed": "arxiv_cscv", "title": "SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving", "link": "https://papers.cool/arxiv/2510.18123", "analysis": {"summary": "This paper systematically studies safety and security challenges of using natural language as the communication medium in V2X collaborative driving, presenting a taxonomy of attack strategies and proposing the SafeCoop defense pipeline with semantic firewalls and multi‑source consensus. Experiments in CARLA across 32 critical scenarios demonstrate significant improvements in driving performance and attack detection under malicious conditions.", "summary_cn": "本文系统性地研究了在 V2X 协同驾驶中使用自然语言作为通信介质所带来的安全与安全性挑战，提出了攻击策略分类并设计了 SafeCoop 防御流水线，包括语义防火墙和多源共识机制。 在 CARLA 仿真中的 32 种关键场景实验表明，在恶意攻击下该系统显著提升了驾驶性能和攻击检测能力。", "keywords": "natural language communication, V2X, collaborative driving, safety, security, adversarial attacks, semantic firewall, CARLA simulation, agentic defense, multi-source consensus", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}, "usage": {"completion_tokens": 790, "prompt_tokens": 3503, "total_tokens": 4293, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 614, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069981, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030481, "upstream_inference_completions_cost": 0.000395}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:58.008031", "feed": "arxiv_cscv", "title": "Online In-Context Distillation for Low-Resource Vision Language Models", "link": "https://papers.cool/arxiv/2510.18117", "analysis": {"summary": "The paper introduces Online In-Context Distillation (ICD), a method where a small vision-language model learns from a stronger teacher at inference time using sparse cross-modal demonstrations, enabling strong performance with minimal teacher annotations and limited compute. It presents an analysis of when vision-language in-context learning is feasible, proposes a novel demonstration selection strategy, test-time teacher scaling, and student uncertainty conditioning to reduce queries. Experiments show up to 33% performance gains for small models with as few as 4% of teacher data, approaching the teacher's zero-shot ability.", "summary_cn": "本文提出了在线上下文蒸馏（Online In-Context Distillation，ICD）方法，让小型视觉语言模型在推理时通过稀疏的跨模态示例向更强的教师模型学习，从而在计算和注释资源极其有限的情况下实现显著性能提升。文章先分析了视觉语言模型何时能够进行上下文学习，并提出了跨模态示例选择、教师测试时尺度缩放以及学生不确定性条件化等新技术，以减少教师查询次数。实验表明，在仅使用约 4% 教师标注的情况下，小模型性能提升最高可达 33%，并可逼近教师的 zero-shot 表现。", "keywords": "online in-context distillation, vision-language models, low-resource adaptation, teacher-student distillation, cross-modal demonstration selection, test-time scaling, uncertainty conditioning", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zhiqi Kang", "Rahaf Aljundi", "Vaggelis Dorovatas", "Karteek Alahari"]}, "usage": {"completion_tokens": 835, "prompt_tokens": 3458, "total_tokens": 4293, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 522, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071556, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029806, "upstream_inference_completions_cost": 0.0004175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:58.478850", "feed": "arxiv_cscv", "title": "CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization", "link": "https://papers.cool/arxiv/2510.17847", "analysis": {"summary": "The paper proposes CoIDO, a dual-objective framework that jointly optimizes data importance and diversity for visual instruction tuning of multimodal large language models. By training a lightweight plug-in scorer on a small random sample and using a homoscedastic uncertainty formulation, CoIDO efficiently selects a 20% subset of data that achieves 98.2% of full-data fine-tuning performance on LLaVA-1.5-7B across ten downstream tasks.", "summary_cn": "本文提出 CoIDO 框架，通过联合优化数据重要性与多样性，实现视觉指令微调数据的高效挑选。利用在少量随机样本上训练的轻量插件评分器，并基于同方差不确定性公式平衡两者，CoIDO 在整个数据集上选取 20% 子集，在 LLaVA-1.5-7B 上的十个下游任务中达到约 98.2% 的全数据微调性能。", "keywords": "data selection, visual instruction tuning, multimodal LLMs, importance-diversity optimization, efficient training, CoIDO, homoscedastic uncertainty", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yichen Yan", "Ming Zhong", "Qi Zhu", "Xiaoling Gu", "Jinpeng Chen", "Huan Li"]}, "usage": {"completion_tokens": 681, "prompt_tokens": 3465, "total_tokens": 4146, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 432, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063961, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029911, "upstream_inference_completions_cost": 0.0003405}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:59.201321", "feed": "arxiv_cscv", "title": "MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation", "link": "https://papers.cool/arxiv/2510.17866", "analysis": {"summary": "MUSE is a training‑free framework for zero‑shot 2D object detection and segmentation that uses 3D‑derived multi‑view templates and uncertainty‑aware similarity matching. It combines class and patch embeddings with GeM pooling, a joint absolute‑relative similarity metric, and an uncertainty‑aware prior to refine scores, achieving state‑of‑the‑art results on the BOP Challenge 2025 without any additional training.", "summary_cn": "MUSE 是一种无需训练的零样本二维目标检测和分割框架，利用来自三维未见对象的多视角模板和不确定性感知相似度匹配。它在嵌入阶段结合类别嵌入和使用 GeM（广义均值池化）归一化的局部贴片嵌入，在匹配阶段采用绝对与相对相似度的联合度量，并通过不确定性感知的对象先验对相似度进行校正，从而在 BOP Challenge 2025 中实现了最先进的表现。", "keywords": "zero-shot detection, uncertainty-aware similarity, multi-view templates, GeM pooling, object segmentation, training-free framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sungmin Cho", "Sungbum Park", "Insoo Oh"]}, "usage": {"completion_tokens": 735, "prompt_tokens": 3413, "total_tokens": 4148, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 512, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065881, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029131, "upstream_inference_completions_cost": 0.0003675}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:59.534707", "feed": "arxiv_cscv", "title": "Robotic Classification of Divers' Swimming States using Visual Pose Keypoints as IMUs", "link": "https://papers.cool/arxiv/2510.17863", "analysis": {"summary": "The paper proposes a hybrid method that converts 3D human joint keypoints extracted from underwater video into a pseudo‑inertial measurement unit (pseudo‑IMU) to classify scuba divers' swimming states. By deploying this classifier on an autonomous underwater vehicle, the system can detect anomalous behaviors indicative of medical emergencies such as cardiac arrest, addressing the challenge of wireless signal loss for traditional wearable sensors. Experiments with simulated distress scenarios demonstrate the approach’s effectiveness for robotic diver‑monitoring and safety enhancement.", "summary_cn": "本文提出一种混合方法，将水下视频中提取的三维人体关节关键点转化为伪惯性测量单元（pseudo‑IMU），用于分类潜水员的泳姿状态。该系统可在自主水下机器人上运行，检测诸如心脏骤停等医学急救的异常行为，克服了传统穿戴式传感器在水中无线信号衰减的问题。通过模拟紧急情境的实验表明，该方法在机器人潜水员监测与安全提升方面具备实用性。", "keywords": "underwater activity recognition, pose estimation, pseudo-IMU, diver safety, anomaly detection, AUV monitoring, visual inertial sensing, robotic surveillance", "scoring": {"interpretability": 3, "understanding": 4, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Demetrious T. Kutzke", "Ying-Kun Wu", "Elizabeth Terveen", "Junaed Sattar"]}, "usage": {"completion_tokens": 765, "prompt_tokens": 3376, "total_tokens": 4141, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 480, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066826, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028576, "upstream_inference_completions_cost": 0.0003825}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:59.633188", "feed": "arxiv_cscv", "title": "Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model", "link": "https://papers.cool/arxiv/2510.17851", "analysis": {"summary": "The paper proposes a latent diffusion model that generates post‑treatment glioblastoma MRI scans from pre‑treatment scans using concatenated conditioning on tumor location and classifier‑free guidance informed by survival data. Trained on a dataset of 140 patients with paired pre‑ and post‑treatment T1‑Gd MRI and expert tumor delineations, the model aims to enable early visual prediction of treatment response. Results demonstrate that the approach can capture tumor evolution trends, offering a potential tool for personalized medicine.", "summary_cn": "本文提出一种潜在扩散模型，利用预处理 MRI 中的肿瘤定位信息以及基于生存数据的 classifier‑free 引导，将术后 MRI 生成于术前 MRI，实现早期视觉化的治疗反应预测。模型在包含 140 名患者的配对术前/术后 T1‑Gd MRI 数据及专家标注的肿瘤分割上进行训练和测试，展示了对肿瘤演变趋势的捕捉能力，为个性化医疗提供潜在工具。", "keywords": "latent diffusion, MRI prediction, glioblastoma, treatment response prediction, disease progression modeling, classifier-free guidance, medical imaging, generative models, survival conditioning", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandre G. Leclercq", "Sébastien Bougleux", "Noémie N. Moreau", "Alexis Desmonts", "Romain Hérault", "Aurélien Corroyer-Dulmont"]}, "usage": {"completion_tokens": 763, "prompt_tokens": 3455, "total_tokens": 4218, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 493, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067911, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029761, "upstream_inference_completions_cost": 0.0003815}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:22:59.847142", "feed": "arxiv_cscv", "title": "DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing", "link": "https://papers.cool/arxiv/2510.18218", "analysis": {"summary": "The paper proposes DualHash, a stochastic primal‑dual algorithm with provable complexity bounds for deep hashing, addressing the discrete quantization challenge by transforming W‑type regularization via Fenchel duality to obtain a closed‑form proximal operator. Two algorithmic variants are derived, achieving \\(O(\\varepsilon^{-4})\\) and \\(O(\\varepsilon^{-3})\\) complexities, and experiments on three image‑retrieval datasets show improved retrieval performance.", "summary_cn": "本文提出 DualHash，一种具有理论复杂度保证的随机原始‑对偶深度哈希算法，通过 Fenchel 对偶将 W‑型正则化转化至对偶空间，从而获得闭式近端算子并解决二值化难题。文中给出加速动量版 (\\(O(\\varepsilon^{-4})\\)) 与方差约减版 (\\(O(\\varepsilon^{-3})\\)) 两种实现，并在三个图像检索数据库上验证了其性能提升。", "keywords": "deep hashing, binary codes, stochastic primal-dual, Fenchel duality, variance reduction, image retrieval", "scoring": {"interpretability": 1, "understanding": 4, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Luxuan Li", "Xiao Wang", "Chunfeng Cui"]}, "usage": {"completion_tokens": 589, "prompt_tokens": 3429, "total_tokens": 4018, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 297, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058821, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029371, "upstream_inference_completions_cost": 0.0002945}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:01.191513", "feed": "arxiv_cscv", "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain", "link": "https://papers.cool/arxiv/2510.17801", "analysis": {"summary": "This work introduces RoboBench, a comprehensive benchmark for evaluating multimodal large language models as embodied brains in robotic manipulation. It defines five evaluation dimensions—instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis—across 14 capabilities, 25 tasks, and thousands of QA pairs, and includes an MLLM-as‑world‑simulator framework to test plan feasibility. Experiments on 14 MLLMs highlight fundamental limitations in implicit instruction understanding, spatiotemporal reasoning, cross‑scenario planning, fine‑grained affordance prediction, and execution failure diagnosis.", "summary_cn": "本研究提出 RoboBench，一个用于评估多模态大语言模型（MLLM）作为具身大脑在机器人操作任务中表现的全面基准。它从指令理解、感知推理、通用规划、可供性预测和失败分析五个维度，覆盖 14 项能力、25 项任务和数千个问答对，并引入 MLLM‑as‑world‑simulator 框架评估计划的可执行性。对 14 种 MLLM 的实验揭示了在隐式指令理解、时空推理、跨场景规划、细粒度可供性以及执行失败诊断方面的基本局限。", "keywords": "multimodal large language model, embodied AI, benchmark, instruction comprehension, perception reasoning, planning, affordance prediction, failure analysis, robotic manipulation, evaluation framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"]}, "usage": {"completion_tokens": 909, "prompt_tokens": 3543, "total_tokens": 4452, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 691, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00050532, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00014172, "upstream_inference_completions_cost": 0.0003636}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:01.520745", "feed": "arxiv_cscv", "title": "Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers", "link": "https://papers.cool/arxiv/2510.18358", "analysis": {"summary": "The paper proposes Hydra Ensembles, an efficient transformer ensemble that creates diverse members by pruning attention heads and combines them using a novel grouped multi-head attention mechanism. This approach achieves uncertainty quantification performance comparable to or better than Deep Ensembles while retaining inference speed close to a single model, and it includes an analysis showing that careful pruning preserves calibration. Experiments on image and text classification, including zero-shot ImageNet‑1k, demonstrate consistent improvements over existing methods without additional training.", "summary_cn": "本文提出了 Hydra Ensembles，一种通过裁剪注意力头部并使用新型分组多头注意力层将其合并的高效 Transformer 集成方法。该方法在保持接近单模型推理速度的同时，提供与 Deep Ensembles 相当或更优的不确定性量化性能，并通过实验表明谨慎的裁剪能够保持模型校准。实验在图像和文本分类任务（包括零样本 ImageNet‑1k）上展示了相对于现有方法的一致提升，无需额外训练。", "keywords": "uncertainty quantification, deep ensembles, transformer pruning, Hydra Ensembles, calibrated inference, efficient ensembles, zero-shot classification", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Firas Gabetni", "Giuseppe Curci", "Andrea Pilzer", "Subhankar Roy", "Elisa Ricci", "Gianni Franchi"]}, "usage": {"completion_tokens": 700, "prompt_tokens": 3378, "total_tokens": 4078, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 445, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00041512, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013512, "upstream_inference_completions_cost": 0.00028}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:02.406538", "feed": "arxiv_cscv", "title": "A Generalizable Light Transport 3D Embedding for Global Illumination", "link": "https://papers.cool/arxiv/2510.18189", "analysis": {"summary": "The paper introduces a generalizable 3D light transport embedding that predicts global illumination directly from point‑cloud scene representations using a scalable transformer. By encoding geometric and material features into neural primitives and retrieving them with cross‑attention at render time, the method can estimate diffuse irradiance across diverse indoor scenes and be fine‑tuned for other tasks such as glossy radiance fields and path guiding. Results show view‑consistent predictions without relying on rasterized or path‑traced cues.", "summary_cn": "本文提出了一种通用的 3D 光传输嵌入（3D light transport embedding），利用可扩展的 Transformer 从点云场景表示直接预测全局光照。它将几何和材质特征编码为神经基元，在渲染时通过交叉注意力检索并聚合这些基元，以估计多种室内场景的漫反射辐照度，并可在有限微调后用于光泽材质的空间‑方向辐射场或路径引导。实验表明该方法在无需光栅化或路径追踪提示的情况下实现了一致的视图预测。", "keywords": "global illumination, neural rendering, 3D light transport embedding, transformer, point cloud, irradiance estimation, path guiding", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bing Xu", "Mukund Varma T", "Cheng Wang", "Tzumao Li", "Lifan Wu", "Bartlomiej Wronski", "Ravi Ramamoorthi", "Marco Salvi"]}, "usage": {"completion_tokens": 736, "prompt_tokens": 3470, "total_tokens": 4206, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 480, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066786, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029986, "upstream_inference_completions_cost": 0.000368}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:02.744673", "feed": "arxiv_cscv", "title": "Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch", "link": "https://papers.cool/arxiv/2510.17858", "analysis": {"summary": "The paper introduces an ultra-efficient post-training technique that shortcuts large pre-trained flow-matching diffusion models into few-step samplers by self-distilling the velocity field, eliminating the need for step-size embeddings and requiring minimal compute. The method can also be integrated during pretraining to produce models that inherently generate efficient flows and enables a few-shot distillation for massive diffusion models with negligible cost. Experiments show a 3-step Flux model trained in under one A100-day without quality loss.", "summary_cn": "该论文提出一种超高效的后训练方法，通过对速度场进行自蒸馏，将大规模预训练的流匹配扩散模型快捷为少步采样器，省去步长嵌入并几乎不耗算力。该技术既可在后训练阶段加入，也可嵌入预训练过程，使模型天然学习高效少步流，并实现对数十亿参数扩散模型的少样本蒸馏，成本极低。实验表明，3 步 Flux 模型在不足一天的 A100 计算下即可训练完成，质量保持领先。", "keywords": "flow matching, diffusion models, velocity field self-distillation, few-step sampler, shortcutting, post-training, few-shot distillation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xu Cai", "Yang Wu", "Qianli Chen", "Haoran Wu", "Lichuan Xiang", "Hongkai Wen"]}, "usage": {"completion_tokens": 918, "prompt_tokens": 3436, "total_tokens": 4354, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 705, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00075376, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029476, "upstream_inference_completions_cost": 0.000459}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:03.231911", "feed": "arxiv_cscv", "title": "Metrics and evaluations for computational and sustainable AI efficiency", "link": "https://papers.cool/arxiv/2510.17885", "analysis": {"summary": "The paper introduces a unified, reproducible methodology for evaluating AI model inference that combines computational performance with environmental impact metrics such as energy use and location‑adjusted carbon emissions. It applies this framework across a range of hardware platforms and software stacks, producing Pareto frontiers that clarify trade‑offs between accuracy, latency, energy, and carbon under realistic serving conditions.", "summary_cn": "本文提出了一套统一且可复现的 AI 推理评估方法，将计算性能与能源消耗、地点校正的碳排放等环境影响指标相结合。通过在多种硬件平台和软件栈上实验，生成展示准确率、延迟、能耗和碳排放权衡的 Pareto 前沿，帮助实现可持续的 AI 部署。", "keywords": "AI efficiency, carbon emissions, benchmarking, inference latency, energy consumption, sustainable AI, multi-precision models, hardware evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}, "usage": {"completion_tokens": 546, "prompt_tokens": 3433, "total_tokens": 3979, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 323, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056731, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029431, "upstream_inference_completions_cost": 0.000273}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:04.090506", "feed": "arxiv_cscv", "title": "Conformal Lesion Segmentation for 3D Medical Images", "link": "https://papers.cool/arxiv/2510.17897", "analysis": {"summary": "The paper proposes Conformal Lesion Segmentation (CLS), a risk-constrained framework that calibrates data-driven thresholds via conformal prediction to guarantee that the false negative rate (FNR) of 3D lesion segmentation remains below a user-specified tolerance with a desired confidence level. By holding out a calibration set, CLS computes critical thresholds for each sample that meet the FNR target, then uses the (1-α) quantile of these thresholds as the test-time decision boundary, providing statistical guarantees while improving segmentation accuracy. Experiments on six 3D‑LS datasets and five backbone models demonstrate the method's statistical soundness and practical performance gains for clinical deployment.", "summary_cn": "本文提出了“Conformal Lesion Segmentation (CLS)”风险约束框架，通过共形预测校准数据驱动阈值，以确保 3D 病灶分割的假阴性率 (FNR) 在给定容忍度 ε 下，并在期望置信水平 α 下提供统计保证。CLS 在校准集上为每个样本计算满足 FNR 目标的关键阈值，然后取这些阈值的 (1-α) 分位数作为测试时的决策阈值，实现了统计可靠性并提升了分割精度。六个 3D‑LS 数据集和五种主干模型的实验验证了方法的统计可靠性和在临床部署中的实际性能提升。", "keywords": "conformal prediction, lesion segmentation, false negative rate, risk-constrained segmentation, 3D medical imaging, calibration, uncertainty quantification", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Binyu Tan", "Zhiyuan Wang", "Jinhao Duan", "Kaidi Xu", "Heng Tao Shen", "Xiaoshuang Shi", "Fumin Shen"]}, "usage": {"completion_tokens": 725, "prompt_tokens": 3517, "total_tokens": 4242, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 361, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066941, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030691, "upstream_inference_completions_cost": 0.0003625}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:06.583101", "feed": "arxiv_cscv", "title": "From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation", "link": "https://papers.cool/arxiv/2510.18263", "analysis": {"summary": "The paper addresses the trade-off between identity preservation and prompt adherence in subject-driven image generation by improving online reinforcement learning methods. It introduces Customized-GRPO, featuring Synergy-Aware Reward Shaping and Time-Aware Dynamic Weighting to align reward signals with diffusion temporal dynamics, mitigating competitive degradation seen in naive GRPO. Experiments show the method yields images with better fidelity and editability compared to baselines.", "summary_cn": "本文针对主题驱动图像生成中身份保持与提示匹配之间的权衡，提出改进的在线强化学习方法。通过引入Customized‑GRPO，包含协同感知奖励塑形（Synergy‑Aware Reward Shaping）和时间感知动态权重（Time‑Aware Dynamic Weighting），使奖励信号与扩散过程的时间动态保持一致，从而克服了简单GRPO导致的竞争退化。实验表明该方法在保持关键身份特征的同时，更准确地遵循复杂文本提示，优于基线。", "keywords": "reinforcement learning, diffusion models, subject-driven image generation, reward shaping, temporal dynamics, identity preservation, prompt adherence, Customized-GRPO, synergy-aware reward shaping, dynamic weighting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziwei Huang", "Ying Shu", "Hao Fang", "Quanyu Long", "Wenya Wang", "Qiushi Guo", "Tiezheng Ge", "Leilei Gan"]}, "usage": {"completion_tokens": 670, "prompt_tokens": 3421, "total_tokens": 4091, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 423, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00040484, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013684, "upstream_inference_completions_cost": 0.000268}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:07.592773", "feed": "arxiv_cscv", "title": "ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy", "link": "https://papers.cool/arxiv/2510.18014", "analysis": {"summary": "This paper introduces ManzaiSet, a large-scale multimodal dataset comprising facial video and audio recordings of 241 participants reacting to Japanese manzai comedy performances, and presents analyses revealing distinct viewer response types and a positive viewing order effect. The dataset aims to mitigate Western bias in affective computing and supports culturally aware emotion AI development. Automated humor classification experiments show no significant differences across viewer types after statistical correction.", "summary_cn": "本文推出了 ManzaySet，一个包含 241 名参与者观看日本相声（manzai）表演时面部视频和音频的多模态大型数据集，并通过分析发现了三类观众反应模式以及观看顺序的正向效应。该数据集旨在降低情感计算中的西方中心偏差，促进面向非西方文化的情感 AI 开发。自动幽默分类实验表明在进行多重检验校正后，不同观众类型之间没有显著差异。", "keywords": "multimodal dataset, viewer responses, Japanese manzai, affective computing, emotion AI, facial expression, humor classification, cultural bias", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kazuki Kawamura", "Kengo Nakai", "Jun Rekimoto"]}, "usage": {"completion_tokens": 612, "prompt_tokens": 4062, "total_tokens": 4674, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 278, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003561, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002031, "upstream_inference_completions_cost": 0.000153}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:07.744754", "feed": "arxiv_cscv", "title": "Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models", "link": "https://papers.cool/arxiv/2510.18303", "analysis": {"summary": "The paper introduces Med-RwR, a multimodal medical reasoning-with-retrieval framework that enables large language models to actively retrieve external medical knowledge based on visual and textual cues during diagnosis. A two-stage reinforcement learning approach with specialized rewards encourages the model to combine visual findings and clinical text for effective retrieval, and a confidence‑driven image re‑retrieval mechanism adapts at test time when uncertainty is high. Experiments on several medical benchmarks, including a new echocardiography dataset, show significant performance gains and improved generalization to under‑represented domains.", "summary_cn": "本文提出了 Med‑RwR，一个多模态医学推理‑检索框架，使大语言模型在诊断过程中能够主动依据视觉诊断信息和文本临床信息检索外部医学知识。通过两阶段强化学习并设计专属奖励，模型被激励有效利用视觉和文本线索进行检索，同时在测试时加入置信度驱动的图像再检索，以应对低信心预测。多项医学基准实验，包括新建的超声心动图基准，显示该方法显著提升性能并在陌生领域具有良好泛化能力。", "keywords": "multimodal LLM, medical reasoning, retrieval-augmented generation, reinforcement learning, confidence-driven re-retrieval, hallucination reduction, external knowledge integration, echocardiography benchmark", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Lehan Wang", "Yi Qin", "Honglong Yang", "Xiaomeng Li"]}, "usage": {"completion_tokens": 739, "prompt_tokens": 4135, "total_tokens": 4874, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 352, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003915, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020675, "upstream_inference_completions_cost": 0.00018475}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:07.874187", "feed": "arxiv_cscv", "title": "Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing", "link": "https://papers.cool/arxiv/2510.17816", "analysis": {"summary": "The paper introduces WiAnchor, a training framework that leverages near-field Wi-Fi signals from personal devices to enable multi-person human activity recognition across domains, even when some activity categories are missing. It enlarges inter-class margins during pre-training, uses an anchor-matching mechanism for fine-tuning to filter subject-specific interference, and refines predictions via feature similarity to anchors, achieving over 90% cross-domain accuracy on a newly collected dataset.", "summary_cn": "本文提出 WiAnchor 框架，利用个人设备的近场 Wi‑Fi 信号实现多人与跨域的人体活动识别，即使部分活动类别缺失。该框架在预训练阶段扩大类间特征间距，在微调阶段采用锚点匹配机制过滤受试者特定干扰，并通过与锚点的特征相似度进一步提升识别准确率，在新构建的数据集上实现超过 90% 的跨域准确率。", "keywords": "Wi-Fi sensing, human activity recognition, cross-domain adaptation, near-field effect, anchor matching, fine-tuning, multi-person HAR", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xin Li", "Jingzhi Hu", "Yinghui He", "Hongbo Wang", "Jin Gan", "Jun Luo"]}, "usage": {"completion_tokens": 884, "prompt_tokens": 3447, "total_tokens": 4331, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 717, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00057015, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00017235, "upstream_inference_completions_cost": 0.0003978}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:09.338938", "feed": "arxiv_cscv", "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation", "link": "https://papers.cool/arxiv/2510.18751", "analysis": {"summary": "This paper presents ALGOS, a system that combines vision-language models with GeoSAM-assisted human evaluation to segment harmful algal blooms in remote-sensing images and estimate their severity. By fine-tuning on NASA's Cyanobacteria Aggregated Manual Labels, the method achieves strong performance on both mask generation and bloom severity prediction, offering a scalable approach for automated HAB monitoring.", "summary_cn": "本文提出了ALGOS系统，利用视觉语言模型和GeoSAM辅助的人类评估，对遥感图像中的有害藻华进行分割并估计其严重程度。通过在NASA的蓝藻聚合手工标签（CAML）上进行微调，实现了高质量的掩码生成和藻华严重程度预测，为自动化藻华监测提供了可扩展的解决方案。", "keywords": "algal bloom, cyanobacteria, vision-language model, remote sensing, segmentation, severity estimation, GeoSAM, environmental monitoring", "scoring": {"interpretability": 3, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"]}, "usage": {"completion_tokens": 746, "prompt_tokens": 4018, "total_tokens": 4764, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 453, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003874, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002009, "upstream_inference_completions_cost": 0.0001865}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:10.540809", "feed": "arxiv_cscv", "title": "FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo", "link": "https://papers.cool/arxiv/2510.18193", "analysis": {"summary": "FST.ai 2.0 is an explainable AI ecosystem for Olympic and Paralympic Taekwondo that combines graph convolutional network‑based pose action recognition, credal‑set epistemic uncertainty modeling, and visual explainability overlays to assist referees, coaches, and athletes in real time. The system provides interactive dashboards for human‑AI collaboration, fairness monitoring, and policy analytics, achieving an 85% reduction in decision review time and high referee trust. The work demonstrates how transparent, uncertainty‑aware AI can be integrated into sport officiating and performance assessment.", "summary_cn": "FST.ai 2.0 是一个面向奥运会和残奥会跆拳道的可解释 AI 生态系统，融合了基于图卷积网络的姿态动作识别、credal 集合的认知不确定性建模以及可视化解释叠加，以实时辅助裁判、教练和运动员。系统提供交互式仪表板，实现人机协作、公平性监测和政策分析，实验显示决策复审时间降低 85%，裁判对 AI 决策的信任度高。该工作展示了透明且具不确定性意识的 AI 如何在体育裁判和运动员评估中得到可靠集成。", "keywords": "explainable AI, graph convolutional networks, pose action recognition, credal sets, epistemic uncertainty, sports fairness, human-AI collaboration, decision support, Taekwondo, interactive dashboards", "scoring": {"interpretability": 7, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Keivan Shariatmadar", "Ahmad Osman", "Ramin Ray", "Usman Dildar", "Kisam Kim"]}, "usage": {"completion_tokens": 953, "prompt_tokens": 3463, "total_tokens": 4416, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 679, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00077531, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029881, "upstream_inference_completions_cost": 0.0004765}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:10.590450", "feed": "arxiv_cscv", "title": "Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis", "link": "https://papers.cool/arxiv/2510.18229", "analysis": {"summary": "The paper proposes a generation-based debiasing framework for object detection that uses a representation score to identify gaps beyond class frequency and employs precise visual blueprints with a generative alignment strategy to create high‑fidelity, unbiased training images. Experiments show notable improvements in mean average precision for underrepresented object groups, narrowing performance gaps and outperforming prior layout‑to‑image synthesis methods.", "summary_cn": "本文提出一种基于生成的目标检测去偏框架，利用“表示分数”(representation score) 诊断超越类别频次的表征缺口，并通过精确的视觉蓝图和生成对齐策略合成高质量、无偏的训练图像。实验表明，对稀疏对象组的 mAP 明显提升，缩小了性能差距，并超越了之前的布局到图像合成方法。", "keywords": "object detection, debiasing, generative augmentation, representation score, blueprint prompt, layout-to-image synthesis, data imbalance, fairness, mAP", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Xinhao Cai", "Liulei Li", "Gensheng Pei", "Tao Chen", "Jinshan Pan", "Yazhou Yao", "Wenguan Wang"]}, "usage": {"completion_tokens": 624, "prompt_tokens": 3424, "total_tokens": 4048, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 390, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3408}, "cost": 0.00058704, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027504, "upstream_inference_completions_cost": 0.000312}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:11.057165", "feed": "arxiv_cscv", "title": "NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation", "link": "https://papers.cool/arxiv/2510.17914", "analysis": {"summary": "NeuCo-Bench is a benchmark framework that evaluates lossy neural compression and representation learning for Earth Observation by using fixed-size embeddings as task-agnostic representations. It includes an evaluation pipeline, a hidden-task challenge leaderboard to reduce pretraining bias, and a scoring system that balances accuracy and stability, along with the released SSL4EO-S12-downstream dataset. Initial results from a public CVPR EARTHVISION challenge and ablations with foundation models demonstrate its utility for standardized assessment of neural embeddings.", "summary_cn": "NeuCo-Bench 是一个用于评估地球观测（Earth Observation）中（损失）神经压缩和表征学习的基准框架，采用固定大小的嵌入向量作为任务无关的紧凑表示。它包括可复用的评估流水线、通过隐藏任务排行榜降低预训练偏差的挑战模式以及在准确性与稳定性之间平衡的评分系统，并发布了 SSL4EO-S12-downstream 多光谱多时相数据集。初步在 2025 CVPR EARTHVISION 工作坊的公开挑战以及对最先进基础模型的消融实验展示了该框架在标准化神经嵌入评估方面的价值。", "keywords": "neural embeddings, Earth observation, benchmark, representation learning, SSL4EO, multispectral, multitemporal, downstream tasks, evaluation pipeline, challenge leaderboard", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rikard Vinge", "Isabelle Wittmann", "Jannik Schneider", "Michael Marszalek", "Luis Gilch", "Thomas Brunschwiler", "Conrad M Albrecht"]}, "usage": {"completion_tokens": 1123, "prompt_tokens": 3999, "total_tokens": 5122, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 736, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0004807, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00019995, "upstream_inference_completions_cost": 0.00028075}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:23:13.370450", "feed": "arxiv_cscv", "title": "Demystifying Transition Matching: When and Why It Can Beat Flow Matching", "link": "https://papers.cool/arxiv/2510.17991", "analysis": {"summary": "The paper investigates when Transition Matching (TM) outperforms Flow Matching (FM) in generative sampling. It proves that for unimodal Gaussian targets TM achieves lower KL divergence and faster convergence than FM, and extends the analysis to Gaussian mixtures, showing TM’s advantage in regimes with well‑separated modes and non‑vanishing variance, while the benefit disappears as variance approaches zero. Experiments on synthetic and real image/video data corroborate the theoretical findings.", "summary_cn": "本文研究了何时过渡匹配（Transition Matching, TM）优于流匹配（Flow Matching, FM）在生成模型中的采样性能。研究证明，在单峰高斯目标下，TM 在有限步数内获得更低的 KL 散度并且收敛更快；在高斯混合分布中，当各模式之间距离较大且方差不接近零时，TM 仍能超越 FM，而方差趋于零时两者表现趋同。实验在合成高斯分布以及图像、视频生成任务上验证了理论结果。", "keywords": "transition matching, flow matching, generative models, KL divergence, Gaussian mixtures, sampling dynamics, convergence analysis", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jaihoon Kim", "Rajarshi Saha", "Minhyuk Sung", "Youngsuk Park"]}, "usage": {"completion_tokens": 979, "prompt_tokens": 3463, "total_tokens": 4442, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 782, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078831, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029881, "upstream_inference_completions_cost": 0.0004895}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:26:34.433204", "feed": "arxiv_cscv", "title": "DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association", "link": "https://papers.cool/arxiv/2510.17860", "analysis": {"summary": "DMTrack presents a deformable state-space tracking framework for UAV-based multi-object tracking, featuring DeformMamba for adaptive trajectory modeling, MotionGate for Kalman-Mamba prediction fusion, and an uncertainty-aware association method to improve identity consistency without relying on appearance cues.", "summary_cn": "DMTrack 提出了一种用于 UAV 多目标跟踪的可变形状态空间框架，包括 DeformMamba 用于自适应轨迹建模、MotionGate 用于卡尔曼与 Mamba 预测的融合以及基于不确定性的关联策略，以在不使用外观模型的情况下提升身份保持和追踪准确性。", "keywords": "multi-object tracking, UAV, deformable state-space, Kalman fusion, uncertainty-aware association, motion modeling, DeformMamba, MotionGate, VisDrone-MOT, UAVDT", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zenghuang Fu", "Xiaofeng Han", "Mingda Jia", "Jin ming Yang", "Qi Zeng", "Muyang Zahng", "Changwei Wang", "Weiliang Meng", "Xiaopeng Zhang"]}, "usage": {"completion_tokens": 648, "prompt_tokens": 4024, "total_tokens": 4672, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 448, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0018944, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0014084, "upstream_inference_completions_cost": 0.000486}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:26:34.867080", "feed": "arxiv_cscv", "title": "OmniNWM: Omniscient Driving Navigation World Models", "link": "https://papers.cool/arxiv/2510.18313", "analysis": {"summary": "OmniNWM introduces an omniscient panoramic navigation world model for autonomous driving that jointly generates RGB, semantic, depth, and 3D occupancy video streams, and employs a normalized panoramic Plucker ray‑map representation for precise long‑horizon control. The model also defines rule‑based dense rewards directly from the generated 3D occupancy to enforce driving compliance and safety, achieving state‑of‑the‑art performance in video generation, control accuracy, and stability.", "summary_cn": "OmniNWM 提出了一种全视角全景导航世界模型，用于自动驾驶场景，能够同时生成 RGB、语义、深度和 3D 占用的全景视频，并通过归一化的全景 Plucker ray‑map 表示实现对全景视频的高精度长时控制。该模型利用生成的 3D 占用直接定义基于规则的密集奖励，以确保驾驶合规与安全，展现了视频生成、控制精度和长时稳定性的最先进表现。", "keywords": "OmniNWM, panoramic world model, Plucker ray-map, 3D occupancy, rule-based dense rewards, autonomous driving, long-horizon control, video generation, state-action-reward modeling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Bohan Li", "Zhuang Ma", "Dalong Du", "Baorui Peng", "Zhujin Liang", "Zhenqiang Liu", "Chao Ma", "Yueming Jin", "Hao Zhao", "Wenjun Zeng", "Xin Jin"]}, "usage": {"completion_tokens": 809, "prompt_tokens": 4018, "total_tokens": 4827, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 550, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00201305, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0014063, "upstream_inference_completions_cost": 0.00060675}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:27:42.797241", "feed": "arxiv_cscv", "title": "Cross-Modal Scene Semantic Alignment for Image Complexity Assessment", "link": "https://papers.cool/arxiv/2510.18377", "analysis": {"summary": "The paper introduces Cross-Modal Scene Semantic Alignment (CM-SSA), a method that leverages image‑text semantic alignment to improve image complexity assessment. CM-SSA consists of a complexity regression branch guided by a scene semantic alignment branch that learns pairwise alignment between images and textual scene descriptions. Experiments on multiple ICA datasets show that CM-SSA outperforms existing state‑of‑the‑art approaches.", "summary_cn": "本文提出了跨模态场景语义对齐 (CM-SSA) 方法，利用图像与文本描述之间的语义对齐来提升图像复杂度评估的效果。CM-SSA 包含一个在场景语义对齐分支指导下的复杂度回归分支，该分支通过成对学习将图像与富含场景信息的文本提示对齐。大量实验表明，该方法在多个 ICA 数据集上显著优于现有的最先进技术。", "keywords": "image complexity assessment, cross-modal alignment, scene semantics, multimodal learning, regression, text prompts", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuqing Luo", "Yixiao Li", "Jiang Liu", "Jun Fu", "Hadi Amirpour", "Guanghui Yue", "Baoquan Zhao", "Padraig Corcoran", "Hantao Liu", "Wei Zhou"]}, "usage": {"completion_tokens": 575, "prompt_tokens": 3464, "total_tokens": 4039, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 322, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058646, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029896, "upstream_inference_completions_cost": 0.0002875}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:28:42.303556", "feed": "arxiv_cscv", "title": "StreamingTOM: Streaming Token Compression for Efficient Video Understanding", "link": "https://papers.cool/arxiv/2510.18269", "analysis": {"summary": "StreamingTOM proposes a training-free, plug‑and‑play two‑stage framework for streaming video vision‑language models that tackles both pre‑LLM token prefill and post‑LLM KV‑cache bottlenecks. Causal Temporal Reduction limits per‑frame token budgets by selecting salient tokens based on frame changes, while Online Quantized Memory stores tokens in 4‑bit format and retrieves them on demand, keeping the active KV‑cache. Experiments show up to 15.7× KV‑cache compression, reduced peak memory, and faster time‑to‑first‑token while retaining competitive accuracy.", "summary_cn": "StreamingTOM 提出一种无需训练、即插即用的两阶段框架，用于流式视频视觉语言模型，解决了 LLM 前置的 token 预填充和后置 KV‑cache 两个瓶颈。因果时间压缩通过基于相邻帧变化和 token 显著性选择有限数量的视觉 token，显著降低每帧的预填充成本；在线量化记忆将 token 存储为 4 位格式，按需检索并解量化，使得 KV‑cache 大小随流长度保持有限。实验表明该方法实现 15.7 倍的 KV‑cache 压缩、降低峰值内存并加快响应速度，同时保持与离线基准接近的准确率。", "keywords": "video understanding, token compression, streaming vision-language, causal temporal reduction, quantized memory, KV-cache compression", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyi Chen", "Keda Tao", "Kele Shao", "Huan Wang"]}, "usage": {"completion_tokens": 610, "prompt_tokens": 3481, "total_tokens": 4091, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 236, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060651, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030151, "upstream_inference_completions_cost": 0.000305}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:10.158850", "feed": "arxiv_cscv", "title": "CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image", "link": "https://papers.cool/arxiv/2510.20776", "analysis": {"summary": "CUPID introduces a generation-based framework that simultaneously infers camera pose, 3D shape, and texture from a single image by casting reconstruction as conditional sampling from a learned 3D object distribution. It employs a two-stage flow-matching pipeline—coarse geometry with pose recovery followed by pose-aligned refinement—to produce high-fidelity voxel and pixel‑voxel correspondences, achieving significant gains in PSNR and Chamfer Distance over prior methods.", "summary_cn": "CUPID 提出了一种基于生成的框架，通过从学习得来的 3D 对象分布进行条件采样，实现单张图片的相机姿态、三维形状和纹理的同步推断。该方法采用两阶段流匹配流程——先生成粗糙几何并恢复姿态，再进行姿态对齐的特征细化，从而在 PSNR 和 Chamfer Distance 上显著优于现有方法。", "keywords": "3D reconstruction, pose estimation, generative models, flow matching, voxel generation", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Binbin Huang", "Haobin Duan", "Yiqun Zhao", "Zibo Zhao", "Yi Ma", "Shenghua Gao"]}, "usage": {"completion_tokens": 460, "prompt_tokens": 3439, "total_tokens": 3899, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 171, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00052521, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029521, "upstream_inference_completions_cost": 0.00023}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:10.991927", "feed": "arxiv_cscv", "title": "AnyPcc: Compressing Any Point Cloud with a Single Universal Model", "link": "https://papers.cool/arxiv/2510.20331", "analysis": {"summary": "AnyPcc introduces a universal point cloud compression framework that combines a Universal Context Model, leveraging spatial and channel-wise priors, with an Instance-Adaptive Fine-Tuning (IAFT) strategy that fine‑tunes a small subset of weights per instance and encodes them in the bitstream. The approach addresses the lack of robust context modeling and out-of-distribution handling, achieving state‑of‑the‑art compression results across 15 diverse datasets.", "summary_cn": "AnyPcc 提出了一种通用点云压缩框架，结合利用空间和通道先验的通用上下文模型（Universal Context Model）以及实例自适应微调（Instance‑Adaptive Fine‑Tuning, IAFT）策略，对每个实例微调少量网络权重并将其编码进比特流。该方法解决了上下文建模不足和分布外数据处理效率低的问题，在 15 个多样化数据集上实现了最先进的压缩性能。", "keywords": "point cloud compression, universal context model, instance-adaptive fine-tuning, geometry compression, out-of-distribution data, neural compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kangli Wang", "Qianxi Yi", "Yuqi Ye", "Shihao Li", "Wei Gao"]}, "usage": {"completion_tokens": 554, "prompt_tokens": 3388, "total_tokens": 3942, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 277, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056456, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028756, "upstream_inference_completions_cost": 0.000277}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.016483", "feed": "arxiv_cscv", "title": "From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging", "link": "https://papers.cool/arxiv/2510.20550", "analysis": {"summary": "The paper introduces ACamera-Net, a lightweight neural network that predicts optimal exposure (ISO) and white‑balance parameters directly from RAW images to improve image quality under challenging lighting conditions. It consists of two modules—ACamera-Exposure and ACamera-Color—and is designed for real‑time inference on edge devices, showing consistent performance gains over traditional auto‑exposure modes without additional enhancement steps.", "summary_cn": "本文提出了 ACamera‑Net，一种轻量级神经网络，可直接从 RAW 图像预测最佳曝光 (ISO) 和白平衡参数，以提升在低光、高动态范围等复杂光照下的成像质量。系统包含曝光估计模块和色温/增益预测模块，针对边缘设备实现实时推理，并在无需额外增强模块的情况下，相比传统自动模式显著提升图像质量和感知稳定性。", "keywords": "camera parameter adjustment, exposure estimation, white balance, adaptive imaging, lightweight neural network, RAW image processing, edge inference", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fuchen Li", "Yansong Du", "Wenbo Cheng", "Xiaoxia Zhou", "Sen Yin"]}, "usage": {"completion_tokens": 551, "prompt_tokens": 3405, "total_tokens": 3956, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 308, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056561, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029011, "upstream_inference_completions_cost": 0.0002755}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.341105", "feed": "arxiv_cscv", "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge", "link": "https://papers.cool/arxiv/2510.20819", "analysis": {"summary": "The paper introduces the Latent Denoising Diffusion Bridge Model (LDDBM), a general framework for translating between arbitrary modalities by learning a shared latent space and using contrastive alignment and predictive losses to ensure semantic consistency. It demonstrates strong performance on tasks such as multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis, establishing a new baseline for general modality translation.", "summary_cn": "本文提出了潜在去噪扩散桥模型（LDDBM），通过在共享潜在空间中学习跨模态桥接，并使用对比对齐损失和预测损失来保证语义一致性，从而实现任意模态之间的翻译。实验在多视图到 3D 形状生成、图像超分辨率和多视图场景合成等任务上取得了强劲表现，奠定了通用模态翻译的新基准。", "keywords": "modality translation, diffusion models, latent diffusion bridge, contrastive alignment, predictive loss, cross-modal generation, latent space, multi-view to 3D, image super-resolution", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nimrod Berman", "Omkar Joglekar", "Eitan Kosman", "Dotan Di Castro", "Omri Azencot"]}, "usage": {"completion_tokens": 580, "prompt_tokens": 3486, "total_tokens": 4066, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 302, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059226, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030226, "upstream_inference_completions_cost": 0.00029}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.369126", "feed": "arxiv_cscv", "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset", "link": "https://papers.cool/arxiv/2510.20661", "analysis": {"summary": "The paper introduces UltraHR-100K, a curated dataset of 100,000 ultra-high-resolution images with detailed captions, and proposes a frequency-aware post‑training framework (DOTS and SWFR) to improve fine‑detail synthesis in text‑to‑image diffusion models. Experiments on the new UltraHR‑eval4K benchmark show notable gains in high‑frequency detail preservation and overall image fidelity.", "summary_cn": "本文推出 UltraHR-100K 数据集，包含 10 万张 3K 以上分辨率的高质量图像及丰富文本描述，并提出频率感知的后训练方法（DOTS 与 SWFR），用于提升文本生成扩散模型在超高分辨率场景下的细节合成能力。实验在 UltraHR‑eval4K 基准上展示了高频细节保留和整体画质的显著提升。", "keywords": "ultra-high-resolution, text-to-image, diffusion models, large-scale dataset, frequency-aware training, detail synthesis, DOTS, SWFR, high-frequency preservation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chen Zhao", "En Ci", "Yunzhe Xu", "Tiehan Fan", "Shanyan Guan", "Yanhao Ge", "Jian Yang", "Ying Tai"]}, "usage": {"completion_tokens": 582, "prompt_tokens": 3490, "total_tokens": 4072, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 333, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059386, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030286, "upstream_inference_completions_cost": 0.000291}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.448519", "feed": "arxiv_cscv", "title": "Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals", "link": "https://papers.cool/arxiv/2510.20267", "analysis": {"summary": "The paper presents a real-time currency detection system for visually impaired users, employing a YOLOv8‑nano model with a custom detection head enhanced by Squeeze‑and‑Excitation blocks. Trained on 30 classes covering USD, EUR, and BDT notes and coins, the model achieves 97.73% accuracy and provides voice feedback to identify detected currency. The goal is to enable independent handling of money for visually impaired individuals using smartphones.", "summary_cn": "本文提出了一套用于视障用户的实时货币检测系统，采用 YOLOv8‑nano 模型并加入 Squeeze‑and‑Excitation 块的自定义检测头。模型在涵盖美元、欧元和孟加拉塔卡的 30 类纸币和硬币数据上训练，取得 97.73% 的准确率，并通过语音反馈提示检测到的货币种类，旨在帮助视障人士独立处理金钱。", "keywords": "currency detection, YOLOv8, visual impairment, voice feedback, object detection, assistive technology", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"]}, "usage": {"completion_tokens": 598, "prompt_tokens": 3458, "total_tokens": 4056, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 323, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059706, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029806, "upstream_inference_completions_cost": 0.000299}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.479452", "feed": "arxiv_cscv", "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models", "link": "https://papers.cool/arxiv/2510.20707", "analysis": {"summary": "The paper introduces MixKV, a method that jointly optimizes importance and diversity to compress key‑value caches in large vision‑language models, addressing head‑wise semantic redundancy. Experiments show consistent performance gains over existing compression techniques across multiple benchmarks, including extreme compression budgets, while preserving inference efficiency and extending to language models. Code is released publicly.", "summary_cn": "本文提出 MixKV 方法，通过同时考虑重要性和多样性，对大规模视觉语言模型的 KV 缓存进行压缩，并针对不同注意力头的语义冗余进行自适应平衡。实验表明，在多项基准测试和极端压缩预算下，MixKV 能显著提升现有压缩方法的效果，同时保持推理效率，并可无缝扩展到大语言模型。代码已公开发布。", "keywords": "KV cache compression, vision-language models, importance and diversity, MixKV, semantic redundancy, memory bottleneck, SnapKV, AdaKV, inference efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xuyang Liu", "Xiyan Gui", "Yuchao Zhang", "Linfeng Zhang"]}, "usage": {"completion_tokens": 635, "prompt_tokens": 3552, "total_tokens": 4187, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 392, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062966, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031216, "upstream_inference_completions_cost": 0.0003175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:11.507403", "feed": "arxiv_cscv", "title": "GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models", "link": "https://papers.cool/arxiv/2510.20586", "analysis": {"summary": "The paper introduces GenColorBench, a benchmark designed to evaluate the color precision of text-to-image generation models using extensive color systems such as ISCC-NBS and CSS3/X11. It provides 44K prompts covering over 400 colors and includes both perceptual and automated assessment methods, revealing performance variations across popular models. The benchmark aims to guide improvements in fine-grained color controllability for applications requiring accurate color representation.", "summary_cn": "本文提出 GenColorBench 基准，用于评估文本到图像生成模型的颜色精度，采用 ISCC-NBS 和 CSS3/X11 等丰富的颜色体系。基准包含 44K 条涉及 400 多种颜色的提示，并结合感知与自动评估方法，揭示了主流模型在颜色生成上的性能差异。此基准旨在推动在需要精确颜色表现的应用场景中提升模型的颜色可控性。", "keywords": "color generation, text-to-image, benchmark, color controllability, ISCC-NBS, CSS3, perceptual assessment, automated evaluation, prompt engineering, visual fidelity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Muhammad Atif Butt", "Alexandra Gomez-Villa", "Tao Wu", "Javier Vazquez-Corral", "Joost Van De Weijer", "Kai Wang"]}, "usage": {"completion_tokens": 665, "prompt_tokens": 3428, "total_tokens": 4093, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 422, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062606, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029356, "upstream_inference_completions_cost": 0.0003325}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:12.001127", "feed": "arxiv_cscv", "title": "AlphaFlow: Understanding and Improving MeanFlow Models", "link": "https://papers.cool/arxiv/2510.20771", "analysis": {"summary": "The paper analyzes the MeanFlow objective, showing it decomposes into trajectory flow matching and trajectory consistency terms that are negatively correlated, leading to optimization conflict. Based on this insight, it proposes the α‑Flow family of objectives and a curriculum that gradually shifts from flow matching to MeanFlow, improving convergence and achieving state‑of‑the‑art FID scores on ImageNet with vanilla DiT backbones.", "summary_cn": "本文分析了 MeanFlow 目标，发现其可分解为轨迹流匹配和轨迹一致性两部分，这两者负相关导致优化冲突。基于此洞察，提出了 α‑Flow 系列目标以及从流匹配平滑过渡到 MeanFlow 的课程策略，提升收敛速度，并在使用普通 DiT 骨干的 ImageNet 实验中取得了最先进的 FID 分数。", "keywords": "MeanFlow, trajectory flow matching, alpha-Flow, generative modeling, diffusion, DiT, curriculum training, FID", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Huijie Zhang", "Aliaksandr Siarohin", "Willi Menapace", "Michael Vasilkovsky", "Sergey Tulyakov", "Qing Qu", "Ivan Skorokhodov"]}, "usage": {"completion_tokens": 539, "prompt_tokens": 3420, "total_tokens": 3959, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 267, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056186, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029236, "upstream_inference_completions_cost": 0.0002695}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:12.138376", "feed": "arxiv_cscv", "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence", "link": "https://papers.cool/arxiv/2510.20579", "analysis": {"summary": "The paper introduces Open-o3 Video, a framework that generates video reasoning with explicit spatio-temporal evidence, highlighting key timestamps, objects, and bounding boxes alongside its answers. It curates two high-quality datasets with unified spatio-temporal annotations and employs a cold-start reinforcement learning strategy with rewards for answer accuracy, temporal alignment, and spatial precision, achieving state-of-the-art performance on V-STAR and multiple video understanding benchmarks. Additionally, the generated reasoning traces enable confidence-aware verification and improve answer reliability.", "summary_cn": "本文提出 Open-o3 Video 框架，能够在视频推理过程中提供显式的时空证据，突出关键时间戳、对象及其边界框并与答案一起展示。作者构建了两套统一时空标注的高质量数据集，并采用冷启动强化学习策略，结合答案准确性、时间对齐和空间精度的奖励，显著提升了 V-STAR 等多项视频理解基准的表现。此外，生成的推理轨迹可用于置信度感知的验证，提升答案可靠性。", "keywords": "video reasoning, spatio-temporal evidence, grounded explanations, reinforcement learning, dataset, Open-o3 Video, visual grounding", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jiahao Meng", "Xiangtai Li", "Haochen Wang", "Yue Tan", "Tao Zhang", "Lingdong Kong", "Yunhai Tong", "Anran Wang", "Zhiyang Teng", "Yujing Wang", "Zhuochen Wang"]}, "usage": {"completion_tokens": 683, "prompt_tokens": 3525, "total_tokens": 4208, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 448, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064961, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030811, "upstream_inference_completions_cost": 0.0003415}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:12.818037", "feed": "arxiv_cscv", "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation", "link": "https://papers.cool/arxiv/2510.20726", "analysis": {"summary": "AutoScape introduces a geometry-consistent long-horizon driving scene generation framework that uses a novel RGB-D diffusion model to produce sparse, geometrically aligned keyframes and a video diffusion model to interpolate dense video frames. By jointly encoding image and depth, conditioning on rendered point clouds from prior keyframes, and applying warp-consistent guidance, the system achieves substantial improvements in long-horizon FID and FVD scores.", "summary_cn": "AutoScape 提出了一种几何一致的长时段驾驶场景生成框架，核心是利用新颖的 RGB-D 扩散模型生成稀疏且几何对齐的关键帧，并通过视频扩散模型在关键帧之间插值生成密集视频。该方法在共享潜在空间中同时处理图像和深度、基于先前关键帧的渲染点云进行条件化，并使用 warp‑consistent 引导，实现了长时段 FID 与 FVD 分数的大幅提升。", "keywords": "geometry-consistent diffusion, RGB-D generation, long-horizon video synthesis, driving scene generation, keyframe interpolation, warp-consistent guidance", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiacheng Chen", "Ziyu Jiang", "Mingfu Liang", "Bingbing Zhuang", "Jong-Chyi Su", "Sparsh Garg", "Ying Wu", "Manmohan Chandraker"]}, "usage": {"completion_tokens": 616, "prompt_tokens": 3379, "total_tokens": 3995, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 344, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059421, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028621, "upstream_inference_completions_cost": 0.000308}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:12.984640", "feed": "arxiv_cscv", "title": "Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling", "link": "https://papers.cool/arxiv/2510.20673", "analysis": {"summary": "The paper introduces Efficient Multi-bit Quantization Network (EMQNet) training, which uses weight bias correction to share batch normalization across bit-widths and eliminate fine-tuning, and a bit-wise coreset sampling strategy to train each precision on a compact, informative subset selected by gradient-based importance scores. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with ResNet and ViT architectures show competitive or superior accuracy while reducing training time up to 7.88×.", "summary_cn": "本文提出了 Efficient Multi-bit Quantization Network (EMQNet) 训练方法，利用权重偏置校正在不同比特宽度间共享批归一化并消除微调需求，并通过基于梯度重要性分数的比特级核集抽样，使每个精度子模型在紧凑且信息量丰富的子集上训练。实验在 CIFAR-10/100、TinyImageNet 和 ImageNet-1K 上使用 ResNet 与 ViT，展示了在保持或提升准确率的同时，训练时间最高可缩短 7.88 倍。", "keywords": "multi-bit quantization, weight bias correction, bit-wise coreset sampling, efficient training, model compression, quantized neural networks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinhee Kim", "Jae Jun An", "Kang Eun Jeon", "Jong Hwan Ko"]}, "usage": {"completion_tokens": 757, "prompt_tokens": 3439, "total_tokens": 4196, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 513, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067371, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029521, "upstream_inference_completions_cost": 0.0003785}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:12.996637", "feed": "arxiv_cscv", "title": "Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation", "link": "https://papers.cool/arxiv/2510.20549", "analysis": {"summary": "The paper introduces SELM-SLAM3, a deep learning‑enhanced visual SLAM framework that incorporates SuperPoint and LightGlue to improve feature extraction and matching under challenging conditions such as low texture, motion blur, and poor lighting. Experiments on TUM RGB‑D, ICL‑NUIM, and TartanAir datasets show that SELM‑SLAM3 outperforms ORB‑SLAM3 by an average of 87.84% and surpasses state‑of‑the‑art RGB‑D SLAM systems by 36.77%, demonstrating its suitability for reliable navigation assistance for visually impaired users.", "summary_cn": "本文提出 SELM‑SLAM3，一种融合 SuperPoint 与 LightGlue 的深度学习驱动视觉 SLAM 框架，旨在在低纹理、运动模糊和光照不佳等困难场景下提升特征提取和匹配的鲁棒性。通过在 TUM RGB‑D、ICL‑NUIM 与 TartanAir 数据集上的实验，SELM‑SLAM3 的定位精度比 ORB‑SLAM3 平均提升 87.84%，并超越已有最先进的 RGB‑D SLAM 系统 36.77%，为视障人士的导航辅助提供了更可靠的技术平台。", "keywords": "visual SLAM, deep learning, SuperPoint, LightGlue, assistive navigation, visually impaired, robustness, localization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Marziyeh Bamdad", "Hans-Peter Hutter", "Alireza Darvishy"]}, "usage": {"completion_tokens": 748, "prompt_tokens": 3392, "total_tokens": 4140, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 416, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066216, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.000374}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:13.058444", "feed": "arxiv_cscv", "title": "Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges", "link": "https://papers.cool/arxiv/2510.20634", "analysis": {"summary": "This systematic review examines 260 deep‑learning studies on dental image analysis, summarizing publicly available datasets, model architectures, training strategies, and evaluation metrics, and discusses current challenges and future research directions.", "summary_cn": "本文系统回顾了 260 篇关于牙科影像分析的深度学习研究，概述了公开数据集、模型结构、训练策略与评估指标，并讨论了当前挑战及未来研究方向。", "keywords": "dental image analysis, deep learning, systematic review, medical imaging datasets, computer-aided diagnosis, AI in dentistry", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenhuan Zhou", "Jingbo Zhu", "Yuchen Zhang", "Xiaohang Guan", "Peng Wang", "Tao Li"]}, "usage": {"completion_tokens": 442, "prompt_tokens": 3511, "total_tokens": 3953, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 293, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00052701, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030601, "upstream_inference_completions_cost": 0.000221}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:13.130247", "feed": "arxiv_cscv", "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives", "link": "https://papers.cool/arxiv/2510.20822", "analysis": {"summary": "HoloCine proposes a novel architecture for generating cinematic multi‑shot long video narratives, addressing the \"narrative gap\" of current text‑to‑video models that only produce isolated clips. It introduces a Window Cross‑Attention mechanism to localize prompts to specific shots and a Sparse Inter‑Shot Self‑Attention pattern that is dense within shots but sparse across shots, enabling efficient minute‑scale video synthesis with consistent characters and cinematic techniques.", "summary_cn": "HoloCine 提出了一种用于生成电影式多镜头长视频叙事的全新架构，弥补了现有文本到视频模型只能生成单独片段的“叙事缺口”。该方法引入了窗口交叉注意力（Window Cross‑Attention）用于将文本提示定位到特定镜头，并采用稀疏镜头间自注意力（Sparse Inter‑Shot Self‑Attention）在镜头内部保持密集、镜头之间保持稀疏，从而实现高效的分钟级视频生成，并展现出角色记忆与电影技巧的持续一致性。", "keywords": "holistic video generation, cinematic storytelling, text-to-video, window cross-attention, sparse inter-shot attention, long video synthesis, multi-shot consistency, generative AI", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihao Meng", "Hao Ouyang", "Yue Yu", "Qiuyu Wang", "Wen Wang", "Ka Leong Cheng", "Hanlin Wang", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Yujun Shen", "Huamin Qu"]}, "usage": {"completion_tokens": 619, "prompt_tokens": 3391, "total_tokens": 4010, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 312, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059751, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028801, "upstream_inference_completions_cost": 0.0003095}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:13.170375", "feed": "arxiv_cscv", "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization", "link": "https://papers.cool/arxiv/2510.20512", "analysis": {"summary": "The paper introduces EchoDistill, a bidirectional concept distillation framework that jointly trains a multi-step teacher diffusion model and a one-step student model to enable rapid personalization of text-to-image diffusion models. By sharing the text encoder and employing adversarial and alignment losses, the student learns novel concepts in a single step, while feedback from the student also refines the teacher, achieving superior personalization performance.", "summary_cn": "本文提出 EchoDistill，一种双向概念蒸馏框架，通过同步训练多步教师扩散模型和一步学生模型，实现文本到图像扩散模型的快速个性化。两者共享文本编码器，并使用对抗损失和对齐损失，使学生在单步生成中学习新概念，同时学生的反馈也优化教师模型，从而在个性化任务中取得显著提升。", "keywords": "diffusion personalization, one-step diffusion, concept distillation, bidirectional distillation, text-to-image generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yixiong Yang", "Tao Wu", "Senmao Li", "Shiqi Yang", "Yaxing Wang", "Joost van de Weijer", "Kai Wang"]}, "usage": {"completion_tokens": 634, "prompt_tokens": 3480, "total_tokens": 4114, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 425, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061836, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030136, "upstream_inference_completions_cost": 0.000317}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:13.826232", "feed": "arxiv_cscv", "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects", "link": "https://papers.cool/arxiv/2510.20605", "analysis": {"summary": "OnlineSplatter is an online feed-forward framework that reconstructs free-moving objects from monocular video by directly generating object-centric 3D Gaussians without requiring camera pose, depth priors, or bundle adjustment. It uses a dual-key memory module to fuse current frame features with temporally aggregated object states, enabling constant computational cost while progressively refining the reconstruction. Experiments on real-world datasets show significant improvements over pose-free baselines, with performance increasing as more observations are added.", "summary_cn": "OnlineSplatter 是一个在线前馈框架，能够在无需相机位姿、深度先验或束束优化的情况下，仅通过单目视频直接生成以对象为中心的 3D 高斯体，实现自由移动对象的重建。该方法采用双键记忆模块，将当前帧特征与时间聚合的对象状态进行融合，从而在保持计算成本恒定的同时逐步细化重建。真实数据集实验表明，相较于已有的无姿态重建基线，OnlineSplatter 在观测数量增加时持续提升性能。", "keywords": "online 3D reconstruction, pose-free, Gaussian splatting, monocular video, memory module, object-centric representation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mark He Huang", "Lin Geng Foo", "Christian Theobalt", "Ying Sun", "De Wen Soh"]}, "usage": {"completion_tokens": 686, "prompt_tokens": 3391, "total_tokens": 4077, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 395, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028801, "upstream_inference_completions_cost": 0.000343}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:14.053497", "feed": "arxiv_cscv", "title": "Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis", "link": "https://papers.cool/arxiv/2510.20531", "analysis": {"summary": "The paper introduces the Fake-in-Facext (FiFa) framework to provide fine‑grained, explainable DeepFake analysis by defining a Facial Image Concept Tree (FICT) for reliable annotation and proposing the Artifact‑Grounding Explanation (AGE) task that outputs textual forgery explanations together with segmentation masks of manipulated artifacts. FiFa‑MLLM, a unified multi‑task multimodal LLM architecture, leverages this annotated data and auxiliary supervision to support diverse multimodal inputs/outputs and achieves state‑of‑the‑art performance on AGE and existing XDFA benchmarks.", "summary_cn": "本文提出了Fake-in-Facext（FiFa）框架，通过构建面部图像概念树（FICT）实现细粒度、可靠的伪造标注，并引入Artifact‑Grounding Explanation（AGE）任务，使文本伪造解释与操作伪造区域的分割掩码交叉输出。基于此数据，FiFa‑MLLM 采用统一的多任务多模态大语言模型结构，支持丰富的输入输出方式，在AGE任务以及现有可解释深度伪造分析（XDFA）数据集上实现了最先进的性能。", "keywords": "deepfake detection, explainable AI, multimodal large language models, artifact grounding, facial image concept tree, segmentation masks, fine-grained analysis, multi-task learning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Lixiong Qin", "Yang Zhang", "Mei Wang", "Jiani Hu", "Weihong Deng", "Weiran Xu"]}, "usage": {"completion_tokens": 705, "prompt_tokens": 3510, "total_tokens": 4215, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 406, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065836, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030586, "upstream_inference_completions_cost": 0.0003525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:14.684480", "feed": "arxiv_cscv", "title": "Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image", "link": "https://papers.cool/arxiv/2510.20539", "analysis": {"summary": "The paper introduces a deep learning framework that jointly estimates the latent sharp image and the underlying 3D camera motion trajectory from a single motion-blurred photograph. Leveraging a differentiable Projective Motion Blur Model, the system provides interpretability by revealing the blur-causing motion and enables reconstruction of the sequence of sharp frames, with post‑inference trajectory refinement via a reblur loss.", "summary_cn": "本文提出一个深度学习框架，能够从单张运动模糊图像中同时估计清晰图像和相机的 3D 旋转轨迹。该方法基于可微分的投影运动模糊模型，通过后处理的重模糊损失优化轨迹，实现对模糊成因的可解释化以及序列锐化图像的重建。", "keywords": "blind deblurring, camera motion trajectory, projective motion blur model, differentiable blur module, deep learning, image restoration, motion blur estimation, reblur loss, trajectory reconstruction", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guillermo Carbajal", "Andrés Almansa", "Pablo Musé"]}, "usage": {"completion_tokens": 898, "prompt_tokens": 3426, "total_tokens": 4324, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 746, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00074226, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029326, "upstream_inference_completions_cost": 0.000449}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:14.690015", "feed": "arxiv_cscv", "title": "ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology", "link": "https://papers.cool/arxiv/2510.20754", "analysis": {"summary": "The paper introduces ACS‑SegNet, an attention‑based dual‑encoder network that combines a CNN and a Vision Transformer to fuse features for semantic tissue segmentation in histopathology images. Experiments on the GCPS and PUMA datasets show improved μIoU and μDice scores compared with existing methods. The code is released publicly.", "summary_cn": "本文提出 ACS‑SegNet，一种基于注意力的双编码器网络，将卷积神经网络 (CNN) 与视觉 Transformer (ViT) 融合以实现组织学图像的语义分割。在 GCPS 与 PUMA 数据集上的实验显示，该模型在 μIoU 与 μDice 指标上均优于现有方法。代码已在 GitHub 开源。", "keywords": "tissue segmentation, histopathology, attention mechanism, dual-encoder, CNN, Vision Transformer, semantic segmentation, medical imaging", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nima Torbati", "Anastasia Meshcheryakova", "Ramona Woitek", "Diana Mechtcheriakova", "Amirreza Mahbod"]}, "usage": {"completion_tokens": 839, "prompt_tokens": 3382, "total_tokens": 4221, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 679, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070616, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028666, "upstream_inference_completions_cost": 0.0004195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:14.695688", "feed": "arxiv_cscv", "title": "From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail", "link": "https://papers.cool/arxiv/2510.20558", "analysis": {"summary": "The paper studies how users perceive visual quality of crowd character representations at varying levels of detail (LoD) and viewing distances. It compares geometric meshes, image‑based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, providing both qualitative and quantitative insights to inform perceptually optimized LoD strategies for crowd rendering.", "summary_cn": "本文调查了用户在不同细节层次（LoD）和观看距离下对人群角色表现的视觉质量感知。通过比较几何网格、基于图像的 impostor、NeRF（神经辐射场）和 3D Gaussian 等表示方式，提供了定性和定量结果，为人群渲染的感知优化 LoD 策略提供指导。", "keywords": "crowd rendering, level of detail, LoD, Neural Radiance Fields, 3D Gaussians, impostors, perceptual evaluation, visual fidelity, performance trade-offs", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaohan Sun", "Carol O'Sullivan"]}, "usage": {"completion_tokens": 553, "prompt_tokens": 3284, "total_tokens": 3837, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 290, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00054846, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027196, "upstream_inference_completions_cost": 0.0002765}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.125684", "feed": "arxiv_cscv", "title": "AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models", "link": "https://papers.cool/arxiv/2510.20348", "analysis": {"summary": "AccuQuant is a post‑training quantization technique for diffusion models that explicitly simulates multiple denoising steps during quantization to reduce the accumulation of quantization errors. By minimizing the discrepancy between the full‑precision and quantized model over a few steps and using an O(1) memory objective, it achieves efficient and accurate quantized diffusion sampling.", "summary_cn": "AccuQuant 是一种针对扩散模型的后训练量化方法，通过显式模拟多个去噪步骤来减小量化误差的累计。该方法在少数去噪步骤上最小化全精度模型与量化模型的输出差异，并采用 O(1) 内存复杂度的目标，实现了高效且精确的量化采样。", "keywords": "diffusion models, post-training quantization, AccuQuant, error accumulation, denoising steps, memory efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Seunghoon Lee", "Jeongwoo Choi", "Byunggwan Son", "Jaehyeon Moon", "Jeimin Jeon", "Bumsub Ham"]}, "usage": {"completion_tokens": 808, "prompt_tokens": 3417, "total_tokens": 4225, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 673, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069591, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029191, "upstream_inference_completions_cost": 0.000404}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.185505", "feed": "arxiv_cscv", "title": "SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding", "link": "https://papers.cool/arxiv/2510.20622", "analysis": {"summary": "The paper proposes SeViCES, a training‑free, model‑agnostic framework that selects informative video frames by combining a temporal‑aware semantic branch (LLM reasoning over captions) with a cluster‑guided visual branch aligned via mutual information, and refines answers through an answer‑consensus module. Extensive experiments on long video understanding benchmarks demonstrate that this consensus‑driven evidence selection improves accuracy and robustness of Video‑LLMs on lengthy videos.", "summary_cn": "本文提出了 SeViCES 框架，它通过（1）基于 LLM 对字幕进行推理的时序感知语义分支和（2）通过互信息对齐的聚类引导视觉分支，统一进行语义‑视觉证据共识以选择关键帧，并通过答案共识细化模块融合两种预测以消除不一致。实验表明，该方法在长视频理解基准上显著提升了 Video‑LLM 的准确性和鲁棒性。", "keywords": "long video understanding, video-LLM, frame selection, semantic-visual consensus, evidence selection, mutual information, multimodal reasoning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuan Sheng", "Yanbin Hao", "Chenxu Li", "Shuo Wang", "Xiangnan He"]}, "usage": {"completion_tokens": 587, "prompt_tokens": 3455, "total_tokens": 4042, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 324, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059111, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029761, "upstream_inference_completions_cost": 0.0002935}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.194456", "feed": "arxiv_cscv", "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas", "link": "https://papers.cool/arxiv/2510.20820", "analysis": {"summary": "LayerComposer introduces a layered canvas representation where each subject occupies its own layer, enabling occlusion-free composition in personalized text-to-image generation. A locking mechanism preserves selected layers with high fidelity while allowing other layers to adapt to context, requiring no architectural changes. Experiments show improved spatial control and identity preservation compared to existing multi-subject generation methods.", "summary_cn": "LayerComposer 提出将每个主体放置在独立图层上的层叠画布表示，实现无遮挡的个性化文本到图像生成。锁定机制在保持选定层高保真度的同时，使其他层能够灵活适应周围环境，且无需修改模型结构。实验表明该方法在空间控制和身份保留方面优于现有的多主体生成技术。", "keywords": "personalized text-to-image generation, layered canvas, multi-subject composition, locking mechanism, spatial control, identity preservation, diffusion models, positional embeddings", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Guocheng Gordon Qian", "Ruihang Zhang", "Tsai-Shien Chen", "Yusuf Dalva", "Anujraaj Argo Goyal", "Willi Menapace", "Ivan Skorokhodov", "Meng Dong", "Arpit Sahni", "Daniil Ostashev", "Ju Hu", "Sergey Tulyakov", "Kuan-Chieh Jackson Wang"]}, "usage": {"completion_tokens": 617, "prompt_tokens": 3384, "total_tokens": 4001, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 415, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059546, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028696, "upstream_inference_completions_cost": 0.0003085}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.267201", "feed": "arxiv_cscv", "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward", "link": "https://papers.cool/arxiv/2510.20696", "analysis": {"summary": "The paper systematically diagnoses failure modes of state-of-the-art vision-language models, highlighting visual hallucinations and over-reliance on textual priors, and introduces an agent-based architecture that couples LLM reasoning with lightweight visual modules for iterative refinement of reasoning chains. Experiments on MMMU and MathVista show significant performance gains over strong baselines, and the authors release their diagnostic framework to support future research.", "summary_cn": "本文系统性地诊断了最先进视觉语言模型的失效模式，揭示了视觉幻觉和对文本先验的过度依赖，并提出一种基于代理的架构，将大语言模型的推理与轻量级视觉模块结合，实现推理链的细化与迭代。实验在 MMMU 与 MathVista 上取得显著提升，作者同时开源了评估框架以促进后续研究。", "keywords": "multimodal large language models, visual reasoning, chain-of-thought, visual hallucination, agent-based architecture, evaluation framework, tool integration", "scoring": {"interpretability": 6, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Jing Bi", "Guangyu Sun", "Ali Vosoughi", "Chen Chen", "Chenliang Xu"]}, "usage": {"completion_tokens": 842, "prompt_tokens": 3370, "total_tokens": 4212, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 683, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070586, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028486, "upstream_inference_completions_cost": 0.000421}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.297885", "feed": "arxiv_cscv", "title": "Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition", "link": "https://papers.cool/arxiv/2510.20284", "analysis": {"summary": "The paper introduces the Knowledge-Informed Neural Network (KINN), a lightweight architecture for complex-valued SAR image recognition that incorporates physics-guided compression via a dictionary processor and a compression‑aggregation‑compression pipeline, instantiated in both CNN and Vision Transformer variants. Extensive experiments on five SAR benchmarks demonstrate state‑of‑the‑art parameter‑efficient performance, strong generalization under data scarcity and domain shift, and enhanced interpretability through physically grounded representations. The work positions KINN as a solution to the trilemma of generalization, interpretability, and efficiency in SAR image analysis.", "summary_cn": "本文提出了 Knowledge‑Informed Neural Network (KINN)，一种轻量级的复值 SAR 图像识别架构，利用字典处理器进行物理引导的压缩，并采用压缩‑聚合‑压缩的流程，分别实现了 CNN（0.7M）和 Vision Transformer（0.95M）版本。 在五个 SAR 基准上的大量实验表明，KINN 在参数效率、数据稀缺及分布外场景下的泛化能力以及通过物理特征实现的可解释性方面均达到了最新水平。 此工作旨在解决 SAR 图像分析中泛化、可解释性和效率三者的冲突，实现可信的 AI 应用。", "keywords": "complex-valued SAR, physics-informed neural network, knowledge-informed neural network, compression-aggregation-compression, interpretability, parameter-efficient, out-of-distribution, vision transformer, synthetic aperture radar, dictionary processor", "scoring": {"interpretability": 7, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Haodong Yang", "Zhongling Huang", "Shaojie Guo", "Zhe Zhang", "Gong Cheng", "Junwei Han"]}, "usage": {"completion_tokens": 820, "prompt_tokens": 3483, "total_tokens": 4303, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 506, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071181, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030181, "upstream_inference_completions_cost": 0.00041}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.492982", "feed": "arxiv_cscv", "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation", "link": "https://papers.cool/arxiv/2510.20812", "analysis": {"summary": "The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight visual-language model draft experts to generate diverse reasoning paths and a strong verdict model to synthesize these paths into final answers for information‑intensive visual question answering. A consensus expert selection step forwards only high‑agreement drafts, improving both accuracy and computational cost on benchmarks like InfographicVQA, ChartMuseum, ChartQAPro, and HR‑Bench 4K. SV demonstrates that aggregating partially correct reasoning can correct errors and rival larger proprietary models without additional training.", "summary_cn": "本文提出了“Speculative Verdict (SV)”框架，通过多个轻量级视觉语言模型草稿专家生成多样化的推理路径，再由强大的判决模型综合这些路径，以回答信息密集型视觉问答任务。共识专家选择机制仅转发高一致性的草稿，从而在提升准确性的同时显著降低计算成本，在 InfographicVQA、ChartMuseum、ChartQAPro 与 HR‑Bench 4K 等基准上取得了持续提升。该方法展示了聚合部分正确推理可纠错，并在无需额外训练的情况下与大型专有模型竞争。", "keywords": "visual question answering, information-intensive images, speculative decoding, draft experts, verdict model, multimodal reasoning, efficiency, consensus selection", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"]}, "usage": {"completion_tokens": 921, "prompt_tokens": 3449, "total_tokens": 4370, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 675, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00075721, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029671, "upstream_inference_completions_cost": 0.0004605}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:15.930013", "feed": "arxiv_cscv", "title": "HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification", "link": "https://papers.cool/arxiv/2510.20669", "analysis": {"summary": "This paper introduces HybridSOMSpikeNet, a hybrid deep learning framework that combines a pre‑trained ResNet‑152 backbone, a differentiable Soft‑SOM for topological clustering, and a spiking neural head to perform ten‑class waste classification with 97.39% accuracy. The Soft‑SOM component enhances interpretability of the learned representations, while the spiking dynamics improve robustness and enable energy‑efficient inference suitable for deployment in real‑world waste management systems. The approach aims to reduce waste misclassification, supporting broader sustainability goals such as improved recycling rates and lower environmental impact.", "summary_cn": "本文提出 HybridSOMSpikeNet—a hybrid deep learning 框架，结合预训练的 ResNet‑152 特征提取、可微分的 Soft‑SOM（软自组织映射）用于拓扑聚类，以及脉冲神经网络头部，以实现十类垃圾分类并达到 97.39% 的测试准确率。Soft‑SOM 部分提升了模型内部表征的可解释性，脉冲动力学则增强了鲁棒性并实现了能耗高效的推理，适合实际垃圾管理部署。该方法旨在降低垃圾误分类，提升回收效率，进而支持更广泛的可持续发展目标，如降低环境影响。", "keywords": "waste classification, hybrid deep learning, differentiable soft self-organizing map, spiking neural network, ResNet-152, sustainability, environmental AI, topological clustering, energy-efficient inference", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "interpretability"}, "authors": ["Debojyoti Ghosh", "Adrijit Goswami"]}, "usage": {"completion_tokens": 1005, "prompt_tokens": 3483, "total_tokens": 4488, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 756, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00080431, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030181, "upstream_inference_completions_cost": 0.0005025}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.186618", "feed": "arxiv_cscv", "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model", "link": "https://papers.cool/arxiv/2510.20803", "analysis": {"summary": "ARGenSeg introduces an autoregressive generation-based paradigm for image segmentation that leverages multimodal large language models (MLLMs) to output visual tokens which are detokenized via a universal VQ-VAE, producing dense pixel-level masks. By employing a next-scale-prediction strategy, the approach reduces inference latency while achieving state-of-the-art accuracy and speed on multiple segmentation benchmarks.", "summary_cn": "ARGenSeg 提出一种基于自回归生成的图像分割范式，利用多模态大语言模型（MLLM）输出视觉令牌，并通过通用 VQ-VAE 解码为密集的像素级掩码。通过下一尺度预测策略，该方法在降低推理延迟的同时，在多个分割数据集上实现了领先的精度和速度。", "keywords": "image segmentation, autoregressive generation, multimodal large language model, VQ-VAE, dense masks, pixel-level understanding, next-scale prediction", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaolong Wang", "Lixiang Ru", "Ziyuan Huang", "Kaixiang Ji", "Dandan Zheng", "Jingdong Chen", "Jun Zhou"]}, "usage": {"completion_tokens": 651, "prompt_tokens": 3405, "total_tokens": 4056, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 442, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061561, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029011, "upstream_inference_completions_cost": 0.0003255}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.207009", "feed": "arxiv_cscv", "title": "Reliable and Reproducible Demographic Inference for Fairness in Face Analysis", "link": "https://papers.cool/arxiv/2510.20482", "analysis": {"summary": "The paper introduces a fully reproducible demographic attribute inference pipeline for fairness auditing in face analysis systems, using modular transfer learning with pretrained face recognition encoders and nonlinear classification heads. It defines a robustness metric based on intra‑identity consistency and shows improved accuracy and lower bias, especially for ethnicity inference, across multiple datasets. The authors release code, pretrained models, and evaluation tools to promote transparency and reproducibility.", "summary_cn": "本文提出了一套可完全复现的人口属性推断流水线，用于人脸分析系统的公平性审计，该流水线采用预训练人脸识别编码器加非线性分类头的模块化迁移学习方法。文中定义了基于同一身份内部一致性的鲁棒性度量，并在多个数据集上展示了尤其在种族属性推断上更高的准确率和更低的偏差。作者公开了数据集元信息、代码、预训练模型以及评估工具，以提升透明度和可复现性。", "keywords": "demographic inference, fairness auditing, face analysis, transfer learning, robustness metric, intra-identity consistency, ethnicity classification, gender classification, reproducibility", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Alexandre Fournier-Montgieux", "Hervé Le Borgne", "Adrian Popescu", "Bertrand Luvison"]}, "usage": {"completion_tokens": 905, "prompt_tokens": 3432, "total_tokens": 4337, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 760, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00074666, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029416, "upstream_inference_completions_cost": 0.0004525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.783995", "feed": "arxiv_cscv", "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging", "link": "https://papers.cool/arxiv/2510.20639", "analysis": {"summary": "The paper introduces BTB3D, a causal convolutional encoder-decoder that produces frequency-aware volumetric tokens for 3D medical imaging, enabling high-resolution vision-language modeling without excessive memory use. A three-stage training curriculum allows the model to learn from short slice excerpts yet handle scans over 300 slices, achieving state-of-the-art performance on report generation and text-to-CT synthesis tasks. The results demonstrate that precise three-dimensional tokenization is crucial for scalable vision-language models in CT imaging.", "summary_cn": "本文提出 BTB3D，一种因果卷积编码-解码结构，生成频率感知的体积令牌，以实现高分辨率的 3D 医学影像视觉‑语言建模且不增加显存开销。通过三阶段训练课程，模型能够从短切片学习并推广到超过 300 切片的 CT 扫描，在报告生成和文本到 CT 合成任务上均取得最新的性能提升。结果表明，精确的三维令牌化是可扩展视觉‑语言模型在 CT 成像中成功的关键因素。", "keywords": "vision-language modeling, 3D medical imaging, CT, tokenization, causal convolution, report generation, text-to-CT synthesis", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Dong Yang", "Pengfei Guo", "Marc Edgar", "Daguang Xu", "Bernhard Kainz", "Bjoern Menze"]}, "usage": {"completion_tokens": 675, "prompt_tokens": 3539, "total_tokens": 4214, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 399, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064771, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031021, "upstream_inference_completions_cost": 0.0003375}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.828492", "feed": "arxiv_cscv", "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers", "link": "https://papers.cool/arxiv/2510.20807", "analysis": {"summary": "The paper introduces a pure transformer model that predicts video frames of dynamic physical simulations directly in pixel space, using causal autoregressive modeling and various spatiotemporal self‑attention layouts. Compared with latent‑space methods, it extends the horizon of physically accurate predictions by up to 50% while keeping comparable video quality scores. Additional interpretability probes reveal internal regions that encode PDE simulation parameters, and these representations generalize to out‑of‑distribution parameter estimation.", "summary_cn": "本文提出了一种纯 transformer 模型，在像素空间中进行因果自回归的视频预测，专注于动态物理仿真，并比较了多种时空自注意力布局。相较于潜在空间方法，该模型将物理上准确的预测时长提升约 50%，且在常规视频质量指标上保持可比性能。进一步的可解释性实验显示网络内部的特定区域能够编码 PDE 仿真参数，并且这种表示能够推广到分布外参数的估计。", "keywords": "video prediction, spatiotemporal transformer, physical simulation, autoregressive, pixel-space, PDE parameter estimation, interpretability, attention", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dean L Slack", "G Thomas Hudson", "Thomas Winterbottom", "Noura Al Moubayed"]}, "usage": {"completion_tokens": 695, "prompt_tokens": 3442, "total_tokens": 4137, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 495, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064316, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029566, "upstream_inference_completions_cost": 0.0003475}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.836143", "feed": "arxiv_cscv", "title": "GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection", "link": "https://papers.cool/arxiv/2510.20268", "analysis": {"summary": "The paper proposes GMFVAD, a method that generates fine-grained multi-modal features by combining visual snippets with caption text to reduce redundant information and improve video anomaly detection. By summarizing the main content of each snippet and enhancing highlighted visual portions with textual cues, the approach achieves state-of-the-art results on four benchmark datasets. Ablation studies confirm that performance gains stem from the redundancy reduction.", "summary_cn": "本文提出 GMFVAD 方法，通过将视频片段的视觉特征与原始视频字幕文本相结合，生成更细粒度的多模态特征，以降低冗余信息并提升视频异常检测效果。该方法对每个片段的主要内容进行摘要，并利用文本信息加强视觉特征的关键部分，在四个主流数据集上实现了最新的性能。消融实验表明，性能提升来源于冗余信息的减少。", "keywords": "video anomaly detection, multi-modal feature, grained feature, text-caption integration, redundancy reduction, surveillance, spatio-temporal correlation, state-of-the-art", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guangyu Dai", "Dong Chen", "Siliang Tang", "Yueting Zhuang"]}, "usage": {"completion_tokens": 686, "prompt_tokens": 3422, "total_tokens": 4108, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 455, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063566, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.000343}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.934124", "feed": "arxiv_cscv", "title": "Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval", "link": "https://papers.cool/arxiv/2510.20393", "analysis": {"summary": "The paper identifies a bias in cross‑modal representation learning for image‑to‑recipe retrieval, where visual features dominate and overlook subtle recipe‑specific ingredients and cooking actions, especially in multicultural datasets. It proposes a causal approach that predicts overlooked culinary elements and injects them into the representation learning process, demonstrating improved retrieval performance on both monolingual Recipe1M and a newly curated multilingual cuisine dataset.", "summary_cn": "本文指出图像到食谱检索的跨模态表示学习存在偏差，即视觉特征主导，忽视了食谱中特有的细微配料和烹饪步骤，尤其在多文化数据集上更为严重。作者提出一种因果方法，预测图像中可能遗漏的烹饪元素并将其注入跨模态表示学习，从而在单语言 Recipe1M 数据集和新构建的多语言文化数据集上显著提升检索性能。", "keywords": "image-to-recipe retrieval, cross-modal representation bias, causal representation learning, multicultural cuisine, ingredient prediction, multimodal retrieval", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Qing Wang", "Chong-Wah Ngo", "Yu Cao", "Ee-Peng Lim"]}, "usage": {"completion_tokens": 690, "prompt_tokens": 3421, "total_tokens": 4111, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 454, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063751, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029251, "upstream_inference_completions_cost": 0.000345}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:16.993007", "feed": "arxiv_cscv", "title": "DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering", "link": "https://papers.cool/arxiv/2510.20285", "analysis": {"summary": "The paper introduces DMC$^3$, a Dual-Modal Counterfactual Contrastive Construction framework for egocentric video question answering. It generates positive and negative counterfactual samples for both text and visual modalities via event description paraphrasing and core interaction mining, and applies contrastive learning to align original and positive samples while separating negatives, achieving state-of-the-art results on EgoTaskQA and QAEGO4D.", "summary_cn": "本文提出 DMC$^3$（Dual-Modal Counterfactual Contrastive Construction）框架，用于第一人称视频问答（Egocentric VideoQA）。通过事件描述改写和核心交互挖掘分别为文本和视觉模态生成正负对照样本，并在对比学习中使原始样本特征靠近正样本特征、远离负样本特征，从而在 EgoTaskQA 与 QAEGO4D 上实现了最新的性能。", "keywords": "egocentric video QA, counterfactual contrastive learning, dual-modal, event description paraphrasing, hand-object interaction, EgoTaskQA, QAEGO4D, video understanding", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiayi Zou", "Chaofan Chen", "Bing-Kun Bao", "Changsheng Xu"]}, "usage": {"completion_tokens": 692, "prompt_tokens": 3493, "total_tokens": 4185, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 437, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064931, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030331, "upstream_inference_completions_cost": 0.000346}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.193040", "feed": "arxiv_cscv", "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion", "link": "https://papers.cool/arxiv/2510.20766", "analysis": {"summary": "The paper proposes Dynamic Position Extrapolation (DyPE), a training-free technique that adapts a diffusion transformer's positional encodings at each diffusion step to match the evolving frequency spectrum, enabling generation of images at resolutions far beyond the model's training data without extra sampling cost. By leveraging the fact that low-frequency components converge early while high frequencies require more steps, DyPE can produce ultra‑high‑resolution outputs (e.g., 16 MP) with state‑of‑the‑art fidelity. Experiments on multiple benchmarks show consistent performance gains, especially at extreme resolutions.", "summary_cn": "本文提出了动态位置外推（DyPE）技术，它在扩散过程的每一步动态调整扩散Transformer的位置信息编码，使其频谱与当前生成阶段相匹配，从而在无需额外采样成本的情况下生成远超训练分辨率的图像。该方法利用低频结构早期收敛、高频结构需更多扩散步数的特性，实现了如1600万像素等超高分辨率图像的高保真合成。多项基准测试表明，在极高分辨率下性能提升尤为显著。", "keywords": "diffusion models, positional encoding, ultra-high-resolution image synthesis, spectral progression, diffusion transformer, DyPE", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Noam Issachar", "Guy Yariv", "Sagie Benaim", "Yossi Adi", "Dani Lischinski", "Raanan Fattal"]}, "usage": {"completion_tokens": 706, "prompt_tokens": 3414, "total_tokens": 4120, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 428, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064446, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029146, "upstream_inference_completions_cost": 0.000353}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.494387", "feed": "arxiv_cscv", "title": "Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature", "link": "https://papers.cool/arxiv/2510.20794", "analysis": {"summary": "The paper introduces a multi-object tracking framework that fuses radar and camera data, using common features to enable online calibration and autonomous association of detections. By positioning radar as a primary sensor for accurate range information and employing feature matching with category-consistency checks, the method improves tracking precision in both controlled and real traffic environments.", "summary_cn": "本文提出一种融合雷达与摄像头的多目标跟踪框架，利用两者的公共特征实现在线标定并自动关联检测结果。通过将雷达视为提供精确距离信息的核心传感器，并结合特征匹配与类别一致性检查，提升了在受控环境和真实交通场景中的跟踪精度。", "keywords": "radar-camera fusion, multi-object tracking, online calibration, common feature matching, sensor fusion, autonomous driving, perception robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Lei Cheng", "Siyang Cao"]}, "usage": {"completion_tokens": 728, "prompt_tokens": 3465, "total_tokens": 4193, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 545, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066311, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029911, "upstream_inference_completions_cost": 0.000364}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.502490", "feed": "arxiv_cscv", "title": "Positional Encoding Field", "link": "https://papers.cool/arxiv/2510.20385", "analysis": {"summary": "The paper introduces the Positional Encoding Field (PE-Field), extending traditional 2D positional encodings to a structured 3D field with depth-aware and hierarchical components, allowing diffusion transformers to reason about geometry directly in 3D space. Experiments show that DiTs retain global coherence even when positional encodings are perturbed, suggesting spatial coherence is mainly governed by the encodings, and PE-Field-augmented DiTs achieve state-of-the-art results on novel view synthesis and controllable spatial image editing.", "summary_cn": "本文提出了位置编码场（PE-Field），将传统的二维位置编码扩展到结构化的三维场，并加入深度感知和层次化编码，使扩散 Transformer 能够直接在三维空间中进行几何推理。实验表明，即使位置编码被扰动，DiT 仍能保持全局一致性，说明空间连贯性主要由编码决定，基于 PE-Field 的 DiT 在单图新视角合成和可控空间图像编辑任务上达到了最新的性能。", "keywords": "positional encoding, diffusion transformer, PE-Field, novel view synthesis, volumetric reasoning, depth-aware encoding, hierarchical encoding, 3D field, spatial image editing", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yunpeng Bai", "Haoxiang Li", "Qixing Huang"]}, "usage": {"completion_tokens": 729, "prompt_tokens": 3392, "total_tokens": 4121, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 450, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065266, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.0003645}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.537918", "feed": "arxiv_cscv", "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata", "link": "https://papers.cool/arxiv/2510.20708", "analysis": {"summary": "The paper introduces ALICE‑LRI, a sensor‑agnostic algorithm that automatically infers the intrinsic geometry of spinning LiDAR sensors and generates lossless range‑image projections without any manufacturer calibration data. By reverse‑engineering beam configurations and per‑beam corrections, the method preserves every point of the original point cloud and achieves real‑time performance, as demonstrated on the full KITTI and DurLAR datasets. A compression case study further shows downstream benefits of the perfect geometric preservation.", "summary_cn": "本文提出 ALICE‑LRI，一种无需制造商标定文件即可自动推断旋转 LiDAR 传感器固有几何并生成无信息损失的范围图像的通用算法。该方法通过逆向解析激光束配置和每束校正，实现了对原始点云的完整点保留，并在 KITTI 与 DurLAR 数据集上展示了实时性能。压缩案例研究进一步验证了几何完全保留在下游应用中的显著质量提升。", "keywords": "LiDAR, range image, lossless projection, intrinsic calibration, point cloud, sensor-agnostic, compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Samuel Soutullo", "Miguel Yermo", "David L. Vilariño", "Óscar G. Lorenzo", "José C. Cabaleiro", "Francisco F. Rivera"]}, "usage": {"completion_tokens": 731, "prompt_tokens": 3485, "total_tokens": 4216, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 507, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066761, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030211, "upstream_inference_completions_cost": 0.0003655}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.546775", "feed": "arxiv_cscv", "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning", "link": "https://papers.cool/arxiv/2510.20286", "analysis": {"summary": "This paper introduces the Instruction-as-Reasoning paradigm for GUI grounding, treating natural-language instructions as dynamic reasoning pathways and training models to select the most effective one. A two-stage framework (supervised fine-tuning on diverse synthesized instructions followed by reinforcement-learning-based pathway selection) yields UI-Ins-7B/32B models that achieve state-of-the-art accuracy on several UI grounding benchmarks and demonstrate emergent reasoning and strong agentic performance.", "summary_cn": "本文提出了“指令即推理”(Instruction-as-Reasoning)范式，用于 GUI 定位任务，将自然语言指令视为可动态选择的推理路径，并通过监督微调多样化指令和随后强化学习的两阶段训练，使模型能够在推理时挑选最有效的路径。基于此的 UI-Ins-7B/32B 在多个 GUI 定位基准上取得领先表现，并展现出新颖的推理组合能力和较强的代理潜力。", "keywords": "GUI grounding, instruction-as-reasoning, multi-perspective instructions, reinforcement learning, UI agents, grounding benchmarks", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liangyu Chen", "Hanzhang Zhou", "Chenglin Cai", "Jianan Zhang", "Panrong Tong", "Quyu Kong", "Xu Zhang", "Chen Liu", "Yuqi Liu", "Wenxuan Wang", "Yue Wang", "Qin Jin", "Steven Hoi"]}, "usage": {"completion_tokens": 1031, "prompt_tokens": 3561, "total_tokens": 4592, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 900, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00082901, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031351, "upstream_inference_completions_cost": 0.0005155}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:17.930695", "feed": "arxiv_cscv", "title": "Causal Debiasing for Visual Commonsense Reasoning", "link": "https://papers.cool/arxiv/2510.20281", "analysis": {"summary": "The paper studies bias in Visual Commonsense Reasoning (VCR) datasets, identifying co-occurrence and statistical shortcuts in both text and image modalities. It introduces VCR-OOD benchmarks to evaluate out-of-distribution generalization and applies a causal backdoor adjustment using a dictionary of correct answers to remove prediction shortcuts, showing improved performance across datasets.", "summary_cn": "本文分析了视觉常识推理（VCR）数据集中的共现偏差和统计偏差，指出了文本和视觉两方面的预测捷径。作者构建了 VCR-OOD 基准用于评估模型在两模态上的分布外泛化能力，并采用基于正确答案字典的因果后门调整方法消除预测捷径，实验证明该去偏方法在多个数据集上均有效。", "keywords": "causal debiasing, visual commonsense reasoning, VCR, OOD dataset, backdoor adjustment, dataset bias, multimodal reasoning, shortcut mitigation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jiayi Zou", "Gengyun Jia", "Bing-Kun Bao"]}, "usage": {"completion_tokens": 754, "prompt_tokens": 3346, "total_tokens": 4100, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 530, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065826, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028126, "upstream_inference_completions_cost": 0.000377}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:18.624824", "feed": "arxiv_cscv", "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models", "link": "https://papers.cool/arxiv/2510.20322", "analysis": {"summary": "HyperET introduces an efficient training paradigm that leverages hyperbolic space to align visual representations with textual counterparts across arbitrary granularity levels in multi-modal large language models. By employing learnable matrices with Möbius multiplication (diagonal, block‑diagonal, and banded configurations) and dynamic hyperbolic radius adjustment, the method improves existing MLLM pre‑training and fine‑tuning with less than 1% additional parameters.", "summary_cn": "HyperET 提出一种高效的训练范式，利用超曲面空间在任意粒度层次上对齐视觉表示与文本对应，从而提升多模态大型语言模型的跨模态对齐。该方法通过可学习矩阵的 Möbius 乘法（对角、块对角、带状）以及动态超曲面半径调节，实现了在几乎不增加参数（<1%）的情况下显著提升现有模型的预训练和微调效果。", "keywords": "hyperbolic embeddings, multi-modal LLM, vision-language alignment, Möbius multiplication, hierarchical representations, efficient training, hyperbolic radius adjustment, parameter-efficient, CLIP, SAM", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zelin Peng", "Zhengqin Xu", "Qingyang Liu", "Xiaokang Yang", "Wei Shen"]}, "usage": {"completion_tokens": 787, "prompt_tokens": 3461, "total_tokens": 4248, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 556, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069201, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029851, "upstream_inference_completions_cost": 0.0003935}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:18.657171", "feed": "arxiv_cscv", "title": "Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation", "link": "https://papers.cool/arxiv/2510.20596", "analysis": {"summary": "This paper proposes a novel unsupervised domain adaptation framework for cross-modality segmentation that leverages similarity‑based class‑wise prototypes. It learns prototypes in an embedding space with a similarity constraint to ensure intra‑class representativeness and inter‑class separability, and uses a dictionary to store prototypes from multiple images, avoiding class‑missing issues and enabling contrastive prototype learning. Extensive experiments demonstrate superior performance over state‑of‑the‑art methods.", "summary_cn": "本文提出一种基于相似性原型的跨模态分割无监督领域自适应框架。通过在嵌入空间学习类别原型并施加相似性约束，使原型对每个语义类别具有代表性且相互可分，同时使用字典存储不同图像提取的原型以防止类别缺失并实现原型的对比学习，从而提升分割性能。实验表明该方法优于现有最先进技术。", "keywords": "unsupervised domain adaptation, cross-modality segmentation, similarity-based prototypes, contrastive learning, prototype dictionary, class-wise prototypes, domain shift, embedding space, segmentation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ziyu Ye", "Chen Ju", "Chaofan Ma", "Xiaoyun Zhang"]}, "usage": {"completion_tokens": 786, "prompt_tokens": 3352, "total_tokens": 4138, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 557, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067516, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028216, "upstream_inference_completions_cost": 0.000393}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:18.689936", "feed": "arxiv_cscv", "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence", "link": "https://papers.cool/arxiv/2510.20578", "analysis": {"summary": "EmbodiedBrain is a vision-language foundation model (7B/32B) designed for embodied AI agents, featuring an agent-aligned data structure and a novel Step‑Augmented Group Relative Policy Optimization (Step‑GRPO) training pipeline that incorporates preceding steps as guided precursors. The paper introduces a comprehensive reward system with a Generative Reward Model and a three‑part evaluation suite covering general, planning, and end‑to‑end simulation benchmarks, demonstrating state‑of‑the‑art performance on a newly released challenging simulation environment.", "summary_cn": "EmbodiedBrain 是一种面向具身 AI 代理的视觉语言基础模型（7B/32B），采用面向代理的数据结构和新颖的 Step‑Augmented Group Relative Policy Optimization（Step‑GRPO）训练流程，将前置步骤作为引导前体来提升长程任务成功率。论文还引入了包含生成式奖励模型的综合奖励系统，并构建了包括通用、规划和端到端仿真三部分的评估套件，在新发布的挑战性仿真环境中展示了最先进的性能。", "keywords": "Embodied AI, task planning, vision-language model, multimodal LLM, Step-GRPO, Generative Reward Model, simulation benchmark, embodied foundation model", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Ding Zou", "Feifan Wang", "Mengyu Ge", "Siyuan Fan", "Zongbing Zhang", "Wei Chen", "Lingfeng Wang", "Zhongyou Hu", "Wenrui Yan", "Zhengwei Gao", "Hao Wang", "Weizhao Jin", "Yu Zhang", "Hainan Zhao", "Mingliang Zhang", "Xianxian Xi", "Yaru Zhang", "Wenyuan Li", "Zhengguang Gao", "Yurui Zhu"]}, "usage": {"completion_tokens": 796, "prompt_tokens": 3522, "total_tokens": 4318, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 517, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070566, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030766, "upstream_inference_completions_cost": 0.000398}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.133370", "feed": "arxiv_cscv", "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence", "link": "https://papers.cool/arxiv/2510.20470", "analysis": {"summary": "Conan introduces an evidence‑grounded, multi‑step video reasoning framework that jointly learns to identify relevant frames, reason over cross‑frame clues, and decide when to act, using a large Conan‑91K dataset of automatically generated reasoning traces and an Identification‑Reasoning‑Action RLVR training scheme. The progressive cold‑start strategy enables the model to surpass strong baselines by over 10% accuracy on several multi‑step reasoning benchmarks and generalize to long‑video understanding tasks. Experiments demonstrate the system’s scalability and robustness compared with prior text‑only chain or frame‑retrieval methods.", "summary_cn": "Conan 提出一个基于证据的多步骤视频推理框架，能够同时学习识别相关帧、跨帧推理线索以及决定何时采取行动，使用大规模的 Conan‑91K 数据集（包括帧识别、证据推理和行动决策的自动生成推理轨迹），并采用 Identification‑Reasoning‑Action (AIR) 强化学习训练方案。通过渐进式冷启动策略，模型在多个多步骤推理基准上相较于强基线提升超过 10% 的准确率，并在长视频理解任务中表现出良好的可扩展性和鲁棒性。实验表明该系统在证据定位和推理方面优于仅文本链或单帧检索的已有方法。", "keywords": "video reasoning, multimodal LLM, evidence grounding, reinforcement learning, progressive learning, Conan-91K dataset, multi-step deduction, visual evidence, AIR framework, long-video understanding", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Kun Ouyang", "Yuanxin Liu", "Linli Yao", "Yishuo Cai", "Hao Zhou", "Jie Zhou", "Fandong Meng", "Xu Sun"]}, "usage": {"completion_tokens": 823, "prompt_tokens": 3444, "total_tokens": 4267, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 502, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070746, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029596, "upstream_inference_completions_cost": 0.0004115}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.199242", "feed": "arxiv_cscv", "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling", "link": "https://papers.cool/arxiv/2510.20206", "analysis": {"summary": "RAPO++ proposes a cross-stage prompt optimization framework for text-to-video diffusion models, combining retrieval-augmented prompt enrichment, test-time iterative refinement using multi-source feedback, and LLM fine-tuning to internalize optimization patterns, thereby improving semantic alignment, compositionality, and temporal coherence without altering the generative backbone.", "summary_cn": "RAPO++ 提出了一种跨阶段提示优化框架，用于文本到视频的扩散模型。该框架先通过检索增强的方式丰富用户提示，使其与训练数据对齐；随后在推理时利用多源反馈（如语义对齐、空间忠实度、时间一致性等）迭代优化提示；最后通过微调大型语言模型，使其在推理前即可生成高质量提示，从而提升生成视频的语义匹配、组合推理和时间一致性。", "keywords": "text-to-video generation, prompt optimization, diffusion models, retrieval-augmented prompts, large language model fine-tuning, temporal coherence, compositional reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bingjie Gao", "Qianli Ma", "Xiaoxue Wu", "Shuai Yang", "Guanzhou Lan", "Haonan Zhao", "Jiaxuan Chen", "Qingyang Liu", "Yu Qiao", "Xinyuan Chen", "Yaohui Wang", "Li Niu"]}, "usage": {"completion_tokens": 576, "prompt_tokens": 3555, "total_tokens": 4131, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 306, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060061, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031261, "upstream_inference_completions_cost": 0.000288}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.465902", "feed": "arxiv_cscv", "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation", "link": "https://papers.cool/arxiv/2510.20238", "analysis": {"summary": "COS3D introduces a collaborative prompt-segmentation framework that jointly leverages an instance field and a language field to improve open-vocabulary 3D segmentation. The method learns an instance-to-language feature mapping with a two-stage training strategy and refines prompts during inference via adaptive language-to-instance refinement, achieving state-of-the-art results on benchmark datasets and enabling applications such as image-based 3D segmentation and robotics.", "summary_cn": "COS3D 提出一种协作式提示分割框架，通过同时利用实例域（instance field）和语言域（language field）来提升开放词汇的 3D 分割效果。该方法在训练阶段学习实例到语言的特征映射，并采用两阶段训练策略；在推理阶段通过自适应语言到实例的提示细化实现高质量分割。实验表明其在多个基准上表现领先，并具备图像驱动 3D 分割、层级分割和机器人等应用潜力。", "keywords": "open-vocabulary 3D segmentation, collaborative field, Gaussian splatting, instance-to-language mapping, prompt refinement, multi-modal segmentation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Runsong Zhu", "Ka-Hei Hui", "Zhengzhe Liu", "Qianyi Wu", "Weiliang Tang", "Shi Qiu", "Pheng-Ann Heng", "Chi-Wing Fu"]}, "usage": {"completion_tokens": 646, "prompt_tokens": 3473, "total_tokens": 4119, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 384, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062331, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030031, "upstream_inference_completions_cost": 0.000323}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.472582", "feed": "arxiv_cscv", "title": "SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution", "link": "https://papers.cool/arxiv/2510.20814", "analysis": {"summary": "SpectraMorph is a physics‑guided self‑supervised framework that fuses low‑resolution hyperspectral images with high‑resolution multispectral (or pan‑chromatic) data using a structured latent space that enforces an unmixing bottleneck: endmember signatures are extracted from the hyperspectral image and a compact MLP predicts abundance‑like maps from the multispectral image, with reconstruction via linear mixing. The approach yields interpretable intermediate representations, trains in under a minute, and remains robust even when the multispectral sensor has only a single band, achieving state‑of‑the‑art performance on synthetic and real datasets. Experiments demonstrate consistent improvements over unsupervised baselines and competitive results compared to supervised methods.", "summary_cn": "SpectraMorph 是一种基于物理的自监督融合框架，利用结构化潜在空间强制实现解混瓶颈：从低分辨率高光谱图像提取端元特征，并通过紧凑的多层感知机从多光谱（或全色）图像预测类丰度图，随后通过线性混合重建光谱。该方法产生可解释的中间表示，训练时间不到一分钟，即使多光谱仅有单波段也能保持鲁棒性，并在合成和真实数据集上超越最新的无监督基线，且与监督方法竞争。实验表明其在超分辨率任务上持续取得领先性能。", "keywords": "hyperspectral super-resolution, self-supervised fusion, structured latent space, unmixing bottleneck, interpretability, physics-guided learning, abundance maps, multispectral, pan-chromatic, latent learning", "scoring": {"interpretability": 7, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Ritik Shah", "Marco F Duarte"]}, "usage": {"completion_tokens": 837, "prompt_tokens": 3452, "total_tokens": 4289, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 482, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071566, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029716, "upstream_inference_completions_cost": 0.0004185}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.552373", "feed": "arxiv_cscv", "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization", "link": "https://papers.cool/arxiv/2510.20291", "analysis": {"summary": "The paper introduces a parameter-efficient Mixture-of-Experts framework for cross-modal geo-localization, winning the RoboSense 2025 Track 4 competition. It tackles severe inter-platform heterogeneity and domain gaps between generic training captions and platform-specific queries through a domain-aligned preprocessing pipeline and platform-specific experts, using LLM-based caption refinement, BGE-M3 (text) and EVA-CLIP (image) encoders, and a progressive hard-negative mining strategy.", "summary_cn": "本文提出了一种参数高效的 Mixture-of-Experts 框架用于跨模态地理定位，并在 RoboSense 2025 第四赛道中取得冠军。针对平台间的异构性和通用训练描述与特定测试查询之间的域差距，文中设计了域对齐的预处理流程和平台专属专家，包括基于大语言模型的字幕细化、BGE-M3（文本）与 EVA-CLIP（图像）编码器，以及渐进式硬负例挖掘策略，以提升检索的辨别能力。", "keywords": "cross-modal geo-localization, mixture of experts, domain adaptation, caption refinement, hard-negative mining, satellite imagery, drone navigation, parameter-efficient, multimodal retrieval", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["LinFeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li"]}, "usage": {"completion_tokens": 844, "prompt_tokens": 3392, "total_tokens": 4236, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 610, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071016, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.000422}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.706346", "feed": "arxiv_cscv", "title": "Calibrating Multimodal Consensus for Emotion Recognition", "link": "https://papers.cool/arxiv/2510.20256", "analysis": {"summary": "The paper proposes Calibrated Multimodal Consensus (CMC), a framework for multimodal emotion recognition that generates pseudo unimodal labels via a Pseudo Label Generation Module and employs a Parameter‑free Fusion Module together with a Multimodal Consensus Router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four benchmark datasets show that CMC matches or exceeds state‑of‑the‑art performance, especially in scenarios with conflicting cues.", "summary_cn": "本文提出了“校准多模态共识”(Calibrated Multimodal Consensus, CMC)框架，用伪标签生成模块(Pseudo Label Generation Module)生成单模态伪标签，并通过无参数融合模块(Parameter‑free Fusion Module)和多模态共识路由器(Multimodal Consensus Router)降低文本模态的主导性，缓解跨模态之间的语义不一致。在四个基准数据集上的实验表明，CMC 在整体性能上与最先进方法持平或更优，特别是在存在冲突情感线索的场景中表现突出。", "keywords": "multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, text dominance, multimodal fusion, emotion analysis", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"]}, "usage": {"completion_tokens": 815, "prompt_tokens": 3440, "total_tokens": 4255, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 585, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070286, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029536, "upstream_inference_completions_cost": 0.0004075}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:19.861009", "feed": "arxiv_cscv", "title": "Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning", "link": "https://papers.cool/arxiv/2510.20519", "analysis": {"summary": "Metis-HOME introduces a Hybrid Optimized Mixture-of-Experts framework for multimodal large models, separating a dense model into a thinking branch for complex multi-step reasoning and a non-thinking branch for fast direct inference. A lightweight trainable router dynamically assigns inputs to the appropriate expert, improving both complex reasoning performance and general capabilities while reducing unnecessary computation. Experiments with Qwen2.5-VL-7B show that this hybrid approach mitigates the typical trade‑off between reasoning ability and generalization in multimodal systems.", "summary_cn": "Metis-HOME 提出一种混合优化的专家混合（Mixture‑of‑Experts）框架，将原本的密集模型划分为思考分支（用于复杂多步骤推理）和非思考分支（用于快速直接推断，如通用 VQA 与 OCR）。轻量可训练的路由器能够动态将查询分配至最合适的专家，从而提升复杂推理能力的同时改善模型的通用性能，避免了仅专注推理导致的泛化退化。基于 Qwen2.5-VL-7B 的实验表明，该混合思考范式在多模态系统中有效平衡了推理与通用性的权衡。", "keywords": "multimodal reasoning, mixture of experts, hybrid thinking, dynamic router, Qwen2.5-VL, VQA, OCR, efficient inference, generalization", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xiaohan Lan", "Fanfan Liu", "Haibo Qiu", "Siqi Yang", "Delian Ruan", "Peng Shi", "Lin Ma"]}, "usage": {"completion_tokens": 866, "prompt_tokens": 3464, "total_tokens": 4330, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 537, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073196, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029896, "upstream_inference_completions_cost": 0.000433}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:20.178800", "feed": "arxiv_cscv", "title": "Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization", "link": "https://papers.cool/arxiv/2510.20247", "analysis": {"summary": "The paper introduces a mask-driven positional encoding (MPE) that combines spatial coordinates with object silhouettes, and a strip-convolution context enhancement module (CEM) to capture long-range features for cross‑view object geo‑localization. Integrated into the EDGeo framework, these components improve robustness to annotation shifts and enhance discrimination of elongated structures in satellite imagery, achieving state‑of‑the‑art performance on CVOGL and VIGOR‑Building datasets.", "summary_cn": "本文提出了基于分割掩码的位置信息编码（MPE），将空间坐标与目标轮廓结合，并设计了条形卷积上下文增强模块（CEM），用于捕获卫星图像中长跨度目标的长程特征。将两者集成到 EDGeo 框架后，显著提升了跨视角目标地理定位的鲁棒性和对细长建筑的判别能力，在 CVOGL 与 VIGOR‑Building 数据集上达到了领先的定位精度。", "keywords": "cross-view geo-localization, mask positional encoding, strip convolution, context modeling, satellite imagery, object-aware encoding, deep learning, positional encoding, feature discrimination, EDGeo", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Shuhan Hu", "Yiru Li", "Yuanyuan Li", "Yingying Zhu"]}, "usage": {"completion_tokens": 791, "prompt_tokens": 3463, "total_tokens": 4254, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 554, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069431, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029881, "upstream_inference_completions_cost": 0.0003955}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:20.546361", "feed": "arxiv_cscv", "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context", "link": "https://papers.cool/arxiv/2510.20229", "analysis": {"summary": "The paper investigates why Large Vision-Language Models (LVLMs) produce more hallucinations in longer, free-form responses, attributing the issue to increased reliance on context rather than response length itself. It introduces an \"induce-detect-suppress\" framework that deliberately creates hallucinations via crafted contexts, uses these instances for early detection, and suppresses object-level hallucinations during decoding, achieving consistent improvements across benchmarks. The work provides new insights into hallucination mechanisms and a practical mitigation strategy.", "summary_cn": "本文探讨了大型视觉语言模型（LVLM）在更长自由式回复中更易产生幻觉的原因，认为关键在于对上下文的依赖增加，而非单纯的长度问题。作者提出了“诱导‑检测‑抑制”框架，通过专门设计的上下文诱导幻觉，用诱导实例进行早期检测，并在实际解码时抑制对象级幻觉，显著提升了多个基准的表现。该研究为理解幻觉机制提供了新视角，并提供了实用的缓解方法。", "keywords": "hallucination, LVLM, vision-language, context, detection, mitigation, safety, alignment", "scoring": {"interpretability": 5, "understanding": 8, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"]}, "usage": {"completion_tokens": 735, "prompt_tokens": 3434, "total_tokens": 4169, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 480, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066196, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029446, "upstream_inference_completions_cost": 0.0003675}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:21.475895", "feed": "arxiv_cscv", "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models", "link": "https://papers.cool/arxiv/2510.20217", "analysis": {"summary": "EditInfinity adapts the binary-quantized generative model Infinity for text-driven image editing by exploiting exact intermediate quantized representations to achieve precise image inversion and style preservation. The method introduces an efficient inversion mechanism with text prompting rectification and a holistic smoothing strategy, resulting in high-fidelity edits across add, change, and delete operations on the PIE-Bench benchmark, outperforming diffusion-based baselines.", "summary_cn": "EditInfinity 利用二值量化生成模型 Infinity 的精确中间量化表征，实现对源图像的精准反演和风格保持，从而进行基于文本的图像编辑。该方法提出了结合文本提示校正的高效反演机制以及全局平滑策略，在 PIE-Bench 基准的添加、修改、删除编辑任务上，显著提升编辑保真度，优于扩散模型基线。", "keywords": "binary-quantized generative model, VQ inversion, text-driven image editing, Infinity, image editing fidelity, quantized representations, PIE-Bench", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiahuan Wang", "Yuxin Chen", "Jun Yu", "Guangming Lu", "Wenjie Pei"]}, "usage": {"completion_tokens": 595, "prompt_tokens": 3496, "total_tokens": 4091, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 359, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060126, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030376, "upstream_inference_completions_cost": 0.0002975}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:21.534462", "feed": "arxiv_cscv", "title": "Breakdance Video classification in the age of Generative AI", "link": "https://papers.cool/arxiv/2510.20287", "analysis": {"summary": "The paper investigates the use of modern video foundation models for classifying breakdance videos, a niche but popular dance sport. Experiments compare video encoders and video-language decoders, finding that encoder-only models consistently outperform state-of-the-art video-language models for this task. The authors also offer practical guidance on selecting encoders and analyze the behavior of a fine-tuned decoder.", "summary_cn": "本文研究了现代视频基础模型在破舞（breakdance）视频分类任务中的适用性。实验比较了视频编码器和视频语言解码器，结果显示仅使用编码器的模型在此任务上始终优于最先进的视频语言模型。作者还提供了选择编码器的实用建议，并深入分析了微调后的解码器的工作方式。", "keywords": "breakdance, video classification, video foundation models, video encoder, video language model, generative AI, fine-tuning, sports AI, multimodal video, model selection", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Michelle Munson"]}, "usage": {"completion_tokens": 960, "prompt_tokens": 3321, "total_tokens": 4281, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 858, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00075751, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027751, "upstream_inference_completions_cost": 0.00048}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:21.819671", "feed": "arxiv_cscv", "title": "FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing", "link": "https://papers.cool/arxiv/2510.20212", "analysis": {"summary": "FlowCycle introduces an inversion-free, flow-based image editing framework that learns target-aware intermediate states by optimizing learnable noises through a dual cycle-consistent process. By alternating edits from source to target and back, the method preserves editing-irrelevant content while achieving faithful modifications aligned with the textual prompt, outperforming prior approaches in quality and consistency.", "summary_cn": "FlowCycle 提出一种无需逆向的基于 flow 的图像编辑框架，通过可学习噪声在双向循环一致性过程中的优化，生成针对目标感知的中间状态。该方法在源图像到目标图像以及返回源图像的循环中，保留与编辑无关的内容，实现与文本提示高度匹配的编辑，并在质量和一致性上优于现有方法。", "keywords": "text-to-image, flow models, image editing, cycle-consistency, target-aware editing, inversion-free", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanghao Wang", "Zhen Wang", "Long Chen"]}, "usage": {"completion_tokens": 607, "prompt_tokens": 3442, "total_tokens": 4049, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 383, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059916, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029566, "upstream_inference_completions_cost": 0.0003035}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:21.877489", "feed": "arxiv_cscv", "title": "Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment", "link": "https://papers.cool/arxiv/2510.20438", "analysis": {"summary": "The paper introduces FuzzyDistillViT-MobileNet, a knowledge distillation framework that uses fuzzy‑logic‑driven dynamic weighting to transfer knowledge from a Vision Transformer instructor to a MobileNet student for lung cancer classification, combined with pixel‑level image fusion and genetic‑algorithm‑based model selection, achieving over 99% accuracy on histopathology and CT datasets. It emphasizes handling uncertainty by adjusting distillation weights to focus on high‑confidence regions while reducing attention to ambiguous areas, and includes image preprocessing enhancements such as gamma correction and histogram equalization. Experimental results demonstrate high accuracy and robustness across two lung cancer imaging domains.", "summary_cn": "本文提出 FuzzyDistillViT-MobileNet 框架，利用模糊逻辑驱动的动态权重调整将 Vision Transformer (ViT) 作为教师模型的知识蒸馏到 MobileNet 学生模型，用于肺癌分类，并结合像素级图像融合和遗传算法模型选择，在组织病理学和 CT 数据集上实现超过 99% 的准确率。该方法通过在蒸馏过程中动态调整权重，使学生模型聚焦高置信度区域，降低对模糊区域的关注，同时加入伽马校正和直方图均衡等图像预处理技术。实验显示该方法在两个肺癌影像数据集上具备高准确性和鲁棒性。", "keywords": "dynamic knowledge distillation, fuzzy logic, Vision Transformer, MobileNet, lung cancer detection, medical imaging, genetic algorithm, image fusion", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"]}, "usage": {"completion_tokens": 982, "prompt_tokens": 3537, "total_tokens": 4519, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 707, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00080091, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030991, "upstream_inference_completions_cost": 0.000491}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.059478", "feed": "arxiv_cscv", "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "link": "https://papers.cool/arxiv/2510.20162", "analysis": {"summary": "The paper introduces TOMCAT, a test-time method for compositional zero-shot learning that accumulates visual and textual knowledge from unsupervised data to update multimodal prototypes. It employs an adaptive update weight and a dynamic priority queue of high‑confidence images to mitigate distribution shift, and aligns textual and visual prototypes via collaborative representation learning, achieving state‑of‑the‑art results on multiple benchmarks.", "summary_cn": "本文提出了 TOMCAT，一种用于组合零样本学习的测试时方法，通过从无监督数据中积累视觉和文本知识来更新多模态原型。该方法使用自适应更新权重和存储高置信度图像的动态优先队列，以缓解标签空间分布转移，并通过多模态协作表示学习对齐文本与视觉原型，在多个基准上实现了最先进的性能。", "keywords": "compositional zero-shot learning, test-time adaptation, multimodal prototypes, knowledge accumulation, dynamic priority queue", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xudong Yan", "Songhe Feng"]}, "usage": {"completion_tokens": 525, "prompt_tokens": 3409, "total_tokens": 3934, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 289, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00055321, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029071, "upstream_inference_completions_cost": 0.0002625}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.437795", "feed": "arxiv_cscv", "title": "Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding", "link": "https://papers.cool/arxiv/2510.20244", "analysis": {"summary": "The paper proposes DualGround, a dual‑branch architecture for video temporal grounding that explicitly separates global sentence‑level semantics (routed through the [EOS] token) from local phrase‑level semantics (clustered word tokens). By employing token‑role aware cross‑modal interaction, the model improves both moment retrieval and highlight detection, achieving state‑of‑the‑art results on QVHighlights and Charades‑STA benchmarks.", "summary_cn": "本文提出 DualGround 双分支结构，用于视频时序定位，明确区分全局句子层语义（通过 [EOS] 标记）和局部短语层语义（对词汇进行聚类）。通过基于 token 角色的跨模态交互，该模型在 Moment Retrieval 与 Highlight Detection 任务上均实现了领先性能，在 QVHighlights 与 Charades‑STA 基准上取得了最新的成绩。", "keywords": "video temporal grounding, dual-branch architecture, token-role aware cross-modal attention, phrase-level semantics, sentence-level semantics, moment retrieval, highlight detection, CLIP, InternVideo2", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Minseok Kang", "Minhyeok Lee", "Minjung Kim", "Donghyeong Kim", "Sangyoun Lee"]}, "usage": {"completion_tokens": 966, "prompt_tokens": 3502, "total_tokens": 4468, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 813, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078766, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030466, "upstream_inference_completions_cost": 0.000483}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.477505", "feed": "arxiv_cscv", "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects", "link": "https://papers.cool/arxiv/2510.20126", "analysis": {"summary": "The paper presents a system that combines deep learning detection with a physics‑based tracking algorithm to robustly detect and track fast‑moving tiny objects using an RGB‑D camera. By integrating kinematic motion equations and an outlier correction module, the approach reduces average displacement error by up to 70% compared to Kalman‑filter trackers on a custom racquetball dataset. The work targets improved robot perception for autonomous platforms.", "summary_cn": "本文提出了一套将深度学习检测与基于物理的追踪算法相结合的系统，用于在 RGB‑D 相机下鲁棒地检测和跟踪快速移动的微小物体。通过融合运动学方程并加入异常检测与校正模块，该方法在自建的壁球数据集上相比卡尔曼滤波追踪器将平均位移误差降低了最高 70%。该工作旨在提升自主平台机器人的感知能力。", "keywords": "physics-based tracking, 3D tracking, fast moving small objects, RGB-D, outlier detection, kinematics, robot perception", "scoring": {"interpretability": 2, "understanding": 4, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"]}, "usage": {"completion_tokens": 693, "prompt_tokens": 3415, "total_tokens": 4108, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 466, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063811, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029161, "upstream_inference_completions_cost": 0.0003465}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.610544", "feed": "arxiv_cscv", "title": "PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching", "link": "https://papers.cool/arxiv/2510.20178", "analysis": {"summary": "The paper introduces PPMStereo, a method that employs a Pick-and-Play Memory (PPM) module to achieve temporally consistent depth estimation in stereo video while remaining computationally efficient. PPM selects the most relevant past frames ('pick') and adaptively weights them ('play') to aggregate spatio‑temporal information, resulting in state‑of‑the‑art accuracy and consistency. Extensive experiments demonstrate significant improvements over prior dynamic stereo methods.", "summary_cn": "本文提出 PPMStereo，一种利用 Pick-and-Play Memory（PPM）模块实现立体视频时间一致深度估计且计算高效的方法。PPM 通过“pick”步骤挑选最相关的历史帧，再通过“play”步骤对选中的帧进行自适应加权，实现时空信息的聚合，取得了在准确率和一致性上的领先表现。大量实验验证了其相较于现有动态立体匹配方法的显著提升。", "keywords": "dynamic stereo matching, temporally consistent depth estimation, memory buffer, pick-and-play memory, spatio-temporal aggregation, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yun Wang", "Junjie Hu", "Qiaole Dong", "Yongjian Zhang", "Yanwei Fu", "Tin Lun Lam", "Dapeng Wu"]}, "usage": {"completion_tokens": 564, "prompt_tokens": 3535, "total_tokens": 4099, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 286, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059161, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030961, "upstream_inference_completions_cost": 0.000282}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.685038", "feed": "arxiv_cscv", "title": "Revisiting Logit Distributions for Reliable Out-of-Distribution Detection", "link": "https://papers.cool/arxiv/2510.20134", "analysis": {"summary": "The paper introduces LogitGap, a post-hoc out-of-distribution detection method that leverages the gap between the maximum logit and the remaining logits to improve separability of in‑distribution and OOD samples. A training‑free strategy automatically selects the most informative subset of logits for scoring, and both theoretical analysis and extensive experiments on vision and vision‑language models show state‑of‑the‑art performance.", "summary_cn": "本文提出 LogitGap，一种后置 OOD 检测方法，利用最大 logit 与其余 logit 之间的差距增强 ID 与 OOD 样本的可分性。通过训练无关的策略自动挑选最具信息量的 logit 子集进行打分，并提供理论分析与在视觉及视觉‑语言模型上的大量实验，验证了其在各类 OOD 场景中的领先表现。", "keywords": "OOD detection, logits, LogitGap, post-hoc, training-free, reliability, out-of-distribution", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Jiachen Liang", "Ruibing Hou", "Minyang Hu", "Hong Chang", "Shiguang Shan", "Xilin Chen"]}, "usage": {"completion_tokens": 714, "prompt_tokens": 3404, "total_tokens": 4118, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 504, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064696, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028996, "upstream_inference_completions_cost": 0.000357}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.760446", "feed": "arxiv_cscv", "title": "Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos", "link": "https://papers.cool/arxiv/2510.20087", "analysis": {"summary": "The paper introduces Endoshare, a source‑available, cross‑platform tool for merging, standardizing, and de‑identifying endoscopic videos used in minimally invasive surgery. It follows a user‑centered software development process, incorporates privacy‑by‑design principles, and demonstrates high usability and performance across different hardware configurations. The authors suggest further certification and interoperability testing to make Endoshare a viable alternative to proprietary systems.", "summary_cn": "本文提出 Endoshare，这是一款开源、跨平台的工具，用于合并、标准化并去标识微创手术的内镜视频。系统遵循以用户为中心的软件开发流程，采用隐私设计原则，并在不同硬件配置下展示出较高的可用性和处理性能。作者指出仍需进行合规认证和更广泛的互操作性验证，以将 Endoshare 发展为专有系统的可部署替代方案。", "keywords": "surgical video, de-identification, privacy-preserving, data standardization, source-available, usability, video management", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Britty Baby", "Vinkle Srivastav", "Pietro Mascagni", "Nicolas Padoy"]}, "usage": {"completion_tokens": 623, "prompt_tokens": 3554, "total_tokens": 4177, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 388, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062396, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031246, "upstream_inference_completions_cost": 0.0003115}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:22.799962", "feed": "arxiv_cscv", "title": "Transformed Multi-view 3D Shape Features with Contrastive Learning", "link": "https://papers.cool/arxiv/2510.19955", "analysis": {"summary": "The paper investigates how Vision Transformers combined with contrastive learning objectives improve multi-view 3D shape representation, achieving up to 90.6% accuracy on ModelNet10. Extensive experiments show that ViTs capture global shape semantics while contrastive optimization refines local discriminative features, outperforming traditional CNN-based pipelines.", "summary_cn": "本文研究了将视觉Transformer与对比学习目标相结合，以提升多视角3D形状特征表示的效果，在ModelNet10上实现了约90.6%的准确率。大量实验表明，ViT能够捕获整体形状语义，而对比学习优化细化局部判别特征，优于传统CNN方法。", "keywords": "3D shape representation, contrastive learning, vision transformers, multi-view analysis, ModelNet10", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Márcus Vinícius Lobo Costa", "Sherlon Almeida da Silva", "Bárbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"]}, "usage": {"completion_tokens": 536, "prompt_tokens": 3436, "total_tokens": 3972, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 327, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056276, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029476, "upstream_inference_completions_cost": 0.000268}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:23.384563", "feed": "arxiv_cscv", "title": "SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization", "link": "https://papers.cool/arxiv/2510.20189", "analysis": {"summary": "The paper introduces SPAN, a Suspicion Progression Analysis Network that models suspicious intentions in video surveillance as a continuous regression problem rather than discrete classes. By leveraging temporal point process theory, multimodal suspicion coefficient modulation, and concept‑anchored mapping, SPAN captures long‑term dependencies and links actions to intention concepts, achieving lower MSE and higher mAP on the HAI dataset, especially for subtle, low‑frequency behaviors.", "summary_cn": "本文提出 SPAN（Suspicion Progression Analysis Network），将视频监控中的可疑意图从离散分类转为连续回归建模。利用时间点过程理论、跨模态可疑系数调制以及概念锚定映射，捕获可疑行为的长期依赖并将动作与预定义意图概念关联，在 HAI 数据集上显著降低 MSE 并提升 mAP，尤其在低频微弱行为上表现更佳。", "keywords": "temporal intention localization, suspicion progression, continuous regression, video surveillance, multimodal modulation, concept-anchored mapping, temporal point process, anomaly detection", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xinyi Hu", "Yuran Wang", "Yue Li", "Wenxuan Liu", "Zheng Wang"]}, "usage": {"completion_tokens": 635, "prompt_tokens": 3479, "total_tokens": 4114, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 385, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061871, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030121, "upstream_inference_completions_cost": 0.0003175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:23.508269", "feed": "arxiv_cscv", "title": "Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models", "link": "https://papers.cool/arxiv/2510.20042", "analysis": {"summary": "The paper introduces a unified benchmark for evaluating cultural bias in both text-to-image (T2I) generation and image-to-image (I2I) editing across six countries, using a detailed 8‑category schema, era‑aware prompts, automatic metrics, a retrieval‑augmented VQA, and expert human judgments. Experiments on open‑source models reveal that under generic prompts models default to Global‑North, modern depictions, and that iterative I2I edits degrade cultural fidelity despite unchanged conventional metrics. The authors release all data, prompts, and evaluation protocols to enable reproducible tracking of cultural bias in generative image systems.", "summary_cn": "本文提出一个统一的基准，用于在六个国家范围内评估文本到图像（T2I）生成和图像到图像（I2I）编辑的文化偏见，采用 8 类 36 子类的详细结构、时代感知的提示、自动指标、检索增强的 VQA 以及本土专家的人类评审。对开源模型的实验表明，在通用提示下模型倾向于呈现全球北方、现代化的描绘，且迭代的 I2I 编辑会在传统指标保持不变或提升的情况下削弱文化忠实度。作者公开全部图像数据、提示和评估协议，以实现对生成图像模型文化偏见的可复现诊断与追踪。", "keywords": "cultural bias, generative image models, text-to-image, image-to-image editing, cross-cultural evaluation, fairness, VQA, human evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Huichan Seo", "Sieun Choi", "Minki Hong", "Yi Zhou", "Junseo Kim", "Lukman Ismaila", "Naome Etori", "Mehul Agarwal", "Zhixuan Liu", "Jihie Kim", "Jean Oh"]}, "usage": {"completion_tokens": 689, "prompt_tokens": 3485, "total_tokens": 4174, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 308, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064661, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030211, "upstream_inference_completions_cost": 0.0003445}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:23.735217", "feed": "arxiv_cscv", "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks", "link": "https://papers.cool/arxiv/2510.20165", "analysis": {"summary": "The paper introduces IB‑GAN, a generative adversarial network that incorporates the Information Bottleneck principle to learn disentangled representations. By constraining the mutual information between the input and an intermediate stochastic layer of the generator, the model yields a learnable latent distribution that produces interpretable and diverse samples, achieving competitive disentanglement scores on dSprites and superior FID scores on CelebA and 3D Chairs compared to β‑VAEs and InfoGAN.", "summary_cn": "本文提出 IB‑GAN，一种将信息瓶颈（Information Bottleneck）原理引入生成对抗网络以实现可分离表征学习的模型。通过在生成器的中间随机层约束输入与输出之间的互信息，模型能够学习可解释的潜在分布，生成的样本在 dSprites 上的可分离性指标与最新 β‑VAE 相当，并在 CelebA 与 3D Chairs 上的 FID 分数超过 InfoGAN 和 β‑VAE。", "keywords": "disentangled representation, Information Bottleneck, GAN, InfoGAN, latent space, mutual information, unsupervised learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Insu Jeon", "Wonkwang Lee", "Myeongjang Pyeon", "Gunhee Kim"]}, "usage": {"completion_tokens": 930, "prompt_tokens": 3427, "total_tokens": 4357, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 751, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00075841, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029341, "upstream_inference_completions_cost": 0.000465}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:23.894343", "feed": "arxiv_cscv", "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "link": "https://papers.cool/arxiv/2510.19981", "analysis": {"summary": "FutrTrack introduces a modular camera‑LiDAR fusion framework that enhances 3D multiple object tracking by adding a transformer‑based temporal smoother and a fusion‑driven tracker. The system refines bounding box sequences and propagates identities using multimodal BEV features without an explicit motion model, achieving strong results on nuScenes and KITTI benchmarks. This approach demonstrates that query‑based transformer trackers significantly benefit from multimodal sensor integration.", "summary_cn": "FutrTrack 提出一个模块化的摄像头‑LiDAR 融合框架，通过引入基于 Transformer 的时间平滑器和融合驱动的跟踪器来提升 3D 多目标跟踪。系统在无需显式运动模型的情况下，利用多模态鸟瞰图特征精炼边界框序列并传播目标身份，在 nuScenes 与 KITTI 基准上取得了强劲表现。该方法表明查询式 Transformer 跟踪器在多传感器特征融合后能显著提升性能。", "keywords": "camera-LiDAR fusion, transformer, 3D multi-object tracking, BEV fusion, temporal smoother, nuScenes, KITTI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"]}, "usage": {"completion_tokens": 635, "prompt_tokens": 3445, "total_tokens": 4080, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 350, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061361, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029611, "upstream_inference_completions_cost": 0.0003175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.009205", "feed": "arxiv_cscv", "title": "Real Deep Research for AI, Robotics and Beyond", "link": "https://papers.cool/arxiv/2510.20809", "analysis": {"summary": "The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area to identify emerging trends, uncover cross‑domain opportunities, and suggest concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across topics. The work aims to help researchers keep pace with the rapid growth of literature and discover novel research directions.", "summary_cn": "本文提出了一种通用的流水线（Real Deep Research, RDR），用于系统地分析任何研究领域，识别新兴趋势、跨领域机会，并提供具体的研究起点。作者将该框架应用于 AI 与机器人领域，重点关注基础模型和机器人技术，并在附录中展示了各主题的详尽结果，旨在帮助研究者应对快速增长的文献并发现新方向。", "keywords": "literature analysis, trend detection, cross-domain discovery, AI research mapping, robotics, foundation models, systematic review, meta-research", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}, "usage": {"completion_tokens": 629, "prompt_tokens": 3369, "total_tokens": 3998, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 428, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059921, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028471, "upstream_inference_completions_cost": 0.0003145}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.175451", "feed": "arxiv_cscv", "title": "Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection", "link": "https://papers.cool/arxiv/2510.20214", "analysis": {"summary": "The paper introduces CURL, a self‑supervised contrastive learning framework that learns spatial‑temporal representations from long fetal ultrasound videos to detect fetal movements. By using a dual‑contrastive loss and a task‑specific sampling strategy, the method achieves 78% sensitivity and 81.6% AUROC on a dataset of 92 subjects. The results suggest that contrastive representation learning can provide an objective, automated tool for prenatal monitoring.", "summary_cn": "本文提出了 CURL，一种自监督对比学习框架，通过学习长时段胎儿超声视频的空间‑时间表征来检测胎儿运动。该方法利用双重对比损失和任务特定抽样策略，在 92 位受试者的数据上实现了 78% 的灵敏度和 81.6% 的 AUROC。实验表明，对比表征学习有望为产前监测提供客观、自动化的工具。", "keywords": "contrastive learning, ultrasound video, fetal movement detection, self-supervised representation, medical imaging, probabilistic fine-tuning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Talha Ilyas", "Duong Nhu", "Allison Thomas", "Arie Levin", "Lim Wei Yap", "Shu Gong", "David Vera Anaya", "Yiwen Jiang", "Deval Mehta", "Ritesh Warty", "Vinayak Smith", "Maya Reddy", "Euan Wallace", "Wenlong Cheng", "Zongyuan Ge", "Faezeh Marzbanrad"]}, "usage": {"completion_tokens": 781, "prompt_tokens": 3435, "total_tokens": 4216, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 580, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068511, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029461, "upstream_inference_completions_cost": 0.0003905}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.313234", "feed": "arxiv_cscv", "title": "A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development", "link": "https://papers.cool/arxiv/2510.20196", "analysis": {"summary": "The paper conducts a structured review and quantitative profiling of 54 public brain MRI datasets, analyzing modality composition, disease coverage, scale, and image-level characteristics such as voxel spacing and intensity distributions. It evaluates the impact of common preprocessing steps on voxel statistics and demonstrates residual covariate shift using a 3D DenseNet121, highlighting the limits of preprocessing for harmonizing heterogeneous datasets. The findings underline the need for preprocessing-aware and domain-adaptive strategies when developing generalizable brain MRI foundation models.", "summary_cn": "本文对 54 个公开的脑部 MRI 数据集进行结构化回顾和定量分析，评估了模态组成、疾病覆盖、规模以及图像层面的体素间距、方向和强度分布等特征。研究进一步量化了常用预处理（如强度归一化、偏场校正、颅骨去除、空间配准和插值）对体素统计和几何形状的影响，并通过 3D DenseNet121 实验显示标准化预处理后仍存在显著的协变量漂移。结果强调在构建通用脑部 MRI 基础模型时，需要采用考虑预处理差异的领域适应策略。", "keywords": "brain MRI, foundation models, dataset heterogeneity, preprocessing, covariate shift, domain adaptation, 3D DenseNet", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Minh Sao Khue Luu", "Margaret V. Benedichuk", "Ekaterina I. Roppert", "Roman M. Kenzhin", "Bair N. Tuchinov"]}, "usage": {"completion_tokens": 753, "prompt_tokens": 3443, "total_tokens": 4196, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 470, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067231, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029581, "upstream_inference_completions_cost": 0.0003765}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.322974", "feed": "arxiv_cscv", "title": "Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency", "link": "https://papers.cool/arxiv/2510.20092", "analysis": {"summary": "The paper introduces Attentive Convolution (ATConv), a reformulation of the convolution operator that incorporates adaptive routing and lateral inhibition principles inspired by self‑attention, achieving higher accuracy with only 3×3 kernels. Experiments show ATConv consistently outperforms various self‑attention mechanisms on ImageNet classification and improves diffusion‑based image generation efficiency. The authors also present AttNet, a CNN family attaining 84.4% top‑1 accuracy with 27M parameters.", "summary_cn": "本文提出了注意卷积（Attentive Convolution，ATConv），一种在卷积算子中引入自注意力（self‑attention）启发的自适应路由和侧抑制机制的重新表述，只使用 3×3 核即可实现更高的准确率。实验表明，ATConv 在 ImageNet 分类等基础视觉任务上持续超越多种自注意力机制，并在基于扩散的图像生成中提升效率。作者进一步构建了 AttNet 系列 CNN，实现了 84.4% 的 Top‑1 准确率（27M 参数）。", "keywords": "Attentive Convolution, self-attention, convolutional neural network, adaptive routing, lateral inhibition, vision backbone, ImageNet, diffusion models, efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Yu", "Haoyu Chen", "Yan Jiang", "Wei Peng", "Zhaodong Sun", "Samuel Kaski", "Guoying Zhao"]}, "usage": {"completion_tokens": 819, "prompt_tokens": 3594, "total_tokens": 4413, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 568, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00072796, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031846, "upstream_inference_completions_cost": 0.0004095}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.722758", "feed": "arxiv_cscv", "title": "A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance", "link": "https://papers.cool/arxiv/2510.20016", "analysis": {"summary": "The paper proposes a detection framework for fisheye traffic surveillance that uses pre‑ and post‑processing steps and an ensemble of state‑of‑the‑art object detectors to improve performance under severe radial distortion, especially near image boundaries. Experiments on the 2025 AI City Challenge Track 4 show the method achieves an F1 score of 0.6366, ranking 8th out of 62 teams.", "summary_cn": "本文提出了一种用于鱼眼交通监控的检测框架，结合前处理和后处理步骤以及多种最先进目标检测模型的集成，以提升在强径向畸变尤其是图像边缘区域的检测一致性。对 2025 AI City Challenge Track 4 的实验表明，该方法达到 0.6366 的 F1 分数，在 62 支队伍中排名第 8。", "keywords": "fisheye distortion, object detection, traffic surveillance, ensemble detection, pre-processing, post-processing, AI City Challenge, robust detection", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Neema Jakisa Owor", "Joshua Kofi Asamoah", "Tanner Wambui Muturi", "Anneliese Jakisa Owor", "Blessing Agyei Kyem", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"]}, "usage": {"completion_tokens": 802, "prompt_tokens": 3381, "total_tokens": 4183, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 588, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068751, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028651, "upstream_inference_completions_cost": 0.000401}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.871246", "feed": "arxiv_cscv", "title": "Improving Predictive Confidence in Medical Imaging via Online Label Smoothing", "link": "https://papers.cool/arxiv/2510.20011", "analysis": {"summary": "The paper proposes Online Label Smoothing (OLS), a dynamic soft‑labeling technique that adapts to the model's own predictions during training, and evaluates it on the large‑scale RadImageNet medical imaging dataset using ResNet‑50, MobileNetV2, and VGG‑19. OLS consistently improves Top‑1 and Top‑5 accuracy, yields better calibrated predictions, and produces more compact, well‑separated feature embeddings compared to hard labels, conventional label smoothing, and teacher‑free knowledge distillation. These results suggest OLS enhances both predictive performance and reliability for trustworthy AI in healthcare.", "summary_cn": "本文提出在线标签平滑（Online Label Smoothing，OLS）方法，在训练过程中根据模型自身的预测动态调整软标签，并在大规模医学影像数据集 RadImageNet 上使用 ResNet‑50、MobileNetV2 与 VGG‑19 进行评估。与硬标签、传统标签平滑及无教师知识蒸馏相比，OLS 在 Top‑1 与 Top‑5 准确率上均有提升，并显著改善预测校准，使特征嵌入更紧凑且类别间分离度更高，表明该方法可提升医学影像 AI 系统的性能与可靠性。", "keywords": "online label smoothing, calibration, medical imaging, convolutional neural networks, representation learning, RadImageNet, predictive confidence", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Kushan Choudhury", "Shubhrodeep Roy", "Ankur Chanda", "Shubhajit Biswas", "Somenath Kuiry"]}, "usage": {"completion_tokens": 770, "prompt_tokens": 3422, "total_tokens": 4192, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 488, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067766, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.000385}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.894918", "feed": "arxiv_cscv", "title": "Kinaema: a recurrent sequence model for memory and pose in motion", "link": "https://papers.cool/arxiv/2510.20261", "analysis": {"summary": "The paper introduces Kinaema, a recurrent sequence model that maintains an implicit latent memory via a transformer to integrate visual observations for continuous navigation tasks. By compressing observation history into a compact representation, the model can answer pose queries and navigate to goals observed before an episode, demonstrating efficiency compared to classic attention‑based transformers in the newly proposed Mem‑Nav task.", "summary_cn": "本文提出了 Kinaema，一种递归序列模型，通过 transformer 以隐式潜在记忆方式整合测，从而在连续导航任务中实现对历史信息的压缩表示。该模型能够在请求时对查询图像进行位姿预测，并在实际任务开始前已观测到的目标位置进行导航，展示了相较于传统注意力 transformer 在新任务 Mem‑Nav 中的计算效率和有效性。", "keywords": "recurrent transformer, latent memory, visual navigation, pose estimation, Mem-Nav, scene representation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Mert Bulent Sariyildiz", "Philippe Weinzaepfel", "Guillaume Bono", "Gianluca Monaci", "Christian Wolf"]}, "usage": {"completion_tokens": 621, "prompt_tokens": 3416, "total_tokens": 4037, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 396, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060226, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029176, "upstream_inference_completions_cost": 0.0003105}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:24.908484", "feed": "arxiv_cscv", "title": "Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures", "link": "https://papers.cool/arxiv/2510.20193", "analysis": {"summary": "This survey reviews recent advancements in question answering systems that incorporate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. It categorizes approaches by retrieval methods, fusion techniques, and answer generation strategies, and discusses benchmarks, evaluation protocols, and challenges such as cross-modal alignment and latency‑accuracy tradeoffs.", "summary_cn": "本文综述了多媒体检索集成到问答系统中的进展，重点关注将视觉、语言和音频模态与用户查询对齐的架构。文章按检索方法、融合技术和答案生成策略进行分类，并讨论了基准数据集、评估协议以及跨模态对齐、延迟‑准确性权衡等关键挑战。", "keywords": "multimodal QA, retrieval-augmented QA, cross-modal reasoning, vision-language fusion, audio-visual retrieval, benchmark datasets, semantic grounding, latency-accuracy tradeoff", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rahul Raja", "Arpita Vats"]}, "usage": {"completion_tokens": 628, "prompt_tokens": 3347, "total_tokens": 3975, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 403, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059541, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028141, "upstream_inference_completions_cost": 0.000314}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.123947", "feed": "arxiv_cscv", "title": "Fourier-Based GAN Fingerprint Detection using ResNet50", "link": "https://papers.cool/arxiv/2510.19840", "analysis": {"summary": "The paper proposes a forensic method that applies a 2‑dimensional Discrete Fourier Transform to images to expose subtle periodic artifacts left by GANs, then trains a ResNet50 classifier on these Fourier‑domain representations to distinguish StyleGAN‑generated images from real ones, achieving 92.8% accuracy and an AUC of 0.95. The results demonstrate that GAN‑produced images possess distinctive frequency‑domain fingerprints, and the approach outperforms models trained on raw pixel data. This highlights the potential of combining signal‑processing techniques with deep learning for industrial digital‑forensics and content‑authenticity assurance.", "summary_cn": "本文提出一种取证方法，对图像进行二维离散傅里叶变换（2D DFT），以揭示 GAN 生成图像中细微的周期性伪迹，然后在这些频域表示上Net50 分类器，以区分 StyleGAN 生成的图像和真实图像，取得 92.8% 的准确率和 0.95 的 AUC。结果表明，GAN 生成的图像在频域中具有独特的指纹特征，并且该方法显著优于在原始像素上训练的模型。这凸显了将信号处理技术与深度学习相结合用于工业数字取证和内容真实性保障的潜力。", "keywords": "GAN detection, Fourier analysis, frequency fingerprint, ResNet50, image forensics, synthetic image detection, StyleGAN", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Sai Teja Erukude", "Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru"]}, "usage": {"completion_tokens": 791, "prompt_tokens": 3386, "total_tokens": 4177, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 452, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068276, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028726, "upstream_inference_completions_cost": 0.0003955}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.255423", "feed": "arxiv_cscv", "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback", "link": "https://papers.cool/arxiv/2510.20093", "analysis": {"summary": "StableSketcher fine-tunes a diffusion model's variational autoencoder and adds a reinforcement‑learning reward based on visual question answering to improve the fidelity of pixel‑based human sketch generation. The framework achieves better text‑image alignment and releases SketchDUO, a new dataset of sketches paired with captions and QA pairs. Experiments show higher stylistic fidelity compared with the Stable Diffusion baseline.", "summary_cn": "StableSketcher 通过微调扩散模型的变分自编码器并引入基于视觉问答的强化学习奖励函数，以提升像素级手绘草图的生成质量和文本‑图像对齐程度。该框架在此基础上推出 SketchDUO 数据集，包含草图、标题及问答对。实验表明相较于 Stable Diffusion 基准，StableSketcher 在风格忠实度上有显著提升。", "keywords": "diffusion models, sketch generation, visual question answering, reinforcement learning, latent autoencoder, dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"]}, "usage": {"completion_tokens": 621, "prompt_tokens": 3399, "total_tokens": 4020, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 405, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059971, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.0003105}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.584592", "feed": "arxiv_cscv", "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding", "link": "https://papers.cool/arxiv/2510.20155", "analysis": {"summary": "The paper introduces PartNeXt, a large-scale dataset of over 23,000 textured 3D models with fine-grained hierarchical part annotations across 50 categories. It evaluates state-of-the-art part segmentation and a new 3D part-centric question-answering benchmark for 3D-LLMs, showing that existing methods struggle with leaf-level parts and that training on PartNeXt improves performance over PartNet. The dataset aims to advance structured 3D understanding by providing scalable, texture-aware annotations and multi-task evaluation.", "summary_cn": "本文推出 PartNeXt 数据集，包含超过 23,000 个高质量、带纹理的 3D 模型，提供跨 50 类的细粒度层次化部件标签。通过对类别无关部件分割和新设的 3D 部件问答基准进行评估，发现现有方法在细粒度叶层部件上表现不足，并且在 PartNeXt 上训练的 Point‑SAM 相较 PartNet 有显著提升。该数据集通过可扩展的标注、纹理感知标签和多任务评估，推动结构化 3D 理解研究。", "keywords": "PartNeXt, 3D part segmentation, hierarchical annotation, textured 3D models, fine-grained parts, 3D-LLM, point cloud dataset, PartField, SAMPart3D, Point-SAM", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Penghao Wang", "Yiyang He", "Xin Lv", "Yukai Zhou", "Lan Xu", "Jingyi Yu", "Jiayuan Gu"]}, "usage": {"completion_tokens": 1131, "prompt_tokens": 3429, "total_tokens": 4560, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 848, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00085921, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029371, "upstream_inference_completions_cost": 0.0005655}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.735710", "feed": "arxiv_cscv", "title": "GUSL-Dehaze: A Green U-Shaped Learning Approach to Image Dehazing", "link": "https://papers.cool/arxiv/2510.20266", "analysis": {"summary": "The paper introduces GUSL-Dehaze, a lightweight, non‑learning image dehazing method that combines a modified Dark Channel Prior with a green learning pipeline implemented via a U‑shaped architecture. The approach uses unsupervised representation learning, feature‑engineering techniques such as Relevant Feature Test and Least‑Squares Normal Transform, and a transparent supervised step to produce dehazed images while keeping parameter count low and maintaining mathematical interpretability.", "summary_cn": "本文提出 GUSL-Dehaze，一种轻量级、完全不使用深度学习的图像去雾方法，先利用改进的暗通道先验进行初步去雾，再通过 U 形结构的绿色学习管道进行无监督特征提取，并结合 Relevant Feature Test (RFT) 与 Least‑Squares Normal Transform (LNT) 等特征工程技术，最后采用透明的监督学习得到去雾图像，实现参数量低且具有数学可解释性。", "keywords": "image dehazing, dark channel prior, green learning, U-shaped architecture, unsupervised representation learning, feature engineering, lightweight model, mathematical interpretability", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahtab Movaheddrad", "Laurence Palmer", "C. -C. Jay Kuo"]}, "usage": {"completion_tokens": 745, "prompt_tokens": 3474, "total_tokens": 4219, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 486, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067296, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030046, "upstream_inference_completions_cost": 0.0003725}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.806373", "feed": "arxiv_cscv", "title": "Data-Adaptive Transformed Bilateral Tensor Low-Rank Representation for Clustering", "link": "https://papers.cool/arxiv/2510.20077", "analysis": {"summary": "The paper proposes TBTLRR, a transformed bilateral tensor low-rank representation model that learns data-adaptive unitary transforms to define a flexible tensor nuclear norm and captures both global and local correlations for image clustering. It incorporates ℓ_{1/2}-norm and Frobenius regularization to handle complex noise and solves the resulting nonconvex problem with an ADMM-based algorithm, demonstrating superior clustering performance in experiments.", "summary_cn": "本文提出 TBTLRR 模型，通过学习数据自适应的酉变换构建灵活的张量核范数，从而有效捕获图像聚类中的全局和局部相关性。模型融合 ℓ_{1/2} 范数和 Frobenius 正则，以应对复杂噪声，并采用基于 ADMM 的算法求解非凸问题，在实验中表现出色。", "keywords": "tensor low-rank representation, adaptive unitary transform, bilateral tensor, clustering, l_{1/2} norm, ADMM", "scoring": {"interpretability": 1, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Hui Chen", "Xinjie Wang", "Xianchao Xiu", "Wanquan Liu"]}, "usage": {"completion_tokens": 660, "prompt_tokens": 3406, "total_tokens": 4066, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 413, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062026, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029026, "upstream_inference_completions_cost": 0.00033}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:25.845301", "feed": "arxiv_cscv", "title": "Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists", "link": "https://papers.cool/arxiv/2510.20158", "analysis": {"summary": "The paper presents a category-level 8D pose estimation method that predicts the full 3D translation, rotation, and the steering‑handle and pedal angles of articulated bicycles and cyclists from a single RGB image. By jointly estimating 8D pose and 3D keypoints using a mix of synthetic and real data, the approach outperforms state-of-the-art 6D pose estimators on articulated objects. This enables finer-grained travel‑direction inference for vulnerable road users in autonomous driving scenarios.", "summary_cn": "本文提出一种类别级的 8D 位姿估计方法，能够仅凭单张 RGB 图像预测自行车和骑行者的 3D 平移、旋转以及车把和踏板相对于车体的角度。该模型通过联合估计 8D 位姿和 3D 关键点，并结合合成与真实数据进行训练，在关节化目标上优于现有的 6D 位姿估计器。此技术为自动驾驶中的弱势道路使用者提供更精细的行进方向推断，有助于提升安全性。", "keywords": "monocular pose estimation, 8D pose, articulated bicycles, cyclists, autonomous driving, category-level pose, synthetic training data, keypoint detection, VRU safety", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Eduardo R. Corral-Soto", "Yang Liu", "Yuan Ren", "Bai Dongfeng", "Liu Bingbing"]}, "usage": {"completion_tokens": 803, "prompt_tokens": 3529, "total_tokens": 4332, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 534, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071021, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030871, "upstream_inference_completions_cost": 0.0004015}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.136053", "feed": "arxiv_cscv", "title": "AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training", "link": "https://papers.cool/arxiv/2510.20012", "analysis": {"summary": "The paper presents an AI-driven pipeline for pose estimation that quantifies joint-angle trajectories and derived kinematic metrics during resistance‑training exercises, comparing lengthened partial (pROM) and full range‑of‑motion (fROM) protocols across multiple participants. Using a random‑effects meta‑analytic model, the study finds that pROM sets exhibit reduced ROM and shorter eccentric phases, with participant variability driving most of the observed differences, and introduces the %ROM metric to standardize partial‑range definitions. The results highlight how AI‑based motion analysis can deepen biomechanical insight and inform training prescription.", "summary_cn": "本文提出了一套基于 AI 的姿态估计流水线，用于提取阻力训练中关节角度轨迹并计算运动学指标，比较了加长的部分范围 (pROM) 与完整范围 (fROM) 的训练方式。通过随机效应元分析模型，研究发现 pROM 训练的运动范围更小且离心相位更短，且参与者之间的差异是主要变异来源，并引入 %ROM 指标统一部分范围的定义。结果表明 AI 运动分析能够提升对生物力学的理解并改进训练处方。", "keywords": "AI pose estimation, kinematic profiling, range-of-motion, resistance training, meta-analysis, video analysis, %ROM, biomechanics, partial ROM, movement dynamics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Adam Diamant"]}, "usage": {"completion_tokens": 705, "prompt_tokens": 3461, "total_tokens": 4166, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 357, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029851, "upstream_inference_completions_cost": 0.0003525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.289834", "feed": "arxiv_cscv", "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "link": "https://papers.cool/arxiv/2510.20095", "analysis": {"summary": "The paper introduces BIOCAP, a biological multimodal foundation model that leverages synthetic descriptive captions generated by multimodal large language models to complement image labels. By aligning images and captions in a shared latent morphospace, BIOCAP improves species classification and text‑image retrieval performance. The work demonstrates that domain‑specific caption generation can reduce hallucination and provide instance‑specific supervision for organismal biology.", "summary_cn": "本文提出 BIOCAP，一种利用多模态大语言模型生成的合成描述性字幕来补充图像标签的生物多模态基础模型。通过在共享的潜在形态空间中对齐图像和字幕，BIOCAP 在物种分类和文本‑图像检索上取得了显著提升。该工作展示了领域特定的字幕生成能够降低幻觉并为生物学实例提供具体监督。", "keywords": "synthetic captions, multimodal foundation models, biological vision, species classification, text-image retrieval, multimodal large language models, BIOCAP, BIOCLIP, latent morphospace", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"]}, "usage": {"completion_tokens": 737, "prompt_tokens": 3406, "total_tokens": 4143, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 545, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065876, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029026, "upstream_inference_completions_cost": 0.0003685}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.474620", "feed": "arxiv_cscv", "title": "BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography", "link": "https://papers.cool/arxiv/2510.20029", "analysis": {"summary": "BrainPuzzle is a two-stage hybrid framework that first applies reverse time migration to multi-angle transcranial ultrasound measurements and then fuses the resulting fragments with a transformer‑based super‑resolution encoder‑decoder utilizing a graph‑based attention unit to produce accurate speed‑of‑sound maps of the brain. The method addresses the challenges of skull‑induced attenuation, limited aperture, and low signal‑to‑noise ratio, and experiments on synthetic data show improved reconstruction accuracy and image completeness compared to purely physics‑based or data‑driven approaches.", "summary_cn": "BrainPuzzle 提出了一种两阶段的混合框架：第一阶段使用逆时偏移（时间反演声学）对多角度经颅超声数据进行迁移，得到保留结构细节的片段；第二阶段利用基于 Transformer 的超分辨率编码‑解码网络并结合图注意力单元（GAU）将这些片段融合，生成准确的脑部声速（SoS）图像。该方法克服了颅骨衰减、孔径受限和低信噪比等难题，在合成数据实验中展示了比纯物理或纯数据方法更高的重建精度和图像完整性。", "keywords": "ultrasound tomography, speed-of-sound reconstruction, full-waveform inversion, reverse time migration, transformer, graph attention unit, hybrid physics-data, transcranial imaging", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shengyu Chen", "Shihang Feng", "Yi Luo", "Xiaowei Jia", "Youzuo Lin"]}, "usage": {"completion_tokens": 664, "prompt_tokens": 3511, "total_tokens": 4175, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 340, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063801, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030601, "upstream_inference_completions_cost": 0.000332}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.543067", "feed": "arxiv_cscv", "title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts", "link": "https://papers.cool/arxiv/2510.19986", "analysis": {"summary": "The paper proposes a methodology that combines Large Language Models with Retrieval‑Augmented Generation and vector‑based search to automatically assign Iconclass codes to early‑modern religious woodcut illustrations. By generating full‑page textual descriptions that integrate visual and textual cues and then matching them via hybrid vector search, the system attains 87 % and 92 % precision at five‑ and four‑level classification, surpassing conventional image‑ and keyword‑based approaches.", "summary_cn": "本文提出一种将大型语言模型（LLM）与检索增强生成（RAG）以及向量搜索相结合的方法，用于自动为早期现代宗教木刻插图分配 Iconclass 编码。通过生成融合视觉和文本信息的整页描述，并通过混合向量检索匹配相应编码，系统在五级和四级分类上分别实现 87% 与 92% 的精度，显著优于传统的图像和关键词检索方法。", "keywords": "Iconclass, LLM, Retrieval-Augmented Generation, vector search, image classification, digital humanities, religious woodcuts, multimodal description", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Drew B. Thomas"]}, "usage": {"completion_tokens": 816, "prompt_tokens": 3364, "total_tokens": 4180, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 608, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069196, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028396, "upstream_inference_completions_cost": 0.000408}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.690722", "feed": "arxiv_cscv", "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models", "link": "https://papers.cool/arxiv/2510.20468", "analysis": {"summary": "The paper introduces a black-box, one-shot watermark forging attack that uses a preference model trained on procedurally generated images to determine watermark presence and then optimizes a target image via backpropagation to embed the stolen watermark, requiring only a single example and no knowledge of the original watermarking scheme. Experiments across several post-hoc image watermarking methods show that the attack can reliably forge watermarks, exposing a significant security weakness in current watermarking approaches.", "summary_cn": "本文提出一种黑盒单次水印伪造攻击，利用在程序生成图像上训练的偏好模型判断图像是否带有水印，并通过反向传播优化目标图像以嵌入被盗的水印，仅需一张水印图像且无需了解原始水印模型。对多种后置图像水印方法的实验显示，该攻击能够有效伪造水印，揭示了现有水印技术的重大安全缺陷。", "keywords": "watermarking, watermark forging, black-box attack, preference model, post-hoc image watermark, AI-generated content, security, adversarial forgery", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tomáš Souček", "Sylvestre-Alvise Rebuffi", "Pierre Fernandez", "Nikola Jovanović", "Hady Elsahar", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"]}, "usage": {"completion_tokens": 600, "prompt_tokens": 3462, "total_tokens": 4062, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 313, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059866, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029866, "upstream_inference_completions_cost": 0.0003}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:26.800402", "feed": "arxiv_cscv", "title": "Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories", "link": "https://papers.cool/arxiv/2510.20182", "analysis": {"summary": "The paper introduces an evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of multi‑person pedestrian dynamics, including a method to reconstruct 2D bird's‑eye view trajectories without known camera parameters. Using start frames from established datasets and a prompt suite for varying pedestrian densities, the authors assess how well leading models capture plausible multi‑agent behavior and identify failure modes such as merging and disappearing agents.", "summary_cn": "本文提出了一套评估方案，用于基准测试文本到视频（T2V）和图像到视频（I2V）模型在多行人轨迹模拟中的表现，并设计了一种在未知摄像机参数条件下从像素空间重建二维鸟瞰轨迹的方法。通过使用已有数据集的起始帧以及针对不同行人密度的提示集合，作者评估了主流模型在捕捉合理多主体行为方面的能力，并指出了合并和人物消失等失效模式。", "keywords": "video generation, text-to-video, image-to-video, pedestrian trajectory simulation, multi-agent dynamics, evaluation benchmark, trajectory reconstruction, world simulation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Aaron Appelle", "Jerome P. Lynch"]}, "usage": {"completion_tokens": 902, "prompt_tokens": 3381, "total_tokens": 4283, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 749, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073751, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028651, "upstream_inference_completions_cost": 0.000451}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:27.205765", "feed": "arxiv_cscv", "title": "Inverse Image-Based Rendering for Light Field Generation from Single Images", "link": "https://papers.cool/arxiv/2510.20132", "analysis": {"summary": "This paper introduces inverse image-based rendering, a neural rendering pipeline that reconstructs light flows from a single image to synthesize novel views and generate full light fields without additional training or specialized hardware. By storing source ray light flows, using cross-attention to model their relationships, and iteratively updating occluded content, the method outperforms existing novel view synthesis approaches on diverse datasets.", "summary_cn": "本文提出逆向基于图像的渲染（inverse image-based rendering）方法，利用神经渲染管线从单张输入图像重建光流，从而合成任意视点的光场，无需额外硬件或重新训练。该方法先存储源光线的光流，通过交叉注意力建模光线关系，再预测目标光线颜色，并在迭代过程中更新遮挡区域，实现一致的全光场生成，在多个挑战性数据集上优于现有新视点合成技术。", "keywords": "light field, inverse image-based rendering, neural rendering, novel view synthesis, light flow, cross-attention", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hyunjun Jung", "Hae-Gon Jeon"]}, "usage": {"completion_tokens": 816, "prompt_tokens": 3491, "total_tokens": 4307, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 639, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030301, "upstream_inference_completions_cost": 0.000408}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:27.327428", "feed": "arxiv_cscv", "title": "Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning", "link": "https://papers.cool/arxiv/2510.20108", "analysis": {"summary": "The paper identifies partial prototype collapse as a common failure in prototypical self‑supervised learning, where many prototypes converge to nearly identical vectors, reducing target diversity. By analyzing the joint optimization of encoders and prototypes, the authors propose a fully decoupled training scheme that updates prototypes via an online EM‑style Gaussian‑mixture model independent of the encoder loss, eliminating collapse and improving downstream performance.", "summary_cn": "本文指出原型自监督学习中常出现的部分原型坍缩现象，即多个原型向几乎相同的表示收敛，导致目标多样性下降。通过分析编码器与原型的联合优化导致的快捷学习，作者提出一种完全解耦的训练策略，使用在线 EM 式的高斯混合模型独立更新原型，从而消除坍缩并提升下游任务表现。", "keywords": "prototypical self-supervised learning, prototype collapse, Gaussian mixture, EM algorithm, decoupled training, representation diversity", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gabriel Y. Arteaga", "Marius Aasan", "Rwiddhi Chakraborty", "Martine Hjelkrem-Tan", "Thalles Silva", "Michael Kampffmeyer", "Adín Ramírez Rivera"]}, "usage": {"completion_tokens": 568, "prompt_tokens": 3390, "total_tokens": 3958, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 321, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057186, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028786, "upstream_inference_completions_cost": 0.000284}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:27.383363", "feed": "arxiv_cscv", "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation", "link": "https://papers.cool/arxiv/2510.20813", "analysis": {"summary": "GSWorld is a closed-loop, photo‑realistic simulation suite for robotic manipulation that combines 3D Gaussian Splatting with physics engines via a new Gaussian Scene Description File (GSDF) format. The framework enables zero‑shot sim2real policy learning, automated DAgger data collection, reproducible benchmarking, virtual teleoperation, and visual reinforcement learning without requiring real robots. A curated database of robot embodiments and objects supports diverse manipulation tasks.", "summary_cn": "GSWorld 是一个闭环的、逼真渲染的机器人操作仿真平台，基于 3D Gaussian Splatting 与物理引擎，并引入 Gaussian Scene Description File（GSDF）格式，将高斯‑网格表示与机器人 URDF 等对象融合。该框架实现了零样本 sim2real 像素到动作策略学习、自动 DAgger 数据收集、可复现的仿真基准测试、虚拟远程操作以及视觉强化学习，无需真实机器人。系统提供了多种机器人形态和 40 多个对象的数据库，支持多样化的操作任务。", "keywords": "Gaussian Splatting, photo-realistic simulation, robotic manipulation, sim2real, GSDF, DAgger data collection, virtual teleoperation, reinforcement learning, physics engine, closed-loop evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guangqi Jiang", "Haoran Chang", "Ri-Zhao Qiu", "Yutong Liang", "Mazeyu Ji", "Jiyue Zhu", "Zhao Dong", "Xueyan Zou", "Xiaolong Wang"]}, "usage": {"completion_tokens": 708, "prompt_tokens": 3441, "total_tokens": 4149, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 417, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064951, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029551, "upstream_inference_completions_cost": 0.000354}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:28.221635", "feed": "arxiv_cscv", "title": "Synthetic Data for Robust Runway Detection", "link": "https://papers.cool/arxiv/2510.20349", "analysis": {"summary": "The paper proposes generating synthetic runway images using a commercial flight simulator and combining them with a small set of annotated real images to train standard object detection models for accurate runway detection. By controlling the image generation process and applying a customized domain adaptation strategy, the approach improves robustness to adverse conditions such as nighttime scenes that are absent from the real data.", "summary_cn": "本文利用商业飞行模拟器生成合成跑道图像，并与少量标注的真实图像结合，训练目标检测模型，以实现准确的跑道检测。通过控制合成过程并采用定制的领域适应策略，展示了模型在真实数据中未出现的夜间等不利条件下的鲁棒性提升。", "keywords": "synthetic data, runway detection, domain adaptation, object detection, autonomous landing, robustness, flight simulator, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Fabrice Jimenez", "Thomas Oberlin"]}, "usage": {"completion_tokens": 773, "prompt_tokens": 3399, "total_tokens": 4172, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 650, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067571, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.0003865}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:28.303484", "feed": "arxiv_cscv", "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs", "link": "https://papers.cool/arxiv/2510.20762", "analysis": {"summary": "MEIcoder is a biologically informed decoding method that uses neuron-specific most exciting inputs (MEIs), a structural similarity index loss, and adversarial training to reconstruct visual stimuli from primary visual cortex activity. It achieves state-of-the-art performance especially on small datasets, requiring as few as 1,000-2,500 neurons and under 1,000 training samples, and introduces a large benchmark for future work.", "summary_cn": "MEIcoder 是一种结合神经元特定的最激发输入（MEI）、结构相似性指数（SSIM）损失以及对抗训练的生物信息解码方法，用于从初级视觉皮层（V1）的神经活动中重建视觉刺激。该方法在小规模数据集表现卓越，仅需约 1,000-2,500 个神经元和不足 1,000 条训练样本即可实现高保真图像重建，并提供了一个包含 160,000 多样本的统一基准以促进后续研究。", "keywords": "visual decoding, neural activity, most exciting inputs, MEIcoder, brain-machine interface, adversarial training, SSIM loss, neuroscience, small dataset, representation learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jan Sobotka", "Luca Baroni", "Ján Antolík"]}, "usage": {"completion_tokens": 784, "prompt_tokens": 3448, "total_tokens": 4232, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 551, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068856, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029656, "upstream_inference_completions_cost": 0.000392}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:29.224685", "feed": "arxiv_cscv", "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses", "link": "https://papers.cool/arxiv/2510.20027", "analysis": {"summary": "The paper introduces a real-time render-aware filtering technique for 3D Gaussian Splatting models that mitigates visual noise when rendering from camera far outside the training distribution. By computing sensitivity scores from intermediate gradients and targeting anisotropic orientation instabilities, the filter improves fidelity, realism, and consistency without requiring retraining, outperforming NeRF-based baselines such as BayesRays.", "summary_cn": "本文提出一种实时渲染感知的过滤方法，用于在 3D Gaussian Splatting 模型从训练分布之外的相机位姿进行渲染时抑制视觉噪声。该方法利用中间梯度计算的敏感度分数，专注于各向异性方向的不稳定性，从而在无需重新训练的情况下提升图像的真实感和一致性，优于 BayesRays 等 NeRF 基准。", "keywords": "3D Gaussian Splatting, novel view synthesis, out-of-distribution, render-aware filtering, gradient sensitivity, neural rendering, NeRF, BayesRays, real-time", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Damian Bowness", "Charalambos Poullis"]}, "usage": {"completion_tokens": 969, "prompt_tokens": 3423, "total_tokens": 4392, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 787, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00077731, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029281, "upstream_inference_completions_cost": 0.0004845}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:29.298431", "feed": "arxiv_cscv", "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples", "link": "https://papers.cool/arxiv/2510.20800", "analysis": {"summary": "The paper introduces a fast LLM adaptation method that, using a single gradient step on only 100 examples, identifies a small set of weight matrices for low‑rank factor based on singular‑value gradients, eliminating exhaustive layer‑wise search and achieving up to 24.6% accuracy gains without fine‑tuning. It demonstrates that evaluating on a tiny sample set suffices because downstream performance is dominated by prompting style rather than dataset size, resulting in a practical and robust compression‑based adaptation pipeline.", "summary_cn": "本文提出一种快速的 LLM 适应方法，仅使用 100 条样本进行一次梯度更新，即可通过奇异值梯度识别少数需要进行低秩分解的权重矩阵，省去逐层全面搜索，并在无需微调的情况下实现最高 24.6% 的准确率提升。研究表明，下游任务表现主要受提示风格影响，因而只需少量样本即可评估，从而形成一种实用且鲁棒的基于压缩的适应流程。", "keywords": "LLM adaptation, low-rank factorization, singular value gradient, matrix pruning, LASER, efficient fine-tuning, model compression, single-step adaptation", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}, "usage": {"completion_tokens": 914, "prompt_tokens": 3499, "total_tokens": 4413, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 706, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00076121, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030421, "upstream_inference_completions_cost": 0.000457}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:30.579508", "feed": "arxiv_cscv", "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking", "link": "https://papers.cool/arxiv/2510.20335", "analysis": {"summary": "The paper introduces Dino-Diffusion Parking (DDP), a domain‑agnostic autonomous parking pipeline that leverages visual foundation models and diffusion‑based planning to achieve robust perception and motion planning under distribution shifts. Trained only in a standard CARLA setting, DDP transfers zero‑shot to adverse weather and lighting conditions, attaining >90 % success across multiple out‑of‑distribution scenarios, and shows promising sim‑to‑real performance in a 3D Gaussian splatting reconstruction of a real parking lot.", "summary_cn": "本文提出 Dino‑Diffusion Parking (DDP) 系统，将视觉基础模型与基于扩散的规划相结合，实现跨域鲁棒的自动泊车。模型仅在标准 CARLA 环境下训练，却能在零样本条件下适应恶劣天气、光照等分布转移，在所有测试的 OOD 场景中停车成功率超过 90%，并在基于真实停车场的 3D Gaussian splatting 环境中展示了有前景的模拟到真实转移能力。", "keywords": "autonomous parking, domain generalization, diffusion planning, visual foundation models, zero-shot transfer, sim-to-real, robustness, 3D Gaussian splatting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Zixuan Wu", "Hengyuan Zhang", "Ting-Hsuan Chen", "Yuliang Guo", "David Paz", "Xinyu Huang", "Liu Ren"]}, "usage": {"completion_tokens": 1076, "prompt_tokens": 3391, "total_tokens": 4467, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 877, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00082601, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028801, "upstream_inference_completions_cost": 0.000538}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:34.201096", "feed": "arxiv_cscv", "title": "FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals", "link": "https://papers.cool/arxiv/2510.19917", "analysis": {"summary": "The paper presents FINDER, a framework that treats datasets as realizations of random fields and extracts stochastic features via the Kosambi‑Karhunen‑Loéve expansion, enabling classification in low‑signal‑to‑noise regimes by analyzing eigenspace residuals. It demonstrates state‑of‑the‑art performance on tasks such as Alzheimer's disease staging and deforestation detection, and discusses the method's expected advantages, failure modes, and limitations.", "summary_cn": "本文提出 FINDER 框架，将数据视为随机场的实现，通过 Kosambi‑Karhunen‑Loéve 展开提取随机特征，并利用特征的特征空间残差进行分类，尤其在信噪比低、样本稀少的情形下表现优秀。作者在阿尔茨海默病分期和森林砍伐遥感检测等数据匮乏的科学任务上实现了领先水平，并讨论了该方法的适用条件、失败模式及局限性。", "keywords": "noisy data classification, stochastic features, Karhunen–Loève expansion, eigen-decomposition, feature inference, robustness, Hilbert space mapping, data-deficient domains", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Trajan Murphy", "Akshunna S. Dogra", "Hanfeng Gu", "Caleb Meredith", "Mark Kon", "Julio Enrique Castrillion-Candas"]}, "usage": {"completion_tokens": 690, "prompt_tokens": 3443, "total_tokens": 4133, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 410, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064081, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029581, "upstream_inference_completions_cost": 0.000345}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:35.104902", "feed": "arxiv_cscv", "title": "Filter-Based Reconstruction of Images from Events", "link": "https://papers.cool/arxiv/2510.20071", "analysis": {"summary": "The paper introduces FIBAR, a filter‑based reconstruction algorithm that converts raw events from a moving event camera into intensity images using a temporal IIR filter and a novel stale‑pixel detection combined with Gaussian blurring. It runs on a standard laptop CPU at tens of millions of events per second and, while noisier and prone to ghosting compared to neural‑network approaches, can support simple tasks such as fiducial marker detection. Code and qualitative comparisons to a neural method (FireNet) are provided.", "summary_cn": "本文提出了 FIBAR，一种基于滤波的异步重建算法，通过时间数字 IIR 滤波器整合事件相机的强度变化，并使用新颖的陈旧像素检测与高斯模糊来降低噪声。该方法可在普通笔记本 CPU 上实现每秒数千万事件的实时处理，虽比神经网络方法（FireNet）产生更多噪声和幽灵影像，但已足以完成如标记检测等简单任务。文中提供了代码以及与神经网络方法的定性比较。", "keywords": "event camera, asynchronous reconstruction, filter-based method, IIR filter, stale pixel detection, Gaussian blur, CPU implementation, fiducial marker detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bernd Pfrommer"]}, "usage": {"completion_tokens": 671, "prompt_tokens": 3439, "total_tokens": 4110, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 370, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00061167, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027617, "upstream_inference_completions_cost": 0.0003355}}, "model": "openai/gpt-oss-120b"}
