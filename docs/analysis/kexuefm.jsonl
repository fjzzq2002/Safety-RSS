{"timestamp": "2025-10-22T03:02:48.679346", "feed": "kexuefm", "title": "随机矩阵的谱范数的快速估计", "link": "https://kexue.fm/archives/11335", "analysis": {"summary": "The article presents a fast estimation method for the spectral norm (largest singular value) of an $n\\times m$ matrix whose entries are independent standard normal variables, confirming that its expected value is roughly $\\sqrt{n}+\\sqrt{m}$ and detailing efficient computational techniques for this estimate.", "summary_cn": "本文介绍了一种快速估计 $n\\times m$ 标准正态随机矩阵谱范数（最大奇异值）的方法，证明其期望值约为 $\\sqrt{n}+\\sqrt{m}$，并给出实现高效估计的算法。", "keywords": "spectral norm, random matrix, Gaussian matrix, operator norm, estimation, singular value, random matrix theory", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 472, "prompt_tokens": 3380, "total_tokens": 3852, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 320, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0007902, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.000507, "upstream_inference_completions_cost": 0.0002832}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:02:57.272698", "feed": "kexuefm", "title": "MuP之上：1. 好模型的三个特征", "link": "https://kexue.fm/archives/11340", "analysis": {"summary": "The article examines the connection between the Muon optimizer (Momentum Orthogonalized by Newton‑Schulz) and the Maximal Update Parametrization (MuP), describing how their different origins converge on similar optimization goals and outlining three traits of a good model based on the author’s experimentation.", "summary_cn": "本文探讨了 Muon（Momentum Orthogonalized by Newton‑Schulz）优化器与 MuP（Maximal Update Parametrization）之间的关系，指出它们虽出发点不同却在模型优化目标上趋同，并总结出作者在实践中得到的好模型的三个特征。", "keywords": "MuP, Muon, model optimization, maximal update parametrization, Newton‑Schulz orthogonalization, training dynamics, deep learning, optimizer design", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 640, "prompt_tokens": 3459, "total_tokens": 4099, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 522, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00039436, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013836, "upstream_inference_completions_cost": 0.000256}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:02:59.802668", "feed": "kexuefm", "title": "重新思考学习率与Batch Size（二）：平均场", "link": "https://kexue.fm/archives/11280", "analysis": {"summary": "The article revisits the relationship between learning rate and batch size, focusing on the use of mean-field approximations to simplify the derivation of optimal learning rates for algorithms like SignSGD and SoftSignSGD. By moving the expectation inside the nonlinear functions, the author derives an approximate formula for the optimal learning rate involving expectations of the sign gradients and the Hessian.", "summary_cn": "本文重新审视学习率与批量大小的关系，重点介绍利用平均场近似来简化 SignSGD 与 SoftSignSGD 等算法的学习率推导。通过将求期望移入非线性函数内部，作者得到一个涉及符号梯度期望和 Hessian 的近似最优学习率公式。", "keywords": "learning rate, batch size, SignSGD, SoftSignSGD, mean-field approximation, optimization, gradient expectation, Hessian", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 496, "prompt_tokens": 4231, "total_tokens": 4727, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 217, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00033555, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00021155, "upstream_inference_completions_cost": 0.000124}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:03:00.650655", "feed": "kexuefm", "title": "DiVeQ：一种非常简洁的VQ训练方案", "link": "https://kexue.fm/archives/11328", "analysis": {"summary": "DiVeQ proposes a very concise training scheme for vector quantization (VQ) that replaces the traditional auxiliary loss with a new Straight-Through Estimator (STE) technique. By eliminating the extra loss term, the method simplifies the VQ training pipeline and reduces the need for additional hyperparameters, while maintaining performance.", "summary_cn": "DiVeQ 提出了一种极简的 VQ（向量量化）训练方案，使用全新的 STE（Straight-Through Estimator）技巧取代传统的 Aux Loss。该方法不再需要额外的辅助损失和超参数调节，使 VQ 的训练过程更加端到端且简洁。", "keywords": "vector quantization, VQ-VAE-through estimator, DiVeQ, discrete tokenization, deep learning, auxiliary loss removal, training simplification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 629, "prompt_tokens": 3370, "total_tokens": 3999, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 427, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 64}, "cost": 0.00081552, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00050102, "upstream_inference_completions_cost": 0.0003145}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:03:04.659242", "feed": "kexuefm", "title": "AdamW的Weight RMS的渐近估计", "link": "https://kexue.fm/archives/11307", "analysis": {"summary": "The article revisits the mean-field approximation for AdamW and demonstrates that the RMS of model weights can be asymptotically estimated from the optimizer's hyperparameters, a result that is counterintuitive because weight magnitudes are usually thought to be learned from data. It reproduces the derivation of the weight RMS asymptotic estimate and discusses its implications.", "summary_cn": "本文重新审视 AdamW 的平均场近似，展示模型权重的均方根（RMS）可以通过优化器的超参数提前获得渐近估计，这一结论令人意外，因为权重模长通常被认为是从训练数据中学习得到的。文章复现了权重 RMS 的渐近估计推导并讨论了其意义。", "keywords": "AdamW, weight RMS, mean-field approximation, asymptotic estimation, optimizer hyperparameters, gradient descent", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 617, "prompt_tokens": 3978, "total_tokens": 4595, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 338, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00035315, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001989, "upstream_inference_completions_cost": 0.00015425}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:03:11.506523", "feed": "kexuefm", "title": "重新思考学习率与Batch Size（三）：Muon", "link": "https://kexue.fm/archives/11285", "analysis": {"summary": "The article revisits the relationship between learning rate and batch size, extending prior mean‑field analyses to the Muon optimizer, which uses a non‑element‑wise update rule. It adapts the previous mean‑field framework to derive conclusions about how Muon's learning rate should scale with batch size.", "summary_cn": "本文重新审视学习率与批量大小的关系，将之前的平均场分析方法扩展到采用非逐元素更新规则的 Muon 优化器，并推导出 Muon 学习率随批量大小的缩放结论。", "keywords": "learning rate, batch size, Muon optimizer, mean field theory, optimization", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 485, "prompt_tokens": 3358, "total_tokens": 3843, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 312, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00032832, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013432, "upstream_inference_completions_cost": 0.000194}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:06:19.283134", "feed": "kexuefm", "title": "为什么线性注意力要加Short Conv？", "link": "https://kexue.fm/archives/11320", "analysis": {"summary": "The article examines why recent linear attention models add a short convolution to the Q, K, V projections, arguing that it compensates for the loss of expressivity caused by linearization and enhances token-mixing depth. The author proposes a more precise hypothesis about the mechanism, suggesting that the convolution injects local context and improves information flow across tokens.", "summary_cn": "本文探讨了为何最新的线性注意力模型在 Q、K、V 投影上加入 Short Conv，认为这是为了弥补线性化导致的表达能力下降并加强 Token 之间的混合。作者给出更具体的猜测机制，指出卷积引入局部上下文，提升信息在 Tokens 之间的流动。", "keywords": "linear attention, short convolution, token mixing, model architecture, QKV projection, expressivity", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 661, "prompt_tokens": 3957, "total_tokens": 4618, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 392, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.0003631, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00019785, "upstream_inference_completions_cost": 0.00016525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:07:49.619239", "feed": "kexuefm", "title": "为什么Adam的Update RMS是0.2？", "link": "https://kexue.fm/archives/11267", "analysis": {"summary": "The article discusses an observed phenomenon that the root mean square (RMS) of Adam optimizer updates remains around 0.2–0.3 during large‑scale LLM training with beta1=0.9 and beta2=0.95, and questions why this value appears consistently. It mentions a technique called \"Match Adam Update RMS\" used to transfer from Adam to the Muon optimizer by fixing Muon's update RMS to 0.2, enabling reuse of learning rate and weight decay settings. The post seeks theoretical explanations for this empirical constant.", "summary_cn": "本文讨论了在大规模语言模型训练中，使用 Adam 优化器（beta1=0.9、beta2=0.95）时，其更新的均方根（RMS）大约保持在 0.2 到 0.3 之间的现象，并提出了 \"Match Adam Update RMS\" 技巧，即将 Muon 优化器的更新 RMS 固定为 0.2，以便直接复用 Adam 的学习率和权重衰减。文章试图从理论上解释这一经验常数的来源。", "keywords": "Adam optimizer, update RMS, Muon optimizer, LLM training, learning rate transfer, beta1, beta2", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 4, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 526, "prompt_tokens": 4069, "total_tokens": 4595, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 169, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00033495, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00020345, "upstream_inference_completions_cost": 0.0001315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:11:35.858797", "feed": "kexuefm", "title": "重新思考学习率与Batch Size（一）：现状", "link": "https://kexue.fm/archives/11260", "analysis": {"summary": "This article revisits the relationship between learning rate and batch size, reviewing the classic second-order analysis introduced by OpenAI and highlighting its complexity when applied to non‑SGD optimizers. It proposes a simplified derivation pathway that is more general and lightweight, and explores how the analysis might be extended to the Muon optimizer. The goal is to provide clearer theoretical insight and practical guidance for scaling hyperparameters across different optimizers.", "summary_cn": "本文重新审视学习率与批次大小的关系，回顾了 OpenAI 提出的二阶近似分析，并指出该方法在非 SGD 优化器下计算复杂。作者提出了一条更通用、更简洁的推导路径，并探讨了将该分析推广到 Muon 优化器的可能性，旨在为不同优化器的超参数缩放提供更清晰的理论指导。", "keywords": "learning rate scaling, batch size, optimizer, second-order analysis, Muon optimizer, SGD, hyperparameter theory, training dynamics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 638, "prompt_tokens": 3991, "total_tokens": 4629, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 333, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00035905, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00019955, "upstream_inference_completions_cost": 0.0001595}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:12:49.329000", "feed": "kexuefm", "title": "重新思考学习率与Batch Size（四）：EMA", "link": "https://kexue.fm/archives/11301", "analysis": {"summary": "The article revisits the role of exponential moving average (EMA) – commonly known as momentum – in optimizers such as Adam, Lion and Muon, and examines why SignSGD is not a perfect theoretical proxy for Adam. It highlights that SignSGD’s update norm stays constant at 1, unlike Adam where momentum changes the magnitude, and investigates how EMA influences learning‑rate and batch‑size scaling rules.", "summary_cn": "本文重新审视了 EMA（动量）在 Adam、Lion、Muon 等优化器中的作用，并探讨了 SignSGD 为什么并非 Adam 的理想理论近似。文章指出 SignSGD 的更新范数始终为 1，而 Adam 受动量影响其幅度会变化，进而分析 EMA 对学习率和批量大小尺度规则的影响。", "keywords": "EMA, momentum, Adam, SignSGD, learning rate scaling, batch size scaling, optimizer analysis", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["苏剑林"]}, "usage": {"completion_tokens": 610, "prompt_tokens": 3403, "total_tokens": 4013, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 394, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.00038012, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00013612, "upstream_inference_completions_cost": 0.000244}}, "model": "openai/gpt-oss-120b"}
