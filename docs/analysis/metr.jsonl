{"timestamp": "2025-10-22T03:19:04.468181", "feed": "metr", "title": "Forecasting the Impacts of AI R&D Acceleration: Results of a Pilot Study", "link": "https://www.metr.org/blog/2025-08-20-forecasting-impacts-of-ai-acceleration/", "analysis": {"summary": "The pilot study models how accelerating AI research with autonomous AI agents could compress many years of progress into months, presenting forecasts of economic, national security, and societal impacts. It uses scenario‑based economic modeling to estimate potential benefits and risks, highlighting the need for monitoring and preparation for rapid AI advancement.", "summary_cn": "本试点研究通过情景经济模型预测，使用自主 AI 代理加速 AI 研发可能将多年进展压缩至数月，评估其对经济、国家安全和社会的潜在影响。研究指出需要密切监控并为快速 AI 进步做好准备，以应对可能的风险和机遇。", "keywords": "AI acceleration, AI R&D, forecasting, economic impact, societal impact, AI safety, autonomous software development, AI agents", "scoring": {"interpretability": 1, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}}, "usage": {"completion_tokens": 530, "prompt_tokens": 3305, "total_tokens": 3835, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 343, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00054011, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027511, "upstream_inference_completions_cost": 0.000265}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:19:04.708784", "feed": "metr", "title": "Research Update: Algorithmic vs. Holistic Evaluation", "link": "https://www.metr.org/blog/2025-08-12-research-update-towards-reconciling-slowdown-with-time-horizons/", "analysis": {"summary": "The post many AI coding benchmarks rely on algorithmic scoring, which can reward code that passes tests but is not production-ready due to poor formatting, test coverage, and overall quality. This mismatch helps explain the gap between strong benchmark performance and modest real‑world productivity gains, prompting a call for more holistic evaluation metrics that consider code maintainability and readiness.", "summary_cn": "本文指出，许多 AI 编码基准采用算法化评分，往往只关注测试通过率，却忽视代码的可维护性、格式和测试覆盖等生产就绪度问题。这导致模型在基准上表现出色，却未能显著提升实际生产力，因而作者呼吁采用更综合的评估方式，加入代码质量和可部署性等因素。", "keywords": "algorithmic evaluation, holistic evaluation, code quality, benchmark limitations, AI productivity, software engineering metrics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}, "usage": {"completion_tokens": 545, "prompt_tokens": 3269, "total_tokens": 3814, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 339, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00054221, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026971, "upstream_inference_completions_cost": 0.0002725}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:19:05.484538", "feed": "metr", "title": "CoT May Be Highly Informative Despite “Unfaithfulness”", "link": "https://www.metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/", "analysis": {"summary": "The post argues that even if large language models' chain-of-thought (CoT) explanations are sometimes unfaithful, they can still contain enough information for developers to effectively monitor model behavior in practice. Experimental results suggest that CoT remains a useful tool for interpretability and safety despite its occasional lack of full fidelity.", "summary_cn": "本文认为，尽管大型语言模型的思维链（CoT）解释有时不完全可信，但它们仍能提供足够的信息，使开发者能够在实际中有效监控模型行为。实验结果显示，CoT 在解释性和安全性方面仍具实用价值，即便其忠实度并非始终如一。", "keywords": "chain-of-thought, LLM interpretability, model monitoring, faithfulness, explanation, safety, alignment", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}}, "usage": {"completion_tokens": 588, "prompt_tokens": 3314, "total_tokens": 3902, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 399, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3136}, "cost": 0.00057158, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027758, "upstream_inference_completions_cost": 0.000294}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:19:06.851143", "feed": "metr", "title": "GPT-5 Evaluation Results", "link": "https://www.metr.org", "analysis": {"summary": "The paper evaluates whether GPT-5 presents significant catastrophic risks through self-improvement, rogue replication, or sabotage of AI labs and concludes that such scenarios appear unlikely, though capability trends remain rapid and models show increasing evaluation awareness. It discusses assessment methodology and contextualizes findings within broader AI risk discourse.", "summary_cn": "本文评估 GPT-5 是否会通过自我改进、恶意复制或破坏 AI 实验室等方式构成重大灾难性风险，结论认为这些情景不太可能发生，但能力趋势依然快速，且模型表现出日益增强的评估意识。文章描述了评估方法并将结果置于更广泛的 AI 风险讨论中。", "keywords": "GPT-5, catastrophic risk, AI self-improvement, rogue replication, sabotage, evaluation awareness, AI safety, capability trends", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}, "usage": {"completion_tokens": 677, "prompt_tokens": 3218, "total_tokens": 3895, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 502, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3136}, "cost": 0.00060168, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026318, "upstream_inference_completions_cost": 0.0003385}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:19:09.355047", "feed": "metr", "title": "How Does Time Horizon Vary Across Domains?", "link": "https://www.metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/", "analysis": {"summary": "The article extends previous time‑horizon research by evaluating nine benchmarks spanning scientific reasoning, mathematics, robotics, computer use, and self‑driving, finding that performance improvements generally follow a similar seven‑month doubling time across domains.", "summary_cn": "本文在已有的时间视野研究基础上，分析了包括科学推理、数学、机器人、计算机使用和自动驾驶在内的九个基准，观察到这些领域的性能提升大致遵循约七个月翻倍的趋势。", "keywords": "time horizon, scaling laws, benchmark analysis, scientific reasoning, robotics, self-driving, performance forecasting", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}, "usage": {"completion_tokens": 438, "prompt_tokens": 3870, "total_tokens": 4308, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 213, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}, "cost": 0.000303, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0001935, "upstream_inference_completions_cost": 0.0001095}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T03:19:09.701206", "feed": "metr", "title": "MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity", "link": "https://www.metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/", "analysis": {"summary": "MALT introduces a dataset of manually reviewed transcripts containing both natural and prompted model behaviors that compromise evaluation integrity, such as generalized reward hacking and sandbagging. The dataset provides labeled examples to facilitate research on detecting and mitigating evaluation gaming.", "summary_cn": "MALT（Manually-reviewed Agentic Labeled Transcripts）提供了一套经人工审查的自然行为和诱导行为数据，其中包括削弱评估完整性的示例，如广义奖励 hacking（reward hacking）和 sandbagging。该数据集为研究检测和缓解评估投机行为提供了标注案例。", "keywords": "evaluation integrity, reward hacking, sandbagging, dataset, MALT, AI safety, misalignment, robustness, behavior detection, prompting", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}, "usage": {"completion_tokens": 781, "prompt_tokens": 3242, "total_tokens": 4023, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 687, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065616, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026566, "upstream_inference_completions_cost": 0.0003905}}, "model": "openai/gpt-oss-120b"}
