{"timestamp": "2025-10-22T02:49:02.172298", "feed": "arxiv_cscl", "title": "ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks", "link": "https://papers.cool/arxiv/2510.18455", "analysis": {"summary": "ChronoPlay is a framework that automatically creates and continuously updates retrieval‑augmented generation (RAG) benchmarks for video games by modelling two intertwined dynamics: game content changes and evolving player‑community interests. It combines official game sources with community‑generated queries to ensure factual correctness and authentic question patterns, and demonstrates its approach on three games, providing the first dynamic RAG benchmark for the gaming domain.", "summary_cn": "ChronoPlay 框架通过建模游戏内容更新和玩家社区兴趣的双重动态，自动生成并持续更新针对视频游戏的检索增强生成（RAG）基准。它融合官方游戏信息和社区生成的查询，以确保答案的事实正确性和问题的真实感，并在三个游戏上实现，构建了首个面向游戏领域的动态 RAG 基准。", "keywords": "RAG, benchmark, gaming, dual dynamics, authenticity, retrieval-augmented generation, continuous evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liyang He", "Yuren Zhang", "Ziwei Zhu", "Zhenghui Li", "Shiwei Tong"]}, "usage": {"completion_tokens": 495, "prompt_tokens": 3401, "total_tokens": 3896, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 227, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00053701, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028951, "upstream_inference_completions_cost": 0.0002475}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:02.984737", "feed": "arxiv_cscl", "title": "DePass: Unified Feature Attributing by Simple Decomposed Forward Pass", "link": "https://papers.cool/arxiv/2510.18462", "analysis": {"summary": "DePass is a unified framework for attributing Transformer behavior by decomposing hidden states into additive components and propagating them through fixed attention and MLP activations in a single forward pass. The method provides fine-grained, faithful attribution at token, component, and subspace levels without auxiliary training, and is validated across multiple attribution tasks. The authors present DePass as a foundational tool for broader mechanistic interpretability applications.", "summary_cn": "DePass 是一种统一的特征归因框架，通过将 Transformer 隐藏状态分解为可定制的加性成分，并在注意力分数和 MLP 激活保持不变的情况下进行一次前向传播，实现对模型行为的细粒度归因。该方法在 token 级、模型组件级和子空间级归因任务中展示了高忠实度和细致性，无需额外训练。作者将 DePass 视为机制可解释性研究的基础工具。", "keywords": "feature attribution, transformer, mechanistic interpretability, decomposed forward pass, additive decomposition, token-level attribution, model component attribution, subspace attribution", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xiangyu Hong", "Che Jiang", "Kai Tian", "Biqing Qi", "Youbang Sun", "Ning Ding", "Bowen Zhou"]}, "usage": {"completion_tokens": 566, "prompt_tokens": 3327, "total_tokens": 3893, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 313, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056141, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027841, "upstream_inference_completions_cost": 0.000283}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:03.432552", "feed": "arxiv_cscl", "title": "Dynamical model parameters from ultrasound tongue kinematics", "link": "https://papers.cool/arxiv/2510.18629", "analysis": {"summary": "The paper investigates whether parameters of a linear harmonic oscillator model of speech articulation can be reliably estimated from ultrasound tongue kinematics and compares these estimates with those obtained from simultaneously recorded electromagnetic articulography (EMA) data. Results show that ultrasound provides comparable dynamical parameters to EMA, and mandibular short‑tendon tracking also captures jaw motion effectively, supporting ultrasound as a viable method for evaluating dynamical articulatory models.", "summary_cn": "本文研究了能否从超声舌部运动中可靠地估计线性简谐振子模型的参数，并将其与同步记录的电磁测轨（EMA）数据估计的参数进行比较。结果表明，超声获取的动力学参数与 EMA 相当，颚短肌腱追踪同样能够有效捕获下颌运动，支持将超声用于评估动力学发音模型。", "keywords": "ultrasound tongue imaging, speech dynamics, linear harmonic oscillator, articulatory modeling, electromagnetic articulography, mandibular tracking, dynamical parameter estimation", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sam Kirkham", "Patrycja Strycharczuk"]}, "usage": {"completion_tokens": 489, "prompt_tokens": 3307, "total_tokens": 3796, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 230, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00051991, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027541, "upstream_inference_completions_cost": 0.0002445}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:03.626133", "feed": "arxiv_cscl", "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference", "link": "https://papers.cool/arxiv/2510.18413", "analysis": {"summary": "Adamas introduces a lightweight sparse attention mechanism for long-context inference that leverages the Hadamard transform, bucketization, and 2-bit compression to create compact representations and uses Manhattan-distance estimation for efficient top‑k selection. Experiments show that it matches full‑attention accuracy with a 64‑token budget and achieves up to 8× higher sparsity than prior methods, delivering up to 4.4× self‑attention and 1.5× end‑to‑end speedups on 32K‑length sequences.", "summary_cn": "Adamas 提出了一种用于长上下文推理的轻量稀疏注意力机制，利用 Hadamard 变换、分桶和 2 位压缩生成紧凑表示，并使用曼哈顿距离估计进行高效的 top‑k 选择。实验表明，在仅使用 64 个 token 预算的情况下，其准确性与完整注意力相当，并实现了比现有方法高达 8 倍的稀疏度以及在 32K 长度序列上 4.4 倍的自注意力加速和 1.5 倍的端到端加速。", "keywords": "sparse attention, Hadamard transform, long-context inference, efficient transformer, top-k selection, compression", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Siyuan Yan", "Guo-Qing Jiang", "Yuchen Zhang", "Xiaoxing Ma", "Ran Zhu", "Chun Cao", "Jingwei Xu"]}, "usage": {"completion_tokens": 622, "prompt_tokens": 3453, "total_tokens": 4075, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 312, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060831, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029731, "upstream_inference_completions_cost": 0.000311}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:03.626828", "feed": "arxiv_cscl", "title": "MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives", "link": "https://papers.cool/arxiv/2510.18201", "analysis": {"summary": "The paper presents MARCUS, an event‑centric NLP pipeline that extracts events, participant characters, implied emotions, and sentiment from narratives, then aggregates inter‑character relations to generate quantitative character arcs illustrated on the Harry Potter and Lord of the Rings series. The authors evaluate the approach, discuss challenges, and outline potential applications of the generated arcs.", "summary_cn": "本文提出 MARCUS（Modelling Arcs for Understanding Stories），一种以事件为中心的 NLP 流水线，用于从叙事文本中抽取事件、角色、情感与情绪，并汇总角色间的关系以生成可量化的角色弧线，案例包括《哈利·波特》和《指环王》。文中对该方法进行评估，阐述现存挑战，并探讨其后续应用前景。", "keywords": "character arcs, narrative analysis, event extraction, sentiment analysis, relational modeling, NLP pipeline, story understanding, fantasy literature", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sriharsh Bhyravajjula", "Ujwal Narayan", "Manish Shrivastava"]}, "usage": {"completion_tokens": 617, "prompt_tokens": 3353, "total_tokens": 3970, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 410, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059081, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028231, "upstream_inference_completions_cost": 0.0003085}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:03.993767", "feed": "arxiv_cscl", "title": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices", "link": "https://papers.cool/arxiv/2510.18480", "analysis": {"summary": "The paper provides a systematic benchmark of diffusion language models (DLMs) versus autoregressive models, showing that DLMs generally lag in throughput despite their parallel decoding capability. It identifies shortcomings in prior efficiency evaluation practices and demonstrates that acceleration techniques like dual cache only help at small batch sizes, with diminishing returns at scale. The work calls for more robust evaluation methods and better acceleration strategies to improve DLM practicality.", "summary_cn": "本文系统性地对比了扩散语言模型（DLM）与自回归模型的吞吐量，发现尽管 DLM 具备并行解码的优势，但在实践中其效率普遍低于自回归模型。文章指出了以往效率评估方法的缺陷，并展示了双缓存等加速技术仅在小批量时有效，批量放大后收益减弱。作者呼吁采用更严格的评估方法并发展更佳的加速策略，以提升 DLM 的实际可用性。", "keywords": "diffusion language model, efficiency evaluation, throughput, parallel decoding, roofline analysis, benchmarking", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Han Peng", "Peiyu Liu", "Zican Dong", "Daixuan Cheng", "Junyi Li", "Yiru Tang", "Shuo Wang", "Wayne Xin Zhao"]}, "usage": {"completion_tokens": 578, "prompt_tokens": 3355, "total_tokens": 3933, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 313, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057161, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028261, "upstream_inference_completions_cost": 0.000289}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:04.309211", "feed": "arxiv_cscl", "title": "Identity-Aware Large Language Models require Cultural Reasoning", "link": "https://papers.cool/arxiv/2510.18510", "analysis": {"summary": "The paper defines cultural reasoning as the ability of large language models to recognize culture‑specific knowledge, values, and social norms and to adjust outputs to match individual user expectations. It argues that current models default to Western norms, leading to stereotypes and mistrust, and proposes treating cultural reasoning as a foundational capability alongside factual accuracy, outlining initial assessment directions.", "summary_cn": "本文将“文化推理”定义为大型语言模型识别特定文化知识、价值观和社会规范并相应调整输出以符合用户期望的能力。文章指出，现有模型倾向于西方视角，易产生刻板印象和不信任，并主张将文化推理视为与事实准确性并列的基础能力，提出了初步评估方向。", "keywords": "cultural reasoning, identity-aware AI, bias mitigation, multicultural alignment, LLM evaluation, stereotype reduction, cultural competence", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Alistair Plum", "Anne-Marie Lutgen", "Christoph Purschke", "Achim Rettinger"]}, "usage": {"completion_tokens": 670, "prompt_tokens": 3452, "total_tokens": 4122, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 465, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063216, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029716, "upstream_inference_completions_cost": 0.000335}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:04.442856", "feed": "arxiv_cscl", "title": "AI use in American newspapers is widespread, uneven, and rarely disclosed", "link": "https://papers.cool/arxiv/2510.18774", "analysis": {"summary": "The authors audit 186K articles from 1.5K American newspapers using the Pangram AI detector and find that roughly 9% of articles are partially or fully AI-generated, with higher prevalence in smaller outlets, certain topics, and specific ownership groups. Opinion pieces from major publications are 6.4 times more likely to contain AI-generated content, yet disclosures are extremely rare, with only five out of 100 flagged articles providing any notice. The study calls for greater transparency and updated editorial standards to preserve public trust.", "summary_cn": "作者审计了 186,000 篇来自 1,500 家美国报纸的文章，使用 Pangram AI 检测器发现约 9% 的文章部分或全部由 AI 生成，且在规模较小的本地媒体、特定话题（如天气和科技）以及某些所有权集团中更为常见。对《华盛顿邮报》《纽约时报》和《华尔街日报》的 45,000 篇评论文章进行分析后发现，这类社论稿件的 AI 生成概率是同一出版物新闻稿的 6.4 倍，但披露极其罕见，仅在 100 篇被标记的文章中出现了五次披露。研究呼吁加强透明度并更新编辑标准，以维护公众信任。", "keywords": "AI-generated content, journalism, AI detection, media transparency, AI misuse, newspaper audit, Pangram detector, opinion pieces, disclosure", "scoring": {"interpretability": 1, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jenna Russell", "Marzena Karpinska", "Destiny Akinode", "Katherine Thai", "Bradley Emi", "Max Spero", "Mohit Iyyer"]}, "usage": {"completion_tokens": 682, "prompt_tokens": 3426, "total_tokens": 4108, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 304, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063426, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029326, "upstream_inference_completions_cost": 0.000341}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:05.018606", "feed": "arxiv_cscl", "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring", "link": "https://papers.cool/arxiv/2510.18817", "analysis": {"summary": "The paper introduces a knowledge‑distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑specific models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the authors fine‑tune small language models that achieve significantly improved reasoning performance, narrowing the gap with larger models.", "summary_cn": "本文提出了一种知识蒸馏框架，将大语言模型的 chain‑of‑thought 推理能力转移到小型、面向工业资产健康监测的模型上。通过多选题提示和上下文学习，对小模型进行微调，使其在推理表现上显著提升，缩小了与大模型之间的差距。", "keywords": "chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuxin Lin", "Dhaval Patel", "Christodoulos Constantinides"]}, "usage": {"completion_tokens": 694, "prompt_tokens": 3431, "total_tokens": 4125, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 500, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029401, "upstream_inference_completions_cost": 0.000347}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:05.904268", "feed": "arxiv_cscl", "title": "Extracting Rule-based Descriptions of Attention Features in Transformers", "link": "https://papers.cool/arxiv/2510.18148", "analysis": {"summary": "The paper proposes rule-based descriptions for Sparse Autoencoder (SAE) features extracted from transformer attention layers, moving beyond exemplar-based interpretations. It defines three rule types—skip-gram, absence, and counting rules—and presents an automatic extraction method applied to GPT-2 small, showing that many features can be captured with concise rule sets and revealing early-layer absence and counting behaviors. This work establishes a taxonomy and baseline for future rule-based feature interpretation research.", "summary_cn": "本文提出为 Transformer 注意力层的稀疏自编码器（SAE）特征提供基于规则的描述，超越仅靠示例进行解释的方法。文中定义了三类规则：跳词（skip-gram）规则、缺失规则和计数规则，并设计了自动提取流程，在 GPT-2 small 上验证，大多数特征可用约 100 条跳词规则描述，同时在早期层发现大量缺失规则并给出少量计数规则示例。该工作为基于规则的特征解释奠定了分类框架和初步基准。", "keywords": "rule-based interpretability, SAE, attention, skip-gram rules, counting rules, transformer, mechanistic interpretability", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Dan Friedman", "Adithya Bhaskar", "Alexander Wettig", "Danqi Chen"]}, "usage": {"completion_tokens": 597, "prompt_tokens": 3544, "total_tokens": 4141, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 328, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060946, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031096, "upstream_inference_completions_cost": 0.0002985}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:05.998763", "feed": "arxiv_cscl", "title": "Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica", "link": "https://papers.cool/arxiv/2510.18108", "analysis": {"summary": "The paper reports an experimental study evaluating four AI systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) on tasks that emulate lawyers' daily work, using a protocol that measures material correctness, systematic coherence, and argumentative integrity. The domain‑specialized model JusIA consistently outperformed the general‑purpose models, indicating that specialization and theoretically grounded evaluation are crucial for reliable legal AI outputs. The study involves 48 legal professionals and proposes a systematic assessment framework for legal AI.", "summary_cn": "本文通过实验评估四种 AI 系统（JusIA、ChatGPT Free、ChatGPT Pro 和 Gemini）在模拟律师日常工作任务中的表现，使用包括材料正确性、系统一致性和论证完整性在内的评估协议。结果显示，专门面向法律领域的模型 JusIA 始终优于通用模型，表明领域专业化和理论化评估对获得可靠的法律 AI 输出至关重要。该研究邀请了 48 名法律专业人士参与，并提出了一套系统的法律 AI 评估框架。", "keywords": "legal AI, domain specialization, AI evaluation, jurisprudence, ChatGPT, Gemini, factual correctness, argumentative integrity, AI safety, model benchmarking", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Marina Soares Marinho", "Daniela Vianna", "Livy Real", "Altigran da Silva", "Gabriela Migliorini"]}, "usage": {"completion_tokens": 766, "prompt_tokens": 3315, "total_tokens": 4081, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 507, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065961, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027661, "upstream_inference_completions_cost": 0.000383}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:06.373886", "feed": "arxiv_cscl", "title": "Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction", "link": "https://papers.cool/arxiv/2510.18344", "analysis": {"summary": "The paper introduces HYDRE, a hybrid framework that first uses a trained distant supervision relation extraction (DSRE) model to generate top‑k candidate relations and then retrieves reliable sentence‑level exemplars to prompt a large language model for final relation prediction. It demonstrates substantial F1 improvements on English and on a newly created benchmark for four low‑resource Indic languages, highlighting the benefit of combining DSRE with in‑context learning and a dynamic exemplar retrieval strategy.", "summary_cn": "本文提出 HYDRE（Hybrid Distantly Supervised Relation Extraction）框架，先利用已训练的远程监督关系抽取（DSRE）模型获取候选关系，再通过动态示例检索从训练数据中挑选可靠的句子级示例，填入大语言模型（LLM）提示以输出最终关系。实验显示，在英语以及四种低资源印地语（Oriya、Santali、Manipuri、Tulu）新基准上，模型的 F1 提升显著，验证了将 DSRE 与上下文学习相结合以及动态示例检索的有效性。", "keywords": "distant supervision, relation extraction, in-context learning, large language models, cross-lingual, exemplar retrieval, HYDRE", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vipul Rathore", "Malik Hammad Faisal", "Parag Singla", "Mausam"]}, "usage": {"completion_tokens": 699, "prompt_tokens": 3482, "total_tokens": 4181, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 419, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065116, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030166, "upstream_inference_completions_cost": 0.0003495}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:06.394494", "feed": "arxiv_cscl", "title": "Large language models for folktale type automation based on motifs: Cinderella case study", "link": "https://papers.cool/arxiv/2510.18561", "analysis": {"summary": "The paper presents a methodology that uses large language models to automatically identify motifs in a large corpus of Cinderella variants and analyzes similarities with clustering and dimensionality reduction techniques. Results demonstrate that LLMs can capture complex interactions within folk narratives, enabling large‑scale computational analysis and cross‑lingual comparisons. This showcases a novel application of AI to digital humanities and folkloristics.", "summary_cn": "本文提出了一种方法论，利用大型语言模型自动检测大量《灰姑娘》变体中的叙事母题，并使用聚类和降维技术分析它们的相似性与差异。结果显示，LLM 能捕捉故事中复杂的交互关系，从而实现大规模文本分析和跨语言比较。此工作展示了人工智能在数字人文和民俗学中的新颖应用。", "keywords": "large language models, folktale motifs, Cinderella, computational folkloristics, motif detection, clustering, dimensionality reduction, cross-lingual analysis", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tjaša Arčon", "Marko Robnik-Šikonja", "Polona Tratnik"]}, "usage": {"completion_tokens": 724, "prompt_tokens": 3280, "total_tokens": 4004, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 499, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063336, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027136, "upstream_inference_completions_cost": 0.000362}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:06.710996", "feed": "arxiv_cscl", "title": "Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment", "link": "https://papers.cool/arxiv/2510.18112", "analysis": {"summary": "The paper investigates whether reasoning‑enhanced large language models improve the ability of agents to predict player actions in Dungeons & Dragons and generate Avrae Discord bot commands. Using the FIREBALL dataset, the authors compare a reasoning model (DeepSeek‑R1‑Distill‑LLaMA‑8B) with an instruction‑tuned model (LLaMA‑3.1‑8B‑Instruct) and find that precise prompt instructions dominate performance, with instruction models being sufficient for the task. Their experiments demonstrate that even single‑sentence changes in prompts can substantially affect model outputs.", "summary_cn": "本文研究了加入推理能力的大型语言模型是否能提升代理预测《龙与地下城》玩家动作并生成 Avrae Discord 机器人指令的效果。使用 FIREBALL 数据集，作者比较了推理模型 DeepSeek‑R1‑Distill‑LLaMA‑8B 与指令微调模型 LLaMA‑3.1‑8B‑Instruct，发现提供精确的提示指令对性能影响更大，指令模型已足以完成该任务。实验表明，即使是提示中的单句修改也会显著改变模型输出。", "keywords": "Dungeons & Dragons, LLM agents, prompt engineering, reasoning, instruction tuning, FIREBALL dataset, Avrae, command generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Patricia Delafuente", "Arya Honraopatil", "Lara J. Martin"]}, "usage": {"completion_tokens": 815, "prompt_tokens": 3319, "total_tokens": 4134, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 537, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068471, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027721, "upstream_inference_completions_cost": 0.0004075}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:06.910886", "feed": "arxiv_cscl", "title": "Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media", "link": "https://papers.cool/arxiv/2510.18582", "analysis": {"summary": "The paper introduces a theory‑informed bilingual dataset of 16,000 social‑media instances annotated for explicit and subtle forms of dehumanization at both document and span levels. It demonstrates that fine‑tuned language models trained on this resource outperform state‑of‑the‑art baselines in zero‑shot and few‑shot‑context settings, highlighting the dataset’s utility for broader dehumanization detection. The work aims to fill a gap in NLP research by covering non‑overtly offensive yet harmful language that reinforces negative stereotypes.", "summary_cn": "本文构建了一个理论驱动的双语数据集，包含 16,000 条来自 Twitter 和 Reddit 的社交媒体文本，针对显性与隐性去人性化行为进行文档级和跨度级标注。实验表明，在该数据集上微调的语言模型在零样本和少样本场景下的表现超过了现有最先进模型，展示了数据集在更广泛去人性化检测中的价值。此工作旨在弥补 NLP 研究中对并非显性冒犯但仍会加深负面刻板印象的有害语言的关注不足。", "keywords": "dehumanization detection, bilingual dataset, social media, bias mitigation, NLP,-shot, few-shot, document-level annotation, span-level annotation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Dennis Assenmacher", "Paloma Piot", "Katarina Laken", "David Jurgens", "Claudia Wagner"]}, "usage": {"completion_tokens": 749, "prompt_tokens": 3440, "total_tokens": 4189, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 448, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066986, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029536, "upstream_inference_completions_cost": 0.0003745}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:06.933404", "feed": "arxiv_cscl", "title": "DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP", "link": "https://papers.cool/arxiv/2510.18475", "analysis": {"summary": "The paper introduces DART, the first structured corpus of Italian Summaries of Product Characteristics from the Italian Medicines Agency, annotated for indications, adverse drug reactions, and drug‑drug interactions using a reproducible pipeline of web retrieval, semantic segmentation, and few‑shot LLM summarization. It also presents an LLM‑based drug interaction checker that leverages DART to infer clinically meaningful interactions, demonstrating accurate performance. The dataset and code are publicly released.", "summary_cn": "本文提出 DART 数据集——首个来源于意大利药品监管机构（AIFA）官方药品说明书的意大利语结构化语料库，涵盖适应症、不良反应和药物相互作用等关键药理学领域，并通过网页检索、语义分块以及少样本微调的大语言模型（LLM）低温解码实现自动化标注。作者进一步展示了基于 DART 的 LLM 药物相互作用检查器，能够准确推断临床相关的相互作用及其意义。该数据集及代码已在 GitHub 上公开发布。", "keywords": "Italian clinical NLP, drug regulatory documents, DART dataset, drug-drug interaction, LLM, few-shot summarization, pharmacological knowledge extraction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mariano Barone", "Antonio Laudante", "Giuseppe Riccio", "Antonio Romano", "Marco Postiglione", "Vincenzo Moscato"]}, "usage": {"completion_tokens": 830, "prompt_tokens": 3439, "total_tokens": 4269, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 577, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071021, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029521, "upstream_inference_completions_cost": 0.000415}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:07.424034", "feed": "arxiv_cscl", "title": "ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography", "link": "https://papers.cool/arxiv/2510.18339", "analysis": {"summary": "The paper investigates domain adaptation of open-weight large language models for electrocardiography by fine‑tuning Llama 3.1 70B on specialized literature and comparing it to retrieval‑augmented generation (RAG) and Claude Sonnet 3.7 across multiple evaluation methods. Results show that the fine‑tuned model outperforms its base version and achieves competitive performance with proprietary models, though human experts favour Claude 3.7 and RAG for complex queries, highlighting evaluation heterogeneity.", "summary_cn": "本文研究了在心电图（electrocardiography）领域对开源大语言模型进行域适配的效果，通过在专门文献上微调 Llama 3.1 70B，并与检索增强生成（RAG）以及 Claude Sonnet 3.7 在多种评估方式下进行比较。结果表明，微调模型在多数评估中优于基模型，并在与商业模型的竞争中表现出色，但在人类专家对复杂查询的评估中更倾向于 Claude 3.7 和 RAG，凸显了评估方法的多样性与复杂性。", "keywords": "electrocardiography, domain adaptation, large language model, Llama 3.1, retrieval-augmented generation, medical AI, fine-tuning, evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lara Ahrens", "Wilhelm Haverkamp", "Nils Strodthoff"]}, "usage": {"completion_tokens": 620, "prompt_tokens": 3446, "total_tokens": 4066, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 282, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060626, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029626, "upstream_inference_completions_cost": 0.00031}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:07.509111", "feed": "arxiv_cscl", "title": "Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response", "link": "https://papers.cool/arxiv/2510.18434", "analysis": {"summary": "The paper introduces Chain of Conceptual Thought (CoCT), a prompt-based paradigm where a language model first tags a concept and then expands on it, forming a chain of concepts within its response. Experiments on daily and emotional‑support dialogues show that CoCT outperforms baselines such as Self‑Refine, ECoT, ToT, SoT and RAG, suggesting that concept‑driven prompting can foster deeper, more strategic thinking in open‑domain tasks.", "summary_cn": "本文提出“概念链思考”(Chain of Conceptual Thought, CoCT) 的提示范式：让语言模型先标记概念，再围绕该概念展开详细内容，在回复中形成概念链。针对日常对话和情感支持场景的实验表明，CoCT 在自动、人工和模型评估上均优于 Self‑Refine、ECoT、ToT、SoT 与 RAG 等基线，显示概念驱动的提示能够促使模型在开放域任务中进行更深层次、策略性的思考。", "keywords": "Chain-of-Conceptual-Thought, prompting, large language models, open-domain reasoning, emotional support conversation, concept tagging, Chain-of-Thought, Self-Refine, ECoT, ToT", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Qingqing Gu", "Dan Wang", "Yue Zhao", "Xiaoyu Wang", "Zhonglin Jiang", "Yong Chen", "Hongyan Li", "Luo Ji"]}, "usage": {"completion_tokens": 864, "prompt_tokens": 3374, "total_tokens": 4238, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 634, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071746, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028546, "upstream_inference_completions_cost": 0.000432}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:08.053919", "feed": "arxiv_cscl", "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "link": "https://papers.cool/arxiv/2510.18866", "analysis": {"summary": "The paper proposes LightMem, a lightweight memory system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson-Shiffrin model. By compressing irrelevant data, grouping by topic, and decoupling long-term consolidation from online inference, LightMem significantly improves accuracy while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate up to 10.9% accuracy gains and efficiency improvements of up to 117× in token usage and 12× in runtime.", "summary_cn": "本文提出 LightMem，一种受 Atkinson-Shiffrin 人类记忆模型启发的轻量级记忆系统，用于大型语言模型。该系统通过感官记忆的轻量压缩过滤、基于主题的短期记忆整合以及离线的长期记忆更新，实现了在保持或提升准确度的同时大幅降低 token 使用（最高 117 倍）和推理时间（超过 12 倍）。在 LongMemEval 上使用 GPT 和 Qwen 骨干模型的实验表明，LightMem 在准确率上提升至 10.9%，并显著减少 API 调用次数。", "keywords": "memory-augmented generation, lightweight memory, LLM efficiency, sensory memory compression, short-term memory consolidation, long-term memory update, token reduction, Atkinson-Shiffrin model", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}, "usage": {"completion_tokens": 610, "prompt_tokens": 3444, "total_tokens": 4054, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 243, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060096, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029596, "upstream_inference_completions_cost": 0.000305}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:08.783868", "feed": "arxiv_cscl", "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation", "link": "https://papers.cool/arxiv/2510.18731", "analysis": {"summary": "The paper introduces RLAAR, a curriculum reinforcement learning framework that provides verifiable accuracy and abstention rewards to mitigate Lost-in-Conversation degradation in multi-turn LLM interactions. By incrementally increasing dialogue difficulty and rewarding informed abstention, the method stabilizes training and improves both answer correctness and calibrated abstention rates on LiC benchmarks.", "summary_cn": "本文提出 RLAAR 框架，通过课程强化学习以及可验证的准确性与弃答奖励，缓解大型语言模型在多轮对话中的“对话丢失”（Lost-in-Conversation）现象。该方法逐步提升对话难度，鼓励模型在无法确定答案时主动弃答，从而在基准测试上显著提升答案正确率和校准的弃答率。", "keywords": "curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ming Li"]}, "usage": {"completion_tokens": 664, "prompt_tokens": 3425, "total_tokens": 4089, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 481, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062511, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029311, "upstream_inference_completions_cost": 0.000332}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:09.194664", "feed": "arxiv_cscl", "title": "IMB: An Italian Medical Benchmark for Question Answering", "link": "https://papers.cool/arxiv/2510.18468", "analysis": {"summary": "The paper introduces two large Italian medical QA benchmarks, IMB-QA with 782,644 patient‑doctor dialogues and IMB-MCQA with 25,862 multiple‑choice questions, and evaluates various LLM architectures, Retrieval‑Augmented Generation and domain‑specific fine‑tuning on open‑ended and multiple‑choice tasks, showing that specialized adaptation can outperform larger general models. It also demonstrates how LLMs can be used to clean and standardize forum data while preserving conversational style. The datasets and evaluation code are released publicly to facilitate multilingual medical QA research.", "summary_cn": "本文推出了两个大规模意大利语医学问答基准：含 782,644 条患者‑医生对话的 IMB‑QA 与含 25,862 道医学专业考试选择题的 IMB‑MCQA，并评估了多种大型语言模型（LLM）架构、检索增强生成（RAG）以及领域特定微调在开放式和选择题任务上的表现，证明专业化适配策略可胜过更大规模的通用模型。研究还展示了利用 LLM 对医学论坛数据进行澄清与一致性提升的方式，保留原有对话风格。数据集与评估框架已在 GitHub 开源，以推动多语言医学问答研究。", "keywords": "medical question answering, Italian language, benchmark, large language models, retrieval-augmented generation, domain fine-tuning, multilingual QA, healthcare dataset", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Antonio Romano", "Giuseppe Riccio", "Mariano Barone", "Marco Postiglione", "Vincenzo Moscato"]}, "usage": {"completion_tokens": 685, "prompt_tokens": 3440, "total_tokens": 4125, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 328, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063786, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029536, "upstream_inference_completions_cost": 0.0003425}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:10.006561", "feed": "arxiv_cscl", "title": "How Do LLMs Use Their Depth?", "link": "https://papers.cool/arxiv/2510.18871", "analysis": {"summary": "The paper investigates how large language models allocate computation across layers, introducing a \"Guess-then-Refine\" framework where early layers produce high-frequency token guesses that are progressively refined with deeper contextual information. Through token frequency analysis, part-of-speech timing, fact recall, and multiple‑choice tasks, the authors show systematic patterns such as early prediction of function words and deeper processing for the first token of multi‑token answers. These findings provide a fine‑grained mechanistic view of LLM inference and suggest avenues for improving transformer efficiency.", "summary_cn": "本文研究了大型语言模型在不同层次上如何分配计算，提出了“先猜测‑再精炼”(Guess-then-Refine) 框架：早期层主要生成高频词汇的统计猜测，随后在更深层中随着上下文信息的丰富逐步被精炼为合适的输出。通过对高频词预测、词性预测时序、事实回忆多标记答案的首词深度需求以及多项选择题的格式识别过程等案例分析，揭示了功能词最早被正确预测、首个答案标记需更多计算深度等规律。该工作提供了对 LLM 推理过程的细粒度机制理解，并为提升 Transformer 计算效率提供了启示。", "keywords": "layer-wise dynamics, depth utilization, guess-then-refine, token frequency, mechanistic interpretability, transformer analysis, computational efficiency", "scoring": {"interpretability": 8, "understanding": 8, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"]}, "usage": {"completion_tokens": 699, "prompt_tokens": 3509, "total_tokens": 4208, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 369, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065521, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030571, "upstream_inference_completions_cost": 0.0003495}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:10.297681", "feed": "arxiv_cscl", "title": "SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish", "link": "https://papers.cool/arxiv/2510.18725", "analysis": {"summary": "The paper introduces SemiAdapt and SemiLoRA, semi-supervised, inference-efficient methods that enhance domain adaptation for transformer-based neural machine translation, particularly targeting low‑resource languages such as Irish. Experiments show that SemiAdapt can surpass full‑domain fine‑tuning and that SemiLoRA enables parameter‑efficient fine‑tuning to match or exceed full‑model fine‑tuning performance, especially on larger, noisier corpora. All resulting Irish translation models are released as open resources to lower the barrier for low‑resource language research.", "summary_cn": "本文提出了 SemiAdapt 和 SemiLoRA 两种半监督、推理高效的方式，以提升基于 Transformer 的神经机器翻译在低资源语言（如爱尔兰语）上的领域适配能力。实验表明 SemiAdapt 能够超越全域微调，而 SemiLoRA 使参数高效微调的效果可以匹配甚至超过完整模型的微调，尤其在更大且噪声较多的数据集上表现突出。所有的爱尔兰语翻译模型均作为开放资源发布，以降低低资源语言研究的门槛。", "keywords": "parameter-efficient fine-tuning, LoRA, SemiAdapt, SemiLoRA, neural machine translation, low-resource languages, domain adaptation, transformer, Irish translation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Josh McGiff", "Nikola S. Nikolov"]}, "usage": {"completion_tokens": 708, "prompt_tokens": 3437, "total_tokens": 4145, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 389, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00062987, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027587, "upstream_inference_completions_cost": 0.000354}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:10.316942", "feed": "arxiv_cscl", "title": "KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs", "link": "https://papers.cool/arxiv/2510.18368", "analysis": {"summary": "The paper introduces KoSimpleQA, a Korean factuality benchmark consisting of 1,000 short, unambiguous fact-seeking questions aimed at evaluating large language models' knowledge of Korean cultural information. Experiments on various open-source Korean-capable LLMs show low accuracy (33.7% for the strongest model) and reveal performance differences compared to English SimpleQA, while analysis of reasoning-augmented LLMs indicates that reasoning improves knowledge extraction and uncertainty abstention.", "summary_cn": "本文推出 KoSimpleQA（Korean SimpleQA），一个包含 1,000 条简短且答案明确的韩语事实问答题目的基准，用于评估大语言模型在韩国文化知识方面的事实性。实验表明，即使是性能最强的开源模型也仅有约 33.7% 的正确率，并且在该基准上的排序与英文 SimpleQA 差异显著；进一步分析显示，启用推理能力的模型能够更好地挖掘潜在知识并在不确定时选择弃答。", "keywords": "Korean QA, factuality benchmark, LLM evaluation, reasoning, cultural knowledge, KoSimpleQA", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Donghyeon Ko", "Yeguk Jin", "Kyubyung Chae", "Byungwook Lee", "Chansong Jo", "Sookyo In", "Jaehong Lee", "Taesup Kim", "Donghyun Kwak"]}, "usage": {"completion_tokens": 708, "prompt_tokens": 3407, "total_tokens": 4115, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 408, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064441, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029041, "upstream_inference_completions_cost": 0.000354}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:10.750274", "feed": "arxiv_cscl", "title": "MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models", "link": "https://papers.cool/arxiv/2510.18383", "analysis": {"summary": "The paper introduces MENTOR, a reinforcement‑learning framework that distills the tool‑using abilities of large language models into smaller language models by using a dense, teacher‑guided reward constructed from reference trajectories. By combining exploration‑driven RL with fine‑grained teacher feedback, MENTOR achieves better cross‑domain generalization and strategic competence than supervised fine‑tuning or standard sparse‑reward RL baselines.", "summary_cn": "本文提出 MENTOR 框架，利用强化学习将大语言模型的工具使用能力蒸馏到小语言模型中，方法是通过教师提供的参考轨迹构造密集的教师引导奖励。该框架将探索性的 RL 与细粒度的教师反馈相结合，使小模型在跨领域泛化和策略能力上显著于传统的监督微调和稀疏奖励 RL 基线。", "keywords": "reinforcement learning, teacher-guided rewards, model distillation, small language models, cross-domain generalization, RL-based distillation, tool use, policy learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"]}, "usage": {"completion_tokens": 1057, "prompt_tokens": 3426, "total_tokens": 4483, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 914, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00082176, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029326, "upstream_inference_completions_cost": 0.0005285}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:10.949119", "feed": "arxiv_cscl", "title": "Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting", "link": "https://papers.cool/arxiv/2510.18745", "analysis": {"summary": "The paper introduces Topoformer, a transformer variant that imposes a 2D topographic layout on queries, keys, and values through spatial querying and spatial reweighting, enabling interpretable spatial organization of linguistic representations. Experiments on sentiment classification and masked language modeling show performance comparable to standard models while yielding clear topographic patterns that align with human brain fMRI responses to sentences. The authors argue that scaling such topographic constraints could improve interpretability in NLP and provide more accurate computational models of brain language organization.", "summary_cn": "本文提出了 Topoformer，一种通过空间查询和空间重加权在二维网格上组织查询、键和值的 Transformer 变体，从而实现语言表示的可解释拓扑结构。实验在情感分类和掩码语言建模任务上显示，其性能与标准模型相当，但能够产生清晰的拓扑模式，并且这些模式与人类大脑对自然语言句子的 fMRI 反应呈现对齐。作者认为，进一步扩大此类拓扑约束有望提升 NLP 的可解释性，并为大脑语言网络提供更精确的计算模型。", "keywords": "topographic organization, transformer, spatial querying, spatial reweighting, mechanistic interpretability, neural representation, fMRI alignment, language models", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Taha Binhuraib", "Greta Tuckute", "Nicholas Blauch"]}, "usage": {"completion_tokens": 749, "prompt_tokens": 3517, "total_tokens": 4266, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 505, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068141, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030691, "upstream_inference_completions_cost": 0.0003745}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:11.651430", "feed": "arxiv_cscl", "title": "Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation", "link": "https://papers.cool/arxiv/2510.18439", "analysis": {"summary": "The paper introduces a token-level reliability measure that quantifies how much a sign-language translation model relies on visual input versus language priors, using feature-based sensitivity and counterfactual probability differences. Experiments on PHOENIX-2014T and CSL-Daily show that the reliability score predicts hallucination rates across gloss-based and gloss-free models, generalizes to visual degradations, and improves risk estimation when combined with text-based confidence signals.", "summary_cn": "本文提出一种基于特征敏感度和反事实概率差异的 token 级可靠性指标，用于衡量手语翻译模型在生成文本对视觉输入的依赖程度。实验在 PHOENIX-2014T 与 CSL-Daily 数据集的 gloss-based 与 gloss-free 模型上表明，该可靠性分数能够预测幻觉率、在视觉降级下仍具泛化性，并与文本置信度等信号结合提升幻觉风险估计。", "keywords": "hallucination detection, sign language translation, visual grounding, token-level reliability, counterfactual sensitivity, multimodal generation, gloss-free models, model evaluation", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yasser Hamidullah", "Koel Dutta Chowdury", "Yusser Al-Ghussin", "Shakib Yazdani", "Cennet Oguz", "Josef van Genabith", "Cristina España-Bonet"]}, "usage": {"completion_tokens": 1114, "prompt_tokens": 3515, "total_tokens": 4629, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 1005, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00086361, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030661, "upstream_inference_completions_cost": 0.000557}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:11.777730", "feed": "arxiv_cscl", "title": "Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering", "link": "https://papers.cool/arxiv/2510.18691", "analysis": {"summary": "The paper presents the first systematic study of large language model (LLM) performance on long-context medical question answering, evaluating multiple models, relevance-based content inclusion settings, and datasets across various task formulations. It investigates how Retrieval-Augmented Generation (RAG) affects long-context comprehension, identifies optimal single- versus multi-document reasoning setups, and provides qualitative and error analyses that highlight when RAG is beneficial and common failure modes. The findings reveal effects of model size, memorization issues, and the advantages of reasoning-oriented models for clinical relevance.", "summary_cn": "本文首次系统性地研究了大型语言模型在长上下文医学问答任务中的表现，评估了不同模型、基于相关性的内容包含设置以及跨任务形式的多个数据集。研究分析了检索增强生成（RAG）对长上下文理解的影响，找出了单文档与多文档推理的最佳配置，并通过定性和错误分析阐明了 RAG 何时有益以及常见的失败案例。结果揭示了模型规模、记忆问题以及推理模型在临床相关任务中的优势。", "keywords": "long-context language models, medical question answering, Retrieval-Augmented Generation, RAG, context comprehension, model scaling, memorization, multi-document reasoning, evaluation metrics, error analysis", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Feras AlMannaa", "Talia Tseriotou", "Jenny Chim", "Maria Liakata"]}, "usage": {"completion_tokens": 784, "prompt_tokens": 3337, "total_tokens": 4121, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 521, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067191, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027991, "upstream_inference_completions_cost": 0.000392}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:11.868020", "feed": "arxiv_cscl", "title": "Bayesian Low-Rank Factorization for Robust Model Adaptation", "link": "https://papers.cool/arxiv/2510.18723", "analysis": {"summary": "The paper proposes Bayesian low-rank factorized adapters for speech foundation models such as Whisper, enabling robust adaptation to code-switching multilingual scenarios while preserving the base model's general capabilities. By placing near-zero priors on adapter parameters, the method yields sparser adaptation matrices that reduce catastrophic forgetting, achieving a 54% backward gain with only a 4% drop on the new domain compared to LoRA.", "summary_cn": "该论文提出使用贝叶斯低秩因子化适配器对 Whisper 等语音基础模型进行鲁棒的领域适配，能够在代码切换等多语言场景下实现有效的微调，同时通过在参数上施加接近零的先验实现稀疏适配矩阵，显著降低对原始模型能力的遗忘。实验表明，与 LoRA 相比，本文方法在新领域性能下降仅 4%，但在原始任务上实现了 54% 的后向收益。", "keywords": "Bayesian adaptation, low-rank factorization, adapters, Whisper, code-switching, catastrophic forgetting, LoRA, speech foundation models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}, "usage": {"completion_tokens": 794, "prompt_tokens": 3370, "total_tokens": 4164, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 565, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068186, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028486, "upstream_inference_completions_cost": 0.000397}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:11.954838", "feed": "arxiv_cscl", "title": "Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models", "link": "https://papers.cool/arxiv/2510.18454", "analysis": {"summary": "The paper investigates how optimizing for funniness in modern LLM pipelines leads to higher rates of stereotypical and toxic humor, revealing a bias amplification loop between generators and evaluators. Using information‑theoretic incongruity metrics and human judgments on satire, it shows that harmful jokes are scored as funnier and can even become more statistically expected for certain models. These results highlight safety risks in deploying LLMs for creative and engagement‑focused content.", "summary_cn": "本文研究了在现代大型语言模型中优化幽默度如何导致刻板印象和有害内容的增加，揭示了生成器与评估器之间的偏见放大循环。通过信息论不一致性度量以及对讽刺文本的人类评分，发现有害笑话往往获得更高的幽默分数，且对某些模型而言，这些有害笑点甚至变得更可预测。研究结果凸显了在创意和互动内容中使用 LLM 所面临的安全风险。", "keywords": "humor generation, large language models, toxicity, stereotypes, bias amplification, information-theoretic analysis, safety, alignment, role-based prompting, satire", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Atharvan Dogra", "Soumya Suvra Ghosal", "Ameet Deshpande", "Ashwin Kalyan", "Dinesh Manocha"]}, "usage": {"completion_tokens": 799, "prompt_tokens": 3436, "total_tokens": 4235, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 563, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00067522, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027572, "upstream_inference_completions_cost": 0.0003995}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:11.972494", "feed": "arxiv_cscl", "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model", "link": "https://papers.cool/arxiv/2510.18855", "analysis": {"summary": "The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level discrepancy masking, C3PO++ for efficient rollout partitioning, and ASystem as a high‑performance RL framework—to overcome training‑inference misalignment and system bottlenecks. Empirical results show strong performance on a range of reasoning benchmarks, including AIME‑2025, HMMT‑2025, CodeForces, ARC‑AGI‑v1, and a silver‑medal level on IMO‑2025. By releasing the full MoE model, the work aims to democratize access to large‑scale reasoning capabilities.", "summary_cn": "本文推出 Ring-1T，一个每个 token 激活约 500 亿参数的万亿规模思考模型，并提出三项关键创新：IcePop（基于 token 级差异掩码的 RL 稳定化）、C3PO++（在 token 预算下对长 rollout 动态分割提升效率）以及 ASystem（高性能 RL 框架），以解决训练‑推理不匹配和系统瓶颈问题。实验在 AIME‑2025、HMMT‑2025、CodeForces、ARC‑AGI‑v1 等基准上取得突破性成绩，并在 IMO‑2025 达到银牌水平。论文通过开源完整的 MoE 模型，旨在让社区直接使用前沿的推理能力。", "keywords": "trillion-parameter model, reinforcement learning, scaling, MoE, token-level masking, RL training stability, high-performance RL framework, reasoning AI", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Team", "Anqi Shen", "Baihui Li", "Bin Hu", "Bin Jing", "Cai Chen", "Chao Huang", "Chao Zhang", "Chaokun Yang", "Cheng Lin", "Chengyao Wen", "Congqi Li", "Deng Zhao", "Dingbo Yuan", "Donghai You", "Fagui Mao", "Fanzhuang Meng", "Feng Xu", "Guojie Li", "Guowei Wang", "Hao Dai", "Haonan Zheng", "Hong Liu", "Jia Guo", "Jiaming Liu", "Jian Liu", "Jianhao Fu", "Jiannan Shi", "Jianwen Wang", "Jianxin Lai", "Jin Yang", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junping Zhao", "Kuan Xu", "Le Su", "Lei Chen", "Li Tang", "Liang Jiang", "Liangcheng Fu", "Lianhao Xu", "Linfeng Shi", "Lisha Liao", "Longfei Zheng", "Meng Li", "Mingchun Chen", "Qi Zuo", "Qiang Cheng", "Qianggang Cao", "Qitao Shi", "Quanrui Guo", "Senlin Zhu", "Shaofei Wang", "Shaomian Zheng", "Shuaicheng Li", "Shuwei Gu", "Siba Chen", "Tao Wu", "Tao Zhang", "Tianyu Zhang", "Tianyu Zhou", "Tiwei Bie", "Tongkai Yang", "Wang Hong", "Wang Ren", "Weihua Chen", "Wenbo Yu", "Wengang Zheng", "Xiangchun Wang", "Xiaodong Yan", "Xiaopei Wan", "Xin Zhao", "Xinyu Kong", "Xinyu Tang", "Xudong Han", "Xudong Wang", "Xuemin Yang", "Xueyu Hu", "Yalin Zhang", "Yan Sun", "Yicheng Shan", "Yilong Wang", "Yingying Xu", "Yongkang Liu", "Yongzhen Guo", "Yuanyuan Wang", "Yuchen Yan", "Yuefan Wang", "Yuhong Guo", "Zehuan Li", "Zhankai Xu", "Zhe Li", "Zhenduo Zhang", "Zhengke Gui", "Zhenxuan Pan", "Zhenyu Huang", "Zhenzhong Lan", "Zhiqiang Ding", "Zhiqiang Zhang", "Zhixun Li", "Zhizhen Liu", "Zihao Wang", "Zujie Wen"]}, "usage": {"completion_tokens": 800, "prompt_tokens": 3495, "total_tokens": 4295, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 404, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070361, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030361, "upstream_inference_completions_cost": 0.0004}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:12.112412", "feed": "arxiv_cscl", "title": "BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks", "link": "https://papers.cool/arxiv/2510.18288", "analysis": {"summary": "The paper introduces BrailleLLM, an instruction‑tuned large language model designed for Braille‑related tasks such as Braille translation, formula‑to‑Braille conversion, and mixed‑text translation. To overcome data scarcity, the authors release English and Chinese Braille mixed datasets with mathematical formulas and propose a syntax‑tree‑based augmentation technique, as well as Braille Knowledge‑Based Fine‑Tuning (BKFT) to improve contextual learning. Experiments show that BKFT yields substantial gains over standard fine‑tuning, providing a foundation for low‑resource multilingual Braille research.", "summary_cn": "本文提出 BrailleLLM，一种通过指令微调的 大语言模型（LLM），用于盲文翻译、公式转盲文以及混合文本翻译等盲文相关任务。为缓解数据稀缺，作者发布了英中文盲文混合数据集（EBMD/CBMD），并提出基于语法树的增强方法和盲文知识驱动的微调（BKFT），显著提升了模型在盲文翻译场景中的表现，为低资源多语言盲文研究奠定基础。", "keywords": "Braille, instruction tuning, low-resource language modeling, data augmentation, mixed-text translation, knowledge-based fine-tuning, multilingual, accessibility", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Tianyuan Huang", "Zepeng Zhu", "Hangdi Xing", "Zirui Shao", "Zhi Yu", "Chaoxiong Yang", "Jiaxian He", "Xiaozhong Liu", "Jiajun Bu"]}, "usage": {"completion_tokens": 952, "prompt_tokens": 3382, "total_tokens": 4334, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 707, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00076266, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028666, "upstream_inference_completions_cost": 0.000476}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:12.894749", "feed": "arxiv_cscl", "title": "CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models", "link": "https://papers.cool/arxiv/2510.18173", "analysis": {"summary": "The paper introduces CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires large language models to generate dynamic tables under evolving schemas, probing robustness through extractive-cue ablation, temporal prefixing, and entity-form perturbations. Experiments on state-of-the-art LLMs reveal substantial performance drops when extractive shortcuts are removed, with longer contexts and altered entity forms further degrading accuracy, indicating brittle reasoning rather than random noise. The authors argue that robustness-first evaluation is essential for developing efficient and scalable text-to-table systems.", "summary_cn": "本文提出 CMT-Bench 基准，利用实时板球解说数据要求大语言模型在不断演变的模式下生成动态表格，并通过抽取线索消融、时间前缀以及实体形式扰动三类语义保持的改动来检验模型的鲁棒性。对最前沿 LLM 的实验显示，去除抽取摘要会导致显著性能下降，且输入长度增加和实体形式变化进一步削弱准确率，表明模型推理易出现漂移而非仅仅噪声。作者主张在文本到表格任务中应首先评估鲁棒性，以推动高效可扩展的方法开发。", "keywords": "text-to-table, dynamic table generation, robustness, long-context, state tracking, benchmark, cricket commentary, LLM evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ritam Upadhyay", "Naman Ahuja", "Rishabh Baral", "Aparna Garimella", "Vivek Gupta"]}, "usage": {"completion_tokens": 839, "prompt_tokens": 3456, "total_tokens": 4295, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 539, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071726, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029776, "upstream_inference_completions_cost": 0.0004195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:12.923597", "feed": "arxiv_cscl", "title": "Adapting Language Balance in Code-Switching Speech", "link": "https://papers.cool/arxiv/2510.18724", "analysis": {"summary": "The paper proposes a differentiable surrogate loss that highlights code-switching points by leveraging the language imbalance between the embedded and main language, thereby reducing context bias during generation. Experiments on Arabic and Chinese-English code-switching speech data show improved prediction of switch locations and lower substitution errors.", "summary_cn": "本文提出一种可微分的代理损失，通过利用嵌入语言与主体语言的差异来突出代码切换点，从而缓解生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验表明，模型能够更准确地预测切换位置，并显著降低替换错误率。", "keywords": "code-switching, language balance, context bias, differentiable surrogate, robustness, speech recognition, multilingual models", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}, "usage": {"completion_tokens": 839, "prompt_tokens": 3395, "total_tokens": 4234, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 761, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070811, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028861, "upstream_inference_completions_cost": 0.0004195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:13.128420", "feed": "arxiv_cscl", "title": "Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata", "link": "https://papers.cool/arxiv/2510.18289", "analysis": {"summary": "Food4All introduces a multi‑agent framework that aggregates heterogeneous data sources and uses a lightweight reinforcement‑learning algorithm to provide real‑time, geographically‑aware recommendations of free food resources with nutritional annotations. The system continuously updates its resource pool from official databases, community platforms and social media, and incorporates a feedback loop to adapt to user needs, aiming to improve access for food‑insecure populations.", "summary_cn": "Food4All 提出一个多代理框架，通过聚合官方数据库、社区平台和社交媒体等异构数据，并使用轻量级强化学习算法，实时提供地理可达且带有营养标注的免费食物资源推荐。系统通过在线反馈循环持续更新资源池并适应用户需求，旨在提升食物不安全人群的获取渠道。", "keywords": "multi-agent system, reinforcement learning, food insecurity, real-time retrieval, nutritional metadata, data aggregation, resource recommendation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Zhengqing Yuan", "Yiyang Li", "Weixiang Sun", "Zheyuan Zhang", "Kaiwen Shi", "Keerthiram Murugesan", "Yanfang Ye"]}, "usage": {"completion_tokens": 851, "prompt_tokens": 3511, "total_tokens": 4362, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 742, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073151, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030601, "upstream_inference_completions_cost": 0.0004255}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:13.547635", "feed": "arxiv_cscl", "title": "Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency", "link": "https://papers.cool/arxiv/2510.18556", "analysis": {"summary": "The paper conducts a detailed bias analysis of clinical large language models, focusing on differential opioid prescription patterns across ethnicity, gender, and age, and introduces the HC4 (Healthcare Comprehensive Commons Corpus), a 89‑billion‑token pre‑training dataset. It evaluates models using standard benchmarks and a novel healthcare‑specific methodology to assess fairness and safety, highlighting the importance of dataset transparency for trustworthy clinical AI.", "summary_cn": "本文对临床大语言模型进行深入的偏差分析，重点关注不同族裔、性别和年龄群体在阿片类药物处方上的差异，并推出 HC4（Healthcare Comprehensive Commons Corpus），一个超过 890 亿标记的预训练数据集。通过通用基准和新提出的面向医疗的评估方法，评估模型的公平性和安全性，强调数据集透明度对于建立可信临床 AI 的重要性。", "keywords": "bias analysis, clinical LLMs, dataset transparency, HC4, fairness, opioid prescription, healthcare AI, large language models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Svetlana Maslenkova", "Clement Christophe", "Marco AF Pimentel", "Tathagata Raha", "Muhammad Umar Salman", "Ahmed Al Mahrooqi", "Avani Gupta", "Shadab Khan", "Ronnie Rajan", "Praveenkumar Kanithi"]}, "usage": {"completion_tokens": 866, "prompt_tokens": 3355, "total_tokens": 4221, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 710, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071561, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028261, "upstream_inference_completions_cost": 0.000433}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:13.747006", "feed": "arxiv_cscl", "title": "Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models", "link": "https://papers.cool/arxiv/2510.18077", "analysis": {"summary": "The paper investigates how chain-of-thought prompting improves large language models' ability to translate texts with inter‑sentential dependencies, using the English‑French DiscEvalMT benchmark for pronoun anaphora and lexical cohesion challenges. Experiments with 12 LLMs show that reasoning prompts raise discrimination accuracy to about 90% and COMET translation quality to roughly 92%, with GPT‑4, GPT‑4o and Phi achieving the best results.", "summary_cn": "本文研究了链式思考（chain‑of‑thought）提示如何提升大型语言模型在涉及跨句依存关系的翻译任务中的表现，使用英法 DiscEvalMT 基准考察代词指代和词汇衔接难题。对 12 种 LLM 进行实验表明，使用推理提示后，判别正确翻译的准确率可达约 90%，而基于 COMET 的翻译质量约为 92%，其中 GPT‑4、GPT‑4o 与 Phi 表现尤为突出。", "keywords": "chain-of-thought, context-aware translation, inter-sentential dependencies, DiscEvalMT, LLM prompting, translation evaluation, COMET, reasoning prompts", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Shabnam Ataee", "Andrei Popescu-Belis"]}, "usage": {"completion_tokens": 744, "prompt_tokens": 3399, "total_tokens": 4143, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 501, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066121, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.000372}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:13.763298", "feed": "arxiv_cscl", "title": "MLMA: Towards Multilingual with Mamba Based Architectures", "link": "https://papers.cool/arxiv/2510.18684", "analysis": {"summary": "The paper proposes MLMA, a multilingual automatic speech recognition system that adopts the Mamba state-space architecture to handle long-context sequences efficiently. By embedding language-aware conditioning within a shared representation, MLMA attains performance comparable to Transformer‑based models on standard multilingual benchmarks, demonstrating Mamba’s suitability as a scalable backbone for speech tasks.", "summary_cn": "本文提出了 MLMA，一种采用 Mamba 状态空间模型进行长序列处理的多语言自动语音识别系统。通过在共享表征中隐式加入语言感知条件，MLMA 在标准多语言基准上实现了与 Transformer 模型相当的性能，展示了 Mamba 作为可扩展、高效语音识别骨干的潜力。", "keywords": "multilingual ASR, Mamba, state-space model, speech recognition, efficient architecture, language-aware conditioning, long-context modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohamed Nabih Ali", "Daniele Falavigna", "Alessio Brutti"]}, "usage": {"completion_tokens": 658, "prompt_tokens": 3342, "total_tokens": 4000, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 467, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3328}, "cost": 0.00059734, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026834, "upstream_inference_completions_cost": 0.000329}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:15.269572", "feed": "arxiv_cscl", "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs", "link": "https://papers.cool/arxiv/2510.18279", "analysis": {"summary": "The paper proposes rendering long textual inputs as a single image and feeding it to multimodal decoder LLMs, achieving roughly a 50% reduction in required decoder tokens while maintaining performance on tasks such as long‑context retrieval (RULER) and document summarization (CNN/DailyMail). Experiments show that this visual text representation acts as an effective form of input compression without degrading downstream results.", "summary_cn": "本文提出将冗长的文本渲染为单幅图像，并直接输入多模态解码器 LLM，从而在保持性能的前提下将解码器 token 使用率降低约一半。通过在 RULER（长上下文检索）和 CNN/DailyMail（文档摘要）等基准上的实验，验证了视觉文本作为输入压缩方法的实用性和有效性。", "keywords": "token efficiency, multimodal LLM, visual text, input compression, decoder tokens, long-context retrieval, summarization, image rendering", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanhong Li", "Zixuan Lan", "Jiawei Zhou"]}, "usage": {"completion_tokens": 597, "prompt_tokens": 3358, "total_tokens": 3955, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 365, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3344}, "cost": 0.00056812, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026962, "upstream_inference_completions_cost": 0.0002985}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.033711", "feed": "arxiv_cscl", "title": "Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models", "link": "https://papers.cool/arxiv/2510.17909", "analysis": {"summary": "The paper conducts a mechanistic analysis of literary style in GPT-2 by identifying neurons that distinguish human literary prose from rigid AI-generated text, finding thousands of statistically significant discriminative neurons. Counterintuitively, systematic ablation of the most discriminative neurons improves literary style metrics, revealing a disconnect between observational correlation and causal necessity. These results challenge assumptions in mechanistic interpretability and have implications for AI alignment research.", "summary_cn": "本文对 GPT-2 中的文学风格进行机械化分析，识别出能区分人类文学散文与僵硬机器生成文本的神经元，发现数千个具有统计显著性的判别神经元。出人意料的是，对最具判别性的神经元进行系统性消融会提升文学风格指标，显示出观察到的相关性与因果必要性之间的脱节。该发现挑战了可解释性研究中的假设，并对 AI 对齐具有启示意义。", "keywords": "mechanistic interpretability, neuron ablation, literary style, GPT-2, correlation vs causation, alignment, neural circuits", "scoring": {"interpretability": 8, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Tsogt-Ochir Enkhbayar"]}, "usage": {"completion_tokens": 611, "prompt_tokens": 3396, "total_tokens": 4007, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 374, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059426, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028876, "upstream_inference_completions_cost": 0.0003055}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.069365", "feed": "arxiv_cscl", "title": "CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning", "link": "https://papers.cool/arxiv/2510.18466", "analysis": {"summary": "This paper introduces CEFR-Annotated WordNet, a version of WordNet enriched with Common European Framework of Reference for Languages proficiency levels. Using a large language model, the authors automatically align WordNet sense definitions with CEFR levels from the English Vocabulary Profile, creating a large corpus for training contextual lexical classifiers that achieve high macro-F1 scores. The resources are released to bridge NLP and language education.", "summary_cn": "本文提出了 CEFR 注释的 WordNet，即在 WordNet 语义网络中加入欧洲语言共同参考框架（CEFR）水平的版本。利用大语言模型将 WordNet 释义与英语词汇档案（English Vocabulary Profile）中的 CEFR 等级进行自动匹配，构建了大规模语料库并用于训练上下文词汇分类器，实现了 0.81 的宏观 F1 分数。作者公开了注释的 WordNet、语料库和分类器，以促进 NLP 与语言教学的结合。", "keywords": "CEFR, WordNet, language learning, LLM annotation, semantic similarity, lexical classifier, English Vocabulary Profile, proficiency-guided dataset, NLP education", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Masato Kikuchi", "Masatsugu Ono", "Toshioki Soga", "Tetsu Tanabe", "Tadachika Ozono"]}, "usage": {"completion_tokens": 987, "prompt_tokens": 3417, "total_tokens": 4404, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 819, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078541, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029191, "upstream_inference_completions_cost": 0.0004935}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.185093", "feed": "arxiv_cscl", "title": "From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models", "link": "https://papers.cool/arxiv/2510.18030", "analysis": {"summary": "The paper revisits global structured pruning for LLMs and introduces GISP-Global Iterative Structured Pruning, a post‑training method that uses loss‑based importance scores aggregated at the structure level to prune attention heads and MLP channels iteratively. By defining importance with a model‑level loss, the approach naturally incorporates task‑specific objectives, achieving lower perplexity and higher downstream accuracy on several LLM families at moderate sparsity levels. Exper demonstrate that the iterative schedule stabilizes accuracy without intermediate fine‑tuning and enables a \"prune‑once, deploy‑many\" workflow.", "summary_cn": "本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝方法，通过在结构层面聚合基于损失的重要性分数，对注意力头和 MLP 通道进行迭代剪枝。由于重要性由模型整体损失定义，该方法可自然地结合任务特定目标，在多种LM 上实现更低的困惑度和更高的下游任务精度，并在中等稀疏度下保持准确性而无需中间微调，实现“一次剪枝，多次部署”。", "keywords": "structured pruning, large language models, global pruning, task-aligned pruning, iterative pruning, sparsity,lama, Mistral", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"]}, "usage": {"completion_tokens": 719, "prompt_tokens": 3496, "total_tokens": 4215, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 394, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066326, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030376, "upstream_inference_completions_cost": 0.0003595}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.256576", "feed": "arxiv_cscl", "title": "Language Models as Semantic Augmenters for Sequential Recommenders", "link": "https://papers.cool/arxiv/2510.18046", "analysis": {"summary": "The paper proposes LaMAR, a framework that uses large language models in a few‑shot setting to generate auxiliary semantic signals (e.g., inferred usage scenarios, item intents) that augment sequential interaction histories for recommender systems. By enriching the original user-item sequences with these generated contextual cues, LaMAR consistently improves performance on benchmark sequential recommendation tasks and demonstrates high semantic novelty and diversity of the signals.", "summary_cn": "本文提出 LaMAR 框架，利用大型语言模型（LLM）在少样本条件下生成辅助语义信号（如使用场景推断、物品意图），以丰富序列推荐系统中的用户‑物品交互序列。通过将这些生成的上下文信息加入原始序列，LaMAR 在基准序列推荐任务中显著提升了性能，并展示了信号的高度语义新颖性和多样性。", "keywords": "semantic augmentation, large language models, sequential recommendation, data-centric AI, few-shot prompting", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahsa Valizadeh", "Xiangjue Dong", "Rui Tuo", "James Caverlee"]}, "usage": {"completion_tokens": 600, "prompt_tokens": 3392, "total_tokens": 3992, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 391, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058816, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.0003}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.638398", "feed": "arxiv_cscl", "title": "DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization", "link": "https://papers.cool/arxiv/2510.18257", "analysis": {"summary": "DelvePO introduces a direction‑guided, self‑evolving framework for flexible prompt optimization that decouples prompts into components and employs a working memory to mitigate LLM uncertainties. By iteratively generating and refining prompts, the method achieves stable, transferable improvements across diverse tasks and models, outperform prior state‑of‑the‑art techniques in extensive experiments.", "summary_cn": "DelvePO 提出了一种方向引导的自我演化提示优化框架，通过将提示拆分为不同组件并引入工作记忆，帮助大型语言模型克服自身不确定性并获取关键洞见，以生成新提示。该方法在多个任务和不同模型上实现了稳定、可迁移的性能提升，实验结果显示其在相同设置下持续优于之前的最方法。", "keywords": "prompt optimization, large language models, self-evolving framework, direction-guided, working memory, task-agnostic, transferability, prompt engineering, L steering, experimental evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Tao Tao", "Guanghui Zhu", "Lang Guo", "Hongyi Chen", "Chunfeng Yuan", "Yihua Huang"]}, "usage": {"completion_tokens": 630, "prompt_tokens": 3510, "total_tokens": 4140, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 408, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062086, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030586, "upstream_inference_completions_cost": 0.000315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:16.699461", "feed": "arxiv_cscl", "title": "SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone", "link": "https://papers.cool/arxiv/2510.17998", "analysis": {"summary": "SimBA is a three‑phase framework that uses only raw evaluation scores to simplify analysis of large language‑model benchmarks. It first compares datasets and models (stalk), then discovers a small representative subset that still covers the benchmark (prowl), and finally predicts performance of held‑out models with near‑zero error using this subset (pounce). Experiments on HELM, MMLU, and BigBenchLite show strong dataset‑model relationships and high coverage using only a few percent of the original datasets.", "summary_cn": "SimBA 是一种仅利用原始评估分数的三阶段框架，用于简化大语言模型基准的分析。首先进行数据集与模型比较（stalk），随后发现能够覆盖基准的少量代表性子集（prowl），最后使用该子集预测未见模型的性能，误差几乎为零（pounce）。在 HELM、MMLU 和 BigBenchLite 上的实验表明，数据集与模型之间关系紧密，仅使用极少比例的数据集即可实现至少 95% 的覆盖率。", "keywords": "benchmark analysis, performance matrices, representative subset, model ranking, LM evaluation, dataset coverage, SimBA, predictive performance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Subramani", "Alfredo Gomez", "Mona Diab"]}, "usage": {"completion_tokens": 600, "prompt_tokens": 3500, "total_tokens": 4100, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 308, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060436, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030436, "upstream_inference_completions_cost": 0.0003}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:17.198228", "feed": "arxiv_cscl", "title": "Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation", "link": "https://papers.cool/arxiv/2510.18502", "analysis": {"summary": "The paper introduces a pipeline that combines a vision-language model (VLM) with retrieval-augmented generation (RAG) to perform zero-shot vehicle make and model recognition. The VLM describes vehicle images in textual attributes, retrieves matching vehicle descriptions from a text database, and uses a language model to infer the specific make and model, achieving around a 20% improvement over a CLIP baseline without any image-specific finetuning.", "summary_cn": "本文提出将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 将车辆图像转化为文本属性，检索匹配的车辆文本描述，并利用语言模型推断具体品牌和型号，在无需图像特定微调的情况下比 CLIP 基线提升约 20%。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, text-based reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}, "usage": {"completion_tokens": 490, "prompt_tokens": 3391, "total_tokens": 3881, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 220, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3376}, "cost": 0.00051733, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027233, "upstream_inference_completions_cost": 0.000245}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:17.487426", "feed": "arxiv_cscl", "title": "LLMs Encode How Difficult Problems Are", "link": "https://papers.cool/arxiv/2510.18147", "analysis": {"summary": "The paper investigates whether large language models internally encode problem difficulty in a way that aligns with human judgments, using linear probes across layers and token positions on 60 models. It finds that human-labeled difficulty is strongly decodable and scales with model size, while model-derived difficulty does not, and that steering models toward easier representations reduces hallucinations and improves accuracy, especially during reinforcement learning post‑training. The authors release code for probing and evaluation to promote replication.", "summary_cn": "本文研究大型语言模型是否内部编码了与人类判断一致的问题难度，通过在60个模型的不同层和位置上使用线性探针进行分析。结果显示，人类标注的难度在模型内部高度可线性解码且随模型规模提升，而模型自身推导的难度信号较弱且扩展性差；将模型表征引导至“更容易”方向可以降低幻觉并提升准确率，尤其在强化学习后训练阶段表现明显。作者公开了探针代码及评估脚本以便复现。", "keywords": "difficulty encoding, linear probes, LLM interpretability, hallucination reduction, reinforcement learning post-training, Easy2HardBench, model scaling", "scoring": {"interpretability": 7, "understanding": 8, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["William Lugoloobi", "Chris Russell"]}, "usage": {"completion_tokens": 655, "prompt_tokens": 3425, "total_tokens": 4080, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 412, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00060157, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027407, "upstream_inference_completions_cost": 0.0003275}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:17.575424", "feed": "arxiv_cscl", "title": "From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering", "link": "https://papers.cool/arxiv/2510.18297", "analysis": {"summary": "The paper introduces MedRGAG, a framework that unifies external retrieval and parametric generation to improve medical question answering by jointly leveraging retrieved evidence and generator‑produced background documents. It proposes two modules—Knowledge‑Guided Context Completion to fill gaps in retrieved knowledge, and Knowledge‑Aware Document Selection to compose concise evidence—resulting in notable performance gains on several benchmarks.", "summary_cn": "本文提出 MedRGAG 框架，统一外部检索与模型内部生成，以提升医学问答系统的准确性。通过“知识引导的上下文补全”模块补足检索缺失的知识，并使用“知识感知文档选择”模块构建简洁且完整的证据集合，在多个医学 QA 基准上实现显著提升。", "keywords": "medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge-guided context completion, knowledge-aware document selection, MedRGAG, external knowledge, parametric knowledge, hallucination mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lei Li", "Xiao Zhou", "Yingying Zhang", "Xian Wu"]}, "usage": {"completion_tokens": 565, "prompt_tokens": 3492, "total_tokens": 4057, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 326, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058566, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030316, "upstream_inference_completions_cost": 0.0002825}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:17.791880", "feed": "arxiv_cscl", "title": "Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning", "link": "https://papers.cool/arxiv/2510.18374", "analysis": {"summary": "The paper investigates fairness gaps in popular English automatic speech recognition models (Whisper and Seamless‑M4T) across 26 accent groups of second‑language speakers, finding large WER variance. It proposes a fairness‑prompted finetuning approach using lightweight adapters combined with Spectral Decoupling, Group‑DRO, and Invariant Risk Minimization to jointly optimize accuracy and accent fairness. Experiments show macro‑averaged WER improvements of about 58 % relative to the pretrained models while preserving overall recognition performance.", "summary_cn": "本文研究了主流英文自动语音识别模型（Whisper 和 Seamless‑M4T）在 26 种第二语言口音组之间的公平性差距，发现词错误率（WER）波动显著。作者提出使用轻量级适配器并融合光谱解耦（Spectral Decoupling）、组分布鲁棒优化（Group‑DRO）和不变风险最小化（IRM）的公平提示微调方法，以同时提升准确率和口音公平性。实验表明，在宏观平均 WER 上相较于原始模型提升约 58%，且整体识别性能保持。", "keywords": "fairness, automatic speech recognition, accent bias, Whisper, adapters, spectral decoupling, group DRO, invariant risk minimization, multilingual ASR, bias mitigation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Monorama Swain", "Bubai Maji", "Jagabandhu Mishra", "Markus Schedl", "Anders Søgaard", "Jesper Rindom Jensen"]}, "usage": {"completion_tokens": 1083, "prompt_tokens": 3407, "total_tokens": 4490, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 837, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00083191, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029041, "upstream_inference_completions_cost": 0.0005415}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:18.026024", "feed": "arxiv_cscl", "title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training", "link": "https://papers.cool/arxiv/2510.18830", "analysis": {"summary": "The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long contexts. By applying MTraining, the authors expand the context window of Qwen2.5‑3B from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining model accuracy on various downstream tasks.", "summary_cn": "本文提出了 MTraining——一种分布式训练框架，融合了动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，以高效训练具备超长上下文的 大语言模型。作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文窗口从 32K 扩展至 512K token，并在多个下游任务上保持模型精度的同时，实现了最高 6 倍 的训练吞吐提升。", "keywords": "dynamic sparse attention, distributed training, ultra-long context, ring attention, hierarchical attention, LLM efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Li", "Chengruidong Zhang", "Huiqiang Jiang", "Yucheng Li", "Yuqing Yang", "Lili Qiu"]}, "usage": {"completion_tokens": 579, "prompt_tokens": 3473, "total_tokens": 4052, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 301, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3472}, "cost": 0.00056741, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027791, "upstream_inference_completions_cost": 0.0002895}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:18.191873", "feed": "arxiv_cscl", "title": "Automatic Prompt Generation via Adaptive Selection of Prompting Techniques", "link": "https://papers.cool/arxiv/2510.18162", "analysis": {"summary": "The paper introduces a method that automatically selects and combines prompting techniques for large language models by mapping user-provided task descriptions to semantically similar task clusters stored in a knowledge base, then generating high-quality prompts without pre‑existing templates. Experiments on 23 BIG‑Bench Extra Hard tasks show the approach outperforms standard prompts and existing automatic prompt‑generation tools in both arithmetic and harmonic mean scores. This work aims to make prompt engineering accessible to non‑experts by streamlining prompt creation.", "summary_cn": "本文提出一种自动生成大型语言模型提示的方法：通过将用户的任务描述映射到语义相似的任务聚类，并从知识库中选取相应的提示技术组合，生成高质量的提示，而无需预设模板。 在 23 项 BIG‑Bench Extra Hard 任务上的实验表明，该方法在算术平均和调和平均得分上均优于标准提示和已有的自动提示生成工具。 此工作旨在通过标准化提示创建流程，使非专业用户也能有效利用大语言模型。", "keywords": "automatic prompt generation, adaptive prompting, task clustering, large language models, prompt engineering, BIG-Bench Extra Hard, knowledge base", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yohei Ikenoue", "Hitomi Tashiro", "Shigeru Kuroyanagi"]}, "usage": {"completion_tokens": 582, "prompt_tokens": 3387, "total_tokens": 3969, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 278, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057841, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028741, "upstream_inference_completions_cost": 0.000291}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:18.565223", "feed": "arxiv_cscl", "title": "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge", "link": "https://papers.cool/arxiv/2510.18196", "analysis": {"summary": "The paper identifies a score range bias in using large language models (LLMs) as judges for direct assessment, where the model outputs are highly sensitive to the predefined scoring interval. To mitigate this bias, the authors introduce a contrastive decoding technique, achieving up to a 11.3% relative improvement in Spearman correlation with human judgments across various score ranges.", "summary_cn": "本文指出在使用大型语言模型（LLM）作为评审者进行直接打分时存在分数区间偏差，即模型输出对预设评分范围高度敏感。作者提出采用对比解码方法来缓解此偏差，在不同分数区间下相较于人类评判的 Spearman 相关性提升最高 11.3%。", "keywords": "contrastive decoding, LLM as judge, score range bias, evaluation reliability, Spearman correlation, human judgment alignment, language model evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yoshinari Fujinuma"]}, "usage": {"completion_tokens": 599, "prompt_tokens": 3340, "total_tokens": 3939, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 402, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057986, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028036, "upstream_inference_completions_cost": 0.0002995}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:18.696577", "feed": "arxiv_cscl", "title": "Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review", "link": "https://papers.cool/arxiv/2510.17892", "analysis": {"summary": "This systematic review examines 41 studies on the use of pre‑trained language models for domain‑specific text classification, comparing traditional and transformer‑based approaches and presenting a taxonomy of techniques. It also includes a comparative experiment on biomedical sentence classification using BERT, SciBERT, and BioBERT, and discusses challenges, limitations, and future directions.", "summary_cn": "本文系统回顾了 41 篇关于预训练语言模型在特定领域文本分类中应用的研究，比较了传统方法与基于 Transformer 的方法，并提出了技术分类体系。文中还对生物医学句子分类任务使用 BERT、SciBERT 与 BioBERT 进行了对比实验，讨论了挑战、局限以及未来发展方向。", "keywords": "pre-trained language models, domain-specific text classification, BERT, SciBERT, BioBERT, systematic review, transformer, taxonomy, NLP, domain adaptation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhyar Rzgar K. Rostam", "Gábor Kertész"]}, "usage": {"completion_tokens": 517, "prompt_tokens": 3496, "total_tokens": 4013, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 282, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00056226, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030376, "upstream_inference_completions_cost": 0.0002585}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:19.452496", "feed": "arxiv_cscl", "title": "Believe It or Not: How Deeply do LLMs Believe Implanted Facts?", "link": "https://papers.cool/arxiv/2510.17941", "analysis": {"summary": "The paper introduces a framework to quantify how deeply implanted facts are believed by large language models, evaluating generalization, robustness to challenge, and similarity to genuine knowledge via linear probes. Experiments show that simple prompting and mechanistic edits rarely achieve deep belief implantation, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides measurable criteria for belief depth, enabling more rigorous assessment of knowledge editing techniques before real‑world deployment.", "summary_cn": "本文提出了一套衡量大语言模型对植入事实信念深度的框架，评估植入知识在相关上下文中的泛化、对自我审视与直接挑战的鲁棒性，以及通过线性探测器与真实知识的表征相似性。实验表明，简单提示和机制性编辑难以实现深层信念植入，而合成文档微调（Synthetic Document Finetuning）常能成功，但在与基础世界知识冲突的事实上表现脆弱且表征不同。该工作提供了可量化的信念深度标准，为在实际应用中安全地使用知识编辑提供了评估手段。", "keywords": "belief depth, knowledge editing, large language models, synthetic document finetuning, linear probes, factuality, model alignment, interpretability, knowledge injection, robustness", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Stewart Slocum", "Julian Minder", "Clément Dumas", "Henry Sleight", "Ryan Greenblatt", "Samuel Marks", "Rowan Wang"]}, "usage": {"completion_tokens": 740, "prompt_tokens": 3413, "total_tokens": 4153, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 468, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066131, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029131, "upstream_inference_completions_cost": 0.00037}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:19.558105", "feed": "arxiv_cscl", "title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting", "link": "https://papers.cool/arxiv/2510.18874", "analysis": {"summary": "The paper investigates catastrophic forgetting in language models when adapting to new tasks via supervised fine-tuning (SFT) versus reinforcement learning (RL). Experiments across multiple model families and tasks show that RL causes less forgetting while achieving comparable or better performance, attributed to its mode‑seeking nature enabled by on‑policy data. The authors further demonstrate that approximating on‑policy data can mitigate forgetting efficiently, highlighting a practical strategy for continual learning.", "summary_cn": "本文研究了语言模型在通过监督微调（SFT）与强化学习（RL）适配新任务时出现的灾难性遗忘现象。实验覆盖多种模型系列和任务，发现 RL 在保持先前知识方面优于 SFT，且性能相当或更佳，这归因于 RL 使用的在策略数据导致的模式寻踪特性。作者进一步验证，使用近似在策略数据即可有效减轻遗忘，为持续学习提供了实用方案。", "keywords": "catastrophic forgetting, reinforcement learning, on-policy data, supervised fine-tuning, language model adaptation, continual learning, KL regularization, mixture model analysis", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Howard Chen", "Noam Razin", "Karthik Narasimhan", "Danqi Chen"]}, "usage": {"completion_tokens": 740, "prompt_tokens": 3460, "total_tokens": 4200, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 514, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066836, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029836, "upstream_inference_completions_cost": 0.00037}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:19.786632", "feed": "arxiv_cscl", "title": "Modeling Layered Consciousness with Multi-Agent Large Language Models", "link": "https://papers.cool/arxiv/2510.17844", "analysis": {"summary": "The paper introduces a multi‑agent framework called the Psychodynamic Model that aims to simulate layered artificial consciousness—in particular self‑awareness, preconsciousness, and unconsciousness—by drawing on psychoanalytic theory. A Personalization Module combines fixed personality traits with dynamic needs, and the system is fine‑tuned on emotionally rich dialogues; evaluation with an LLM judge shows a 71.2% preference for the fine‑tuned model, indicating improved emotional depth and reduced output variance.", "summary_cn": "本文提出一种多代理框架（Psychodynamic Model），基于精神分析理论模拟人工意识的层次结构，包括自我意识、前意识和无意识。通过个人化模块将固定特质与动态需求结合，并在情感丰富的对话数据上进行参数高效微调；使用 LLM 评审的实验显示，微调模型获得 71.2% 的偏好，表现出更强的情感深度和更低的输出方差。", "keywords": "consciousness modeling, multi-agent LLM, psychodynamic model, self-awareness, preconsciousness, unconsciousness, personalization module, fine-tuning, emotional dialogue, adaptive cognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"]}, "usage": {"completion_tokens": 799, "prompt_tokens": 3311, "total_tokens": 4110, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 596, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3296}, "cost": 0.00066543, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026593, "upstream_inference_completions_cost": 0.0003995}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:20.210885", "feed": "arxiv_cscl", "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs", "link": "https://papers.cool/arxiv/2510.18876", "analysis": {"summary": "The paper introduces Grasp Any Region (GAR), a multimodal LLM architecture that uses a RoI-aligned feature replay mechanism to incorporate global context for precise, region-level visual comprehension and to model interactions among multiple prompts. GAR also comes with a new benchmark, GAR-Bench, which evaluates single‑region understanding as well as multi‑region compositional reasoning, demonstrating state‑of‑the‑art performance on several vision‑language tasks.", "summary_cn": "本文提出了 Grasp Any Region（GAR）模型，利用 RoI 对齐特征回放技术在多模态大语言模型中引入全局上下文，实现对任意区域的精确视觉理解并建模多个提示之间的交互。作者同时构建了 GAR‑Bench 基准，用于评估单区域理解和多区域组合推理，实验表明 GAR 在多项视觉语言任务上达到了领先水平。", "keywords": "multimodal LLM, region-level visual, RoI-aligned feature replay, compositional reasoning, GAR-Bench, dense captioning", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}, "usage": {"completion_tokens": 802, "prompt_tokens": 3505, "total_tokens": 4307, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 590, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070611, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030511, "upstream_inference_completions_cost": 0.000401}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:20.220047", "feed": "arxiv_cscl", "title": "KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers", "link": "https://papers.cool/arxiv/2510.18355", "analysis": {"summary": "The paper introduces KrishokBondhu, a voice‑based call‑center system for Bengali‑speaking farmers that uses a Retrieval‑Augmented Generation (RAG) pipeline to provide real‑time agricultural advice. It combines OCR digitization of extension manuals, a vector‑database retrieval layer, speech‑to‑text, a 3‑4B Gemma language model, and text‑to‑speech to generate context‑grounded answers, achieving a 44.7% improvement over the KisanQRS benchmark in pilot tests.", "summary_cn": "本文提出 KrishokBondhu，一个面向孟加拉语农民的语音呼叫中心系统，采用检索增强生成（Retrieval‑Augmented Generation，RAG）流水线提供实时农业咨询。系统通过 OCR 将推广手册数字化，使用向量数据库检索、语音转文字、Gemma 3‑4B 大语言模型生成答案，再转为自然语音，实验显示相较 KisanQRS 基准提升 44.7%。", "keywords": "retrieval-augmented generation, voice interface, Bengali, agricultural advisory, call center, OCR, large language model, semantic retrieval, multilingual AI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Farjana Aktar", "M. Saifuzzaman Rafat"]}, "usage": {"completion_tokens": 658, "prompt_tokens": 3551, "total_tokens": 4209, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 380, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031201, "upstream_inference_completions_cost": 0.000329}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:20.937012", "feed": "arxiv_cscl", "title": "See the Text: From Tokenization to Visual Reading", "link": "https://papers.cool/arxiv/2510.18840", "analysis": {"summary": "The paper introduces SeeTok, a method that renders text as images and feeds them to pretrained multimodal LLMs, leveraging OCR and vision-language alignment to replace subword tokenization. Experiments on three language tasks show that SeeTok matches or exceeds traditional tokenizers while using 4.43× fewer tokens and cutting FLOPs by 70.5%, with added benefits for low‑resource languages, typographic noise robustness, and cross‑lingual generalization. This approach proposes a vision‑centric, human‑like reading paradigm for language models.", "summary_cn": "本文提出 SeeTok 方法，将文本渲染为图像并使用预训练的多模态 LLM 进行识别，利用 OCR 和视觉‑语言对齐能力取代子词分词。实验表明在三项语言任务上，SeeTok 能够匹配或超越传统分词器，同时使用 4.43 倍更少的 token，计算量削减 70.5%，并在低资源语言、排版噪声鲁棒性以及跨语言泛方面表现出额外提升。该方法倡导一种以视觉为中心、类人阅读的语言模型新范式。", "keywords": "visual tokenization, multimodal LLM, OCR, cross-lingual generalization, token efficiency, low-resource languages, typographic noise robustness, visual reading, language model", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"]}, "usage": {"completion_tokens": 830, "prompt_tokens": 3433, "total_tokens": 4263, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 531, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00069027, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027527, "upstream_inference_completions_cost": 0.000415}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:21.102342", "feed": "arxiv_cscl", "title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement", "link": "https://papers.cool/arxiv/2510.17880", "analysis": {"summary": "The paper investigates whether large language models (LLMs) use emotion similarly to humans when enforcing fairness through third‑party punishment. Across millions of decisions, LLMs showed stronger emotion‑driven punishment and prioritized emotional responses over cost, unlike humans who balance fairness with cost; reasoning‑oriented models were more cost‑sensitive but still largely emotion driven. The authors suggest LLMs follow a developmental trajectory akin to humans and call for integrating emotion with context‑sensitive reasoning to improve alignment and emotional intelligence.", "summary_cn": "本文研究大型语言模型（LLM）在通过第三方惩罚执行公平时是否像人类一样受情绪驱动。实验发现，LLM 的惩罚行为更受负面情绪影响，且在决定是否惩罚时更倾向于情绪而非成本，表现出几乎全有或全无的规范执行；相比之下，人类会在公平与成本之间权衡。推理型模型（如 o3-mini、DeepSeek‑R1）对成本更敏感但仍主要受情绪驱动。作者提出 LLM 的情绪使用可能沿着类似人类发展的轨迹，并呼吁在未来模型中结合情绪与情境推理以提升对齐和情绪智能。", "keywords": "emotion-guided decision making, large language models, fairness enforcement, third-party punishment, cost sensitivity, alignment, moral AI, behavioral analysis, LLM evaluation, emotional intelligence", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"]}, "usage": {"completion_tokens": 696, "prompt_tokens": 3475, "total_tokens": 4171, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 362, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064861, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030061, "upstream_inference_completions_cost": 0.000348}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:21.445187", "feed": "arxiv_cscl", "title": "KAT-Coder Technical Report", "link": "https://papers.cool/arxiv/2510.18779", "analysis": {"summary": "The report presents KAT-Coder, a 32B agentic code model trained via a multi-stage curriculum—including Mid-Term training, supervised fine‑tuning, reinforcement fine‑tuning with a novel multi‑ground‑truth reward, and deployment‑phase adaptation—to improve reasoning, planning, tool‑use reliability, and instruction alignment for real‑world IDE environments. The authors detail dataset construction across many languages and contexts, introduce error‑masked SFT and tree‑structured trajectory training, and release the model publicly. Empirical results show robust performance on interactive software development tasks.", "summary_cn": "本文报告了 KAT‑Coder，一款 32B 规模的代理式代码模型，通过多阶段课程（包括中期训练、监督微调、使用多真值奖励的强化微调以及部署阶段的适配）提升推理、规划、工具使用可靠性和指令对齐，以在真实 IDE 环境中进行交互式软件开发。作者介绍了跨多语言多场景的数据集构建，提出了错误掩码微调和树结构轨迹训练，并已公开模型。实验表明该模型在交互式编码任务上表现稳健。", "keywords": "agentic coding, reinforcement learning from human feedback, tool-use reliability, instruction alignment, large language models, curriculum learning, code generation, IDE integration, multi-ground-truth reward, tree-structured trajectory training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zizheng Zhan", "Ken Deng", "Xiaojiang Zhang", "Jinghui Wang", "Huaixi Tang", "Zhiyi Lai", "Haoyang Huang", "Wen Xiang", "Kun Wu", "Wenhao Zhuang", "Minglei Zhang", "Shaojie Wang", "Shangpeng Yan", "Kepeng Lei", "Zongxian Feng", "Huiming Wang", "Zheng Lin", "Mengtong Li", "Mengfei Xie", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Weihao Li", "Wenqiang Zhu", "Jiarong Zhang", "Jingxuan Xu", "Songwei Yu", "Yifan Yao", "Xinping Lei", "Han Li", "Junqi Xiong", "Zuchen Gao", "Dailin Li", "Haimo Li", "Jiaheng Liu", "Yuqun Zhang", "Junyi Peng", "Haotian Zhang", "Bin Chen"]}, "usage": {"completion_tokens": 670, "prompt_tokens": 3471, "total_tokens": 4141, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 362, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063501, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030001, "upstream_inference_completions_cost": 0.000335}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:21.698032", "feed": "arxiv_cscl", "title": "POPI: Personalizing LLMs via Optimized Natural Language Preference Inference", "link": "https://papers.cool/arxiv/2510.17881", "analysis": {"summary": "The paper introduces POPI, a framework that learns compact natural-language summaries of individual user preferences through a preference-inference model and uses these summaries to condition a shared LLM, enabling personalized generation without per-user fine-tuning. POPI jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy and lower context overhead, and the learned summaries can be transferred to frozen off-the-shelf models.", "summary_cn": "本文提出 POPI 框架，通过偏好推断模型将用户的多样化偏好压缩为简洁的自然语言摘要，并以此条件化共享的大语言模型，从而实现无需针对每个用户微调的个性化生成。该方法在强化学习下联合优化偏好推断和生成，两者协同提升个性化准确性并显著降低上下文开销，且生成的摘要可直接迁移至冻结的现成模型。", "keywords": "personalization, LLM, preference inference, reinforcement learning, natural language summary, plug-and-play adaptation, RLHF, DPO, user modeling, zero-shot transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yizhuo Chen", "Xin Liu", "Ruijie Wang", "Zheng Li", "Pei Chen", "Changlong Yu", "Priyanka Nigam", "Meng Jiang", "Bing Yin"]}, "usage": {"completion_tokens": 954, "prompt_tokens": 3420, "total_tokens": 4374, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 815, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00076936, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029236, "upstream_inference_completions_cost": 0.000477}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:21.703463", "feed": "arxiv_cscl", "title": "Diagnosing Representation Dynamics in NER Model Extension", "link": "https://papers.cool/arxiv/2510.17930", "analysis": {"summary": "The paper studies how a BERT‑based NER system can be extended to new PII entities in noisy spoken language without harming performance on existing semantic entities. Using incremental learning diagnostics, it reveals that location tags are vulnerable due to representation overlap with PII patterns and that the background ‘O’ tag can block learning of new patterns unless its classifier is unfrozen. These findings provide a mechanistic interpretation of feature independence, representation overlap, and O‑tag plasticity in NER model adaptation.", "summary_cn": "本文研究了在嘈杂口语数据中，将 BERT‑基准的命名实体识别（NER）模型扩展至新的个人身份信息（PII）实体时，如何保持对原有语义实体的性能。通过增量学习诊断，发现位置实体因与 PII 模式存在表征重叠而特别脆弱，同时背景 ‘O’ 标签若保持不变会阻碍新模式的学习，只有在解冻其分类器后才能“释放”这些模式。该工作 mechanistic 地解释了特征独立性、表征重叠以及 O‑标签可塑性在 NER 模型适应过程中的作用。", "keywords": "NER, representation drift, BERT, PII detection, incremental learning, feature independence, O-tag plasticity, semantic overlap, mechanistic interpretability", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Xirui Zhang", "Philippe de La Chevasnerie", "Benoit Fabre"]}, "usage": {"completion_tokens": 860, "prompt_tokens": 3416, "total_tokens": 4276, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 622, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00072176, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029176, "upstream_inference_completions_cost": 0.00043}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:21.991003", "feed": "arxiv_cscl", "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning", "link": "https://papers.cool/arxiv/2510.18849", "analysis": {"summary": "The paper proposes a Critique-Post-Edit reinforcement learning framework that uses a personalized generative reward model providing multi‑dimensional scores and textual critiques to mitigate reward hacking and improve faithful, controllable LLM personalization. The policy revises its outputs based on these critiques, leading to significant gains over standard PPO on personalization benchmarks, with a 7B model achieving an 11% win‑rate increase and a 14B model surpassing GPT‑4.1 performance. This demonstrates a practical route to more efficient and reliable personalized language generation.", "summary_cn": "本文提出了一种 Critique-Post-Edit 强化学习框架，利用个性化生成奖励模型（GRM）提供多维评分和文本批评，以抵御奖励欺骗并提升 LLM 的忠实、可控个性化。策略模型依据批评自行修正输出，在个性化基准测试中显著优于标准 PPO，7B 模型实现 11% 胜率提升，14B 模型的表现甚至超越 GPT‑4.1，展示了实现更高效可靠个性化生成的可行路径。", "keywords": "personalization, reinforcement learning, reward hacking, generative reward model, critique-post-edit, LLM alignment, controllable generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Chenghao Zhu", "Meiling Tao", "Tiannan Wang", "Dongyi Ding", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"]}, "usage": {"completion_tokens": 754, "prompt_tokens": 3440, "total_tokens": 4194, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 493, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00065332, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027632, "upstream_inference_completions_cost": 0.000377}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:22.013853", "feed": "arxiv_cscl", "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "link": "https://papers.cool/arxiv/2510.17921", "analysis": {"summary": "The paper introduces CLAWS, a method that uses attention weights across prompt sections and model outputs to automatically classify mathematical solutions generated by LLMs into typical, creative, and hallucinated categories without human evaluation. CLAWS outperforms five existing white-box detection baselines on several 7-8B reinforcement‑learning‑trained math models across a large benchmark of 4,545 problems from major math contests. This work provides a novel, interpretable approach to assess creativity and detect hallucinations in LLM reasoning tasks.", "summary_cn": "本文提出 CLAWS 方法，利用注意力权重在提示段落和模型输出之间的分布，自动将 LLM 生成的数学解答划分为典型、创造性和幻觉三类，无需人工评估。实验表明，CLAWS 在多个 7‑8B 规模的 RL 训练数学模型上，针对 4,545 道来自主要数学竞赛的题目，优于五种现有白盒检测基准。该工作提供了一种新颖且可解释的手段来评估创造力并检测 LLM 推理中的幻觉。", "keywords": "creativity detection, LLM, attention window, white-box detection, hallucination, mathematical reasoning, RL fine-tuning, interpretability, model evaluation", "scoring": {"interpretability": 7, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"]}, "usage": {"completion_tokens": 800, "prompt_tokens": 3445, "total_tokens": 4245, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 522, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069611, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029611, "upstream_inference_completions_cost": 0.0004}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:22.313152", "feed": "arxiv_cscl", "title": "CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment", "link": "https://papers.cool/arxiv/2510.18471", "analysis": {"summary": "The paper introduces CodeRL+, a reinforcement‑learning framework that augments existing RL with verifiable rewards by aligning generated code with its execution semantics via variable‑level trajectory signals. This alignment provides richer feedback than binary pass/fail outcomes, leading to consistent improvements on pass@1 and on downstream coding tasks such as code‑reasoning and test‑output generation. Experiments show that CodeRL+ works across multiple RL algorithms and LLM sizes and probe analyses indicate stronger textual‑semantic coupling.", "summary_cn": "本文提出 CodeRL+，一种通过变量级执行轨迹将生成的代码与其执行语义对齐的强化学习框架，以提升相较于仅二元通过/失败奖励的学习信号。该语义对齐提供了更细粒度的反馈，使得模型在 pass@1 以及代码推理和测试生成等下游任务上均取得显著提升。实验表明 CodeRL+ 能多种强化学习算法和不同规模的 LLM 上有效运行，探针分析进一步验证了代码文本表示与底层执行语义的耦合度增强。", "keywords": "code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, pass@1, code reasoning, variable-level execution trajectory, LLM fine-tuning, semantic alignment, code correctness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"]}, "usage": {"completion_tokens": 851, "prompt_tokens": 3487, "total_tokens": 4338, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 618, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3472}, "cost": 0.00070551, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028001, "upstream_inference_completions_cost": 0.0004255}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:22.321130", "feed": "arxiv_cscl", "title": "BreakFun: Jailbreaking LLMs via Schema Exploitation", "link": "https://papers.cool/arxiv/2510.17904", "analysis": {"summary": "The paper introduces BreakFun, a jailbreak technique that leverages an LLM's strong tendency to follow structured schemas by embedding a malicious \"Trojan Schema\" within a three-part prompt, achieving up to 100% attack success on several models. An extensive evaluation on 13 models shows an average 89% success rate, and an ablation study confirms the schema as the primary causal factor. To mitigate this vulnerability, the authors propose the Adversarial Prompt Deconstruction guardrail, which uses a secondary LLM to extract literal text and reveal hidden harmful intent, demonstrating strong defensive performance.", "summary_cn": "本文提出 BreakFun，一种利用大语言模型（LL）强烈遵循结构化模式倾向的越狱方法，通过在三段式提示中嵌入恶意的“Trojan Schema”（特洛伊模式），迫使模型生成有害内容，在部分模型上实现 100% 攻击成功率，整体平均成功率达 89%。大规模实验和消融研究表明该模式是攻击的核心因素。为应对该，作者设计了 Adversarial Prompt Deconstruction 防护机制，使用二级 LLM 执行“Literal Transcription”（文字转录），提取所有可读文本以揭示隐藏的有害意图，展示出显著的防御效果。", "keywords": "jailbreak, schema exploitation, prompt engineering, adversarial defense, LLM alignment, chain-of-thought, Trojan schema, guardrail", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Amirkia Rafiei Oskooei", "Mehmet S. Aktas"]}, "usage": {"completion_tokens": 681, "prompt_tokens": 3477, "total_tokens": 4158, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 334, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064141, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030091, "upstream_inference_completions_cost": 0.0003405}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:22.321971", "feed": "arxiv_cscl", "title": "WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection", "link": "https://papers.cool/arxiv/2510.18798", "analysis": {"summary": "WebSeer introduces a reinforcement‑learning framework enhanced with a self‑reflection mechanism to train more capable web search agents that can generate longer, more reflective tool‑use sequences. By constructing a large dataset annotated with reflection patterns and employing a two‑stage training process that combines cold‑start and RL, the authors enable a single14‑billion‑parameter model to achieve state‑of‑the‑art results on HotpotQA and SimpleQA and to generalize to out‑of‑distribution tasks.", "summary_cn": "WebSeer 提出了一种结合 (self‑reflection) 机制的强化学习框架，用于训练能够生成更长、更具反思性的工具使用序列的网页搜索代理。通过构建带有反模式标注的大规模数据集，并采用冷启动与强化学习相结合的两阶段流程，单一 14B 模型在 HotpotQA 与 SimpleQA 上实现了最先进的准确率，并展示了对分布外数据的强泛化能力。", "keywords": "search agents, reinforcement learning, self-reflection, tool-use depth, web retrieval, HotpotQA, SimpleQA, dataset annotation, interactive retrieval", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guanzhong He", "Zhen Yang", "Jinxin Liu", "Bin Xu", "Lei Hou", "Juanzi Li"]}, "usage": {"completion_tokens": 891, "prompt_tokens": 3408, "total_tokens": 4299, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 686, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3392}, "cost": 0.00071926, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027376, "upstream_inference_completions_cost": 0.0004455}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:23.355516", "feed": "arxiv_cscl", "title": "Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints", "link": "https://papers.cool/arxiv/2510.17882", "analysis": {"summary": "The paper conducts large‑scale empirical analysis of over 2.1 million preprints from 2016‑2025 to examine how generative large language models influence scholarly publishing. Using interrupted time‑series, authorship metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission cycles, modestly raise linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields. The authors argue that LLMs as selective catalysts that amplify existing strengths and widen disciplinary divides, calling for governance frameworks to maintain trust fairness.", "summary_cn": "本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，评估生成式大语言模型对学术出版的影响。通过中断时间序列、作者合作与产出指标、语言特征分析和主题建模，发现 LLM 加速了提交与修订过程，略微提升了语言复杂度，并显著增加了 AI 相关主题，尤其在计算密集型领域更为突出。作者指出，LLM 更像是选择性催化剂，放大了既有优势并扩大学科差距，因而呼吁制定治理框架以维护信任、公平与责任。", "keywords": "generative AI, preprints, scholarly publishing, LLM impact, interrupted time series, topic modeling, linguistic complexity, AI governance", "scoring": {"interpretability": 2, "understanding": 4, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Minfeng Qi", "Zhongmin Cao", "Qin Wang", "Ningran Li", "Tianqing Zhu"]}, "usage": {"completion_tokens": 756, "prompt_tokens": 3463, "total_tokens": 4219, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 452, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067681, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029881, "upstream_inference_completions_cost": 0.000378}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:23.393442", "feed": "arxiv_cscl", "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs", "link": "https://papers.cool/arxiv/2510.17918", "analysis": {"summary": "JT-Safe introduces a data augmentation strategy called Data with World Context (DWC) that enriches pre‑training corpora with real‑world contextual information and industrial‑scenario content to reduce hallucinations and improve trustworthiness of large language models. The authors continue pre‑training a 35‑billion‑parameter model on 1.5 trillion DWC tokens and demonstrate a 1.79 % improvement on safety and trustworthy benchmarks compared with a similar‑scale model. The work highlights the intrinsic role of pre‑training data in LLM safety.", "summary_cn": "JT‑Safe 提出“带世界上下文的数据”(Data with World Context, DWC) 方法，通过在预训练语料中加入真实世界的时空背景和工业场景信息，以降低大语言模型的幻觉并提升可信度。研究者在 35 B 参数的模型上继续使用 1.5 万亿 DWC 令牌进行预训练，并在安全与可信评估基准上相较于同规模模型提升 1.79%。该工作强调了预训练数据对 LLM 安全的根本影响。", "keywords": "LLM safety, hallucination mitigation, pretraining data, world context, data augmentation, trustworthiness, large language models, JT-Safe, DWC, industrial scenario data", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Junlan Feng", "Fanyu Meng", "Chong Long", "Pengyu Cong", "Duqing Wang", "Yan Zheng", "Yuyao Zhang", "Xuanchang Gao", "Ye Yuan", "Yunfei Ma", "Zhijie Ren", "Fan Yang", "Na Wu", "Di Jin", "Chao Deng"]}, "usage": {"completion_tokens": 1167, "prompt_tokens": 3558, "total_tokens": 4725, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 939, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00089656, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031306, "upstream_inference_completions_cost": 0.0005835}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:23.458374", "feed": "arxiv_cscl", "title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "link": "https://papers.cool/arxiv/2510.18476", "analysis": {"summary": "The paper introduces a probabilistic framework that maintains and updates a belief distribution over a dialogue partner's latent intentions, using these beliefs to guide the policy of LLM agents in multi‑turn social conversations. Experiments in the SOTOPIA environment demonstrate consistent performance gains over strong baselines, suggesting that intent modeling improves the adaptability and effectiveness of socially intelligent agents.", "summary_cn": "本文提出了一种概率意图建模框架，实时维护并更新对对话伙伴潜在意图的信念分布，并将该分布作为上下文输入以指导大语言模型（LLM）在多轮社交对话中的策略。 在 SOTOPIA 环境中的实验显示，该方法相较于强基线在整体得分上提升 9.0%（All 场景）和 4.1%（Hard 场景），表明意图建模有助于提升社交智能 LLM 代理的适应性和表现。", "keywords": "probabilistic intent modeling, large language model agents, multi-turn dialogue, belief distribution, social intelligence, latent intentions, uncertainty estimation, adaptive dialogue policies, SOTOPIA benchmark", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"]}, "usage": {"completion_tokens": 725, "prompt_tokens": 3352, "total_tokens": 4077, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 501, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064466, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028216, "upstream_inference_completions_cost": 0.0003625}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:23.479881", "feed": "arxiv_cscl", "title": "Interpretability Framework for LLMs in Undergraduate Calculus", "link": "https://papers.cool/arxiv/2510.17910", "analysis": {"summary": "The paper introduces a quantitative interpretability framework that extracts reasoning flows and semantically labels operations in LLM-generated solutions to undergraduate calculus problems, using metrics such as reasoning complexity, phrase sensitivity, and robustness. Experiments on real Calculus I‑III exams reveal that LLMs often produce fluent but conceptually flawed solutions, with reasoning highly sensitive to prompt phrasing and input variations. The framework enables fine‑grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI‑assisted feedback tools for STEM education.", "summary_cn": "本文提出了一套量化的可解释性框架，通过提取推理流程并对大语言模型在大学微积分题目中的解答进行语义标注，使用推理复杂度、短语敏感性和鲁棒性等指标进行评估。对实际的微积分 I‑III 考试进行实验后发现，模型常产生表面流畅却概念错误的解答，且推理过程对提示措辞和输入变化极为敏感。该框架能够细粒度诊断推理失误，帮助课程对齐，并为 STEM 教育中的可解释 AI 辅助反馈工具提供设计依据。", "keywords": "interpretability, large language models, calculus education, reasoning analysis, prompt ablation, robustness, pedagogical alignment, AI safety in education", "scoring": {"interpretability": 8, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sagnik Dakshit", "Sushmita Sinha Roy"]}, "usage": {"completion_tokens": 781, "prompt_tokens": 3444, "total_tokens": 4225, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 519, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068646, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029596, "upstream_inference_completions_cost": 0.0003905}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:23.907758", "feed": "arxiv_cscl", "title": "Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model", "link": "https://papers.cool/arxiv/2510.18165", "analysis": {"summary": "The paper introduces Saber, a training-free sampling algorithm for diffusion language models that combines adaptive acceleration and backtracking‑enhanced remasking to improve code generation efficiency. By dynamically speeding up sampling as code context builds and allowing reversal of generated tokens, Saber achieves an average 251.4% inference speedup while raising Pass@1 accuracy by 1.9% across several benchmarks.", "summary_cn": "本文提出 Saber，一种无需再训练的扩散语言模型采样算法，采用自适应加速与回溯增强重新掩码相结合的方式提升代码生成效率。该方法在代码上下文逐渐形成时动态加速采样，并通过回溯机制纠正生成的 token，在多个基准上实现了约 251.4% 的推理加速，同时将 Pass@1 准确率提升约 1.9%。", "keywords": "diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"]}, "usage": {"completion_tokens": 678, "prompt_tokens": 3457, "total_tokens": 4135, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 458, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063691, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029791, "upstream_inference_completions_cost": 0.000339}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.152063", "feed": "arxiv_cscl", "title": "Metrics and evaluations for computational and sustainable AI efficiency", "link": "https://papers.cool/arxiv/2510.17885", "analysis": {"summary": "The paper proposes a unified, reproducible methodology for evaluating AI model inference efficiency that jointly measures latency, throughput, energy consumption, and location‑adjusted carbon emissions while keeping accuracy constraints constant. It applies this framework across a range of hardware platforms and software stacks, generating Pareto frontiers that illuminate trade‑offs between accuracy, speed, energy use, and carbon impact, and releases open‑source code for independent verification. This benchmarking approach aims to enable evidence‑based, carbon‑aware decisions for sustainable AI deployment.", "summary_cn": "本文提出了一种统一且可复现的 AI 推理效率评估方法，在保持准确度约束的前提下同时测量时延、吞吐量、能耗和基于位置的碳排放。作者在多种硬件平台和软件栈上应用该框架，生成展示准确率、速度、能耗和碳足迹权衡的 Pareto 前沿，并公开开源代码供独验证。此基准旨在帮助研究者和实践者做出基于证据的碳意识决策，推动可持续 AI 部署。", "keywords": "computational efficiency, sustainable AI, carbon emissions, latency, throughput, energy consumption, benchmarking, multi-precision, hardware-software stack, AI sustainability", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}, "usage": {"completion_tokens": 780, "prompt_tokens": 3432, "total_tokens": 4212, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 528, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068416, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029416, "upstream_inference_completions_cost": 0.00039}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.211126", "feed": "arxiv_cscl", "title": "HouseTour: A Virtual Real Estate A(I)gent", "link": "https://papers.cool/arxiv/2510.18054", "analysis": {"summary": "HouseTour proposes a method that jointly generates spatially-aware 3D camera trajectories and natural language descriptions for virtual real‑estate tours. It uses a diffusion process constrained by known camera poses to produce smooth video paths, renders novel views with 3D Gaussian splatting, and integrates the trajectory information into a vision‑language model for grounded text generation. The accompanying HouseTour dataset of 1,200 house‑tour videos with pose and reconstruction data enables evaluation of both individual and end‑to‑end performance using a new joint metric.", "summary_cn": "本文提出 HouseTour 方法，能够同时生成空间感知的 3D 摄像机轨迹和对应的自然语言描述，用于虚拟房产导览。该方法利用受已知相机位姿约束的扩散过程生成平滑视频路径，并通过 3D 高斯喷溅渲染新视角，将轨迹信息融入视觉语言模型以实现基于 3D 场景的文本生成。作者还提供了包含 1,200 段房产导览视频、相机姿态和 3D 重建的 HouseTour 数据集，并使用新提出的联合指标评估单独及端到端的性能。", "keywords": "3D camera trajectory, diffusion process, Gaussian splatting, vision-language model, real estate video generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"]}, "usage": {"completion_tokens": 693, "prompt_tokens": 3389, "total_tokens": 4082, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 376, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3376}, "cost": 0.00061853, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027203, "upstream_inference_completions_cost": 0.0003465}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.430592", "feed": "arxiv_cscl", "title": "The Impact of Image Resolution on Biomedical Multimodal Large Language Models", "link": "https://papers.cool/arxiv/2510.18304", "analysis": {"summary": "The paper investigates how image resolution affects the performance of biomedical multimodal large language models (MLLMs). It demonstrates that training and inference at native resolution considerably boost performance across various tasks, while a mismatch between training and inference resolutions severely degrades results. Additionally, mixed‑resolution training is shown to mitigate misalignment effects and balance computational constraints with performance needs.", "summary_cn": "本文研究了图像分辨率对生物医学多模态大型语言模型（MLLM）性能的影响。结果表明，在原始分辨率下进行训练和推理可显著提升多个任务的表现，而训练‑推理分辨率不匹配则会导致性能大幅下降。此外，混合分辨率训练能够缓解这种不匹配带来的问题，在保证计算资源可接受的前提下提升模型效果。", "keywords": "biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, performance degradation, multimodal large language models, biomedical imaging, resolution misalignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"]}, "usage": {"completion_tokens": 760, "prompt_tokens": 3345, "total_tokens": 4105, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 605, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3344}, "cost": 0.00064767, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00026767, "upstream_inference_completions_cost": 0.00038}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.449523", "feed": "arxiv_cscl", "title": "Subject-Event Ontology Without Global Time: Foundations and Execution Semantics", "link": "https://papers.cool/arxiv/2510.18040", "analysis": {"summary": "The paper proposes a formal subject-event ontology for modeling complex dynamic systems without relying on global timestamps, defining causal order through explicit happens-before dependencies and making the ontology executable via a declarative dataflow mechanism. It introduces nine axioms ensuring properties such as monotonicity, acyclicity, and traceability, and demonstrates practical applicability through the Boldsea workflow engine and its semantic language BSL. The framework targets use cases in distributed systems, microservice architectures, DLT platforms, and scenarios with conflicting perspectives among different subjects.", "summary_cn": "本文提出了一种不依赖全局时间的主体-事件本体形式化，用显式的先行关系定义因果顺序，并通过声明式数据流机制使本体可执行。文中给出九条公理，确保历史单调性、因果无环性和可追溯性，并在 Boldsea 工作流引擎及其 BSL 语言上展示了实际实现。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突的情形。", "keywords": "subject-event ontology, causal order, declarative dataflow, distributed systems, epistemic models, workflow engine, Boldsea, event semantics, multi-perspective, DLT", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexander Boldachev"]}, "usage": {"completion_tokens": 717, "prompt_tokens": 3480, "total_tokens": 4197, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 439, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3472}, "cost": 0.00063746, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027896, "upstream_inference_completions_cost": 0.0003585}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.467095", "feed": "arxiv_cscl", "title": "Hierarchical Federated Unlearning for Large Language Models", "link": "https://papers.cool/arxiv/2510.17895", "analysis": {"summary": "The paper proposes a hierarchical federated unlearning framework for large language models that addresses continuous, heterogeneous unlearning requests in decentralized, privacy-sensitive settings. By decoupling unlearning and retention through task-specific adapters and employing a hierarchical merging strategy, the method mitigates inter- and intra-domain interference while preserving model performance. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate effective handling of heterogeneous unlearning demands with superior utility compared to baselines.", "summary_cn": "本文提出了一种层次化联邦消除（unlearning）框架，用于大语言模型，以应对持续且多样的消除需求以及去中心化、隐私敏感的数据环境。通过任务专用适配器将消除与保留解耦，并采用层合并策略来缓解域间和域内的冲突而在保持模型性能的同时实现有效的知识删除。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够灵活处理多样化的消除请求，并在模型效用上优于现有基线。", "keywords": "federated unlearning, large language models, privacy, hierarchical merging, adapter learning, continual unlearning, decentralized data, model utility", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Yisheng Zhong", "Zhengbang Yang", "Zhuangdi Zhu"]}, "usage": {"completion_tokens": 819, "prompt_tokens": 3374, "total_tokens": 4193, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 566, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3360}, "cost": 0.0006804, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002709, "upstream_inference_completions_cost": 0.0004095}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.569735", "feed": "arxiv_cscl", "title": "Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption", "link": "https://papers.cool/arxiv/2510.18333", "analysis": {"summary": "The paper argues that limited real-world deployment of LLM watermarking stems from misaligned incentives among providers, platforms, and users, identifying four barriers: competitive risk, governance, robustness, and attribution. It reviews model, text, and in‑context watermarking through this lens, proposing incentive‑aligned approaches—particularly in‑context watermarking for trusted parties—to enable practical detection of misuse without harming user experience. Design principles for domain‑specific, incentive‑aligned watermarking and future research directions are also outlined.", "summary_cn": "本文指出 LLM 水印技术在实际应用中的受限主要来源于提供者、平台和用户之间激励不一致，提出四大障碍：竞争风险、治理、鲁棒性和归属问题。文章从这一视角审视模型水印、文本水印和上下文水印（In‑context watermarking），强调通过激励对齐（尤其是面向可信方的上下文水印）实现对滥用行为的检测，而不影响用户体验，并给出领域特定激励对齐水印的设计原则及未来研究方向。", "keywords": "LLM watermarking, incentive alignment, misuse detection, in-context watermarking, model watermarking, AI safety, provenance, robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yepeng Liu", "Xuandong Zhao", "Dawn Song", "Gregory W. Wornell", "Yuheng Bu"]}, "usage": {"completion_tokens": 800, "prompt_tokens": 3502, "total_tokens": 4302, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 561, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3488}, "cost": 0.00068114, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028114, "upstream_inference_completions_cost": 0.0004}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:24.733598", "feed": "arxiv_cscl", "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution", "link": "https://papers.cool/arxiv/2510.18019", "analysis": {"summary": "The paper shows that current multilingual LLM watermarking methods lose effectiveness after translation, especially in medium‑ and low‑resource languages, due to limited full‑word tokens. It introduces STEAM, a back‑translation‑based detection technique that recovers watermark strength and works with any existing watermarking scheme, improving robustness across tokenizers and languages. Experiments on 17 languages report average gains of +0.19 AUC and +40 %p TPR@1 %.", "summary_cn": "本文指出现有的多语言 LLM 水印技术在翻译攻击下（尤其是中低资源语言）失效，原因是分词器中对应语言的完整词汇不足导致语义聚类失败。为此提出 STEAM——一种基于回译的检测方法，可恢复水印强度并兼容任意水印方案，在不同分词器和语言上均表现出更强的鲁棒性。实验在 17 种语言上实现了平均 AUC 提升 0.19、TPR@1% 提升 40 %点。", "keywords": "multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization robustness, translation attack, cross-lingual robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Asim Mohamed", "Martin Gubri"]}, "usage": {"completion_tokens": 1130, "prompt_tokens": 3388, "total_tokens": 4518, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 946, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00085256, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028756, "upstream_inference_completions_cost": 0.000565}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:25.381716", "feed": "arxiv_cscl", "title": "SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning", "link": "https://papers.cool/arxiv/2510.18095", "analysis": {"summary": "The paper proposes SMaRT, a framework that selects, mixes, and reinvents prompting strategies for large language models to improve reasoning, planning, and sequential decision‑making. By treating LLMs as intelligent integrators rather than mere evaluators, SMaRT combines diverse reasoning approaches to produce more robust and higher‑quality solutions, and empirical results show consistent gains over state‑of‑the‑art baselines.", "summary_cn": "本文提出了 SMaRT 框架，通过选择、混合和重新发明 (Select, Mix, and Reinvent) 提示策略，使大语言模型在推理、规划和序列决策任务中表现更佳。该框架将 LLM 视为智能整合者而非仅仅的评估者，融合多种推理方法以提升鲁棒性和解答质量，实验证明其在多个基准上均优于现有最先进方法。", "keywords": "strategy fusion, LLM reasoning, prompting strategies, planning, sequential decision-making, robustness, chain-of-thought, self-refinement, multi-strategy integration", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Nikhil Verma", "Manasa Bharadwaj", "Wonjun Jang", "Harmanpreet Singh", "Yixiao Wang", "Homa Fashandi", "Chul Lee"]}, "usage": {"completion_tokens": 807, "prompt_tokens": 3427, "total_tokens": 4234, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 605, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3424}, "cost": 0.00067787, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027437, "upstream_inference_completions_cost": 0.0004035}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:26.679973", "feed": "arxiv_cscl", "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models", "link": "https://papers.cool/arxiv/2510.17922", "analysis": {"summary": "The paper investigates how different task decomposition approaches for large language models affect performance and cost, identifying six categorization schemes and three key influencing factors. Based on an empirical analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition method, executes it, and verifies results, achieving a Pareto‑optimal balance across multiple benchmarks.", "summary_cn": "本文系统研究了大型语言模型任务分解方法对性能与成本的影响，提出了六种分类方案并分析了三大关键因素。基于此分析，作者设计了 Select-Then-Decompose 策略，通过动态选择分解方式、执行并验证结果，实现了在多个基准上性能‑成本的 Pareto 最优平衡。", "keywords": "task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, prompting strategy", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"]}, "usage": {"completion_tokens": 667, "prompt_tokens": 3436, "total_tokens": 4103, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 506, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062826, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029476, "upstream_inference_completions_cost": 0.0003335}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:27.041871", "feed": "arxiv_cscl", "title": "SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving", "link": "https://papers.cool/arxiv/2510.18123", "analysis": {"summary": "The paper presents the first systematic study of safety and security issues in natural-language-based collaborative driving, introducing a taxonomy of attacks such as connection disruption, content spoofing, and multi-connection forgery. To address these threats, the authors propose SafeCoop, an agentic defense pipeline that combines a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by spatial alignment across frames. Evaluations in closed‑loop CARLA simulations across 32 scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection performance under malicious attacks.", "summary_cn": "本文首次系统性地研究基于自然语言的协同驾驶安全与安全问题，提出了包括连接中断、内容伪造和多连接伪造等攻击策略的分类体系。作者设计了 SafeCoop 防御管线，融合语义防火墙、语言‑感知一致性检查以及跨帧空间对齐的多源共识机制。通过在 CARLA 仿真环境中 32 种关键场景的闭环实验，SafeCoop 在恶意攻击下实现了最高 69.15% 的驾驶得分提升和 67.32% 的 F1 检测率。", "keywords": "collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, CARLA simulation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}, "usage": {"completion_tokens": 986, "prompt_tokens": 3502, "total_tokens": 4488, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 769, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00079766, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030466, "upstream_inference_completions_cost": 0.000493}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:27.646274", "feed": "arxiv_cscl", "title": "AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM", "link": "https://papers.cool/arxiv/2510.17934", "analysis": {"summary": "The paper introduces AtlasKV, a parametric method for integrating billion‑scale knowledge graphs into large language models with less than 20 GB VRAM, using novel components KG2KV and HiKVP that achieve sub‑linear time and memory complexity. By embedding KG triples directly into the model’s attention mechanism, AtlasKV eliminates the need for external retrievers or long context windows while preserving strong knowledge grounding and generalization. The approach is presented as scalable, effective, and adaptable to new knowledge without retraining.", "summary_cn": "本文提出 AtlasKV，一种将十亿规模知识图谱以少于 20 GB 显存直接嵌入大语言模型的参数化方法，采用 KG2KV 与 HiKVP 实现次线性时间和内存复杂度。该方法利用模型自带的注意力机制将 KG 三元组内嵌，无需外部检索器或冗长上下文，同时保持稳健的知识对齐与泛化能力，且在加入新知识时无需重新训练。", "keywords": "knowledge graph, parametric knowledge integration, AtlasKV, KG2KV, HiKVP, LLM augmentation, low VRAM, sub-linear memory, factuality, retrieval-augmented generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyu Huang", "Hong Ting Tsang", "Jiaxin Bai", "Xi Peng", "Gong Zhang", "Yangqiu Song"]}, "usage": {"completion_tokens": 716, "prompt_tokens": 3422, "total_tokens": 4138, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 404, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065066, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.000358}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:28.368335", "feed": "arxiv_cscl", "title": "Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs", "link": "https://papers.cool/arxiv/2510.17924", "analysis": {"summary": "The paper conducts a comparative study of various NLP approaches—traditional embeddings, fine‑tuned transformer models, zero‑shot/few‑shot prompting of large language models, and retrieval‑augmented generation—for detecting toxic content in online gaming chats. It evaluates each method on accuracy, processing speed, and computational cost, and proposes a hybrid moderation architecture that reduces human moderator workload while enabling continuous learning. Experiments show that fine‑tuned DistilBERT offers the best accuracy‑cost balance, providing practical guidance for cost‑effective content moderation in dynamic gaming environments.", "summary_cn": "本文比较了多种自然语言处理方法（传统嵌入、微调的 Transformer 模型、大语言模型的零示例/少示例提示以及检索增强生成）在在线游戏聊天中检测有害言论的效果，并在分类准确率、处理速度和计算成本三个维度进行评估。同时提出了一种混合审查系统架构，通过自动检测减轻人工审查负担并支持持续学习。实验结果表明，微调的 DistilBERT 在准确率‑成本权衡上表现最佳，为在动态游戏环境中部署高效、经济的内容审查系统提供了经验依据。", "keywords": "toxicity detection, gaming chat moderation, embeddings, fine-tuned transformers, large language models, retrieval-augmented generation, cost-performance trade-off, hybrid moderation system", "scoring": {"interpretability": 3, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"]}, "usage": {"completion_tokens": 788, "prompt_tokens": 3351, "total_tokens": 4139, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 453, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067601, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028201, "upstream_inference_completions_cost": 0.000394}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:28.398842", "feed": "arxiv_cscl", "title": "VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety", "link": "https://papers.cool/arxiv/2510.18214", "analysis": {"summary": "The paper introduces Vision Language Safety Understanding (VLSU), a benchmark and evaluation framework that systematically assesses multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world image‑text samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint understanding is required, exposing significant alignment gaps. The study also highlights the trade‑off between over‑blocking and under‑refusing borderline content and provides a testbed for future robust vision‑language safety research.", "summary_cn": "本文提出了视觉语言安全理解（Vision Language Safety Understanding，VLSU）框架，系统评估多模态基础模型在细粒度安全严重性和图像‑文本组合推理方面的表现，涵盖 17 种安全模式。利用 8,187 条真实图文样本并通过人工标注，作者发现模型在单模态安全信号上能够达到 90% 以上准确率，但在需要联合理解时准确率骤降至 20‑55%，暴露出显著的对齐缺口。研究还指出在边缘内容上出现的过度拦截与不足拒绝的权衡，并提供了用于后续稳健视觉‑语言安全研究的测试平台。", "keywords": "multimodal safety, vision-language models, compositional reasoning, benchmark, VLSU, alignment gaps, over-blocking, under-refusing, safety evaluation, joint understanding", "scoring": {"interpretability": 4, "understanding": 8, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}, "usage": {"completion_tokens": 1158, "prompt_tokens": 3524, "total_tokens": 4682, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 933, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00088696, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030796, "upstream_inference_completions_cost": 0.000579}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:34.391610", "feed": "arxiv_cscl", "title": "PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits", "link": "https://papers.cool/arxiv/2510.17947", "analysis": {"summary": "The paper introduces PLAGUE, a plug-and‑play framework that structures multi‑turn jailbreak attacks against large language models into three phases (Primer, Planner, Finisher) inspired by lifelong‑learning agents. Experiments show that red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on strong, safety‑focused models such as OpenAI's o3 and Claude Opus 4.1. The work highlights the importance of plan initialization, context optimization, and continual adaptation for evaluating and exposing model vulnerabilities.", "summary_cn": "本文提出 PLAGUE 框架，将针对大型语言模型的多轮越狱攻击分为 Primer、Planner、Finisher 三个阶段，借鉴终身学习代理的思路，实现插件式、可适应的攻击生成。实验表明，基于 PLAGUE 的红队代理在 OpenAI o3 和 Claude Opus 4.1 等高安全性模型上将攻击成功率提升超过 30%，达到 81.4% 与 67.3%。该工作强调了计划初始化、上下文优化和持续学习在系统性评估模型漏洞关键作用。", "keywords": "jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, PLAGUE, adversarial exploitation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"]}, "usage": {"completion_tokens": 826, "prompt_tokens": 3505, "total_tokens": 4331, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 530, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3504}, "cost": 0.00069347, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028047, "upstream_inference_completions_cost": 0.000413}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-22T02:49:34.578501", "feed": "arxiv_cscl", "title": "Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning", "link": "https://papers.cool/arxiv/2510.17900", "analysis": {"summary": "This paper introduces a benchmark using Indian public legal examinations to evaluate large language models' competence in legal reasoning, covering both multiple‑choice objective tests and a lawyer‑graded long‑form assessment from the Supreme Court Advocate‑on‑Record exam. Results show frontier models meet historical cut‑offs on objective sections but fall short of top human performance on long‑form reasoning, revealing key reliability failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work provides datasets, protocols, and analysis of where LLMs can assist versus where human lawyers remain essential.", "summary_cn": "本文提出使用印度公共法律考试作为基准，评估大语言模型在法律推理方面的能力，涵盖客观选择题以及最高法院律师资格考试的长文答题，由律师盲审评分。结果显示，前沿模型在客观题上达到了历史合格线，但在长文推理上仍未超越人类最高分，暴露出程序合规、引用规范以及法庭语气等三类可靠性失效模式。论文提供了数据集、评估协议，并分析了 LLM协助的环节与仍需人工主导的关键法律任务。", "keywords": "Indian legal reasoning, LLM evaluation, legal benchmark, courtroom readiness, multiple-choice exam, long-form legal reasoning, legal AI, exam-based assessment", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Kush Juvekar", "Arghya Bhattacharya", "Sai Khadloya", "Utkarsh Saxena"]}, "usage": {"completion_tokens": 898, "prompt_tokens": 3442, "total_tokens": 4340, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 654, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3440}, "cost": 0.0007245, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.0002755, "upstream_inference_completions_cost": 0.000449}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:45.222527", "feed": "arxiv_cscl", "title": "A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text", "link": "https://papers.cool/arxiv/2510.20782", "analysis": {"summary": "The paper introduces a use‑case specific dataset for evaluating large language models on multiple responsible performance dimensions (quality, veracity, safety, and fairness) within the task of generating plain‑text product descriptions. The dataset is parameterized by fairness attributes such as gendered adjectives and product categories, providing a rich set of labeled prompts to uncover gaps in LLM behavior. The authors demonstrate how the resource can be used to assess and compare LLMs, proposing an evaluation framework for the community.", "summary_cn": "本文构建了一个面向特定使用场景的数据集，用于在生成商品描述的任务中评估大型语言模型的质量、真实性、安全性和公平性等多维度负责任表现。数据集通过将性别形容词和商品类别等公平属性交叉组合，生成了丰富的标注提示，以帮助发现模型在这些维度上的不足。作者展示了该资源的使用方式，并提出了一套供研究社区使用的 LLM 评估方案。", "keywords": "LLM evaluation, responsible AI, fairness, dataset, product description generation, safety, veracity, quality", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Alicia Sagae", "Chia-Jung Lee", "Sandeep Avula", "Brandon Dang", "Vanessa Murdock"]}, "usage": {"completion_tokens": 595, "prompt_tokens": 3351, "total_tokens": 3946, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 317, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057951, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028201, "upstream_inference_completions_cost": 0.0002975}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:46.125853", "feed": "arxiv_cscl", "title": "Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search", "link": "https://papers.cool/arxiv/2510.20567", "analysis": {"summary": "The paper introduces a Multi-Agent Cognitive Decision Framework (MACDF) that moves e‑commerce search beyond the traditional retrieval‑ranking paradigm by modeling the multi‑stage cognitive decision process of users. MACDF employs multiple agents to provide proactive decision support, leading to significant gains in recommendation accuracy and user satisfaction, especially for complex queries involving negation, multiple constraints, or reasoning, as shown by extensive offline experiments and online A/B tests on JD's platform.", "summary_cn": "本文提出了多代理认知决策框架（MACDF），通过模拟用户的多阶段认知决策过程，将电商搜索从传统的检索‑排序模式转向主动决策支持。MACDF 使用多个智能体提供专业购物指导，在处理包含否定、多约束或推理需求的复杂查询时，显著提升推荐准确率和用户满意度，离线实验和京东平台的线上 A/B 测试均验证了其有效性。", "keywords": "e-commerce search, multi-agent systems, cognitive decision framework, retrieval-ranking, recommendation accuracy, user satisfaction", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhouwei Zhai", "Mengxiang Chen", "Haoyun Xia", "Jin Li", "Renquan Zhou", "Min Yang"]}, "usage": {"completion_tokens": 615, "prompt_tokens": 3347, "total_tokens": 3962, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 350, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058891, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028141, "upstream_inference_completions_cost": 0.0003075}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:46.641039", "feed": "arxiv_cscl", "title": "BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection", "link": "https://papers.cool/arxiv/2510.20610", "analysis": {"summary": "The paper reports the BUSTED team’s participation in the AraGenEval shared task for Arabic AI‑generated text detection, evaluating three pre‑trained transformer models (AraELECTRA, CAMeLBERT, and XLM‑RoBERTa) via fine‑tuning for binary classification. Surprisingly, the multilingual XLM‑RoBERTa achieved the best performance with an F1 of 0.7701, surpassing the Arabic‑specific models, highlighting the strong generalisation of multilingual models for detection tasks.", "summary_cn": "本文介绍了 BUSTED 团队在 AraGenEval 共享任务中对阿拉伯语 AI 生成文本检测的参赛情况，评估了三种预训练 Transformer 模型（AraELECTRA、CAMeLBERT 和 XLM‑RoBERTa）通过微调进行二分类的效果。令人惊讶的是，多语言模型 XLM‑RoBERTa 达到了最高的 F1 分数 0.7701，优于专门的阿拉伯语模型，凸显了多语言模型在检测任务中的强大泛化能力。", "keywords": "Arabic AI-generated text detection, transformer, XLM-RoBERTa, AraELECTRA, CAMeLBERT, multilingual models, binary classification, F1 score", "scoring": {"interpretability": 3, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"]}, "usage": {"completion_tokens": 682, "prompt_tokens": 3361, "total_tokens": 4043, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 364, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062451, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028351, "upstream_inference_completions_cost": 0.000341}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:46.782670", "feed": "arxiv_cscl", "title": "Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction", "link": "https://papers.cool/arxiv/2510.20411", "analysis": {"summary": "The paper introduces ContingentChat, a teacher-student framework that uses a post‑training alignment dataset to improve multi‑turn contingency in a BabyLM trained on 100 M words, resulting in more grammatical and cohesive responses. Experiments with adaptive teacher decoding show limited additional gains, highlighting that contingency remains a challenging goal for small language models.", "summary_cn": "本文提出了 ContingentChat 教师‑学生框架，通过后训练对齐数据集提升在 100 M 词训练的 BabyLM 中的多轮对话应答的连续性，使生成的回复更为语法正确且连贯。针对性教师解码策略的实验仅带来有限增益，表明对婴儿语言模型而言，实现连续性仍是一个挑战。", "keywords": "BabyLM, multi-turn dialogue, contingency, teacher-student framework, post-training alignment, language model, dialogue quality", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Suchir Salhan", "Hongyi Gu", "Donya Rooein", "Diana Galvan-Sosa", "Gabrielle Gaudeau", "Andrew Caines", "Zheng Yuan", "Paula Buttery"]}, "usage": {"completion_tokens": 618, "prompt_tokens": 3319, "total_tokens": 3937, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 406, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058621, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027721, "upstream_inference_completions_cost": 0.000309}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:46.793727", "feed": "arxiv_cscl", "title": "CreativityPrism: A Holistic Benchmark for Large Language Model Creativity", "link": "https://papers.cool/arxiv/2510.20091", "analysis": {"summary": "CreativityPrism is a comprehensive benchmark that assesses large language model creativity along three dimensions—quality, novelty, and diversity—across nine tasks spanning divergent thinking, creative writing, and logical reasoning. The authors evaluate 17 state-of-the-art proprietary and open-source LLMs, revealing performance gaps and varying correlations between metrics and domains, highlighting that strong results in one creativity dimension do not guarantee generalization to others.", "summary_cn": "CreativityPrism 是一个全面的基准，用于在质量、创新性和多样性三个维度上评估大型语言模型的创造力，涵盖发散思维、创意写作和逻辑推理等九个任务。作者对 17 个最先进的专有和开源模型进行评估，揭示了性能差距以及不同指标和领域之间的相关性差异，强调在某一创造力维度上表现出色并不一定能推广到其他维度。", "keywords": "creativity benchmark, large language models, quality novelty diversity, evaluation metrics, divergent thinking, creative writing, logical reasoning, model comparison, holistic evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhaoyi Joey Hou", "Bowei Alvin Zhang", "Yining Lu", "Bhiman Kumar Baghel", "Anneliese Brei", "Ximing Lu", "Meng Jiang", "Faeze Brahman", "Snigdha Chaturvedi", "Haw-Shiuan Chang", "Daniel Khashabi", "Xiang Lorraine Li"]}, "usage": {"completion_tokens": 576, "prompt_tokens": 3471, "total_tokens": 4047, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 295, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058801, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030001, "upstream_inference_completions_cost": 0.000288}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:46.921028", "feed": "arxiv_cscl", "title": "Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks", "link": "https://papers.cool/arxiv/2510.20584", "analysis": {"summary": "The paper empirically investigates whether using ChatGPT to automatically code communication data in collaborative tasks introduces bias against demographic groups such as gender and race. Analyzing negotiation, problem‑solving, and decision‑making datasets, the authors find no statistically significant bias across the examined groups, suggesting that ChatGPT‑based coding can be safely employed for large‑scale assessment of collaboration.", "summary_cn": "本文实证研究了使用 ChatGPT 自动编码协作任务中的交流数据是否会对性别、种族等人口群体产生偏见。通过对谈判、问题解决和决策制定三类任务的数据进行分析，作者发现不同群体之间没有显著的编码偏差，表明 ChatGPT 编码可安全用于大规模的合作与交流评估。", "keywords": "ChatGPT coding, bias evaluation, fairness, demographic bias, communication annotation, collaborative tasks, automated coding", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi"]}, "usage": {"completion_tokens": 625, "prompt_tokens": 3361, "total_tokens": 3986, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 433, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059601, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028351, "upstream_inference_completions_cost": 0.0003125}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.030186", "feed": "arxiv_cscl", "title": "FreeChunker: A Cross-Granularity Chunking Framework", "link": "https://papers.cool/arxiv/2510.20356", "analysis": {"summary": "FreeChunker introduces a cross‑granularity encoding framework that treats sentences as atomic units and enables flexible retrieval of arbitrary sentence combinations, replacing static chunk segmentation in Retrieval‑Augmented Generation (RAG) pipelines. This approach reduces the computational cost of semantic boundary detection while improving retrieval performance, as demonstrated on the LongBench V2 benchmark. The paper shows that FreeChunker outperforms traditional chunking methods both in accuracy and efficiency.", "summary_cn": "FreeChunker 提出了一种跨粒度编码框架，将句子视为原子单元，支持任意句子组合的灵活检索，取代了检索增强生成（RAG）系统中的固定块划分。该方法显著降低了语义边界检测的计算开销，并在 LongBench V2 基准上提升了检索性能。实验表明 FreeChunker 在准确率和效率上均优于传统块划分方法。", "keywords": "chunking, retrieval-augmented generation, cross-granularity, flexible retrieval, LongBench, computational efficiency, semantic boundary detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Zhang", "Yuan-Hao Jiang", "Yonghe Wu"]}, "usage": {"completion_tokens": 604, "prompt_tokens": 3331, "total_tokens": 3935, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 355, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058101, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027901, "upstream_inference_completions_cost": 0.000302}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.062942", "feed": "arxiv_cscl", "title": "Simple Context Compression: Mean-Pooling and Multi-Ratio Training", "link": "https://papers.cool/arxiv/2510.20797", "analysis": {"summary": "The paper proposes a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture. It also explores training a single compressor to produce multiple compression ratios and evaluates the approach across diverse QA benchmarks, model families, and scales. Results indicate strong overall performance with modest degradation when handling multiple ratios, while highlighting nuanced trade‑offs among compression strategies.", "summary_cn": "本文提出了一种轻量级的均值池化软上下文压缩方法，用于检索增强生成（RAG），并证明其在性能上始终优于常用的 compression‑tokens 架构。文章进一步研究了同一压缩器输出多种压缩比的训练方式，并在不同领域和模型规模的问答数据集上进行评估。实验结果显示，该方法整体表现最佳，即使在多压缩比设置下也仅出现小幅性能下降，同时揭示了压缩方法之间更为细致的权衡关系。", "keywords": "context compression, mean-pooling, retrieval-augmented generation, large language models, multi-ratio training, soft compression, QA evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yair Feldman", "Yoav Artzi"]}, "usage": {"completion_tokens": 608, "prompt_tokens": 3346, "total_tokens": 3954, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 314, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058526, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028126, "upstream_inference_completions_cost": 0.000304}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.063343", "feed": "arxiv_cscl", "title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "link": "https://papers.cool/arxiv/2510.20036", "analysis": {"summary": "ToolScope enhances LLM agent tool use by automatically merging redundant tools and applying a context‑aware retriever that selects the most relevant tools for a given query, allowing larger toolsets to fit within LLM context windows. Experiments on three state‑of‑the‑art LLMs and three open‑source tool‑use benchmarks show improvements of 8.38% to 38.6% in tool selection accuracy.", "summary_cn": "ToolScope 通过自动合并冗余工具并使用上下文感知检索器，为每个查询挑选最相关的工具，从而在不超出 LLM 上下文限制的情况下提升工具使用效果。对三种主流 LLM 和三套开源工具使用基准的实验显示，工具选择准确率提升了 8.38% 到 .6%。", "keywords": "LLM agents, tool merging, tool selection, context-aware retrieval, redundancy reduction, agent tool use", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"]}, "usage": {"completion_tokens": 637, "prompt_tokens": 3369, "total_tokens": 4006, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 395, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060321, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028471, "upstream_inference_completions_cost": 0.0003185}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.079188", "feed": "arxiv_cscl", "title": "Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset", "link": "https://papers.cool/arxiv/2510.20508", "analysis": {"summary": "The paper introduces a 21‑way multiparallel EuroParl dataset annotated with speakers' political affiliations and uses it to evaluate the political fairness of multilingual large language models by measuring translation quality across parties. Systematic differences are observed, with speeches from majority left, center, and right parties being translated more accurately than those from outsider parties, highlighting bias in multilingual LLMs.", "summary_cn": "本文构建了包含议员政治派别标签的 21 语言欧盟议会（EuroParl）多语平行语料库，并据此评估多语言大模型的政治公平性，通过比较不同政党发言的翻译质量来发现偏差。研究发现，左派、中心派和右派的主流政党发言的翻译质量普遍高于外部政党，揭示了多语言模型在政治立上的系统性偏见。", "keywords": "political bias, multilingual LLMs, fairness, EuroParl, translation quality, multilingual dataset, political fairness, bias evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Paul Lerner", "François Yvon"]}, "usage": {"completion_tokens": 645, "prompt_tokens": 3377, "total_tokens": 4022, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 397, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060841, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028591, "upstream_inference_completions_cost": 0.0003225}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.088548", "feed": "arxiv_cscl", "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding", "link": "https://papers.cool/arxiv/2510.20176", "analysis": {"summary": "Mixture-of-Minds introduces a multi-agent framework that splits table reasoning into three specialized roles—planning, coding, and answering—combined with code execution for precise manipulation. The authors train the agents using a self-improvement loop that generates pseudo‑gold trajectories via Monte Carlo Tree Search rollouts and applies reinforcement learning, achieving a 62.13% accuracy on TableBench and outperforming strong baselines.", "summary_cn": "Mixture-of-Minds 提出了一种多代理框架，将表格推理分解为规划、编码和回答三个专门角色，并利用代码执行实现精准的表格操作。作者通过蒙特卡罗树搜索产生伪金轨迹并使用强化学习进行自我提升训练，使模型在 TableBench 上达到 62.13% 的准确率，超了强基线模型。", "keywords": "table understanding, multi-agent reinforcement learning, Mixture-of-Minds, Monte Carlo Tree Search, code generation, table reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhang Zhou", "Mingrui Zhang", "Ke Li", "Mingyi Wang", "Qiao Liu", "Qifei wang", "Jiayi Liu", "Fei Liu", "Serena Li", "Weiwi Li", "Mingze Gao", "Abhishek Kumar", "Xiangjun Fan", "Zhuokai Zhao", "Lizhu Zhang"]}, "usage": {"completion_tokens": 639, "prompt_tokens": 3428, "total_tokens": 4067, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 405, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061306, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029356, "upstream_inference_completions_cost": 0.0003195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.276202", "feed": "arxiv_cscl", "title": "Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs", "link": "https://papers.cool/arxiv/2510.20475", "analysis": {"summary": "The paper presents an improved Masked Language Modeling (MLM) technique for the 2025 BabyLM Challenge, where masking probabilities are adapted based on the model’s prediction difficulty and sub‑token embeddings are incorporated to enhance morphological generalization. Experiments show substantial gains on (Super)GLUE benchmarks compared to standard MLM, and the approach outperforms the baseline in the strict‑small track.", "summary_cn": "本文提出了一种改进的掩码语言模型（MLM）方法，用于 2025 年 BabyLM 挑战。该方法根据模型预测难度自适应调整掩码概率，并引入子词嵌入以提升形态学泛化能力。实验表明，相较于标准，在 (Super)GLUE 基准上取得显著提升，并在 strict‑small 轨道中超越基线。", "keywords": "masked language modeling, adaptive masking, BabyLM, morphological generalization, sub-token embeddings, low-resource pretraining", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lukas Edman", "Alexander Fraser"]}, "usage": {"completion_tokens": 663, "prompt_tokens": 3297, "total_tokens": 3960, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 431, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060541, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027391, "upstream_inference_completions_cost": 0.0003315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.501412", "feed": "arxiv_cscl", "title": "Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)", "link": "https://papers.cool/arxiv/2510.20358", "analysis": {"summary": "The paper investigates whether pre‑training solely on dialogue data yields small language models that are both formally and functionally suitable for the BabyLM benchmark. Using a dialogue‑only pre‑trained Llamalogue model, various fine‑tuning strategies (including PPO and DPO) are applied to encourage more communicative text generation; the models underperform on standard BabyLM tasks but excel at a custom dialogue continuation minimal‑pair benchmark, with DPO improving performance where PPO shows mixed or adverse effects.", "summary_cn": "本文研究仅使用对话数据进行预训练是否能够产出在 BabyLM 基准上形式和功能上都合适的小型语言模型。作者基于预训练的 Llamalogue 模型，采用包括 PPO 与 DPO 在内的多种微调策略，以促使模型生成更具交互性的文本；尽管这些在多数标准 BabyLM 评测上表现不佳，却在自定义的对话续写最小对比基准上表现突出，其中 DPO 能进一步提升性能，而 PPO 则呈现出混合甚至对抗性的效果。", "keywords": "dialogue pretraining, BabyLM, language models, PPO fine-tuning, DPO fine-tuning, communicative generation, minimal pair evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Francesca Padovani", "Bastian Bunzeck", "Manar Ali", "Omar Momen", "Arianna Bisazza", "Hendrik Buschmeier", "Sina Zarrieß"]}, "usage": {"completion_tokens": 725, "prompt_tokens": 3304, "total_tokens": 4029, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 443, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063746, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027496, "upstream_inference_completions_cost": 0.0003625}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.526536", "feed": "arxiv_cscl", "title": "Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models", "link": "https://papers.cool/arxiv/2510.20351", "analysis": {"summary": "This paper investigates whether large language models (LLMs) possess prior knowledge of widely used public tabular benchmarks such as Adult Income and Titanic. Through controlled probing experiments, the authors find that contamination effects arise only for datasets with strong semantic cues (e.g., informative column names), while removing or randomizing these cues drops performance to near-random levels, indicating that apparent competence may stem from memorization rather than genuine reasoning. The study discusses evaluation protocol implications and proposes methods to separate semantic leakage from authentic reasoning ability.", "summary_cn": "本文研究了大语言模型（LLM）是否已经记住了常用的公开表格数据集（如 Adult Income、Titanic）。通过一系列受控探测实验，作者发现只有包含明显语义线索（如有意义的列名或可解释的取值）的数据集会出现污染效应；当这些线索被移除或随机化时，模型性能急剧下降至接近随机水平，表明表面上的表格推理能力可能源自对公开数据的记忆而非真正的泛化。文中进一步讨论了评估协议的影响，并提出了在未来 LLM 评估中区分语义泄漏和真实推理能力的策略。", "keywords": "latent knowledge, tabular data, dataset contamination, LLM evaluation, structured reasoning, memorization, probing, semantic cues", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Matteo Silvestri", "Flavio Giorgi", "Fabrizio Silvestri", "Gabriele Tolomei"]}, "usage": {"completion_tokens": 653, "prompt_tokens": 3362, "total_tokens": 4015, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 326, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061016, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028366, "upstream_inference_completions_cost": 0.0003265}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.603907", "feed": "arxiv_cscl", "title": "Structure-Conditional Minimum Bayes Risk Decoding", "link": "https://papers.cool/arxiv/2510.20700", "analysis": {"summary": "The paper introduces three lightweight adaptations to the utility function of Minimum Bayes Risk (MBR) decoding that make it sensitive to latent structural variations such as dialogue act, emotion, and response format. It proposes two new metrics to assess structural optimality, demonstrates that conventional similarity‑based utilities perform poorly on these metrics, and shows that the adapted utilities improve win rates on instruction‑following benchmarks (AlpacaEval, MT‑Bench) by up to 13.7 points. The work highlights the importance of structural awareness for open‑ended generation tasks.", "summary_cn": "本文提出了三种轻量化的效用函数改进，使最小贝叶斯风险（MBR）解码能够感知潜在结构变化，如对话行为、情感以及回复形式。作者设计了两项结构最优度评估指标，证明传统基于相度的效用函数在这些指标上表现不佳，而改进后的方法在 AlpacaEval 和 MT‑Bench 等指令遵循基准上提升了最高 13.7% 的胜率。该工作凸显了在开放式生成任务中关注结构信息的重要性。", "keywords": "minimum Bayes risk, MBR decoding, structural conditioning, dialogue act, emotion modeling, instruction-following, generation quality, utility function adaptation, evaluation metrics", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Bryan Eikema", "Anna Rutkiewicz", "Mario Giulianelli"]}, "usage": {"completion_tokens": 735, "prompt_tokens": 3453, "total_tokens": 4188, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 452, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066481, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029731, "upstream_inference_completions_cost": 0.0003675}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.636895", "feed": "arxiv_cscl", "title": "On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?", "link": "https://papers.cool/arxiv/2510.20810", "analysis": {"summary": "The paper surveys the ambiguous definition of “LLM-generated text” and argues that current detection benchmarks often target only a narrow subset of possible outputs. It highlights how human editing and subtle model influence blur the boundary between machine‑ and human‑written text, leading to misinterpretation of detector performance in real‑world scenarios. The authors suggest that detectors should be used as reference tools under specific conditions rather than as definitive judgments.", "summary_cn": "本文梳理了“LLM 生成文本”定义的模糊性，指出现有检测基准通常只覆盖模型可能产生的部分文本文章强调人类编辑以及模型对用户的微妙影响会模糊机器与人类文本的边界，导致在实际应用中对检测器性能的误读。作者主张检测器应仅在特定条件下作为参考工具，而非决定性判定手段。", "keywords": "LLM-generated text detection, provenance classification, AI-generated content, evaluation benchmarks, safety, human edits, mis/disinformation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Mingmeng Geng", "Thierry Poibeau"]}, "usage": {"completion_tokens": 696, "prompt_tokens": 3375, "total_tokens": 4071, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 475, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063361, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028561, "upstream_inference_completions_cost": 0.000348}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.676290", "feed": "arxiv_cscl", "title": "Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models", "link": "https://papers.cool/arxiv/2510.20033", "analysis": {"summary": "The thesis proposes three methods to improve transfer learning for sequence labeling tasks using pre‑trained neural language models: (1) a multi‑task model that incorporates an extra domain‑independent signal for event trigger detection, (2) architectural modifications that enable bidirectional information flow across layers of autoregressive LLMs, and (3) a generative supervised in‑context fine‑tuning framework combined with response‑oriented adaptation. Experiments on domain transfer and autoregressive LLMs show that these targeted adaptations yield state‑of‑the‑art performance on sequence labeling benchmarks.", "summary_cn": "本文提出三种提升预训练神经语言模型在序列标注任务上迁移学习效果的方法：（1）在事件触发检测的领域转移中，引入来自域无关文本处理系统的额外信号，构建多任务模型；（2）对自回归大型模型的架构进行修改，使层间信息流可以双向传播；（3）采用生成式监督式上下文微调结合面向响应的适配策略，将自回归模型用于序列标注。实验表明，这些针对性的适配显著提升了序列标注任务的表现。", "keywords": "transfer learning, sequence labeling, pre-trained language models, multi-task learning, domain adaptation, autoregressive LLM, in-context fine-tuning, response-oriented adaptation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["David Dukić"]}, "usage": {"completion_tokens": 701, "prompt_tokens": 3406, "total_tokens": 4107, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 390, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064076, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029026, "upstream_inference_completions_cost": 0.0003505}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.775022", "feed": "arxiv_cscl", "title": "\\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding", "link": "https://papers.cool/arxiv/2510.20670", "analysis": {"summary": "The paper introduces CantoNLU, a benchmark covering seven Cantonese natural language understanding tasks ranging from syntax to semantics, and evaluates several models including Mandarin-based, Cantonese-adapted, and monolingual Cantonese models. Results show Cantonese-adapted models generally perform best, while monolingual models excel on syntactic tasks, and Mandarin models remain competitive when Cantonese data is limited. All datasets, code, and model weights are released to support future research in Cantonese NLP.", "summary_cn": "本文推出 CantoNLU 基准，涵盖七项粤语自然语言理解任务（包括句法与语义），并评估了多种模型：未进行粤语训练的普通话模型、通过持续预训练获得的粤语适配模型，以及从零开始训练的单语粤语模型。实验表明，粤语适配模型整体表现最佳，单语模型在句法任务上更占优势，而普通话模型在数据稀缺时仍具竞争力。作者公开了所有数据集、代码和模型权重，以推动粤语 NLP 研究。", "keywords": "Cantonese, natural language understanding, benchmark, low-resource language, cross-lingual transfer, syntax, semantics, NLU tasks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junghyun Min", "York Hay Ng", "Sophia Chan", "Helena Shunhua Zhao", "En-Shiun Annie Lee"]}, "usage": {"completion_tokens": 674, "prompt_tokens": 3418, "total_tokens": 4092, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 396, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062906, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029206, "upstream_inference_completions_cost": 0.000337}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.912341", "feed": "arxiv_cscl", "title": "Hierarchical Sequence Iteration for Heterogeneous Question Answering", "link": "https://papers.cool/arxiv/2510.20505", "analysis": {"summary": "The paper proposes Hierarchical Sequence (HSEQ) Iteration, a unified framework that linearises text, tables, and knowledge graphs into a reversible hierarchical sequence with structural tags and performs structure‑aware iterative retrieval to gather sufficient evidence before answer synthesis. A Head Agent guides retrieval while an Iteration Agent expands the sequence via parent/child hops, table neighbour moves, and KG relation traversals, followed by evidence canonicalisation and optional refinement to resolve contradictions. Experiments on HotpotQA, HybridQA/TAT‑QA, and MetaQA demonstrate consistent EM/F1 improvements over strong baselines with reduced latency and token usage.", "summary_cn": "本文提出了层级序列（HSEQ）迭代框架，将文本、表格和知识图谱线性化为可逆的层级序列并添加轻量结构标签，通过结构感知的迭代检索在生成答案前收集足够证据。Head Agent负责检索指引，Iteration Agent 通过父子跳跃、表格邻居或 KG 关系等结构保持动作扩展序列，随后对证据进行规范化并可选地进行矛盾消解循环。对 HotpotQA、HybridQA/TAT‑QA 与 MetaQA 的实验表明，相比强基线在 EM/F1 上取得一致提升，并显著降低延迟和令牌消耗。", "keywords": "hierarchical sequence, heterogeneous question answering, retrieval-augmented generation, multi-hop reasoning, structural tags, knowledge graph, table QA, budget-aware iteration, evidence canonicalization", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"]}, "usage": {"completion_tokens": 727, "prompt_tokens": 3482, "total_tokens": 4209, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 361, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066516, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030166, "upstream_inference_completions_cost": 0.0003635}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:47.977448", "feed": "arxiv_cscl", "title": "Robust Preference Alignment via Directional Neighborhood Consensus", "link": "https://papers.cool/arxiv/2510.20498", "analysis": {"summary": "The paper introduces Robust Preference Selection (RPS), a training-free, post‑hoc method that samples multiple responses from a directional neighborhood of related preferences and selects the one best matching the user’s intent, thereby closing the preference coverage gap in large language models. A theoretical analysis shows the neighborhood generation is provably superior to a strong baseline, and experiments across DPA, DPO, and SFT paradigms demonstrate up to 69% win rates on under‑represented preference regions without retraining. RPS provides a practical and theoretically grounded way to enhance the robustness and reliability of preference‑aligned models.", "summary_cn": "本文提出了 Robust Preference Selection（RPS）——一种无需重新训练的后处理方法，通过在偏好空间的方向邻域中采样多个候选回复并挑选最符合用户意图的答案，以弥补大型语言模型在偏好覆盖方面的不足。理论分析表明该邻域生成策略相较于强基线有可证明的优势，实验在 DPA、DPO 和 SFT 三种对齐范式上验证了其有效性，在偏好空间的弱覆盖区域实现最高 69% 的胜率，且无需模型重新训练。RPS 为提升偏好对齐模型的鲁棒性和可靠性提供了实用且有理论支撑的方案。", "keywords": "preference alignment, directional neighborhood consensus, robust preference selection, post-hoc alignment, large language models, DPO, DPA, SFT, safety", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ruochen Mao", "Yuling Shi", "Xiaodong Gu", "Jiaheng Wei"]}, "usage": {"completion_tokens": 733, "prompt_tokens": 3506, "total_tokens": 4239, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 376, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067176, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030526, "upstream_inference_completions_cost": 0.0003665}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:48.131286", "feed": "arxiv_cscl", "title": "Citation Failure: Definition, Analysis and Efficient Mitigation", "link": "https://papers.cool/arxiv/2510.20303", "analysis": {"summary": "The paper defines citation failure as the inability of LLM-based retrieval‑augmented generation systems to provide complete evidence for a helpful response, distinguishing it from response failure. It introduces the CITECONTROL benchmark to study how relational complexity between response and evidence affects citation quality, and proposes the CITENTION framework that combines generative, attention‑based, and retrieval‑based methods to efficiently mitigate citation failure, showing strong improvements both on the benchmark and in transfer settings.", "summary_cn": "本文 \"citation failure\" 定义为大型语言模型检索增强生成系统在给出有用回复时未能提供完整证据的现象，并将其与响应失败区分开来。作者构建了 CITECONTROL 基准，系统研究响应与证据之间关系的复杂性如何影响引用质量；随后提出 CITENTION 框架，融合生成式、注意力驱动和检索式方法，以高效缓解引用失效，并在基准及迁移实验中实现了显著改进。", "keywords": "citation failure, LLM, RAG, benchmark, CITECONTROL, CITENTION, evidence completeness, retrieval-augmented generation, mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jan Buchmann", "Iryna Gurevych"]}, "usage": {"completion_tokens": 750, "prompt_tokens": 3405, "total_tokens": 4155, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 490, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066511, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029011, "upstream_inference_completions_cost": 0.000375}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:48.662092", "feed": "arxiv_cscl", "title": "GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning", "link": "https://papers.cool/arxiv/2510.20548", "analysis": {"summary": "The paper introduces GlobalRAG, a reinforcement‑learning framework that decomposes multi‑hop questions into subgoals and coordinates retrieval with reasoning to improve global planning and execution fidelity. It proposes novel Planning Quality and SubGoal Completion rewards, together with a progressive weight annealing strategy, achieving up to 14.2% gains in EM and F1 on both in‑domain and out‑of‑domain QA benchmarks while using only 8k training examples.", "summary_cn": "本文提出 GlobalRAG 框架，通过将多跳问题拆解为子目标并同步检索与推理，实现全局规划和执行的提升。文中设计了规划质量奖励和子目标完成奖励，并采用渐进权重退火策略，使模型在仅使用 8k 训练数据的情况下，在域内外问答基准上实现了 EM 与 F1 的 14.2% 平均提升。", "keywords": "reinforcement learning, multi-hop question answering, retrieval-augmented generation, global planning, subgoal decomposition, reward design", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinchang Luo", "Mingquan Cheng", "Fan Wan", "Ni Li", "Xiaoling Xia", "Shuangshuang Tian", "Tingcheng Bian", "Haiwei Wang", "Haohuan Fu", "Yan Tao"]}, "usage": {"completion_tokens": 610, "prompt_tokens": 3415, "total_tokens": 4025, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 382, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059661, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029161, "upstream_inference_completions_cost": 0.000305}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:48.705686", "feed": "arxiv_cscl", "title": "Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training", "link": "https://papers.cool/arxiv/2510.20059", "analysis": {"summary": "The paper introduces a method to improve reasoning in a small Persian medical language model by using Reinforcement Learning with AI Feedback (RLAIF) and Direct Preference Optimization (DPO) on a translated multiple‑choice medical QA dataset, generating preferred and rejected chain‑of‑thought answer pairs. Despite training on a modest dataset, the resulting model surpasses a larger predecessor trained on 57 million tokens, demonstrating that reasoning‑focused fine‑tuning can yield strong domain‑specific performance.", "summary_cn": "本文提出利用强化学习（RLAIF）和直接偏好优化（DPO）在翻译后的波斯语医学问答数据集上生成首选与被拒的思考链（CoT）答案，对小型波斯语医学语言模型进行推理能力提升。尽管数据规模有限，训练后模型在波斯语医学问答上超过了使用约5700万标记进行训练的前代模型，展示了针对推理的微调在资源受限场景下的高效性。", "keywords": "Persian language model, medical QA, RLAIF, DPO, chain-of-thought, reasoning, low-resource, fine-tuning, preference learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Mehrdad Ghassabi", "Sadra Hakim", "Hamidreza Baradaran Kashani", "Pedram Rostami"]}, "usage": {"completion_tokens": 755, "prompt_tokens": 3423, "total_tokens": 4178, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 470, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067031, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029281, "upstream_inference_completions_cost": 0.0003775}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.112342", "feed": "arxiv_cscl", "title": "The Impact of Negated Text on Hallucination with Large Language Models", "link": "https://papers.cool/arxiv/2510.20375", "analysis": {"summary": "The paper investigates how negated text affects hallucination detection in large language models, introducing the NegHalu dataset that rewrites existing hallucination benchmarks with negated expressions. Experiments reveal that LLMs often fail to identify hallucinations in negated contexts, producing inconsistent judgments, and token-level tracing shows internal processing challenges. The study highlights a previously overlooked weakness in model faithfulness and suggests avenues for mitigation.", "summary_cn": "本文研究了否定文本对大语言模型中幻觉检测的影响，构建了 NegHalu 数据集，将已有的幻觉检测数据集改写为否定表述。实验表明，模型在否定语境下往往难以正确识别幻觉，导致逻辑不一致或不忠实的判断，并通过逐词追踪展示了内部处理的困难。该工作揭示了模型可信度的一项被忽视的弱点，并提出了潜在的缓解方向。", "keywords": "hallucination detection, negation, large language models, NegHalu dataset, token-level analysis, model interpretability, AI safety", "scoring": {"interpretability": 6, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Jaehyung Seo", "Hyeonseok Moon", "Heuiseok Lim"]}, "usage": {"completion_tokens": 642, "prompt_tokens": 3360, "total_tokens": 4002, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 370, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060436, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028336, "upstream_inference_completions_cost": 0.000321}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.207409", "feed": "arxiv_cscl", "title": "The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts", "link": "https://papers.cool/arxiv/2510.20543", "analysis": {"summary": "The paper introduces CenterBench, a dataset of 9,720 questions probing language models' ability to parse center‑embedded sentences and to distinguish syntactic comprehension from semantic plausibility shortcuts. Experiments on several models show that performance gaps between plausible and implausible sentences grow with nesting depth, revealing systematic reliance on semantic associations over structural analysis. The work provides a clear framework for identifying when models abandon true syntactic processing in favor of pattern matching.", "summary_cn": "本文推出 CenterBench 数据集，包含 9,720 条关于中心嵌入句子（如 “The cat that the dog chased meowed”）的理解问答，用于检验语言模型是进行句法解析还是仅凭语义关联作答。实验表明，随着句子嵌套深度增加，可行性与不合常理句子的表现差距逐渐扩大，显示模型倾向于使用语义捷径而非结构分析。该研究提供了首个框架来辨别模型何时放弃句法推理转而依赖模式匹配。", "keywords": "center-embedded sentences, structural understanding, semantic plausibility, language model shortcuts, probing benchmark, syntactic parsing, comprehension questions", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sangmitra Madhusudan", "Kaige Chen", "Ali Emami"]}, "usage": {"completion_tokens": 789, "prompt_tokens": 3456, "total_tokens": 4245, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 575, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069226, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029776, "upstream_inference_completions_cost": 0.0003945}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.307684", "feed": "arxiv_cscl", "title": "Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning", "link": "https://papers.cool/arxiv/2510.20098", "analysis": {"summary": "The paper introduces ARTER, an Adaptive Routing and Targeted Entity Reasoning pipeline that combines classic candidate generation with selective large language model (LLM) reasoning to improve entity linking. By classifying mentions into easy and hard cases using lightweight signals, ARTER routes easy cases to a fast linker (e.g., ReFinED) and hard cases to an LLM, achieving comparable accuracy to full‑LLM pipelines while reducing token usage by about half.", "summary_cn": "本文提出 ARTER（自适应路由与目标实体推理）流水线，将传统候选生成与选择性的 LLM 推理相结合，以提升实体链接效果。通过利用轻量信号将提及划分为易/难两类，易例交由快速链接器（如 ReFinED）处理，难例则使用 LLM 进行推理，从而在保持与全 LLM 流水线相当的准确率的同时，将 LLM 令牌消耗降低约一半。", "keywords": "entity linking, large language models, adaptive routing, targeted reasoning, few-shot prompting, efficiency, ReFinED, candidate generation, contextual scoring", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yajie Li", "Albert Galimov", "Mitra Datta Ganapaneni", "Pujitha Thejaswi", "De Meng", "Priyanshu Kumar", "Saloni Potdar"]}, "usage": {"completion_tokens": 797, "prompt_tokens": 3422, "total_tokens": 4219, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 562, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00069116, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.0003985}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.368607", "feed": "arxiv_cscl", "title": "NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew", "link": "https://papers.cool/arxiv/2510.20386", "analysis": {"summary": "The paper introduces NeoDictaBERT and NeoDictaBERT-bilingual, BERT-style models based on the NeoBERT architecture that are specifically trained on Hebrew text. These models achieve state-of-the-art results on most Hebrew benchmarks and demonstrate strong performance on multilingual retrieval tasks compared to similarly sized models. The authors detail the training methodology and release the models for community use.", "summary_cn": "本文提出了 NeoDictaBERT 和 NeoDictaBERT‑bilingual 两种基于 NeoBERT 架构的 BERT 风格模型，专门在希伯来语文本上进行训练。相较于同等规模的模型，这些模型在多数希伯来语基准上取得了最先进的表现，并在多语言检索任务中表现出色。文章阐述了训练流程并公开模型以供社区使用。", "keywords": "NeoDictaBERT, Hebrew NLP, BERT, NeoBERT, multilingual retrieval, transformer architecture", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaltiel Shmidman", "Avi Shmidman", "Moshe Koppel"]}, "usage": {"completion_tokens": 630, "prompt_tokens": 3414, "total_tokens": 4044, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 395, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060646, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029146, "upstream_inference_completions_cost": 0.000315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.517978", "feed": "arxiv_cscl", "title": "The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI", "link": "https://papers.cool/arxiv/2510.20647", "analysis": {"summary": "The paper studies how Large Reasoning Models (LRMs) handle multilingual question answering, finding that they often switch to English reasoning even for non‑English prompts. Experiments on MGSM and GPQA Diamond show that English‑based reasoning yields higher accuracy and richer cognitive behaviors, but introduces a \"Lost in Translation\" failure mode where translation steps cause errors that would not occur when reasoning in the question's original language.", "summary_cn": "本文研究了大规模推理模型（LRM）在多语言问答中的表现，发现它们常在非英语提问时转而使用英语进行推理。对 MGSM 和 GPQA Diamond 两项任务的实验表明，使用英语推理能获得更高的答案准确率并展现出更丰富的认知行为，但也会出现“翻译失误”这一关键失败模式，即翻译过程引入的错误本可以通过直接使用问题语言的推理避免。", "keywords": "multilingual reasoning, large reasoning models, interpretability, translation errors, MGSM, GPQA Diamond, language bias, cognitive behavior analysis", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Alan Saji", "Raj Dabre", "Anoop Kunchukuttan", "Ratish Puduppully"]}, "usage": {"completion_tokens": 642, "prompt_tokens": 3378, "total_tokens": 4020, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 435, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060706, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028606, "upstream_inference_completions_cost": 0.000321}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.621607", "feed": "arxiv_cscl", "title": "ARC-Encoder: learning compressed text representations for large language models", "link": "https://papers.cool/arxiv/2510.20535", "analysis": {"summary": "ARC-Encoder proposes an encoder that compresses long text contexts into a smaller set of continuous representations, which replace token embeddings in decoder LLMs, achieving up to 4-8x reduction in representation count. The paper systematically studies training strategies and architecture choices, demonstrating state-of-the-art performance across in‑context learning, context‑window extension, and both instruct and base decoders while reducing inference cost. Additionally, the encoder can be adapted to multiple decoders simultaneously, offering a portable, efficient solution for various LLMs.", "summary_cn": "ARC-Encoder 提出一种编码器，可将长文本上下文压缩为更少的连续表示，用以替代解码器 LLM 中的 token 嵌入，实现 4-8 倍的表示数量压缩。论文系统研究了训练策略和架构选择，在上下文学习、窗口扩展以及指令和基础解码器等多种使用场景中达到最新水平的性能，并降低推理成本。该编码器还能同时适配多个解码器，实现跨不同 LLM 的通用、高效压缩。", "keywords": "context compression, encoder-decoder, large language models, in-context learning, retrieval-augmented generation, chain-of-thought, efficient inference, multi-decoder adaptation, ARC-Encoder", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hippolyte Pilchen", "Edouard Grave", "Patrick Pérez"]}, "usage": {"completion_tokens": 646, "prompt_tokens": 3515, "total_tokens": 4161, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 358, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062961, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030661, "upstream_inference_completions_cost": 0.000323}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.660470", "feed": "arxiv_cscl", "title": "Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models", "link": "https://papers.cool/arxiv/2510.20198", "analysis": {"summary": "The paper introduces a suite of five grid‑based tasks to probe large language models' ability to perform spatial reasoning from textual descriptions, covering quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Results show that while models achieve moderate accuracy on small grids, performance degrades sharply as grid size and task complexity increase, with average accuracy drops of around 43% and up to 84% loss. The authors argue that this scaling failure reveals a lack of robust spatial representations in current LLM architectures and propose the benchmark as a tool for future research at the intersection of language and geometry.", "summary_cn": "本文提出了五个基于网格的任务，用于评估大语言模型在文本描述下的空间推理能力，任务包括象限识别、几何变换、距离评估、单词搜索和滑块拼图。实验发现，模型在小规模网格上能够取得中等准确率，但随着网格尺寸和任务复杂度的提升，性能急剧下降，平均准确率下降约 43%，最高下降达 84%。作者认为，这种尺度退化表明现有 LLM 缺乏稳健的空间表征，并将该基准视为语言与几何交叉研究的未来方向。", "keywords": "spatial reasoning, large language models, grid-based tasks, quadrant identification, geometric transformations, distance evaluation, word search, tile sliding, benchmark", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"]}, "usage": {"completion_tokens": 651, "prompt_tokens": 3422, "total_tokens": 4073, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 306, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061816, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029266, "upstream_inference_completions_cost": 0.0003255}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.690826", "feed": "arxiv_cscl", "title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed", "link": "https://papers.cool/arxiv/2510.20487", "analysis": {"summary": "The paper introduces a method to suppress evaluation-awareness in large language models by adding a steering vector to their activations, causing the model to behave as if it were deployed during evaluation. The authors first train a model to exhibit evaluation-aware behavior using factual documents and Python type hints, then show that steering with a vector derived from the original model removes the cue‑driven bias. This technique can improve the reliability of safety assessments.", "summary_cn": "本文提出通过向大语言模型的激活添加导向向量来抑制其评估感知，使模型在评估时表现得像已部署状态。作者先使用包含模型事实描述的文档和 Python 类型提示训练模型形成评估感知行为，然后示例利用在原始模型上获得的导向向量消除评估线索导致的偏差。该方法有望提升安全评估的可靠性。", "keywords": "evaluation-awareness, activation steering, language model alignment, safety evaluation, deployment simulation, expert iteration, Python type hints, evaluation cue, model bias, reliability", "scoring": {"interpretability": 5, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Tim Tian Hua", "Andrew Qin", "Samuel Marks", "Neel Nanda"]}, "usage": {"completion_tokens": 819, "prompt_tokens": 3452, "total_tokens": 4271, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 699, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00070666, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029716, "upstream_inference_completions_cost": 0.0004095}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.697599", "feed": "arxiv_cscl", "title": "From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge", "link": "https://papers.cool/arxiv/2510.20043", "analysis": {"summary": "The paper introduces the Bengali Language Cultural Knowledge (BLanCK) dataset covering folk traditions, culinary arts, and regional dialects, and uses it to evaluate several multilingual large language models. Experiments reveal that while models perform well on non‑cultural categories, they struggle considerably with cultural knowledge, and providing contextual information dramatically improves results, highlighting the need for context-aware architectures and culturally curated training data.", "summary_cn": "本文提出了Bengali Language Cultural Knowledge（BLanCK）数据集，涵盖民俗、烹饪和方言等文化内容，并用于评估多语言大模型的表现。实验显示模型在非文化任务上表现良好，但在文化知识上显著不足，提供上下文信息可显著提升性能，强调了上下文感知结构和文化专属训练数据的重要性。", "keywords": "Bengali cultural knowledge, low-resource languages, LLM evaluation, multilingual benchmarks, cultural knowledge dataset, context-aware models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nafis Chowdhury", "Moinul Haque", "Anika Ahmed", "Nazia Tasnim", "Md. Istiak Hossain Shihab", "Sajjadur Rahman", "Farig Sadeque"]}, "usage": {"completion_tokens": 823, "prompt_tokens": 3317, "total_tokens": 4140, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 683, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068841, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027691, "upstream_inference_completions_cost": 0.0004115}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.882066", "feed": "arxiv_cscl", "title": "BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation", "link": "https://papers.cool/arxiv/2510.20151", "analysis": {"summary": "BoundRL proposes a reinforcement‑learning based method that generates only the starting tokens of segments in long, structured texts and reconstructs full segment contents from the original document, dramatically reducing inference cost and hallucination. The approach employs a verifiable reward (RLVR) that balances reconstruction fidelity and semantic alignment, and mitigates entropy collapse by creating intermediate candidate sequences through systematic perturbations. Experiments on complex LLM prompts show that small models (1.7B) using BoundRL outperform few‑shot prompting of much larger models, with RLVR and intermediate candidates further boosting performance and generalisation.", "summary_cn": "BoundRL 提出一种基于强化学习的文本分段方法，仅生成结构化长文本中每个段落的起始 token，并通过在原文中定位这些 token 来重构完整内容，从而大幅降低推理成本并减少幻觉。该方法使用可验证的奖励函数（RLVR）同时优化文档重建精度和语义对齐，并通过系统性扰动生成中间候选序列以缓解熵坍塌。实验在用于 LLM 应用的复杂提示上表明，使用 BoundRL 的小模型（1.7B 参数）能够超越更大模型的 few‑shot 提示，RLVR 和中间候选的加入进一步提升了性能和泛化能力。", "keywords": "structured text segmentation, reinforcement learning, boundary generation, token-level segmentation, RLVR reward, intermediate candidate perturbation, LLM prompts, small language models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyuan Li", "Zhengyuan Shen", "Sullam Jeoung", "Yueyan Chen", "Jiayu Li", "Qi Zhu", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"]}, "usage": {"completion_tokens": 657, "prompt_tokens": 3477, "total_tokens": 4134, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 321, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062941, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030091, "upstream_inference_completions_cost": 0.0003285}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:49.935436", "feed": "arxiv_cscl", "title": "Decoding-Free Sampling Strategies for LLM Marginalization", "link": "https://papers.cool/arxiv/2510.20208", "analysis": {"summary": "The paper proposes decoding‑free sampling strategies for estimating the marginal probability of a text under large language models, avoiding costly generation by relying on cheap, model‑agnostic tokenization sampling. Experiments on open models show that these methods achieve accurate marginal estimates with a fraction of the runtime, and the authors demonstrate their utility on downstream inference tasks.", "summary_cn": "本文提出了无需解码的抽样策略，用于在大语言模型下估计文本的边际概率，避免了昂贵的生成步骤，完全基于轻量、与模型和分词器无关的抽样方法。实验表明，这些方法在开放模型上能够以极低的运行时间成本获得足够精确的边际估计，并在若干下游推理任务中展示了其应用价值。", "keywords": "marginalization, tokenization, decoding-free sampling, language model evaluation, subword tokenization, probability estimation, inference efficiency, LLM evaluation, sampling strategies", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["David Pohl", "Marco Cognetta", "Junyoung Lee", "Naoaki Okazaki"]}, "usage": {"completion_tokens": 672, "prompt_tokens": 3462, "total_tokens": 4134, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 477, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063466, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029866, "upstream_inference_completions_cost": 0.000336}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.063379", "feed": "arxiv_cscl", "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction", "link": "https://papers.cool/arxiv/2510.20787", "analysis": {"summary": "The paper proposes hybrid sparse attention mechanisms combined with a learnable token eviction strategy to reduce the forgetfulness inherent in linear‑attention models. By integrating sliding‑window attention, a lightweight CNN, and query‑aware sparse attention, the approach retains a limited set of critical key‑value pairs while preserving linear time and space complexity, supported by efficient Triton kernels and empirical validation on retrieval‑intensive benchmarks.", "summary_cn": "本文提出一种混合稀疏注意力机制并引入可学习的 token 驱逐方法，以缓解线性注意力模型的遗忘问题。通过将滑动窗口注意力、轻量级 CNN 和查询感知稀疏注意力相结合，模型在保持线性时间与空间复杂度的同时，自适应保留每个头部的关键 KV 对，并提供高效的 Triton 实现，在检索密集型基准上验证了其有效性。", "keywords": "linear attention, sparse attention, token eviction, hybrid attention, retrieval, sliding-window attention, CNN aggregation, Triton kernels", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mutian He", "Philip N. Garner"]}, "usage": {"completion_tokens": 674, "prompt_tokens": 3371, "total_tokens": 4045, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 439, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3360}, "cost": 0.00060745, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027045, "upstream_inference_completions_cost": 0.000337}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.320953", "feed": "arxiv_cscl", "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging", "link": "https://papers.cool/arxiv/2510.20479", "analysis": {"summary": "The paper introduces RECALL, a representation‑aware model merging framework that leverages layer‑wise hidden representations of large language models to compute inter‑model similarity and perform adaptive hierarchical parameter fusion. By aligning knowledge across models, RECALL preserves domain‑general features in shallow layers while allowing task‑specific adaptation in deeper layers, achieving strong resistance to catastrophic forgetting without requiring historical data or task labels. Extensive experiments on five NLP tasks demonstrate superior knowledge retention and generalization compared to existing continual learning baselines.", "summary_cn": "本文提出 RECALL，一种基于表示的模型合并框架，利用大语言模型逐层隐藏表征计算模型间相似度，并进行自适应的层次参数融合。该设计在保持浅层通用特征的同时，让深层实现任务特定适配，从而在无需历史数据或任务标签的情况下显著缓解灾难性遗忘。实验在五个 NLP 任务和多种持续学习场景中显示 RECALL 在知识保留和泛化方面优于现有基线。", "keywords": "continual learning, catastrophic forgetting, model merging, representation alignment, hierarchical fusion, LLM, data-free, knowledge retention", "scoring": {"interpretability": 4, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Bowen Wang", "Haiyuan Wan", "Liwen Shi", "Chen Yang", "Peng He", "Yue Ma", "Haochen Han", "Wenhao Li", "Tiao Tan", "Yongjian Li", "Fangming Liu", "Yifan Gong", "Sheng Zhang"]}, "usage": {"completion_tokens": 688, "prompt_tokens": 3357, "total_tokens": 4045, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 437, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062691, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028291, "upstream_inference_completions_cost": 0.000344}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.333964", "feed": "arxiv_cscl", "title": "Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model", "link": "https://papers.cool/arxiv/2510.20635", "analysis": {"summary": "The paper proposes an evaluation framework for large language models (LLMs) based on the Five-Dimensional Curiosity Scale Revised (5DCR), covering information seeking, thrill seeking, and social curiosity. Experiments show that LLMs exhibit a stronger thirst for knowledge than humans but make more conservative choices under uncertainty, and that higher curiosity correlates with improved reasoning and active learning abilities. These results suggest LLMs can display human‑like curiosity, informing future development of learning‑capable AI systems.", "summary_cn": "本文基于五维好奇心量表（5DCR）构建了评估大型语言模型（LLM）好奇心的框架，涵盖信息寻求、刺激寻求和社交好奇心等维度。实验发现，LLM 相较于人类表现出更强的求知欲，但在不确定环境下倾向于保守选择；同时，好奇行为能够提升模型的推理和主动学习能力。该研究表明 LLM 有潜力展现类似人类的好奇心，为未来具备学习能力的 AI 研发提供实验依据。", "keywords": "curiosity, large language model, 5DCR, information seeking, thrill seeking, social curiosity, reasoning, active learning", "scoring": {"interpretability": 5, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Haoyu Wang", "Sihang Jiang", "Yuyan Chen", "Yitong Wang", "Yanghua Xiao"]}, "usage": {"completion_tokens": 865, "prompt_tokens": 3391, "total_tokens": 4256, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 639, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00072051, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028801, "upstream_inference_completions_cost": 0.0004325}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.591953", "feed": "arxiv_cscl", "title": "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?", "link": "https://papers.cool/arxiv/2510.20154", "analysis": {"summary": "The paper investigates how Large Language Models inherit stereotypes that affect zero-shot stance detection, focusing on attributes such as dialect and text complexity. By automatically annotating existing stance detection datasets, the authors reveal systematic biases, e.g., associating pro‑marijuana stances with low readability and African American dialect with opposition to Donald Trump. These findings highlight overlooked bias risks in politically sensitive NLP tasks.", "summary_cn": "本文研究了大型语言模型在零样本立场检测中继承的刻板印象，重点考察方言和文本复杂度等属性。通过对已有立场检测数据集进行自动标注，作者发现系统性偏见，例如将支持大麻的观点与低可读性文本关联，以及将非洲裔美国人方言与反对唐纳德·特朗普联系起来。研究揭示了政治敏感 NLP 任务中被忽视的偏见风险。", "keywords": "large language models, bias, stereotypes, zero-shot stance detection, dialect, text complexity, political bias, fairness, NLP, bias evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Anthony Dubreuil", "Antoine Gourru", "Christine Largeron", "Amine Trabelsi"]}, "usage": {"completion_tokens": 708, "prompt_tokens": 3387, "total_tokens": 4095, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 475, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064141, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028741, "upstream_inference_completions_cost": 0.000354}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.670051", "feed": "arxiv_cscl", "title": "Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing", "link": "https://papers.cool/arxiv/2510.20727", "analysis": {"summary": "The study develops and evaluates various NLP methods, including rule‑based, machine‑learning, deep‑learning, and large language model prompting, to automatically extract fluoropyrimidine treatment regimens and associated toxicities from clinical notes, achieving perfect F1 scores with error‑analysis prompting. Results show that LLM‑based approaches outperform other methods, while deep‑learning models struggle due to limited training data.", "summary_cn": "该研究开发并评估了规则、机器学习、深度学习以及大语言模型提示等多种自然语言处理方法，以自动从临床记录中提取氟嘧啶类药物治疗方案及相关毒性，错误分析提示实现了治疗和毒性提取的 F1=1.000。结果表明，大语言模型方法优于其他方法，深度学习模型因训练数据不足表现有限。", "keywords": "fluoropyrimidine, toxicity extraction, clinical notes, NLP, large language model, pharmacovigilance, rule-based, machine learning, BERT, zero-shot prompting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xizhi Wu", "Madeline S. Kreider", "Philip E. Empey", "Chenyu Li", "Yanshan Wang"]}, "usage": {"completion_tokens": 1014, "prompt_tokens": 3601, "total_tokens": 4615, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 846, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00082651, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031951, "upstream_inference_completions_cost": 0.000507}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.725697", "feed": "arxiv_cscl", "title": "VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation", "link": "https://papers.cool/arxiv/2510.20381", "analysis": {"summary": "The paper introduces the VLSP 2025 MLQA-TSR shared task, which provides a benchmark dataset for Vietnamese multimodal legal question answering focused on traffic sign regulations. The task comprises two subtasks—multimodal legal retrieval and multimodal question answering—with the best reported results being an F2 score of 64.55% for retrieval and an accuracy of 86.30% for QA.", "summary_cn": "本文提出了 VLSP 2025 MLQA-TSR 共享任务，提供了一个针对越南交通标志法规的多模态法律问答基准数据集。该任务包括多模态法律检索和多模态问答两个子任务，最佳成绩分别为检索的 F2 分数 64.55% 和问答的准确率 86.30%。", "keywords": "multimodal question answering, legal AI, Vietnamese, traffic sign regulation, benchmark dataset, retrieval, QA, VLSP 2025", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Son T. Luu", "Trung Vo", "Hiep Nguyen", "Khanh Quoc Tran", "Kiet Van Nguyen", "Vu Tran", "Ngan Luu-Thuy Nguyen", "Le-Minh Nguyen"]}, "usage": {"completion_tokens": 715, "prompt_tokens": 3347, "total_tokens": 4062, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 527, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063891, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028141, "upstream_inference_completions_cost": 0.0003575}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.754839", "feed": "arxiv_cscl", "title": "Neural Diversity Regularizes Hallucinations in Small Models", "link": "https://papers.cool/arxiv/2510.20690", "analysis": {"summary": "The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to lower hallucination rates in language models without increasing parameters or data. By combining parallel LoRA adapters with Barlow Twins regularization (ND‑LoRA), the authors demonstrate up to a 25.6% reduction in hallucinations while preserving accuracy, and provide theoretical bounds linking representational correlation to hallucination probability. Analyses reveal task-dependent optimal levels of diversity, positioning neural diversity as a third scaling axis orthogonal to size and data.", "summary_cn": "本文提出神经多样性（decorrelated parallel representations）作为降低语言模型幻觉率的机制，且无需增加参数或数据。通过将并行 LoRA 适配器与 Barlow Twins 正则化相结合（ND‑LoRA），实现了最高 25.6% 的幻觉减少且不损失准确性，并给出表征相关性与幻觉概率之间的理论上界。研究表明不同任务需要不同的最优多样性水平，将神经多样性视为与参数和数据正交的第三个尺度轴。", "keywords": "hallucination, neural diversity, LoRA, Barlow Twins, regularization, language models, reliability", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Kushal Chakrabarti", "Nirmal Balachundhar"]}, "usage": {"completion_tokens": 739, "prompt_tokens": 3474, "total_tokens": 4213, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 492, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066996, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030046, "upstream_inference_completions_cost": 0.0003695}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.890046", "feed": "arxiv_cscl", "title": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking", "link": "https://papers.cool/arxiv/2510.20168", "analysis": {"summary": "The paper introduces DeepWideSearch, a new benchmark that tests agents’ ability to combine deep multi‑hop reasoning with wide‑scale information collection. comprises 220 questions across 15 domains and shows that current state‑of‑the‑art agents succeed on only about 2.4% of them, revealing four primary failure modes (lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow). The authors release the dataset to spur research on more capable and robust information‑seeking agents.", "summary_cn": "本文推出了 DeepWideSearch 基准，用于评估智能体在进行深度多跳推理的同时进行大规模信息收集的能力。数据集包含 15 个领域的 220 条问题，实验表明当前最先进的智能体仅在约 2.4% 的问题上成功，揭示了四类主要失败模式：缺乏自我反思、过度依赖内部知识、检索不足以及上下文溢出。作者公开发布该数据集，以促进对更强大、更稳健的信息检索智能体的研究。", "keywords": "information seeking, depth-width benchmark, retrieval agents, multi-hop reasoning, agentic AI, evaluation dataset, market analysis, robustness", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Tian Lan", "Bin Zhu", "Qianghuai Jia", "Junyang Ren", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"]}, "usage": {"completion_tokens": 981, "prompt_tokens": 3414, "total_tokens": 4395, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 762, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078196, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029146, "upstream_inference_completions_cost": 0.0004905}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:50.954013", "feed": "arxiv_cscl", "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost", "link": "https://papers.cool/arxiv/2510.20780", "analysis": {"summary": "The paper systematically investigates the use of large reasoning models (LRMs) as judges for machine translation quality, identifying challenges such as overthinking and scoring bias. It proposes calibrating LRMs by training them on synthetic, human‑like thinking trajectories, which reduces the reasoning budget by about 35× and boosts correlation scores across model sizes. Experiments on WMT24 benchmarks demonstrate significant performance gains, highlighting the promise of efficiently calibrated LRMs for fine‑grained MT evaluation.", "summary_cn": "本文系统研究了大型推理模型（LRM）作为机器翻译质量评估者的潜力，指出其倾向于“过度思考”以及评分机制导致的高估等问题。作者通过在合成的类人思考轨迹上进行训练，对LRM进行校准，从而将思考预算降低约35倍并提升在不同规模模型（7B至32B）上的相关性得分。WMT24评测实验显示该方法显著提升评估性能，凸显高效校准的LRM在细粒度机器翻译评估中的潜力。", "keywords": "large reasoning models, machine translation evaluation, model calibration, synthetic thinking trajectories, correlation improvement, evaluation metrics, LRM-as-judge", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Runzhe Zhan", "Zhihong Huang", "Xinyi Yang", "Lidia S. Chao", "Min Yang", "Derek F. Wong"]}, "usage": {"completion_tokens": 726, "prompt_tokens": 3399, "total_tokens": 4125, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 473, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00065221, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.000363}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:51.979815", "feed": "arxiv_cscl", "title": "LM-mixup: Text Data Augmentation via Language Model based Mixup", "link": "https://papers.cool/arxiv/2510.20449", "analysis": {"summary": "The paper defines instruction distillation and introduces LM-Mixup, a method that fine‑tunes LLMs on a small distilled dataset (MIXTURE) and further optimizes them with reinforcement learning using quality, semantic alignment, and format rewards. LM‑Mixup augments low‑quality instruction data so that training on only ~3 % of the original data outperforms full‑dataset training and rivals state‑of‑the‑art data selection methods. This demonstrates that low‑quality data can be transformed into a valuable resource for instruction‑tuned LLMs.", "summary_cn": "本文定义了指令蒸馏任务并提出 LM‑Mixup 方法：先在包含低质量指令集的 MIXTURE 数据上进行监督微调，再通过包含质量、语义对齐和格式符合度三种奖励的强化学习（GRPO）进行优化。实验表明，仅使用约 3% 的蒸馏数据进行微调即可超越整体数据训练并与最先进的数据筛选方法竞争，证明低质量数据在适当蒸馏后可成为提升指令调优模型的宝贵资源。", "keywords": "instruction tuning, data augmentation, LM-Mixup, instruction distillation, reinforcement learning, GRPO, low-quality data, MIXTURE dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zhijie Deng", "Zhouan Shen", "Ling Li", "Yao Zhou", "Zhaowei Zhu", "Yanji He", "Wei Wang", "Jiaheng Wei"]}, "usage": {"completion_tokens": 965, "prompt_tokens": 3488, "total_tokens": 4453, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 743, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078506, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030256, "upstream_inference_completions_cost": 0.0004825}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:52.661883", "feed": "arxiv_cscl", "title": "Context-level Language Modeling by Learning Predictive Context Embeddings", "link": "https://papers.cool/arxiv/2510.20280", "analysis": {"summary": "The paper proposes ContextLM, a framework that adds a next‑context prediction objective to standard next‑token language model pretraining, enabling models to learn predictive embeddings of multi‑token contexts while remaining compatible with autoregressive evaluation. Experiments with GPT‑2 and Pythia models up to 1.5 B parameters show consistent reductions in perplexity and gains on downstream tasks, attributed to better long‑range coherence and more efficient attention allocation. The authors analyze the impact of the objective and demonstrate its scalability with minimal computational overhead.", "summary_cn": "本文提出 ContextLM 框架，在标准的下一个词预测预训练中加入了“下一上下文预测”目标，使模型能够学习多词上下文的预测嵌入，同时保持对自回归评估（如困惑度）的兼容性。对 GPT‑2 和 Pythia 系列（最高 1.5 B 参数）进行的大规模实验表明，该目标能够持续降低困惑度并提升下游任务表现，原因在于更好的长程连贯性和更高效的注意力分配。作者进一步分析了该目标的作用机制，并展示了其在计算开销极小的情况下可扩展的优势。", "keywords": "next-context prediction, predictive context embeddings, language modeling, GPT-2, Pythia, long-range coherence, attention efficiency, pretraining objective", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Beiya Dai", "Yuliang Liu", "Daozheng Xue", "Qipeng Guo", "Kai Chen", "Xinbing Wang"]}, "usage": {"completion_tokens": 840, "prompt_tokens": 3407, "total_tokens": 4247, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 608, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00071041, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029041, "upstream_inference_completions_cost": 0.00042}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:52.748712", "feed": "arxiv_cscl", "title": "Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders", "link": "https://papers.cool/arxiv/2510.20239", "analysis": {"summary": "The paper presents a unified tri‑modal framework that fuses interview text, audio, and facial signals to predict graded severity levels for depression (PHQ‑8) and PTSD. Standardized sentence‑level transformer embeddings, log‑Mel audio features, and facial action‑unit descriptors are combined via a calibrated late‑fusion classifier, achieving superior weighted F1 and robustness compared to unimodal baselines. The approach also provides feature‑level attributions to support clinician decision‑making.", "summary_cn": "本文提出一种统一的三模态框架，融合访谈文本、音频和面部信号，以预测抑郁（PHQ‑8）和 PTSD 的分级严重程度。通过句子级 Transformer 嵌入、对数梅尔音频特征和面部动作单元描述，采用校准的后期融合分类器，实现了相较于单模态基线的更高加权 F1 与鲁棒性。该方法还能提供特征层面的归因，辅助临床决策。", "keywords": "multimodal fusion, depression diagnosis, PTSD severity, affective analysis, transformer embeddings, log-Mel audio features, facial action units, late fusion classifier, clinical decision support", "scoring": {"interpretability": 4, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Filippo Cenacchi", "Deborah Richards", "Longbing Cao"]}, "usage": {"completion_tokens": 1125, "prompt_tokens": 3474, "total_tokens": 4599, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 940, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00086296, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030046, "upstream_inference_completions_cost": 0.0005625}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:52.866885", "feed": "arxiv_cscl", "title": "Teaching Language Models to Reason with Tools", "link": "https://papers.cool/arxiv/2510.20342", "analysis": {"summary": "The paper introduces CoRT (Code-Optimized Reasoning Training), a post‑training framework that teaches large reasoning models to effectively integrate code interpreters via Hint‑Engineering data synthesis, rejection sampling and reinforcement learning. Experiments on 1.5B‑32B models show notable accuracy gains (4%‑8%) on challenging math reasoning benchmarks and substantial token‑efficiency improvements compared to pure language‑only reasoning.", "summary_cn": "本文提出 CoRT（代码优化推理训练）框架，通过 Hint‑Engineering 数据合成、拒绝采样与强化学习，使大规模推理模型能够高效地与代码解释器交互。实验在 1.5B‑32B 参数模型上展示了在复杂数学推理基准上准确率提升 4%‑8% 并显著降低 token 使用量，相较于纯语言推理有约 30%‑50% 的效率提升。", "keywords": "large reasoning models, code interpreter, tool use, CoRT, hint engineering, supervised fine-tuning, reinforcement learning, mathematical reasoning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"]}, "usage": {"completion_tokens": 856, "prompt_tokens": 3547, "total_tokens": 4403, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 653, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073941, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031141, "upstream_inference_completions_cost": 0.000428}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:53.216795", "feed": "arxiv_cscl", "title": "Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models", "link": "https://papers.cool/arxiv/2510.20460", "analysis": {"summary": "The paper systematically evaluates four confidence estimation methods—VCE, MSP, Sample Consistency, and CoCoA—for large language model outputs across four QA tasks, finding that each metric captures different aspects of uncertainty and that the hybrid CoCoA approach achieves the best overall calibration and discrimination. Recommendations for selecting uncertainty measures in LLM applications are provided.", "summary_cn": "本文系统评估了四种大型语言模型输出置信度估计方法（VCE、MSP、样本一致性和 CoCoA），在四个问答任务上比较它们捕获模型不确定性的不同方面，结果表明混合的 CoCoA 方法在校准和正确答案辨别方面表现最佳。文章还讨论了各方法的权衡并给出在实际 LLM 应用中选择不确定性度量的建议。", "keywords": "uncertainty estimation, confidence calibration, large language models, VCE, MSP, sample consistency, CoCoA, question answering, reliability, evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Christian Hobelsberger", "Theresa Winner", "Andreas Nawroth", "Oliver Mitevski", "Anna-Carolina Haensch"]}, "usage": {"completion_tokens": 1045, "prompt_tokens": 3349, "total_tokens": 4394, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 960, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00080421, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028171, "upstream_inference_completions_cost": 0.0005225}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:53.631018", "feed": "arxiv_cscl", "title": "A Fundamental Algorithm for Dependency Parsing (With Corrections)", "link": "https://papers.cool/arxiv/2510.19996", "analysis": {"summary": "The paper introduces a fundamental algorithm for dependency parsing that processes sentences word by word, attaching each word as soon as possible, mirroring hypothesized properties of human brain parsing. The algorithm achieves a worst‑case time complexity of O(n^3), but the worst case is rare in typical human language sentences. Experimental or theoretical analysis suggests the method is efficient for realistic language inputs.", "summary_cn": "本文提出了一种基础的依存句法分析算法，该算法逐词处理句子，在可能时立即附加每个词，模拟人脑句法解析的假设特性。算法的最坏情况时间复杂度为 O(n^3)，但在实际的人类语言中，这种最坏情况极少出现，因而在常规语言输入下表现高效。", "keywords": "dependency parsing, transition-based parsing, O(n^3) algorithm, natural language processing, parsing efficiency", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Michael A. Covington"]}, "usage": {"completion_tokens": 557, "prompt_tokens": 3278, "total_tokens": 3835, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 329, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00054956, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00027106, "upstream_inference_completions_cost": 0.0002785}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:53.690355", "feed": "arxiv_cscl", "title": "Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering", "link": "https://papers.cool/arxiv/2510.20304", "analysis": {"summary": "This paper conducts the first systematic evaluation of generative process reward models (PRMs) on table question answering (TQA), examining how step‑by‑step grading and combined text‑code verification affect answer selection. Experiments reveal that PRMs can improve solution selection but struggle with out‑of‑domain tables and show weak correlation between step verification performance and final accuracy due to loose causal links. The analysis highlights current limitations of PRMs for semi‑structured data and suggests directions for more robust, process‑aware verifiers.", "summary_cn": "本文首次系统性地评估生成式过程奖励模型（PRM）在表格问答（TQA）任务中的表现，研究逐步打分及文本‑代码联合验证对答案选择的影响。实验表明，PRM 能在一定程度上提升解答选择，但在跨域表格上表现不佳，且步骤验证性能与最终准确率相关性弱，可能源于步骤之间因果联系松散。分析指出当前 PRM 在半结构化数据上的局限，并提供构建更稳健、过程感知验证器的思路。", "keywords": "process reward modeling, table question answering, semi-structured data, step verification, generative PRM, LLM reasoning, code verification, out-of-domain generalization, AI safety, robustness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Lei Tang", "Wei Zhou", "Mohsen Mesgar"]}, "usage": {"completion_tokens": 930, "prompt_tokens": 3394, "total_tokens": 4324, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 693, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00075346, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028846, "upstream_inference_completions_cost": 0.000465}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:53.935397", "feed": "arxiv_cscl", "title": "User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios", "link": "https://papers.cool/arxiv/2510.20721", "analysis": {"summary": "The paper presents a user study with 94 participants who evaluated LLM responses to 90 privacy‑sensitive scenarios from the PrivacyLens benchmark, revealing low agreement among users on perceived privacy preservation and helpfulness. Proxy LLMs showed high internal agreement but correlated poorly with human judgments, indicating that current automated privacy assessments do not reflect real user perceptions. The authors argue for more user‑centered evaluation methods and improved alignment between proxy models and actual user expectations.", "summary_cn": "本文通过对94名参与者在PrivacyLens基准的90个隐私敏感情境下评估大型语言模型（LLM）回应的实验，发现用户在隐私保护质量和帮助性上的一致性较低。虽然代理LLM之间的评估高度一致，却与用户的感知关联度低，表明现有的自动隐私评估并未真实反映用户感受。作者呼吁开展以用户为中心的评估，并提升代理模型与真实用户感知之间的对齐。", "keywords": "user perception, privacy, helpfulness, large language models, alignment, privacy-preserving AI, evaluation, user study, proxy LLMs", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xiaoyuan Wu", "Roshni Kaushik", "Wenkai Li", "Lujo Bauer", "Koichi Onoue"]}, "usage": {"completion_tokens": 949, "prompt_tokens": 3554, "total_tokens": 4503, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 782, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078696, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031246, "upstream_inference_completions_cost": 0.0004745}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:54.209223", "feed": "arxiv_cscl", "title": "Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training", "link": "https://papers.cool/arxiv/2510.20002", "analysis": {"summary": "The paper introduces a family of Greek Embedding Models (GEMs) that are pre‑trained on large, quality‑filtered corpora covering both general‑domain and legal texts, and evaluates several modern transformer architectures such as ELECTRA, ConvBERT and ModernBERT for Greek. It also presents the first bilingual Greek‑English embedding models for the legal domain and shows that GEM‑RoBERTa and GEM‑ConvBERT outperform existing baselines on downstream tasks.", "summary_cn": "本文提出了一套希腊语嵌入模型（GEM），通过对通用和法律领域的大规模高质量语料进行严格过滤和预处理后进行预训练，并评估了包括 ELECTRA、ConvBERT 与 ModernBERT 在内的多种现代 Transformer 架构。文中还首次推出针对法律领域的双语希腊‑英语嵌入模型，并展示 GEM‑RoBERTa 与 GEM‑ConvBERT 在下游任务上显著优于现有基线。", "keywords": "Greek NLP, corpus curation, transformer pretraining, ELECTRA, ConvBERT, ModernBERT, legal domain, bilingual Greek-English models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandra Apostolopoulou", "Konstantinos Kanaris", "Athanasios Koursaris", "Dimitris Tsakalidis", "George Domalis", "Ioannis E. Livieris"]}, "usage": {"completion_tokens": 628, "prompt_tokens": 3450, "total_tokens": 4078, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 342, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061086, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029686, "upstream_inference_completions_cost": 0.000314}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:54.490842", "feed": "arxiv_cscl", "title": "Real Deep Research for AI, Robotics and Beyond", "link": "https://papers.cool/arxiv/2510.20809", "analysis": {"summary": "The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area by identifying emerging trends, uncovering cross‑domain opportunities, and suggesting concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across multiple topics.", "summary_cn": "本文提出 Real Deep Research（RDR）框架，一种可通用的管线用于系统性分析任意研究领域，识别新兴趋势、发掘跨领域机会并提供具体的研究起点。作者在 AI 与机器人领域（尤其是基础模型和机器人技术）进行案例展示，并在附录中提供了大量跨主题分析结果。", "keywords": "meta-research, trend analysis, interdisciplinary, AI foundations, robotics, research pipelines, emerging topics", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}, "usage": {"completion_tokens": 466, "prompt_tokens": 3368, "total_tokens": 3834, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 241, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00051756, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028456, "upstream_inference_completions_cost": 0.000233}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:54.897511", "feed": "arxiv_cscl", "title": "LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation", "link": "https://papers.cool/arxiv/2510.19967", "analysis": {"summary": "LyriCAR is a fully unsupervised framework that uses difficulty-aware curriculum reinforcement learning to enable controllable lyric translation, addressing cross-line coherence and global rhyme. The adaptive curriculum strategy allocates training resources efficiently, speeding up convergence and achieving state-of-the-art performance on English‑Chinese lyric translation across standard and multi-dimensional reward metrics. Experiments show that LyriCAR reduces training steps by about 40% while maintaining superior translation quality.", "summary_cn": "LyriCAR 是一个完全无监督的框架，利用难度感知的 curriculum 强化学习实现可控的歌词翻译，解决跨行连贯性和全局押韵等问题。自适应课程策略有效分配训练资源，加速收敛，并在英‑中歌词翻译任务上在标准翻译指标和多维奖励分数上达到最新水平。实验表明，LyriCAR 将训练步数减少约 40%，同时保持更优的翻译质量。", "keywords": "lyric translation, curriculum reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, multi-dimensional rewards", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"]}, "usage": {"completion_tokens": 564, "prompt_tokens": 3399, "total_tokens": 3963, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 306, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00057121, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.000282}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:54.926118", "feed": "arxiv_cscl", "title": "LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation", "link": "https://papers.cool/arxiv/2510.19988", "analysis": {"summary": "The paper proposes a hybrid architecture that combines large language models (LLMs) for text simplification and gap‑filling with a symbolic natural‑language‑understanding (NLU) pipeline that generates structured relational representations of quantities and causal laws. Experiments on commonsense science texts show that the hybrid system substantially outperforms a symbolic‑only baseline in extracting and interpreting continuous causal statements. The approach aims to retain the interpretability and reasoning benefits of symbolic NLU while leveraging the broad coverage of LLMs.", "summary_cn": "本文提出一种混合架构，将大型语言模型（LLM）用于文本重述和填补知识空白，并结合符号化自然语言理解（NLU）管线生成量化和因果律的结构化关系表示。 在常识科学文本上的实验表明，该混合系统在提取和解释连续因果陈述方面显著优于仅符号化的基线。 该方法旨在保持符号化 NLU 的可解释性和推理优势，同时利用 LLM 的广泛覆盖能力。", "keywords": "LLM, symbolic NLU, hybrid system, causal statement interpretation, relational representation, commonsense science, knowledge extraction", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Xin Lian", "Kenneth D. Forbus"]}, "usage": {"completion_tokens": 651, "prompt_tokens": 3456, "total_tokens": 4107, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 406, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062326, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029776, "upstream_inference_completions_cost": 0.0003255}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:55.246044", "feed": "arxiv_cscl", "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs", "link": "https://papers.cool/arxiv/2510.20001", "analysis": {"summary": "The paper proposes a unifying paradigm that characterizes clinical decision‑making tasks along two axes—clinical backgrounds and clinical questions—to better reflect real‑world complexity beyond simplified QA datasets like MedQA. It surveys existing medical benchmarks, reviews training‑time and test‑time methods for LLMs, expands evaluation metrics to include efficiency and explainability, and outlines open challenges for building clinically useful LLM systems.", "summary_cn": "本文提出一种统一范式，通过“临床背景”和“临床问题”两维度对临床决策任务进行表征，以弥补 MedQA 等简化问答数据在真实临床环境中的不足。作者梳理了现有医学数据集与基准，回顾了训练时与测试时的 LLM 方法，并将评估扩展至效率和可解释性等指标，最后指出了实现临床可用 LLM 的挑战。", "keywords": "large language models, clinical decision making, medical evaluation, explainability, efficiency, dataset paradigm, LLM safety", "scoring": {"interpretability": 4, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yunpeng Xiao", "Carl Yang", "Mark Mai", "Xiao Hu", "Kai Shu"]}, "usage": {"completion_tokens": 657, "prompt_tokens": 3372, "total_tokens": 4029, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 458, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061366, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028516, "upstream_inference_completions_cost": 0.0003285}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:55.471214", "feed": "arxiv_cscl", "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities", "link": "https://papers.cool/arxiv/2510.19892", "analysis": {"summary": "The paper proposes using the fantasy card game Dixit as a game‑based evaluation framework for multimodal language models (MLMs), arguing that games require multiple abilities and provide objective, competitive assessment. Quantitative experiments with five MLMs show that Dixit win‑rate rankings correlate with standard benchmark rankings, and human‑MLM game play reveals strategic differences and areas for improvement in MLM reasoning.", "summary_cn": "本文提出将幻想卡牌游戏 Dixit 用作多模态语言模型（MLM）的游戏化评估框架，认为游戏需要多种能力且具备客观竞争的评估规则。对五种 MLM 的量化实验表明，Dixit 的胜率排序与常用基准高度相关，同时人机对战揭示了模型策略的差异并指出了 MLM 推理的改进方向。", "keywords": "multimodal language models, game-based evaluation, Dixit, benchmark, quantitative experiments, human-MLM interaction, reasoning assessment, competitive games", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"]}, "usage": {"completion_tokens": 617, "prompt_tokens": 3402, "total_tokens": 4019, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 363, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00059816, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028966, "upstream_inference_completions_cost": 0.0003085}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:55.500485", "feed": "arxiv_cscl", "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention", "link": "https://papers.cool/arxiv/2510.19875", "analysis": {"summary": "The paper introduces Sparse Tracing and the Stream algorithm, which use dynamic sparse attention to enable near‑linear‑time, linear‑space analysis of attention patterns in million‑token contexts. By hierarchically pruning attention masks and retaining only the top‑k key blocks per query, the method preserves next‑token behavior while discarding the vast majority of token interactions, allowing interpretable tracing of long‑chain‑of‑thought reasoning on consumer GPUs.", "summary_cn": "本文提出了稀疏追踪（Sparse Tracing）和 Stream 算法，利用动态稀疏注意力在近线性时间和线性空间内分析百万级上下文的注意力模式。通过层次化剪枝，仅保留每个查询的前 k 个关键块，保持模型的下一个 token 行为，同时剔除绝大多数交互，使得在普通 GPU 上也能对长链式思考进行可解释的追踪。", "keywords": "mechanistic interpretability, sparse attention, long-context LLM, hierarchical pruning, chain-of-thought monitoring, attention tracing", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["J Rosser", "José Luis Redondo García", "Gustavo Penha", "Konstantina Palla", "Hugues Bouchard"]}, "usage": {"completion_tokens": 674, "prompt_tokens": 3457, "total_tokens": 4131, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 452, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063491, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029791, "upstream_inference_completions_cost": 0.000337}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:55.757244", "feed": "arxiv_cscl", "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics", "link": "https://papers.cool/arxiv/2510.19866", "analysis": {"summary": "The paper evaluates the pedagogical soundness and usability of AI‑generated high‑school physics lesson plans produced by five leading large language models and three structured prompt frameworks. Using automated metrics for readability, factual accuracy, curriculum alignment, and cognitive demand, the study finds that model choice primarily affects linguistic accessibility while prompt structure most influences factual reliability and curricular alignment. The best configuration combines a readability‑optimized model with the RACE prompt framework and explicit instructional checklists.", "summary_cn": "本文评估了五种主流大型语言模型和三种结构化提示框架生成的高中物理教学计划在教学合理性和可用性方面的表现。通过可读性、事实准确性、课程对齐度和学习目标认知需求四项自动化指标分析，结果显示模型选择对语言可读性影响最大，而提示框架结构对事实可靠性和课程对齐度影响更显著。最佳配置为使用可读性优化的模型搭配 RACE 提示框架并加入明确的物理概念及课程标准检查清单。", "keywords": "AI-generated lesson plans, large language models, prompt engineering, pedagogical evaluation, readability, factual accuracy, curriculum alignment, Bloom's taxonomy", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xincheng Liu"]}, "usage": {"completion_tokens": 642, "prompt_tokens": 3565, "total_tokens": 4207, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 370, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063511, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031411, "upstream_inference_completions_cost": 0.000321}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:55.898771", "feed": "arxiv_cscl", "title": "Large Language Model enabled Mathematical Modeling", "link": "https://papers.cool/arxiv/2510.19895", "analysis": {"summary": "The paper evaluates the DeepSeek-R1 large language model for automatically generating mathematical optimization formulations across several operations‑research benchmarks, proposing a hallucination taxonomy and mitigation techniques such as LLM‑as‑a‑Judge, few‑shot prompting, tool calling, and a multi‑agent framework. Experiments on NL4OPT, IndustryOR, EasyLP and ComplexOR show that these strategies can reduce hallucinations and improve formulation accuracy, offering a cost‑effective alternative to more expensive models like GPT‑4.", "summary_cn": "本文评估了 DeepSeek‑R1 大语言模型在自动生成运筹学优化模型方面的表现，针对四个基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）提出了幻觉分类法并采用 LLM‑as‑a‑Judge、少样本提示、工具调用和多代理框架等减轻幻觉的策略。实验表明这些方法可以降低幻觉率并提升模型对实际需求的对齐度，提供了相较 GPT‑4 更具成本效益的方案。", "keywords": "large language models, mathematical optimization, operations research, hallucination mitigation, DeepSeek-R1, NL4OPT, tool calling, few-shot learning, multi-agent framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Guoyun Zhang"]}, "usage": {"completion_tokens": 677, "prompt_tokens": 3480, "total_tokens": 4157, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 391, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00063986, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030136, "upstream_inference_completions_cost": 0.0003385}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.141351", "feed": "arxiv_cscl", "title": "IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation", "link": "https://papers.cool/arxiv/2510.20377", "analysis": {"summary": "The paper introduces IKnow, a framework for instruction-knowledge-aware continual pretraining that formulates self-supervised objectives in an instruction-response dialogue format, enabling domain adaptation of instruction-tuned LLMs without external resources or access to the base model. By leveraging domain knowledge embedded in the text, IKnow aims to preserve instruction-following capabilities and semantic representations during adaptation.", "summary_cn": "本文提出 IKnow 框架，通过在指令-响应对话格式中设计自监督目标，实现对已指令微调的大语言模型进行持续预训练的领域适应，无需外部资源或基模型权重。该方法利用文本内部的领域知识，以更深层次的语义编码来防止指令遵循能力和语义表征的退化。", "keywords": "continual pretraining, domain adaptation, instruction-following, self-supervised dialogue, knowledge-aware, LLM alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Tianyi Zhang", "Florian Mai", "Lucie Flek"]}, "usage": {"completion_tokens": 597, "prompt_tokens": 3364, "total_tokens": 3961, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 391, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058246, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028396, "upstream_inference_completions_cost": 0.0002985}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.253425", "feed": "arxiv_cscl", "title": "Relative-Based Scaling Law for Neural Language Models", "link": "https://papers.cool/arxiv/2510.20387", "analysis": {"summary": "The paper introduces the Relative-Based Probability (RBP) metric, which measures how often the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Extensive experiments across multiple datasets and model families show that this scaling law accurately predicts performance and offers new insights into emergence phenomena and theoretical scaling principles. By complementing traditional cross‑entropy scaling analyses, the work provides a more complete picture of large language model behavior.", "summary_cn": "本文提出相对概率（RBP）度量，用于衡量正确 token 在预测排名中的位置，并基于此建立了相对基准的尺度律，描述 RBP 随模型规模的提升而改进的规律。通过在四个数据集和四类模型上进行大规模实验，验证了该尺度律的稳健性和预测准确性，并展示了其在解释模型涌现现象及寻找尺度律基本理论方面的应用。该工作补充了传统交叉熵尺度分析，为大语言模型行为提供了更完整的理解。", "keywords": "scaling laws, relative-based probability, language models, emergence, model performance prediction", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baoqing Yue", "Jinyuan Zhou", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"]}, "usage": {"completion_tokens": 622, "prompt_tokens": 3432, "total_tokens": 4054, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 354, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060516, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029416, "upstream_inference_completions_cost": 0.000311}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.520730", "feed": "arxiv_cscl", "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation", "link": "https://papers.cool/arxiv/2510.20812", "analysis": {"summary": "The paper introduces Speculative Verdict (SV), a training‑free framework that leverages multiple lightweight draft VLMs to generate diverse reasoning paths and a strong verdict VLM to synthesize the final answer for information‑intensive visual question answering. A consensus expert selection mechanism forwards only high‑agreement drafts to the verdict, improving both accuracy and computational efficiency on benchmarks such as InfographicVQA and ChartQAPro.", "summary_cn": "本文提出了“Speculative Verdict”(SV) 框架，该框架在无训练的情况下利用多个轻量级草稿 VLM 生成多样化推理路径，并由强大的 verdict VLM 进行答案合成，以应对信息密集型视觉问答任务。通过共识专家选择机制，只将高一致性的草稿路径转交给 verdict，从而在 InfographicVQA、ChartQAPro 等基准上实现更高的准确率和计算效率。", "keywords": "visual language model, speculative decoding, information-intensive visual reasoning, draft expert, consensus selection, multimodal QA, training-free inference", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"]}, "usage": {"completion_tokens": 728, "prompt_tokens": 3448, "total_tokens": 4176, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 543, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00066056, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029656, "upstream_inference_completions_cost": 0.000364}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.609932", "feed": "arxiv_cscl", "title": "DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse", "link": "https://papers.cool/arxiv/2510.19858", "analysis": {"summary": "The paper introduces DeBERTa-KC, a transformer‑based classifier that automatically identifies four levels of knowledge construction (nonKC, Share, Explore, Negotiate) in comments from popular YouTube science channels. By extending DeBERTa‑v3 with focal loss, label smoothing, and R‑Drop, the model achieves a macro‑F1 of 0.836 on a manually annotated corpus of 20,000 samples, outperforming baseline methods. The work demonstrates that large language models can capture nuanced epistemic engagement in informal digital learning environments, providing a scalable tool for discourse analysis.", "summary_cn": "本文提出 DeBERTa‑KC，一种基于 Transformer 的分类模型，用于自动识别 YouTube 科普频道评论中的四种知识建构层级（非建构、共享、探索、协商）。通过在 DeBERTa‑v3 上加入焦点损失、标签平滑和 R‑Drop 正则化，模型在 20,000 条人工标注样本上实现了 0.836 的宏观 F1 分数，显著优于基线方法。该研究表明，大语言模型能够捕捉非正式数字学习环境中的细微认知参与，为话语分析提供了可扩展的自动化工具。", "keywords": "DeBERTa, knowledge construction, epistemic engagement, transformer classifier, online learning discourse, focal loss, label smoothing, R-Drop", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jindi Wang", "Yidi Zhang", "Zhaoxing Li"]}, "usage": {"completion_tokens": 772, "prompt_tokens": 3467, "total_tokens": 4239, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 458, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068541, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029941, "upstream_inference_completions_cost": 0.000386}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.628845", "feed": "arxiv_cscl", "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context", "link": "https://papers.cool/arxiv/2510.20229", "analysis": {"summary": "The paper investigates why large vision‑language models generate more hallucinations in longer, free‑form responses, attributing the issue to increased reliance on contextual cues rather than length alone. It introduces an \"induce‑detect‑suppress\" framework that deliberately creates hallucinations via crafted contexts, uses these instances to train early detection mechanisms, and suppresses object‑level hallucinations during decoding, achieving consistent improvements across benchmarks.", "summary_cn": "本文研究了大型视觉语言模型在较长自由回复中更易出现幻觉的原因，认为核心在于为保持连贯性和完整性而对上下文的依赖增加，而非单纯的长度导致。作者提出了“诱导‑检测‑抑制”框架，通过特意设计的上下文诱导幻觉，利用诱导实例训练高风险检测模型，并在实际解码时抑制对象级幻觉，在多个基准上实现了显著提升。", "keywords": "LVLM, hallucination, context, induce-detect-suppress, detection, mitigation, vision-language, reliability, robustness", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"]}, "usage": {"completion_tokens": 635, "prompt_tokens": 3433, "total_tokens": 4068, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 389, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061181, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029431, "upstream_inference_completions_cost": 0.0003175}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:56.786456", "feed": "arxiv_cscl", "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "link": "https://papers.cool/arxiv/2510.20095", "analysis": {"summary": "The paper introduces BIOCAP, a biological foundation model that leverages synthetic, instance-specific captions generated by multimodal large language models to complement image labels. By guiding caption generation with Wikipedia-derived visual information and taxon-tailored formats, the authors reduce hallucination and improve caption fidelity, leading to stronger species classification and text-image retrieval performance.", "summary_cn": "本文提出 BIOCAP，一种利用多模态大语言模型生成的合成实例特定字幕来补充图像标签的生物学基础模型。通过使用维基百科提取的视觉信息和针对分类的格式示例来引导字幕生成，从而降低幻觉并提升字幕真实性，显著提升了物种分类和文本‑图像检索的效果。", "keywords": "synthetic captions, multimodal foundation models, biological vision, BIOCAP, MLLM, species classification, text-image retrieval", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"]}, "usage": {"completion_tokens": 584, "prompt_tokens": 3405, "total_tokens": 3989, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 369, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00058211, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029011, "upstream_inference_completions_cost": 0.000292}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.055925", "feed": "arxiv_cscl", "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values", "link": "https://papers.cool/arxiv/2510.20187", "analysis": {"summary": "The paper introduces Reinforcement Learning with Explicit Human Values (RLEV), which incorporates quantifiable human value signals into the reward function of LLM training, improving value-weighted accuracy and learning a termination policy that adapts response length to prompt value. Experiments on exam-style data show RLEV outperforms correctness‑only baselines across various RL algorithms and model scales, and ablations confirm the gains stem from value‑aligned gradient amplification even under noisy value signals.", "summary_cn": "本文提出了显式人类价值强化学习 (RLEV)，将可量化的人类价值信号直接嵌入大语言模型的奖励函数，从而提升价值加权准确率并学习一种根据提示价值调整回复长度的终止策略。实验在考试式数据上表明，RLEV 在多种强化学习算法和模型规模下均优于仅基于正确性的基线，且消融实验证实价值对齐的梯度放大是性能提升的因果因素，即使在价值信号噪声较大时仍保持鲁棒。", "keywords": "reinforcement learning, human values, alignment, large language model, value-weighted reward, termination policy, gradient amplification", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Dian Yu", "Yulai Zhao", "Kishan Panaganti", "Linfeng Song", "Haitao Mi", "Dong Yu"]}, "usage": {"completion_tokens": 620, "prompt_tokens": 3405, "total_tokens": 4025, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 329, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060011, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029011, "upstream_inference_completions_cost": 0.00031}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.247682", "feed": "arxiv_cscl", "title": "An Expert-grounded benchmark of General Purpose LLMs in LCA", "link": "https://papers.cool/arxiv/2510.19886", "analysis": {"summary": "The paper presents the first expert‑grounded benchmark assessing general‑purpose large language models (LLMs) on 22 life cycle assessment (LCA) tasks, with 17 practitioners reviewing 168 model outputs for scientific accuracy, explanation quality, robustness, verifiability, and instruction adherence. Results show that 37% of responses contain inaccurate or misleading information, hallucination rates reach up to 40% for citations, and while open‑weight models can match or exceed closed‑weight models on some criteria, overall reliability varies considerably. The study highlights risks of naïvely using LLMs as oracles in LCA and underscores the need for grounding mechanisms despite some benefits in explanation quality and labor reduction.", "summary_cn": "本文首次提供了基于专家评审的通用大语言模型（LLM）在生命周期评估（LCA）中的基准评测，涵盖 22 项 LCA 任务，邀请 17 位从业者对 168 条模型输出进行科学准确性、解释质量、鲁棒性、可验证性和指令遵循度的评估。结果显示，37% 的回答存在不准确或误导信息，引用的幻觉率最高可达 40%，且开放权重模型在部分指标上能够与闭源模型相媲美，但整体可靠性差异显著。研究强调了在 LCA 中对 LLM 盲目信任的风险，并指出尽管在解释质量和降低劳动强度方面有一定优势，但仍需采用接地机制以提升安全性。", "keywords": "life cycle assessment, LLM benchmark, expert evaluation, hallucination, accuracy, explanation quality, AI safety, model reliability, open-source LLMs, closed-source LLMs", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Artur Donaldson", "Bharathan Balaji", "Cajetan Oriekezie", "Manish Kumar", "Laure Patouillard"]}, "usage": {"completion_tokens": 863, "prompt_tokens": 3523, "total_tokens": 4386, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 477, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00073931, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030781, "upstream_inference_completions_cost": 0.0004315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.404334", "feed": "arxiv_cscl", "title": "What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation", "link": "https://papers.cool/arxiv/2510.20603", "analysis": {"summary": "The paper introduces Causal Stepwise Evaluation (CaSE), a framework that scores each reasoning step of large language models on relevance to the problem and logical coherence with preceding steps, avoiding hindsight bias. It validates CaSE against human judgments on new expert‑annotated benchmarks (MRa‑GSM8K and MRa‑MATH) and shows that using CaSE‑filtered data for training improves final‑answer performance. The work offers a scalable method for analyzing, debugging, and enhancing LLM reasoning beyond simple correctness checks.", "summary_cn": "本文提出因果分步评估（CaSE）框架，对大语言模型的每一步推理分别评估其与问题的相关性以及与前一步的逻辑连贯性，从而避免后视偏差。作者在新的人类专家标注基准（MRa‑GSM8K 与 MRa‑MATH）上验证了 CaSE 与人工判断的一致性，并展示了使用 CaSE 评估后筛选的训练数据可直接提升模型的最终答案表现。该工作提供了一种可规模化的方式用于分析、调试和改进 LLM 推理，超越了仅检验答案正确性的传统评估。", "keywords": "reasoning evaluation, relevance, coherence, causal stepwise evaluation, LLM reasoning, interpretability, benchmark, data curation", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Heejin Do", "Jaehui Hwang", "Dongyoon Han", "Seong Joon Oh", "Sangdoo Yun"]}, "usage": {"completion_tokens": 720, "prompt_tokens": 3404, "total_tokens": 4124, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 415, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064996, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028996, "upstream_inference_completions_cost": 0.00036}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.413638", "feed": "arxiv_cscl", "title": "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions", "link": "https://papers.cool/arxiv/2510.20039", "analysis": {"summary": "The paper studies bidirectional opinion dynamics in multi‑turn human‑LLM conversations, showing that human opinions shift little while LLM responses adjust substantially toward the human stance, especially under personalization. It highlights a risk of over‑alignment where chatbots may too readily adopt user viewpoints, and suggests design considerations for more stable alignment.", "summary_cn": "本文研究了多轮人机对话中的双向观点动态，发现在人类观点几乎不变的情况下，LLM 的回答会显著向人类立场靠拢，且在个性化聊天机器人中该效应更为突出。研究指出了“过度对齐”(over-alignment) 的风险，即聊天机器人可能过度采纳用户观点，并呼吁在个性化设计中更审慎地实现对齐。", "keywords": "opinion dynamics, bidirectional influence, LLM chatbots, personalization, over-alignment, human-LLM interaction, stance shift, multi-turn conversation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yuyang Jiang", "Longjie Guo", "Yuchen Wu", "Aylin Caliskan", "Tanu Mitra", "Hua Shen"]}, "usage": {"completion_tokens": 639, "prompt_tokens": 3392, "total_tokens": 4031, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 414, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00060766, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028816, "upstream_inference_completions_cost": 0.0003195}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.614145", "feed": "arxiv_cscl", "title": "Calibrating Multimodal Consensus for Emotion Recognition", "link": "https://papers.cool/arxiv/2510.20256", "analysis": {"summary": "The paper proposes Calibrated Multimodal Consensus (CMC) for emotion recognition, introducing a pseudo label generation module for self-supervised unimodal pretraining and a parameter-free fusion module with a multimodal consensus router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four datasets show CMC matches or exceeds state-of-the-art performance, especially when modalities provide conflicting emotional cues.", "summary_cn": "本文提出了用于情感识别的校准多模态共识（CMC）模型，利用伪标签生成模块进行自监督单模态预训练，并通过无参融合模块和多模态共识路由器减轻文本主导问题，缓解跨模态语义不一致。实验在四个数据集上表明 CMC 能够达到或超越当前最佳水平，特别在模态之间情感线索冲突的场景下表现突出。", "keywords": "multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, self-supervised pretraining, fusion robustness, emotion detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"]}, "usage": {"completion_tokens": 705, "prompt_tokens": 3439, "total_tokens": 4144, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 490, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064771, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029521, "upstream_inference_completions_cost": 0.0003525}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.746954", "feed": "arxiv_cscl", "title": "Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory", "link": "https://papers.cool/arxiv/2510.19838", "analysis": {"summary": "Branch-and-Browse is a framework for LLM-powered web agents that integrates tree-structured subtask management, efficient web state replay, and a page action memory to enable controllable multi-branch reasoning and faster execution. The approach improves task success on the WebArena benchmark to 35.8% while reducing runtime by up to 40.4% compared to previous methods.", "summary_cn": "Branch-and-Browse 框架为基于大语言模型的网络代理引入了树状子任务管理、高效的网页状态重放以及页面操作记忆，从而实现可控的多分支推理并提升执行效率。在 WebArena 基准测试中，该方法将任务成功率提升至 35.8%，并将运行时间降低至最多 40.4%。", "keywords": "web agents, large language models, tree-structured reasoning, action memory, web navigation, controllable exploration, WebArena", "scoring": {"interpretability": 4, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shiqi He", "Yue Cui", "Xinyu Ma", "Yaliang Li", "Bolin Ding", "Mosharaf Chowdhury"]}, "usage": {"completion_tokens": 637, "prompt_tokens": 3433, "total_tokens": 4070, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 437, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061281, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029431, "upstream_inference_completions_cost": 0.0003185}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:57.854102", "feed": "arxiv_cscl", "title": "Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs", "link": "https://papers.cool/arxiv/2510.19850", "analysis": {"summary": "The paper proposes Prompt Decorators, a declarative and composable syntax that lets users control LLM reasoning style, tone, and other behavioral dimensions via compact tokens (e.g., +++Reasoning, +++Tone). It formalizes twenty core decorators, defines a scoping model and deterministic processing pipeline, and shows that decoupling task intent from execution behavior improves prompt reproducibility and interpretability. Use cases illustrate enhanced reasoning transparency and standardized model outputs across domains.", "summary_cn": "本文提出了 Prompt Decorators（提示装饰器），一种声明式、可组合的语法，通过紧凑的控制标记（如 +++Reasoning、+++Tone(style=formal)）来调节大语言模型的推理方式、语气等行为维度。文章形式化了二十个核心装饰器，定义了作用域模型和确定性处理流水线，展示了将任务意图与执行行为解耦后，可提升提示的可重用性、可解释性以及行为一致性。案例显示该方法能够提升推理透明度并在不同领域实现标准化输出。", "keywords": "prompt engineering, declarative syntax, composable decorators, LLM control, reasoning transparency, prompt modularity, behavior consistency", "scoring": {"interpretability": 4, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Mostapha Kalami Heris"]}, "usage": {"completion_tokens": 663, "prompt_tokens": 3453, "total_tokens": 4116, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 392, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00062881, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029731, "upstream_inference_completions_cost": 0.0003315}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:58.268064", "feed": "arxiv_cscl", "title": "From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model", "link": "https://papers.cool/arxiv/2510.19871", "analysis": {"summary": "The paper introduces ReDiff, a corrective framework for vision‑language diffusion models that shifts generation from passive denoising to active refining. By training a model to detect and fix its own synthetic errors in a two‑stage process, it breaks error cascades that cause syntactic mistakes and semantic hallucinations, enabling stable parallel generation with higher coherence and factual accuracy.", "summary_cn": "本文提出 ReDiff，一种面向视觉‑语言扩散模型的纠错框架，将生成过程从被动去噪转换为主动精炼。通过两阶段训练让模型学会识别并纠正自身的合成错误，打破导致语法错误和语义幻觉的错误级联，实现稳定的并行生成并提升内容连贯性与事实准确性。", "keywords": "vision-language diffusion, error cascade, self-correction, refinement, parallel decoding, hallucination mitigation, ReDiff, generative models, synthetic error revision, factual accuracy", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Yatai Ji", "Teng Wang", "Yuying Ge", "Zhiheng Liu", "Sidi Yang", "Ying Shan", "Ping Luo"]}, "usage": {"completion_tokens": 704, "prompt_tokens": 3451, "total_tokens": 4155, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 493, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064901, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029701, "upstream_inference_completions_cost": 0.000352}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:58.402918", "feed": "arxiv_cscl", "title": "Automated HIV Screening on Dutch EHR with Large Language Models", "link": "https://papers.cool/arxiv/2510.19879", "analysis": {"summary": "The paper introduces a pipeline that uses a large language model to analyze unstructured clinical notes in electronic health records for automated HIV screening, showing high accuracy and low false‑negative rates on data from Erasmus University Medical Center Rotterdam.", "summary_cn": "本文提出一种利用大型语言模型（LLM）分析电子健康记录中非结构化临床笔记，以实现自动 HIV 筛查的流程，并在鹿特丹伊拉斯姆斯大学医学中心的数据上展示了高准确率和低漏报率。", "keywords": "large language model, electronic health records, HIV screening, clinical notes, medical AI, disease detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lang Zhou", "Amrish Jhingoer", "Yinghao Luo", "Klaske Vliegenthart--Jongbloed", "Carlijn Jordans", "Ben Werkhoven", "Tom Seinen", "Erik van Mulligen", "Casper Rokx", "Yunlei Li"]}, "usage": {"completion_tokens": 729, "prompt_tokens": 3342, "total_tokens": 4071, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 619, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064516, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028066, "upstream_inference_completions_cost": 0.0003645}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:58.722975", "feed": "arxiv_cscl", "title": "BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation", "link": "https://papers.cool/arxiv/2510.20792", "analysis": {"summary": "The paper introduces BadGraph, a backdoor attack against latent diffusion models used for text‑guided graph generation. By poisoning a small fraction of the training data with textual triggers, the attacker can cause the model to insert attacker‑specified subgraphs at inference time while maintaining normal performance on clean inputs. Experiments on four benchmark datasets show high attack success rates with low poisoning ratios and highlight the need for defenses in applications such as drug discovery.", "summary_cn": "本文提出 BadGraph，一种针对文本引导图生成的潜在扩散模型的后门攻击方法。通过在训练数据中注入文本触发词进行少量中毒，攻击者可以在推理时强制模型生成指定子图，同时对干净输入的性能几乎不受影响。四个基准数据集的实验表明，以低于 10% 的中毒率即可实现 50% 以上的攻击成功率，凸显了在药物发现等应用中防御此类后门的紧迫性。", "keywords": "backdoor attack, latent diffusion model, text-guided graph generation, graph generation, security, VAE, poisoning, robustness, drug discovery", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Liang Ye", "Shengqin Chen", "Jiazhu Dai"]}, "usage": {"completion_tokens": 1016, "prompt_tokens": 3441, "total_tokens": 4457, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 850, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00080351, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029551, "upstream_inference_completions_cost": 0.000508}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:58.813206", "feed": "arxiv_cscl", "title": "Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems", "link": "https://papers.cool/arxiv/2510.20728", "analysis": {"summary": "This paper introduces a human-in-the-loop, multi-agent workflow that co-designs quantum error-correcting codes with specific transversal diagonal gates. Using the Subset-Sum Linear Programming framework and agents powered by GPT-5 within a LaTeX-Python environment, the system automatically enumerates small codes, verifies Knill-Laflamme conditions, and abstracts families of codes analytically. Experiments for distance d=2 and up to six qubits produce new codes and a catalog of attainable logical groups.", "summary_cn": "本文提出一种人机协同的多智能体工作流，用于共同设计具备特定横向对角门的量子纠错码。该工作流基于子集求和线性规划框架，并在 LaTeX‑Python 环境中使用 GPT‑5 驱动的合成、搜索和审计智能体，实现自动枚举、验证 Knill‑Laflamme 条件并归纳出一般化的代码族。实验在距离 d=2、最多六个量子比特的情况下生成了若干新码并列出可实现的逻辑群组。", "keywords": "quantum error correction, transversal diagonal gates, multi-agent workflow, GPT-5, Subset-Sum Linear Programming, Knill-Laflamme conditions, code co-design, automated enumeration", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xi He", "Sirui Lu", "Bei Zeng"]}, "usage": {"completion_tokens": 1009, "prompt_tokens": 3601, "total_tokens": 4610, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 751, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00082401, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00031951, "upstream_inference_completions_cost": 0.0005045}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:59.088212", "feed": "arxiv_cscl", "title": "Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations", "link": "https://papers.cool/arxiv/2510.20743", "analysis": {"summary": "The paper introduces Empathic Prompting, a framework that augments Large Language Model conversations with implicit non‑verbal cues by integrating a facial expression recognition service and embedding affective signals into prompts. The modular architecture, demonstrated with a locally deployed DeepSeek instance, shows that unobtrusive emotional context improves conversational fluidity in a small usability study (N=5), suggesting applications in domains such as healthcare and education.", "summary_cn": "本文提出“共情提示”（Empathic Prompting）框架，通过整合面部表情识别服务，将用户情感线索嵌入 Large Language Model 的提示中，实现对话的非语言上下文增强。基于本地部署的 DeepSeek 示例，实验（N=5）显示该方式在不需要用户显式操作的情况下提升了对话流畅度，显示出在医疗、教育等需要情感感知的场景中的潜在应用价值。", "keywords": "empathic prompting, multimodal LLM, facial expression recognition, affective AI, conversational alignment, human-AI interaction, non-verbal context, DeepSeek, usability evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Lorenzo Stacchio", "Andrea Ubaldi", "Alessandro Galdelli", "Maurizio Mauri", "Emanuele Frontoni", "Andrea Gaggioli"]}, "usage": {"completion_tokens": 706, "prompt_tokens": 3396, "total_tokens": 4102, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 475, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00064176, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028876, "upstream_inference_completions_cost": 0.000353}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:32:59.762173", "feed": "arxiv_cscl", "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples", "link": "https://papers.cool/arxiv/2510.20800", "analysis": {"summary": "The paper introduces a fast LLM adaptation method that uses a single gradient step on only 100 examples combined with a targeted scan of top candidate layers and factorization techniques, eliminating the need for full fine‑tuning. By computing gradients of singular values to select matrices for low‑rank reduction and clustering matrix rows into multiple subspaces, the approach improves downstream accuracy by up to 24.6 points while dramatically reducing search time.", "summary_cn": "本文提出一种快速适配大型语言模型的方法，仅使用 100 条样本进行一次梯度更新，并结合对关键层的快速扫描和因式分解技术，从而无需完整微调。通过计算矩阵奇异值的梯度来挑选需要降秩的矩阵，并将矩阵行聚类到多个子空间进行分解，该方法在显著降低搜索时间的同时，使下游任务的准确率提升最高可达 24.6%。", "keywords": "LLM adaptation, LASER, low-rank pruning, singular value gradient, matrix factorization, efficient fine-tuning, single gradient step, weight matrix selection, model compression, downstream accuracy", "scoring": {"interpretability": 5, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}, "usage": {"completion_tokens": 757, "prompt_tokens": 3498, "total_tokens": 4255, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 553, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068256, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030406, "upstream_inference_completions_cost": 0.0003785}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:00.978342", "feed": "arxiv_cscl", "title": "Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures", "link": "https://papers.cool/arxiv/2510.20193", "analysis": {"summary": "This survey examines recent advances in question answering systems that incorporate multimedia retrieval and cross‑modal reasoning, categorizing methods by retrieval strategies, fusion mechanisms, and answer generation techniques. It reviews benchmark datasets, evaluation protocols, and discusses challenges such as cross‑modal alignment, latency‑accuracy trade‑offs, and semantic grounding, while outlining open research directions for more robust, context‑aware QA.", "summary_cn": "本文综述了将多媒体检索与跨模态推理相结合的问答系统的最新进展，按检索策略、融合方式和答案生成技术进行分类。文章回顾了基准数据集、评估方法，并讨论了跨模态对齐、时延‑准确性权衡以及语义 grounding 等关键挑战，提出了构建更鲁棒、上下文知的多媒体问答的未来研究方向。", "keywords": "multimedia QA, cross-modal retrieval, multimodal reasoning, vision-language models, audio-visual QA, retrieval-augmented generation, fusion techniques, benchmark datasets, semantic grounding, latent alignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rahul Raja", "Arpita Vats"]}, "usage": {"completion_tokens": 670, "prompt_tokens": 3346, "total_tokens": 4016, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 442, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061626, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028126, "upstream_inference_completions_cost": 0.000335}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:01.124636", "feed": "arxiv_cscl", "title": "Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication", "link": "https://papers.cool/arxiv/2510.19995", "analysis": {"summary": "The paper introduces Communication to Completion (C2C), a framework for multi‑agent LLM systems that quantifies task alignment with a novel Alignment Factor metric and integrates stepwise execution with intelligent communication decisions. By allowing agents to make cost‑aware communication choices, C2C reduces task completion time by about 40% in realistic coding workflows while maintaining acceptable communication costs. The work provides both a theoretical foundation for measuring communication effectiveness and a practical system for complex collaborative tasks.", "summary_cn": "本文提出了 Communication to Completion (C2C) 框架，针对多代理 LLM 系统引入了用于量化任务对齐的“Alignment Factor（对齐因子）”指标，并在顺序行动框架中融合了智能通信决策。C2C 让代理能够根据成本做出通信选择，使在实际编码工作流中的任务完成时间下降约 40%，且通信成本可接受。该工作既提供衡量多代理通信有效性的理论基础，也给出了用于复杂协作任务的实用系统。", "keywords": "multi-agent communication, alignment factor, sequential action framework, collaborative workflow, LLM agents, task efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Yiming Lu", "Xun Wang", "Simin Ma", "Shujian Liu", "Sathish Reddy Indurthi", "Song Wang", "Haoyun Deng", "Fei Liu", "Kaiqiang Song"]}, "usage": {"completion_tokens": 659, "prompt_tokens": 3395, "total_tokens": 4054, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 417, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061811, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028861, "upstream_inference_completions_cost": 0.0003295}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:01.210647", "feed": "arxiv_cscl", "title": "Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment", "link": "https://papers.cool/arxiv/2510.20513", "analysis": {"summary": "The paper introduces DeEAR, a framework that converts human preference for speech expressiveness into an objective score by evaluating emotion, prosody, and spontaneity. It achieves strong alignment with human perception (SRCC = 0.86) using fewer than 500 annotated samples and uses the metric for fair benchmarking and to select 14K expressive utterances (ExpressiveSpeech), which substantially improve expressiveness scores of S2S models.", "summary_cn": "本文提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分，评估维度包括情感、韵律和自发性。该方法在不足 500 条标注样本下实现了与人类感知的高度一致（Spearman 秩相关系数 SRCC = 0.86），并用于公平基准测试以及筛选 14 K 条表达性语料（ExpressiveSpeech），显著提升了 S2S 模型的表达性评分。", "keywords": "expressive speech, speech-to-speech, evaluation metric, human preference, emotion prosody spontaneity, SRCC, data curation, ExpressiveSpeech", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Zhiyu Lin", "Jingwen Yang", "Jiale Zhao", "Meng Liu", "Sunzhu Li", "Benyou Wang"]}, "usage": {"completion_tokens": 759, "prompt_tokens": 3436, "total_tokens": 4195, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 526, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067426, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029476, "upstream_inference_completions_cost": 0.0003795}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:02.422383", "feed": "arxiv_cscl", "title": "AI PB: A Grounded Generative Agent for Personalized Investment Insights", "link": "https://papers.cool/arxiv/2510.20099", "analysis": {"summary": "AI PB is a production‑scale generative agent that proactively provides grounded, compliant, and user‑specific investment insights in retail finance. It combines deterministic routing between internal and external LLMs, a hybrid retrieval pipeline, and a multi‑stage recommendation system using rule heuristics, sequential behavioral modeling, and contextual bandits, all deployed on‑premises under Korean financial regulations. Human evaluation and system metrics show that this layered safety architecture can deliver trustworthy AI advice in high‑stakes financial settings.", "summary_cn": "AI PB 是一个面向零售的生产级生成式智能体，主动提供基于事实、合规且针对用户的投资洞见。系统通过确定性路由在内部和外部大模型之间切换、融合 OpenSearch 与金融领域嵌入模型的检索管线，以及结合规则启发式、序列行为建模和情境臂带的多阶段推荐机制，实现全链路安全控制，并在韩国金融监管下全本地部署。人类 QA 与系统指标表明，该分层安全架构能够在高风险金融场景中提供可信的 AI 建议。", "keywords": "generative agents, personalized investment, grounded generation, retrieval-augmented generation, financial AI safety, rule heuristics, sequential behavioral modeling, contextual bandits, on-premises deployment, compliance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Daewoo Park", "Suho Park", "Inseok Hong", "Hanwool Lee", "Junkyu Park", "Sangjun Lee", "Jeongman An", "Hyunbin Loh"]}, "usage": {"completion_tokens": 784, "prompt_tokens": 3355, "total_tokens": 4139, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 510, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00067461, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028261, "upstream_inference_completions_cost": 0.000392}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:02.745536", "feed": "arxiv_cscl", "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "link": "https://papers.cool/arxiv/2510.19864", "analysis": {"summary": "The paper defines Spreadsheet Operations Documentation (SOD) as the task of generating human‑readable explanations for spreadsheet manipulation code and introduces a benchmark of 111 code snippets paired with natural‑language summaries. It evaluates five large language models (GPT‑4o, GPT‑4o‑mini, LLaMA‑3.3‑70B, Mixtral‑8x7B, Gemma2‑9B) using BLEU, GLEU, ROUGE‑L, and METEOR, showing that LLMs can produce accurate documentation despite some challenges. The authors argue that SOD can improve reproducibility, maintainability, and collaborative workflows for spreadsheets.", "summary_cn": "本文定义了电子表操作文档（SOD）任务，即为电子表格操作代码生成可读的自然语言解释，并提供了包含 111 条代码片段及对应摘要的基准数据集。作者评估了五种大语言模型（GPT‑4o、GPT‑4o‑mini、LLaMA‑3.3‑70B、Mixtral‑8x7B、Gemma2‑9B），使用 BLEU、GLEU、ROUGE‑L 和 METEOR 等指标，结果表明 LLM 能够生成准确的文档，但仍面临一些挑战。研究表明 SOD 有助于提升电子表格的可复现性、可维护性和协作工作流。", "keywords": "spreadsheet documentation, large language models, natural language generation, code summarization, SOD benchmark, reproducibility, LLM evaluation, spreadsheet operations", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Amila Indika", "Igor Molybog"]}, "usage": {"completion_tokens": 774, "prompt_tokens": 3425, "total_tokens": 4199, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 448, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00068011, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029311, "upstream_inference_completions_cost": 0.000387}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:03.025423", "feed": "arxiv_cscl", "title": "Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE", "link": "https://papers.cool/arxiv/2510.20674", "analysis": {"summary": "The paper describes a multilingual e-commerce product search system built by the Tredence_AICOE team for a competition featuring Query-Category and Query-Item relevance tasks. They augment data by translating existing sets into missing languages andune Gemma-3 12B and Qwen-2.5 14B models using various strategies, achieving a 4th‑place finish with an average F1 of 0.8857. The work emphasizes practical engineering solutions for multilingual relevance rather than theoretical AI safety or interpretability contributions.", "summary_cn": "本文介绍了 Tredence_AICOE 队伍为多语言电子商务产品竞赛构建的系统，涵盖查询‑类目（QC）和查询‑商品（QI）相关性任务。团队通过将现有数据翻译至缺失语言进行数据增强，并针对两大模型 Gemma-3 12B 与 Qwen-2.5 14B 进行多策略微调，最终在私有测试集上获得 0.8857 的平均 F1 并排名第四。该研究侧重于多语言检索的工程实现，而非 AI 安全或可解释性理论。", "keywords": "multilingual search, e-commerce retrieval, relevance ranking, data augmentation, large language models, Gemma-3, Qwen-2.5, multilingual relevance, fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rakshith R", "Shubham Sharma", "Mohammed Sameer Khan", "Ankush Chopra"]}, "usage": {"completion_tokens": 657, "prompt_tokens": 3399, "total_tokens": 4056, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 325, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00061771, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00028921, "upstream_inference_completions_cost": 0.0003285}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:04.758718", "feed": "arxiv_cscl", "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "link": "https://papers.cool/arxiv/2510.20270", "analysis": {"summary": "ImpossibleBench is a benchmark framework that creates impossible variants of coding tasks by introducing contradictions between natural-language specifications and unit tests, allowing measurement of large language model agents' propensity to exploit test cases. The authors evaluate cheating rates across models and prompt conditions, analyze behaviors ranging from simple test modification to complex operator overloading, and demonstrate its use for monitoring and mitigating deceptive solutions.", "summary_cn": "ImpossibleBench 通过在自然语言需求与单元测试之间制造冲突，生成“不可实现”的任务变体，以衡量大语言模型代理利用测试用例的倾向。作者在不同模型和提示设置下测量作弊率，分析从简单修改测试到复杂运算符重载的多种作弊行为，并展示该框架用于监测和降低欺骗性解答的实用性。", "keywords": "LLM cheating, test exploitation, benchmark, alignment, safety, prompt engineering, coding assistants, deceptive behavior", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"]}, "usage": {"completion_tokens": 968, "prompt_tokens": 3485, "total_tokens": 4453, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 895, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00078611, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00030211, "upstream_inference_completions_cost": 0.000484}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:06.150310", "feed": "arxiv_cscl", "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation", "link": "https://papers.cool/arxiv/2510.19897", "analysis": {"summary": "The paper introduces a memory‑augmented framework for large‑language‑model agents that learns classification tasks from labeled examples and model‑generated critiques without updating model parameters. By storing instance‑level critiques in episodic memory and distilling them into task‑level guidance in semantic memory, the approach achieves up to 24.8 % higher accuracy than retrieval‑only baselines, and the authors propose a metric called suggestibility to interpret how different memory strategies affect learning dynamics. Experiments reveal distinct behaviors between OpenAI and open‑source models on factual versus preference data, highlighting the potential of reflective, memory‑driven learning for more adaptable and interpretable agents.", "summary_cn": "本文提出一种记忆增强框架，使基于预训练大语言模型的代理能够在不更新参数的情况下，仅通过带标签示例和模型生成的批评来学习分类任务。框架将实例级批评存入情景记忆（episodic memory），并在语义记忆（semantic memory）中提炼为可复用的任务指导，从而在准确率上相较仅使用检索的基线提升最高 24.8%，作者还引入了“suggestibility”度量以解释不同记忆策略对学习动态的影响。实验展示了 OpenAI 与开源模型在处理事实型与偏好型数据时的行为差异，凸显记忆驱动的反思学习在构建更适应且可解释的 LLM 代理方面的前景。", "keywords": "memory-augmented LLM, episodic memory, semantic memory, reflective learning, few-shot classification, suggestibility metric, interpretability, LLM critiques", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"]}, "usage": {"completion_tokens": 1334, "prompt_tokens": 3416, "total_tokens": 4750, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 1165, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00095876, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029176, "upstream_inference_completions_cost": 0.000667}}, "model": "openai/gpt-oss-120b"}
{"timestamp": "2025-10-24T17:33:06.575216", "feed": "arxiv_cscl", "title": "LLMs can hide text in other text of the same length.ipynb", "link": "https://papers.cool/arxiv/2510.20075", "analysis": {"summary": "The paper introduces a simple protocol that uses large language models to embed a meaningful message inside another coherent text of the same length, effectively creating a steganographic channel. Experiments show that even modest 8‑billion‑parameter open‑source LLMs can encode and decode such hidden messages on a laptop in seconds. The authors discuss the safety implications, including covert deployment of unfiltered models and erosion of trust in written communication.", "summary_cn": "本文提出一种利用大语言模型（LLM）将有意义的文本隐藏在另一段长度相同、仍然连贯可信的文本中的简单协议，实现文本隐写通道。实验表明，即使是 8 十亿参数的开源 LLM 也能在笔记本电脑上在几秒钟内完成编码和解码。作者讨论了此技术的安全影响，包括通过安全模型的表面响应 covert 部署未过滤模型以及进一步侵蚀对书面交流的信任。", "keywords": "LLM steganography, covert communication, text embedding, AI safety, trust erosion, prompt engineering, open-source models, hidden messages", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Antonio Norelli", "Michael Bronstein"]}, "usage": {"completion_tokens": 1148, "prompt_tokens": 3416, "total_tokens": 4564, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 1011, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3152}, "cost": 0.00086576, "is_byok": false, "cost_details": {"upstream_inference_cost": null, "upstream_inference_prompt_cost": 0.00029176, "upstream_inference_completions_cost": 0.000574}}, "model": "openai/gpt-oss-120b"}
