<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Computer Vision and Pattern Recognition</title>
<link>https://papers.cool/arxiv/cs.CV</link>


<item>
<title>HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</title>
<link>https://papers.cool/arxiv/2510.20822</link>
<guid>https://papers.cool/arxiv/2510.20822</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HoloCine proposes a novel architecture for generating cinematic multi‑shot long video narratives, addressing the "narrative gap" of current text‑to‑video models that only produce isolated clips. It introduces a Window Cross‑Attention mechanism to localize prompts to specific shots and a Sparse Inter‑Shot Self‑Attention pattern that is dense within shots but sparse across shots, enabling efficient minute‑scale video synthesis with consistent characters and cinematic techniques.<br /><strong>Summary (CN):</strong> HoloCine 提出了一种用于生成电影式多镜头长视频叙事的全新架构，弥补了现有文本到视频模型只能生成单独片段的“叙事缺口”。该方法引入了窗口交叉注意力（Window Cross‑Attention）用于将文本提示定位到特定镜头，并采用稀疏镜头间自注意力（Sparse Inter‑Shot Self‑Attention）在镜头内部保持密集、镜头之间保持稀疏，从而实现高效的分钟级视频生成，并展现出角色记忆与电影技巧的持续一致性。<br /><strong>Keywords:</strong> holistic video generation, cinematic storytelling, text-to-video, window cross-attention, sparse inter-shot attention, long video synthesis, multi-shot consistency, generative AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</div>
State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this "narrative gap" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.
<div><strong>Authors:</strong> Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HoloCine proposes a novel architecture for generating cinematic multi‑shot long video narratives, addressing the \"narrative gap\" of current text‑to‑video models that only produce isolated clips. It introduces a Window Cross‑Attention mechanism to localize prompts to specific shots and a Sparse Inter‑Shot Self‑Attention pattern that is dense within shots but sparse across shots, enabling efficient minute‑scale video synthesis with consistent characters and cinematic techniques.", "summary_cn": "HoloCine 提出了一种用于生成电影式多镜头长视频叙事的全新架构，弥补了现有文本到视频模型只能生成单独片段的“叙事缺口”。该方法引入了窗口交叉注意力（Window Cross‑Attention）用于将文本提示定位到特定镜头，并采用稀疏镜头间自注意力（Sparse Inter‑Shot Self‑Attention）在镜头内部保持密集、镜头之间保持稀疏，从而实现高效的分钟级视频生成，并展现出角色记忆与电影技巧的持续一致性。", "keywords": "holistic video generation, cinematic storytelling, text-to-video, window cross-attention, sparse inter-shot attention, long video synthesis, multi-shot consistency, generative AI", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihao Meng", "Hao Ouyang", "Yue Yu", "Qiuyu Wang", "Wen Wang", "Ka Leong Cheng", "Hanlin Wang", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Yujun Shen", "Huamin Qu"]}
]]></acme>

<pubDate>2025-10-23T17:59:59+00:00</pubDate>
</item>
<item>
<title>LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</title>
<link>https://papers.cool/arxiv/2510.20820</link>
<guid>https://papers.cool/arxiv/2510.20820</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LayerComposer introduces a layered canvas representation where each subject occupies its own layer, enabling occlusion-free composition in personalized text-to-image generation. A locking mechanism preserves selected layers with high fidelity while allowing other layers to adapt to context, requiring no architectural changes. Experiments show improved spatial control and identity preservation compared to existing multi-subject generation methods.<br /><strong>Summary (CN):</strong> LayerComposer 提出将每个主体放置在独立图层上的层叠画布表示，实现无遮挡的个性化文本到图像生成。锁定机制在保持选定层高保真度的同时，使其他层能够灵活适应周围环境，且无需修改模型结构。实验表明该方法在空间控制和身份保留方面优于现有的多主体生成技术。<br /><strong>Keywords:</strong> personalized text-to-image generation, layered canvas, multi-subject composition, locking mechanism, spatial control, identity preservation, diffusion models, positional embeddings<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</div>
Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.
<div><strong>Authors:</strong> Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LayerComposer introduces a layered canvas representation where each subject occupies its own layer, enabling occlusion-free composition in personalized text-to-image generation. A locking mechanism preserves selected layers with high fidelity while allowing other layers to adapt to context, requiring no architectural changes. Experiments show improved spatial control and identity preservation compared to existing multi-subject generation methods.", "summary_cn": "LayerComposer 提出将每个主体放置在独立图层上的层叠画布表示，实现无遮挡的个性化文本到图像生成。锁定机制在保持选定层高保真度的同时，使其他层能够灵活适应周围环境，且无需修改模型结构。实验表明该方法在空间控制和身份保留方面优于现有的多主体生成技术。", "keywords": "personalized text-to-image generation, layered canvas, multi-subject composition, locking mechanism, spatial control, identity preservation, diffusion models, positional embeddings", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Guocheng Gordon Qian", "Ruihang Zhang", "Tsai-Shien Chen", "Yusuf Dalva", "Anujraaj Argo Goyal", "Willi Menapace", "Ivan Skorokhodov", "Meng Dong", "Arpit Sahni", "Daniil Ostashev", "Ju Hu", "Sergey Tulyakov", "Kuan-Chieh Jackson Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:55+00:00</pubDate>
</item>
<item>
<title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title>
<link>https://papers.cool/arxiv/2510.20819</link>
<guid>https://papers.cool/arxiv/2510.20819</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Latent Denoising Diffusion Bridge Model (LDDBM), a general framework for translating between arbitrary modalities by learning a shared latent space and using contrastive alignment and predictive losses to ensure semantic consistency. It demonstrates strong performance on tasks such as multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis, establishing a new baseline for general modality translation.<br /><strong>Summary (CN):</strong> 本文提出了潜在去噪扩散桥模型（LDDBM），通过在共享潜在空间中学习跨模态桥接，并使用对比对齐损失和预测损失来保证语义一致性，从而实现任意模态之间的翻译。实验在多视图到 3D 形状生成、图像超分辨率和多视图场景合成等任务上取得了强劲表现，奠定了通用模态翻译的新基准。<br /><strong>Keywords:</strong> modality translation, diffusion models, latent diffusion bridge, contrastive alignment, predictive loss, cross-modal generation, latent space, multi-view to 3D, image super-resolution<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.
<div><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Latent Denoising Diffusion Bridge Model (LDDBM), a general framework for translating between arbitrary modalities by learning a shared latent space and using contrastive alignment and predictive losses to ensure semantic consistency. It demonstrates strong performance on tasks such as multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis, establishing a new baseline for general modality translation.", "summary_cn": "本文提出了潜在去噪扩散桥模型（LDDBM），通过在共享潜在空间中学习跨模态桥接，并使用对比对齐损失和预测损失来保证语义一致性，从而实现任意模态之间的翻译。实验在多视图到 3D 形状生成、图像超分辨率和多视图场景合成等任务上取得了强劲表现，奠定了通用模态翻译的新基准。", "keywords": "modality translation, diffusion models, latent diffusion bridge, contrastive alignment, predictive loss, cross-modal generation, latent space, multi-view to 3D, image super-resolution", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nimrod Berman", "Omkar Joglekar", "Eitan Kosman", "Dotan Di Castro", "Omri Azencot"]}
]]></acme>

<pubDate>2025-10-23T17:59:54+00:00</pubDate>
</item>
<item>
<title>SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution</title>
<link>https://papers.cool/arxiv/2510.20814</link>
<guid>https://papers.cool/arxiv/2510.20814</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SpectraMorph is a physics‑guided self‑supervised framework that fuses low‑resolution hyperspectral images with high‑resolution multispectral (or pan‑chromatic) data using a structured latent space that enforces an unmixing bottleneck: endmember signatures are extracted from the hyperspectral image and a compact MLP predicts abundance‑like maps from the multispectral image, with reconstruction via linear mixing. The approach yields interpretable intermediate representations, trains in under a minute, and remains robust even when the multispectral sensor has only a single band, achieving state‑of‑the‑art performance on synthetic and real datasets. Experiments demonstrate consistent improvements over unsupervised baselines and competitive results compared to supervised methods.<br /><strong>Summary (CN):</strong> SpectraMorph 是一种基于物理的自监督融合框架，利用结构化潜在空间强制实现解混瓶颈：从低分辨率高光谱图像提取端元特征，并通过紧凑的多层感知机从多光谱（或全色）图像预测类丰度图，随后通过线性混合重建光谱。该方法产生可解释的中间表示，训练时间不到一分钟，即使多光谱仅有单波段也能保持鲁棒性，并在合成和真实数据集上超越最新的无监督基线，且与监督方法竞争。实验表明其在超分辨率任务上持续取得领先性能。<br /><strong>Keywords:</strong> hyperspectral super-resolution, self-supervised fusion, structured latent space, unmixing bottleneck, interpretability, physics-guided learning, abundance maps, multispectral, pan-chromatic, latent learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Ritik Shah, Marco F Duarte</div>
Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.
<div><strong>Authors:</strong> Ritik Shah, Marco F Duarte</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SpectraMorph is a physics‑guided self‑supervised framework that fuses low‑resolution hyperspectral images with high‑resolution multispectral (or pan‑chromatic) data using a structured latent space that enforces an unmixing bottleneck: endmember signatures are extracted from the hyperspectral image and a compact MLP predicts abundance‑like maps from the multispectral image, with reconstruction via linear mixing. The approach yields interpretable intermediate representations, trains in under a minute, and remains robust even when the multispectral sensor has only a single band, achieving state‑of‑the‑art performance on synthetic and real datasets. Experiments demonstrate consistent improvements over unsupervised baselines and competitive results compared to supervised methods.", "summary_cn": "SpectraMorph 是一种基于物理的自监督融合框架，利用结构化潜在空间强制实现解混瓶颈：从低分辨率高光谱图像提取端元特征，并通过紧凑的多层感知机从多光谱（或全色）图像预测类丰度图，随后通过线性混合重建光谱。该方法产生可解释的中间表示，训练时间不到一分钟，即使多光谱仅有单波段也能保持鲁棒性，并在合成和真实数据集上超越最新的无监督基线，且与监督方法竞争。实验表明其在超分辨率任务上持续取得领先性能。", "keywords": "hyperspectral super-resolution, self-supervised fusion, structured latent space, unmixing bottleneck, interpretability, physics-guided learning, abundance maps, multispectral, pan-chromatic, latent learning", "scoring": {"interpretability": 7, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Ritik Shah", "Marco F Duarte"]}
]]></acme>

<pubDate>2025-10-23T17:59:26+00:00</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://papers.cool/arxiv/2510.20812</link>
<guid>https://papers.cool/arxiv/2510.20812</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight visual-language model draft experts to generate diverse reasoning paths and a strong verdict model to synthesize these paths into final answers for information‑intensive visual question answering. A consensus expert selection step forwards only high‑agreement drafts, improving both accuracy and computational cost on benchmarks like InfographicVQA, ChartMuseum, ChartQAPro, and HR‑Bench 4K. SV demonstrates that aggregating partially correct reasoning can correct errors and rival larger proprietary models without additional training.<br /><strong>Summary (CN):</strong> 本文提出了“Speculative Verdict (SV)”框架，通过多个轻量级视觉语言模型草稿专家生成多样化的推理路径，再由强大的判决模型综合这些路径，以回答信息密集型视觉问答任务。共识专家选择机制仅转发高一致性的草稿，从而在提升准确性的同时显著降低计算成本，在 InfographicVQA、ChartMuseum、ChartQAPro 与 HR‑Bench 4K 等基准上取得了持续提升。该方法展示了聚合部分正确推理可纠错，并在无需额外训练的情况下与大型专有模型竞争。<br /><strong>Keywords:</strong> visual question answering, information-intensive images, speculative decoding, draft experts, verdict model, multimodal reasoning, efficiency, consensus selection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict
<div><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight visual-language model draft experts to generate diverse reasoning paths and a strong verdict model to synthesize these paths into final answers for information‑intensive visual question answering. A consensus expert selection step forwards only high‑agreement drafts, improving both accuracy and computational cost on benchmarks like InfographicVQA, ChartMuseum, ChartQAPro, and HR‑Bench 4K. SV demonstrates that aggregating partially correct reasoning can correct errors and rival larger proprietary models without additional training.", "summary_cn": "本文提出了“Speculative Verdict (SV)”框架，通过多个轻量级视觉语言模型草稿专家生成多样化的推理路径，再由强大的判决模型综合这些路径，以回答信息密集型视觉问答任务。共识专家选择机制仅转发高一致性的草稿，从而在提升准确性的同时显著降低计算成本，在 InfographicVQA、ChartMuseum、ChartQAPro 与 HR‑Bench 4K 等基准上取得了持续提升。该方法展示了聚合部分正确推理可纠错，并在无需额外训练的情况下与大型专有模型竞争。", "keywords": "visual question answering, information-intensive images, speculative decoding, draft experts, verdict model, multimodal reasoning, efficiency, consensus selection", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:21+00:00</pubDate>
</item>
<item>
<title>Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</title>
<link>https://papers.cool/arxiv/2510.20807</link>
<guid>https://papers.cool/arxiv/2510.20807</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pure transformer model that predicts video frames of dynamic physical simulations directly in pixel space, using causal autoregressive modeling and various spatiotemporal self‑attention layouts. Compared with latent‑space methods, it extends the horizon of physically accurate predictions by up to 50% while keeping comparable video quality scores. Additional interpretability probes reveal internal regions that encode PDE simulation parameters, and these representations generalize to out‑of‑distribution parameter estimation.<br /><strong>Summary (CN):</strong> 本文提出了一种纯 transformer 模型，在像素空间中进行因果自回归的视频预测，专注于动态物理仿真，并比较了多种时空自注意力布局。相较于潜在空间方法，该模型将物理上准确的预测时长提升约 50%，且在常规视频质量指标上保持可比性能。进一步的可解释性实验显示网络内部的特定区域能够编码 PDE 仿真参数，并且这种表示能够推广到分布外参数的估计。<br /><strong>Keywords:</strong> video prediction, spatiotemporal transformer, physical simulation, autoregressive, pixel-space, PDE parameter estimation, interpretability, attention<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</div>
Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.
<div><strong>Authors:</strong> Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pure transformer model that predicts video frames of dynamic physical simulations directly in pixel space, using causal autoregressive modeling and various spatiotemporal self‑attention layouts. Compared with latent‑space methods, it extends the horizon of physically accurate predictions by up to 50% while keeping comparable video quality scores. Additional interpretability probes reveal internal regions that encode PDE simulation parameters, and these representations generalize to out‑of‑distribution parameter estimation.", "summary_cn": "本文提出了一种纯 transformer 模型，在像素空间中进行因果自回归的视频预测，专注于动态物理仿真，并比较了多种时空自注意力布局。相较于潜在空间方法，该模型将物理上准确的预测时长提升约 50%，且在常规视频质量指标上保持可比性能。进一步的可解释性实验显示网络内部的特定区域能够编码 PDE 仿真参数，并且这种表示能够推广到分布外参数的估计。", "keywords": "video prediction, spatiotemporal transformer, physical simulation, autoregressive, pixel-space, PDE parameter estimation, interpretability, attention", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dean L Slack", "G Thomas Hudson", "Thomas Winterbottom", "Noura Al Moubayed"]}
]]></acme>

<pubDate>2025-10-23T17:58:45+00:00</pubDate>
</item>
<item>
<title>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</title>
<link>https://papers.cool/arxiv/2510.20803</link>
<guid>https://papers.cool/arxiv/2510.20803</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ARGenSeg introduces an autoregressive generation-based paradigm for image segmentation that leverages multimodal large language models (MLLMs) to output visual tokens which are detokenized via a universal VQ-VAE, producing dense pixel-level masks. By employing a next-scale-prediction strategy, the approach reduces inference latency while achieving state-of-the-art accuracy and speed on multiple segmentation benchmarks.<br /><strong>Summary (CN):</strong> ARGenSeg 提出一种基于自回归生成的图像分割范式，利用多模态大语言模型（MLLM）输出视觉令牌，并通过通用 VQ-VAE 解码为密集的像素级掩码。通过下一尺度预测策略，该方法在降低推理延迟的同时，在多个分割数据集上实现了领先的精度和速度。<br /><strong>Keywords:</strong> image segmentation, autoregressive generation, multimodal large language model, VQ-VAE, dense masks, pixel-level understanding, next-scale prediction<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</div>
We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.
<div><strong>Authors:</strong> Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ARGenSeg introduces an autoregressive generation-based paradigm for image segmentation that leverages multimodal large language models (MLLMs) to output visual tokens which are detokenized via a universal VQ-VAE, producing dense pixel-level masks. By employing a next-scale-prediction strategy, the approach reduces inference latency while achieving state-of-the-art accuracy and speed on multiple segmentation benchmarks.", "summary_cn": "ARGenSeg 提出一种基于自回归生成的图像分割范式，利用多模态大语言模型（MLLM）输出视觉令牌，并通过通用 VQ-VAE 解码为密集的像素级掩码。通过下一尺度预测策略，该方法在降低推理延迟的同时，在多个分割数据集上实现了领先的精度和速度。", "keywords": "image segmentation, autoregressive generation, multimodal large language model, VQ-VAE, dense masks, pixel-level understanding, next-scale prediction", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaolong Wang", "Lixiang Ru", "Ziyuan Huang", "Kaixiang Ji", "Dandan Zheng", "Jingdong Chen", "Jun Zhou"]}
]]></acme>

<pubDate>2025-10-23T17:58:26+00:00</pubDate>
</item>
<item>
<title>Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</title>
<link>https://papers.cool/arxiv/2510.20794</link>
<guid>https://papers.cool/arxiv/2510.20794</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multi-object tracking framework that fuses radar and camera data, using common features to enable online calibration and autonomous association of detections. By positioning radar as a primary sensor for accurate range information and employing feature matching with category-consistency checks, the method improves tracking precision in both controlled and real traffic environments.<br /><strong>Summary (CN):</strong> 本文提出一种融合雷达与摄像头的多目标跟踪框架，利用两者的公共特征实现在线标定并自动关联检测结果。通过将雷达视为提供精确距离信息的核心传感器，并结合特征匹配与类别一致性检查，提升了在受控环境和真实交通场景中的跟踪精度。<br /><strong>Keywords:</strong> radar-camera fusion, multi-object tracking, online calibration, common feature matching, sensor fusion, autonomous driving, perception robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Lei Cheng, Siyang Cao</div>
This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT
<div><strong>Authors:</strong> Lei Cheng, Siyang Cao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multi-object tracking framework that fuses radar and camera data, using common features to enable online calibration and autonomous association of detections. By positioning radar as a primary sensor for accurate range information and employing feature matching with category-consistency checks, the method improves tracking precision in both controlled and real traffic environments.", "summary_cn": "本文提出一种融合雷达与摄像头的多目标跟踪框架，利用两者的公共特征实现在线标定并自动关联检测结果。通过将雷达视为提供精确距离信息的核心传感器，并结合特征匹配与类别一致性检查，提升了在受控环境和真实交通场景中的跟踪精度。", "keywords": "radar-camera fusion, multi-object tracking, online calibration, common feature matching, sensor fusion, autonomous driving, perception robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Lei Cheng", "Siyang Cao"]}
]]></acme>

<pubDate>2025-10-23T17:54:57+00:00</pubDate>
</item>
<item>
<title>CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</title>
<link>https://papers.cool/arxiv/2510.20776</link>
<guid>https://papers.cool/arxiv/2510.20776</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CUPID introduces a generation-based framework that simultaneously infers camera pose, 3D shape, and texture from a single image by casting reconstruction as conditional sampling from a learned 3D object distribution. It employs a two-stage flow-matching pipeline—coarse geometry with pose recovery followed by pose-aligned refinement—to produce high-fidelity voxel and pixel‑voxel correspondences, achieving significant gains in PSNR and Chamfer Distance over prior methods.<br /><strong>Summary (CN):</strong> CUPID 提出了一种基于生成的框架，通过从学习得来的 3D 对象分布进行条件采样，实现单张图片的相机姿态、三维形状和纹理的同步推断。该方法采用两阶段流匹配流程——先生成粗糙几何并恢复姿态，再进行姿态对齐的特征细化，从而在 PSNR 和 Chamfer Distance 上显著优于现有方法。<br /><strong>Keywords:</strong> 3D reconstruction, pose estimation, generative models, flow matching, voxel generation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</div>
This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit cupid3d.github.io.
<div><strong>Authors:</strong> Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CUPID introduces a generation-based framework that simultaneously infers camera pose, 3D shape, and texture from a single image by casting reconstruction as conditional sampling from a learned 3D object distribution. It employs a two-stage flow-matching pipeline—coarse geometry with pose recovery followed by pose-aligned refinement—to produce high-fidelity voxel and pixel‑voxel correspondences, achieving significant gains in PSNR and Chamfer Distance over prior methods.", "summary_cn": "CUPID 提出了一种基于生成的框架，通过从学习得来的 3D 对象分布进行条件采样，实现单张图片的相机姿态、三维形状和纹理的同步推断。该方法采用两阶段流匹配流程——先生成粗糙几何并恢复姿态，再进行姿态对齐的特征细化，从而在 PSNR 和 Chamfer Distance 上显著优于现有方法。", "keywords": "3D reconstruction, pose estimation, generative models, flow matching, voxel generation", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Binbin Huang", "Haobin Duan", "Yiqun Zhao", "Zibo Zhao", "Yi Ma", "Shenghua Gao"]}
]]></acme>

<pubDate>2025-10-23T17:47:38+00:00</pubDate>
</item>
<item>
<title>AlphaFlow: Understanding and Improving MeanFlow Models</title>
<link>https://papers.cool/arxiv/2510.20771</link>
<guid>https://papers.cool/arxiv/2510.20771</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes the MeanFlow objective, showing it decomposes into trajectory flow matching and trajectory consistency terms that are negatively correlated, leading to optimization conflict. Based on this insight, it proposes the α‑Flow family of objectives and a curriculum that gradually shifts from flow matching to MeanFlow, improving convergence and achieving state‑of‑the‑art FID scores on ImageNet with vanilla DiT backbones.<br /><strong>Summary (CN):</strong> 本文分析了 MeanFlow 目标，发现其可分解为轨迹流匹配和轨迹一致性两部分，这两者负相关导致优化冲突。基于此洞察，提出了 α‑Flow 系列目标以及从流匹配平滑过渡到 MeanFlow 的课程策略，提升收敛速度，并在使用普通 DiT 骨干的 ImageNet 实验中取得了最先进的 FID 分数。<br /><strong>Keywords:</strong> MeanFlow, trajectory flow matching, alpha-Flow, generative modeling, diffusion, DiT, curriculum training, FID<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</div>
MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).
<div><strong>Authors:</strong> Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes the MeanFlow objective, showing it decomposes into trajectory flow matching and trajectory consistency terms that are negatively correlated, leading to optimization conflict. Based on this insight, it proposes the α‑Flow family of objectives and a curriculum that gradually shifts from flow matching to MeanFlow, improving convergence and achieving state‑of‑the‑art FID scores on ImageNet with vanilla DiT backbones.", "summary_cn": "本文分析了 MeanFlow 目标，发现其可分解为轨迹流匹配和轨迹一致性两部分，这两者负相关导致优化冲突。基于此洞察，提出了 α‑Flow 系列目标以及从流匹配平滑过渡到 MeanFlow 的课程策略，提升收敛速度，并在使用普通 DiT 骨干的 ImageNet 实验中取得了最先进的 FID 分数。", "keywords": "MeanFlow, trajectory flow matching, alpha-Flow, generative modeling, diffusion, DiT, curriculum training, FID", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Huijie Zhang", "Aliaksandr Siarohin", "Willi Menapace", "Michael Vasilkovsky", "Sergey Tulyakov", "Qing Qu", "Ivan Skorokhodov"]}
]]></acme>

<pubDate>2025-10-23T17:45:06+00:00</pubDate>
</item>
<item>
<title>DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</title>
<link>https://papers.cool/arxiv/2510.20766</link>
<guid>https://papers.cool/arxiv/2510.20766</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Dynamic Position Extrapolation (DyPE), a training-free technique that adapts a diffusion transformer's positional encodings at each diffusion step to match the evolving frequency spectrum, enabling generation of images at resolutions far beyond the model's training data without extra sampling cost. By leveraging the fact that low-frequency components converge early while high frequencies require more steps, DyPE can produce ultra‑high‑resolution outputs (e.g., 16 MP) with state‑of‑the‑art fidelity. Experiments on multiple benchmarks show consistent performance gains, especially at extreme resolutions.<br /><strong>Summary (CN):</strong> 本文提出了动态位置外推（DyPE）技术，它在扩散过程的每一步动态调整扩散Transformer的位置信息编码，使其频谱与当前生成阶段相匹配，从而在无需额外采样成本的情况下生成远超训练分辨率的图像。该方法利用低频结构早期收敛、高频结构需更多扩散步数的特性，实现了如1600万像素等超高分辨率图像的高保真合成。多项基准测试表明，在极高分辨率下性能提升尤为显著。<br /><strong>Keywords:</strong> diffusion models, positional encoding, ultra-high-resolution image synthesis, spectral progression, diffusion transformer, DyPE<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</div>
Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.
<div><strong>Authors:</strong> Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Dynamic Position Extrapolation (DyPE), a training-free technique that adapts a diffusion transformer's positional encodings at each diffusion step to match the evolving frequency spectrum, enabling generation of images at resolutions far beyond the model's training data without extra sampling cost. By leveraging the fact that low-frequency components converge early while high frequencies require more steps, DyPE can produce ultra‑high‑resolution outputs (e.g., 16 MP) with state‑of‑the‑art fidelity. Experiments on multiple benchmarks show consistent performance gains, especially at extreme resolutions.", "summary_cn": "本文提出了动态位置外推（DyPE）技术，它在扩散过程的每一步动态调整扩散Transformer的位置信息编码，使其频谱与当前生成阶段相匹配，从而在无需额外采样成本的情况下生成远超训练分辨率的图像。该方法利用低频结构早期收敛、高频结构需更多扩散步数的特性，实现了如1600万像素等超高分辨率图像的高保真合成。多项基准测试表明，在极高分辨率下性能提升尤为显著。", "keywords": "diffusion models, positional encoding, ultra-high-resolution image synthesis, spectral progression, diffusion transformer, DyPE", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Noam Issachar", "Guy Yariv", "Sagie Benaim", "Yossi Adi", "Dani Lischinski", "Raanan Fattal"]}
]]></acme>

<pubDate>2025-10-23T17:42:14+00:00</pubDate>
</item>
<item>
<title>ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology</title>
<link>https://papers.cool/arxiv/2510.20754</link>
<guid>https://papers.cool/arxiv/2510.20754</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ACS‑SegNet, an attention‑based dual‑encoder network that combines a CNN and a Vision Transformer to fuse features for semantic tissue segmentation in histopathology images. Experiments on the GCPS and PUMA datasets show improved μIoU and μDice scores compared with existing methods. The code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出 ACS‑SegNet，一种基于注意力的双编码器网络，将卷积神经网络 (CNN) 与视觉 Transformer (ViT) 融合以实现组织学图像的语义分割。在 GCPS 与 PUMA 数据集上的实验显示，该模型在 μIoU 与 μDice 指标上均优于现有方法。代码已在 GitHub 开源。<br /><strong>Keywords:</strong> tissue segmentation, histopathology, attention mechanism, dual-encoder, CNN, Vision Transformer, semantic segmentation, medical imaging<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Diana Mechtcheriakova, Amirreza Mahbod</div>
Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\mu}IoU/{\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and 64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: https://github.com/NimaTorbati/ACS-SegNet
<div><strong>Authors:</strong> Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Diana Mechtcheriakova, Amirreza Mahbod</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ACS‑SegNet, an attention‑based dual‑encoder network that combines a CNN and a Vision Transformer to fuse features for semantic tissue segmentation in histopathology images. Experiments on the GCPS and PUMA datasets show improved μIoU and μDice scores compared with existing methods. The code is released publicly.", "summary_cn": "本文提出 ACS‑SegNet，一种基于注意力的双编码器网络，将卷积神经网络 (CNN) 与视觉 Transformer (ViT) 融合以实现组织学图像的语义分割。在 GCPS 与 PUMA 数据集上的实验显示，该模型在 μIoU 与 μDice 指标上均优于现有方法。代码已在 GitHub 开源。", "keywords": "tissue segmentation, histopathology, attention mechanism, dual-encoder, CNN, Vision Transformer, semantic segmentation, medical imaging", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nima Torbati", "Anastasia Meshcheryakova", "Ramona Woitek", "Diana Mechtcheriakova", "Amirreza Mahbod"]}
]]></acme>

<pubDate>2025-10-23T17:21:06+00:00</pubDate>
</item>
<item>
<title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title>
<link>https://papers.cool/arxiv/2510.20726</link>
<guid>https://papers.cool/arxiv/2510.20726</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AutoScape introduces a geometry-consistent long-horizon driving scene generation framework that uses a novel RGB-D diffusion model to produce sparse, geometrically aligned keyframes and a video diffusion model to interpolate dense video frames. By jointly encoding image and depth, conditioning on rendered point clouds from prior keyframes, and applying warp-consistent guidance, the system achieves substantial improvements in long-horizon FID and FVD scores.<br /><strong>Summary (CN):</strong> AutoScape 提出了一种几何一致的长时段驾驶场景生成框架，核心是利用新颖的 RGB-D 扩散模型生成稀疏且几何对齐的关键帧，并通过视频扩散模型在关键帧之间插值生成密集视频。该方法在共享潜在空间中同时处理图像和深度、基于先前关键帧的渲染点云进行条件化，并使用 warp‑consistent 引导，实现了长时段 FID 与 FVD 分数的大幅提升。<br /><strong>Keywords:</strong> geometry-consistent diffusion, RGB-D generation, long-horizon video synthesis, driving scene generation, keyframe interpolation, warp-consistent guidance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</div>
This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene's appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and 43.0\%, respectively.
<div><strong>Authors:</strong> Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AutoScape introduces a geometry-consistent long-horizon driving scene generation framework that uses a novel RGB-D diffusion model to produce sparse, geometrically aligned keyframes and a video diffusion model to interpolate dense video frames. By jointly encoding image and depth, conditioning on rendered point clouds from prior keyframes, and applying warp-consistent guidance, the system achieves substantial improvements in long-horizon FID and FVD scores.", "summary_cn": "AutoScape 提出了一种几何一致的长时段驾驶场景生成框架，核心是利用新颖的 RGB-D 扩散模型生成稀疏且几何对齐的关键帧，并通过视频扩散模型在关键帧之间插值生成密集视频。该方法在共享潜在空间中同时处理图像和深度、基于先前关键帧的渲染点云进行条件化，并使用 warp‑consistent 引导，实现了长时段 FID 与 FVD 分数的大幅提升。", "keywords": "geometry-consistent diffusion, RGB-D generation, long-horizon video synthesis, driving scene generation, keyframe interpolation, warp-consistent guidance", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiacheng Chen", "Ziyu Jiang", "Mingfu Liang", "Bingbing Zhuang", "Jong-Chyi Su", "Sparsh Garg", "Ying Wu", "Manmohan Chandraker"]}
]]></acme>

<pubDate>2025-10-23T16:44:34+00:00</pubDate>
</item>
<item>
<title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title>
<link>https://papers.cool/arxiv/2510.20708</link>
<guid>https://papers.cool/arxiv/2510.20708</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ALICE‑LRI, a sensor‑agnostic algorithm that automatically infers the intrinsic geometry of spinning LiDAR sensors and generates lossless range‑image projections without any manufacturer calibration data. By reverse‑engineering beam configurations and per‑beam corrections, the method preserves every point of the original point cloud and achieves real‑time performance, as demonstrated on the full KITTI and DurLAR datasets. A compression case study further shows downstream benefits of the perfect geometric preservation.<br /><strong>Summary (CN):</strong> 本文提出 ALICE‑LRI，一种无需制造商标定文件即可自动推断旋转 LiDAR 传感器固有几何并生成无信息损失的范围图像的通用算法。该方法通过逆向解析激光束配置和每束校正，实现了对原始点云的完整点保留，并在 KITTI 与 DurLAR 数据集上展示了实时性能。压缩案例研究进一步验证了几何完全保留在下游应用中的显著质量提升。<br /><strong>Keywords:</strong> LiDAR, range image, lossless projection, intrinsic calibration, point cloud, sensor-agnostic, compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera</div>
3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.
<div><strong>Authors:</strong> Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ALICE‑LRI, a sensor‑agnostic algorithm that automatically infers the intrinsic geometry of spinning LiDAR sensors and generates lossless range‑image projections without any manufacturer calibration data. By reverse‑engineering beam configurations and per‑beam corrections, the method preserves every point of the original point cloud and achieves real‑time performance, as demonstrated on the full KITTI and DurLAR datasets. A compression case study further shows downstream benefits of the perfect geometric preservation.", "summary_cn": "本文提出 ALICE‑LRI，一种无需制造商标定文件即可自动推断旋转 LiDAR 传感器固有几何并生成无信息损失的范围图像的通用算法。该方法通过逆向解析激光束配置和每束校正，实现了对原始点云的完整点保留，并在 KITTI 与 DurLAR 数据集上展示了实时性能。压缩案例研究进一步验证了几何完全保留在下游应用中的显著质量提升。", "keywords": "LiDAR, range image, lossless projection, intrinsic calibration, point cloud, sensor-agnostic, compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Samuel Soutullo", "Miguel Yermo", "David L. Vilariño", "Óscar G. Lorenzo", "José C. Cabaleiro", "Francisco F. Rivera"]}
]]></acme>

<pubDate>2025-10-23T16:22:58+00:00</pubDate>
</item>
<item>
<title>Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</title>
<link>https://papers.cool/arxiv/2510.20707</link>
<guid>https://papers.cool/arxiv/2510.20707</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MixKV, a method that jointly optimizes importance and diversity to compress key‑value caches in large vision‑language models, addressing head‑wise semantic redundancy. Experiments show consistent performance gains over existing compression techniques across multiple benchmarks, including extreme compression budgets, while preserving inference efficiency and extending to language models. Code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出 MixKV 方法，通过同时考虑重要性和多样性，对大规模视觉语言模型的 KV 缓存进行压缩，并针对不同注意力头的语义冗余进行自适应平衡。实验表明，在多项基准测试和极端压缩预算下，MixKV 能显著提升现有压缩方法的效果，同时保持推理效率，并可无缝扩展到大语言模型。代码已公开发布。<br /><strong>Keywords:</strong> KV cache compression, vision-language models, importance and diversity, MixKV, semantic redundancy, memory bottleneck, SnapKV, AdaKV, inference efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</div>
Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \texttt{MixKV} improves baseline methods by an average of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.
<div><strong>Authors:</strong> Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MixKV, a method that jointly optimizes importance and diversity to compress key‑value caches in large vision‑language models, addressing head‑wise semantic redundancy. Experiments show consistent performance gains over existing compression techniques across multiple benchmarks, including extreme compression budgets, while preserving inference efficiency and extending to language models. Code is released publicly.", "summary_cn": "本文提出 MixKV 方法，通过同时考虑重要性和多样性，对大规模视觉语言模型的 KV 缓存进行压缩，并针对不同注意力头的语义冗余进行自适应平衡。实验表明，在多项基准测试和极端压缩预算下，MixKV 能显著提升现有压缩方法的效果，同时保持推理效率，并可无缝扩展到大语言模型。代码已公开发布。", "keywords": "KV cache compression, vision-language models, importance and diversity, MixKV, semantic redundancy, memory bottleneck, SnapKV, AdaKV, inference efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xuyang Liu", "Xiyan Gui", "Yuchao Zhang", "Linfeng Zhang"]}
]]></acme>

<pubDate>2025-10-23T16:17:47+00:00</pubDate>
</item>
<item>
<title>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</title>
<link>https://papers.cool/arxiv/2510.20696</link>
<guid>https://papers.cool/arxiv/2510.20696</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically diagnoses failure modes of state-of-the-art vision-language models, highlighting visual hallucinations and over-reliance on textual priors, and introduces an agent-based architecture that couples LLM reasoning with lightweight visual modules for iterative refinement of reasoning chains. Experiments on MMMU and MathVista show significant performance gains over strong baselines, and the authors release their diagnostic framework to support future research.<br /><strong>Summary (CN):</strong> 本文系统性地诊断了最先进视觉语言模型的失效模式，揭示了视觉幻觉和对文本先验的过度依赖，并提出一种基于代理的架构，将大语言模型的推理与轻量级视觉模块结合，实现推理链的细化与迭代。实验在 MMMU 与 MathVista 上取得显著提升，作者同时开源了评估框架以促进后续研究。<br /><strong>Keywords:</strong> multimodal large language models, visual reasoning, chain-of-thought, visual hallucination, agent-based architecture, evaluation framework, tool integration<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</div>
Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.
<div><strong>Authors:</strong> Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically diagnoses failure modes of state-of-the-art vision-language models, highlighting visual hallucinations and over-reliance on textual priors, and introduces an agent-based architecture that couples LLM reasoning with lightweight visual modules for iterative refinement of reasoning chains. Experiments on MMMU and MathVista show significant performance gains over strong baselines, and the authors release their diagnostic framework to support future research.", "summary_cn": "本文系统性地诊断了最先进视觉语言模型的失效模式，揭示了视觉幻觉和对文本先验的过度依赖，并提出一种基于代理的架构，将大语言模型的推理与轻量级视觉模块结合，实现推理链的细化与迭代。实验在 MMMU 与 MathVista 上取得显著提升，作者同时开源了评估框架以促进后续研究。", "keywords": "multimodal large language models, visual reasoning, chain-of-thought, visual hallucination, agent-based architecture, evaluation framework, tool integration", "scoring": {"interpretability": 6, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Jing Bi", "Guangyu Sun", "Ali Vosoughi", "Chen Chen", "Chenliang Xu"]}
]]></acme>

<pubDate>2025-10-23T16:10:03+00:00</pubDate>
</item>
<item>
<title>Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling</title>
<link>https://papers.cool/arxiv/2510.20673</link>
<guid>https://papers.cool/arxiv/2510.20673</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Efficient Multi-bit Quantization Network (EMQNet) training, which uses weight bias correction to share batch normalization across bit-widths and eliminate fine-tuning, and a bit-wise coreset sampling strategy to train each precision on a compact, informative subset selected by gradient-based importance scores. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with ResNet and ViT architectures show competitive or superior accuracy while reducing training time up to 7.88×.<br /><strong>Summary (CN):</strong> 本文提出了 Efficient Multi-bit Quantization Network (EMQNet) 训练方法，利用权重偏置校正在不同比特宽度间共享批归一化并消除微调需求，并通过基于梯度重要性分数的比特级核集抽样，使每个精度子模型在紧凑且信息量丰富的子集上训练。实验在 CIFAR-10/100、TinyImageNet 和 ImageNet-1K 上使用 ResNet 与 ViT，展示了在保持或提升准确率的同时，训练时间最高可缩短 7.88 倍。<br /><strong>Keywords:</strong> multi-bit quantization, weight bias correction, bit-wise coreset sampling, efficient training, model compression, quantized neural networks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko</div>
Multi-bit quantization networks enable flexible deployment of deep neural networks by supporting multiple precision levels within a single model. However, existing approaches suffer from significant training overhead as full-dataset updates are repeated for each supported bit-width, resulting in a cost that scales linearly with the number of precisions. Additionally, extra fine-tuning stages are often required to support additional or intermediate precision options, further compounding the overall training burden. To address this issue, we propose two techniques that greatly reduce the training overhead without compromising model utility: (i) Weight bias correction enables shared batch normalization and eliminates the need for fine-tuning by neutralizing quantization-induced bias across bit-widths and aligning activation distributions; and (ii) Bit-wise coreset sampling strategy allows each child model to train on a compact, informative subset selected via gradient-based importance scores by exploiting the implicit knowledge transfer phenomenon. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and ViT architectures demonstrate that our method achieves competitive or superior accuracy while reducing training time up to 7.88x. Our code is released at https://github.com/a2jinhee/EMQNet_jk.
<div><strong>Authors:</strong> Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Efficient Multi-bit Quantization Network (EMQNet) training, which uses weight bias correction to share batch normalization across bit-widths and eliminate fine-tuning, and a bit-wise coreset sampling strategy to train each precision on a compact, informative subset selected by gradient-based importance scores. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with ResNet and ViT architectures show competitive or superior accuracy while reducing training time up to 7.88×.", "summary_cn": "本文提出了 Efficient Multi-bit Quantization Network (EMQNet) 训练方法，利用权重偏置校正在不同比特宽度间共享批归一化并消除微调需求，并通过基于梯度重要性分数的比特级核集抽样，使每个精度子模型在紧凑且信息量丰富的子集上训练。实验在 CIFAR-10/100、TinyImageNet 和 ImageNet-1K 上使用 ResNet 与 ViT，展示了在保持或提升准确率的同时，训练时间最高可缩短 7.88 倍。", "keywords": "multi-bit quantization, weight bias correction, bit-wise coreset sampling, efficient training, model compression, quantized neural networks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinhee Kim", "Jae Jun An", "Kang Eun Jeon", "Jong Hwan Ko"]}
]]></acme>

<pubDate>2025-10-23T15:49:02+00:00</pubDate>
</item>
<item>
<title>HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification</title>
<link>https://papers.cool/arxiv/2510.20669</link>
<guid>https://papers.cool/arxiv/2510.20669</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces HybridSOMSpikeNet, a hybrid deep learning framework that combines a pre‑trained ResNet‑152 backbone, a differentiable Soft‑SOM for topological clustering, and a spiking neural head to perform ten‑class waste classification with 97.39% accuracy. The Soft‑SOM component enhances interpretability of the learned representations, while the spiking dynamics improve robustness and enable energy‑efficient inference suitable for deployment in real‑world waste management systems. The approach aims to reduce waste misclassification, supporting broader sustainability goals such as improved recycling rates and lower environmental impact.<br /><strong>Summary (CN):</strong> 本文提出 HybridSOMSpikeNet—a hybrid deep learning 框架，结合预训练的 ResNet‑152 特征提取、可微分的 Soft‑SOM（软自组织映射）用于拓扑聚类，以及脉冲神经网络头部，以实现十类垃圾分类并达到 97.39% 的测试准确率。Soft‑SOM 部分提升了模型内部表征的可解释性，脉冲动力学则增强了鲁棒性并实现了能耗高效的推理，适合实际垃圾管理部署。该方法旨在降低垃圾误分类，提升回收效率，进而支持更广泛的可持续发展目标，如降低环境影响。<br /><strong>Keywords:</strong> waste classification, hybrid deep learning, differentiable soft self-organizing map, spiking neural network, ResNet-152, sustainability, environmental AI, topological clustering, energy-efficient inference<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - interpretability<br /><strong>Authors:</strong> Debojyoti Ghosh, Adrijit Goswami</div>
Accurate waste classification is vital for achieving sustainable waste management and reducing the environmental footprint of urbanization. Misclassification of recyclable materials contributes to landfill accumulation, inefficient recycling, and increased greenhouse gas emissions. To address these issues, this study introduces HybridSOMSpikeNet, a hybrid deep learning framework that integrates convolutional feature extraction, differentiable self-organization, and spiking-inspired temporal processing to enable intelligent and energy-efficient waste classification. The proposed model employs a pre-trained ResNet-152 backbone to extract deep spatial representations, followed by a Differentiable Soft Self-Organizing Map (Soft-SOM) that enhances topological clustering and interpretability. A spiking neural head accumulates temporal activations over discrete time steps, improving robustness and generalization. Trained on a ten-class waste dataset, HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming several state-of-the-art architectures while maintaining a lightweight computational profile suitable for real-world deployment. Beyond its technical innovations, the framework provides tangible environmental benefits. By enabling precise and automated waste segregation, it supports higher recycling efficiency, reduces contamination in recyclable streams, and minimizes the ecological and operational costs of waste processing. The approach aligns with global sustainability priorities, particularly the United Nations Sustainable Development Goals (SDG 11 and SDG 12), by contributing to cleaner cities, circular economy initiatives, and intelligent environmental management systems.
<div><strong>Authors:</strong> Debojyoti Ghosh, Adrijit Goswami</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces HybridSOMSpikeNet, a hybrid deep learning framework that combines a pre‑trained ResNet‑152 backbone, a differentiable Soft‑SOM for topological clustering, and a spiking neural head to perform ten‑class waste classification with 97.39% accuracy. The Soft‑SOM component enhances interpretability of the learned representations, while the spiking dynamics improve robustness and enable energy‑efficient inference suitable for deployment in real‑world waste management systems. The approach aims to reduce waste misclassification, supporting broader sustainability goals such as improved recycling rates and lower environmental impact.", "summary_cn": "本文提出 HybridSOMSpikeNet—a hybrid deep learning 框架，结合预训练的 ResNet‑152 特征提取、可微分的 Soft‑SOM（软自组织映射）用于拓扑聚类，以及脉冲神经网络头部，以实现十类垃圾分类并达到 97.39% 的测试准确率。Soft‑SOM 部分提升了模型内部表征的可解释性，脉冲动力学则增强了鲁棒性并实现了能耗高效的推理，适合实际垃圾管理部署。该方法旨在降低垃圾误分类，提升回收效率，进而支持更广泛的可持续发展目标，如降低环境影响。", "keywords": "waste classification, hybrid deep learning, differentiable soft self-organizing map, spiking neural network, ResNet-152, sustainability, environmental AI, topological clustering, energy-efficient inference", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "interpretability"}, "authors": ["Debojyoti Ghosh", "Adrijit Goswami"]}
]]></acme>

<pubDate>2025-10-23T15:47:09+00:00</pubDate>
</item>
<item>
<title>UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</title>
<link>https://papers.cool/arxiv/2510.20661</link>
<guid>https://papers.cool/arxiv/2510.20661</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces UltraHR-100K, a curated dataset of 100,000 ultra-high-resolution images with detailed captions, and proposes a frequency-aware post‑training framework (DOTS and SWFR) to improve fine‑detail synthesis in text‑to‑image diffusion models. Experiments on the new UltraHR‑eval4K benchmark show notable gains in high‑frequency detail preservation and overall image fidelity.<br /><strong>Summary (CN):</strong> 本文推出 UltraHR-100K 数据集，包含 10 万张 3K 以上分辨率的高质量图像及丰富文本描述，并提出频率感知的后训练方法（DOTS 与 SWFR），用于提升文本生成扩散模型在超高分辨率场景下的细节合成能力。实验在 UltraHR‑eval4K 基准上展示了高频细节保留和整体画质的显著提升。<br /><strong>Keywords:</strong> ultra-high-resolution, text-to-image, diffusion models, large-scale dataset, frequency-aware training, detail synthesis, DOTS, SWFR, high-frequency preservation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai</div>
Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.
<div><strong>Authors:</strong> Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces UltraHR-100K, a curated dataset of 100,000 ultra-high-resolution images with detailed captions, and proposes a frequency-aware post‑training framework (DOTS and SWFR) to improve fine‑detail synthesis in text‑to‑image diffusion models. Experiments on the new UltraHR‑eval4K benchmark show notable gains in high‑frequency detail preservation and overall image fidelity.", "summary_cn": "本文推出 UltraHR-100K 数据集，包含 10 万张 3K 以上分辨率的高质量图像及丰富文本描述，并提出频率感知的后训练方法（DOTS 与 SWFR），用于提升文本生成扩散模型在超高分辨率场景下的细节合成能力。实验在 UltraHR‑eval4K 基准上展示了高频细节保留和整体画质的显著提升。", "keywords": "ultra-high-resolution, text-to-image, diffusion models, large-scale dataset, frequency-aware training, detail synthesis, DOTS, SWFR, high-frequency preservation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chen Zhao", "En Ci", "Yunzhe Xu", "Tiehan Fan", "Shanyan Guan", "Yanhao Ge", "Jian Yang", "Ying Tai"]}
]]></acme>

<pubDate>2025-10-23T15:34:53+00:00</pubDate>
</item>
<item>
<title>Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging</title>
<link>https://papers.cool/arxiv/2510.20639</link>
<guid>https://papers.cool/arxiv/2510.20639</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BTB3D, a causal convolutional encoder-decoder that produces frequency-aware volumetric tokens for 3D medical imaging, enabling high-resolution vision-language modeling without excessive memory use. A three-stage training curriculum allows the model to learn from short slice excerpts yet handle scans over 300 slices, achieving state-of-the-art performance on report generation and text-to-CT synthesis tasks. The results demonstrate that precise three-dimensional tokenization is crucial for scalable vision-language models in CT imaging.<br /><strong>Summary (CN):</strong> 本文提出 BTB3D，一种因果卷积编码-解码结构，生成频率感知的体积令牌，以实现高分辨率的 3D 医学影像视觉‑语言建模且不增加显存开销。通过三阶段训练课程，模型能够从短切片学习并推广到超过 300 切片的 CT 扫描，在报告生成和文本到 CT 合成任务上均取得最新的性能提升。结果表明，精确的三维令牌化是可扩展视觉‑语言模型在 CT 成像中成功的关键因素。<br /><strong>Keywords:</strong> vision-language modeling, 3D medical imaging, CT, tokenization, causal convolution, report generation, text-to-CT synthesis<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</div>
Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding 300 slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent 512*512*241 volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D
<div><strong>Authors:</strong> Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BTB3D, a causal convolutional encoder-decoder that produces frequency-aware volumetric tokens for 3D medical imaging, enabling high-resolution vision-language modeling without excessive memory use. A three-stage training curriculum allows the model to learn from short slice excerpts yet handle scans over 300 slices, achieving state-of-the-art performance on report generation and text-to-CT synthesis tasks. The results demonstrate that precise three-dimensional tokenization is crucial for scalable vision-language models in CT imaging.", "summary_cn": "本文提出 BTB3D，一种因果卷积编码-解码结构，生成频率感知的体积令牌，以实现高分辨率的 3D 医学影像视觉‑语言建模且不增加显存开销。通过三阶段训练课程，模型能够从短切片学习并推广到超过 300 切片的 CT 扫描，在报告生成和文本到 CT 合成任务上均取得最新的性能提升。结果表明，精确的三维令牌化是可扩展视觉‑语言模型在 CT 成像中成功的关键因素。", "keywords": "vision-language modeling, 3D medical imaging, CT, tokenization, causal convolution, report generation, text-to-CT synthesis", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Dong Yang", "Pengfei Guo", "Marc Edgar", "Daguang Xu", "Bernhard Kainz", "Bjoern Menze"]}
]]></acme>

<pubDate>2025-10-23T15:13:13+00:00</pubDate>
</item>
<item>
<title>Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges</title>
<link>https://papers.cool/arxiv/2510.20634</link>
<guid>https://papers.cool/arxiv/2510.20634</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This systematic review examines 260 deep‑learning studies on dental image analysis, summarizing publicly available datasets, model architectures, training strategies, and evaluation metrics, and discusses current challenges and future research directions.<br /><strong>Summary (CN):</strong> 本文系统回顾了 260 篇关于牙科影像分析的深度学习研究，概述了公开数据集、模型结构、训练策略与评估指标，并讨论了当前挑战及未来研究方向。<br /><strong>Keywords:</strong> dental image analysis, deep learning, systematic review, medical imaging datasets, computer-aided diagnosis, AI in dentistry<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 5, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenhuan Zhou, Jingbo Zhu, Yuchen Zhang, Xiaohang Guan, Peng Wang, Tao Li</div>
Efficient analysis and processing of dental images are crucial for dentists to achieve accurate diagnosis and optimal treatment planning. However, dental imaging inherently poses several challenges, such as low contrast, metallic artifacts, and variations in projection angles. Combined with the subjectivity arising from differences in clinicians' expertise, manual interpretation often proves time-consuming and prone to inconsistency. Artificial intelligence (AI)-based automated dental image analysis (DIA) offers a promising solution to these issues and has become an integral part of computer-aided dental diagnosis and treatment. Among various AI technologies, deep learning (DL) stands out as the most widely applied and influential approach due to its superior feature extraction and representation capabilities. To comprehensively summarize recent progress in this field, we focus on the two fundamental aspects of DL research-datasets and models. In this paper, we systematically review 260 studies on DL applications in DIA, including 49 papers on publicly available dental datasets and 211 papers on DL-based algorithms. We first introduce the basic concepts of dental imaging and summarize the characteristics and acquisition methods of existing datasets. Then, we present the foundational techniques of DL and categorize relevant models and algorithms according to different DIA tasks, analyzing their network architectures, optimization strategies, training methods, and performance. Furthermore, we summarize commonly used training and evaluation metrics in the DIA domain. Finally, we discuss the current challenges of existing research and outline potential future directions. We hope that this work provides a valuable and systematic reference for researchers in this field. All supplementary materials and detailed comparison tables will be made publicly available on GitHub.
<div><strong>Authors:</strong> Zhenhuan Zhou, Jingbo Zhu, Yuchen Zhang, Xiaohang Guan, Peng Wang, Tao Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This systematic review examines 260 deep‑learning studies on dental image analysis, summarizing publicly available datasets, model architectures, training strategies, and evaluation metrics, and discusses current challenges and future research directions.", "summary_cn": "本文系统回顾了 260 篇关于牙科影像分析的深度学习研究，概述了公开数据集、模型结构、训练策略与评估指标，并讨论了当前挑战及未来研究方向。", "keywords": "dental image analysis, deep learning, systematic review, medical imaging datasets, computer-aided diagnosis, AI in dentistry", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenhuan Zhou", "Jingbo Zhu", "Yuchen Zhang", "Xiaohang Guan", "Peng Wang", "Tao Li"]}
]]></acme>

<pubDate>2025-10-23T15:05:06+00:00</pubDate>
</item>
<item>
<title>SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</title>
<link>https://papers.cool/arxiv/2510.20622</link>
<guid>https://papers.cool/arxiv/2510.20622</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SeViCES, a training‑free, model‑agnostic framework that selects informative video frames by combining a temporal‑aware semantic branch (LLM reasoning over captions) with a cluster‑guided visual branch aligned via mutual information, and refines answers through an answer‑consensus module. Extensive experiments on long video understanding benchmarks demonstrate that this consensus‑driven evidence selection improves accuracy and robustness of Video‑LLMs on lengthy videos.<br /><strong>Summary (CN):</strong> 本文提出了 SeViCES 框架，它通过（1）基于 LLM 对字幕进行推理的时序感知语义分支和（2）通过互信息对齐的聚类引导视觉分支，统一进行语义‑视觉证据共识以选择关键帧，并通过答案共识细化模块融合两种预测以消除不一致。实验表明，该方法在长视频理解基准上显著提升了 Video‑LLM 的准确性和鲁棒性。<br /><strong>Keywords:</strong> long video understanding, video-LLM, frame selection, semantic-visual consensus, evidence selection, mutual information, multimodal reasoning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He</div>
Long video understanding remains challenging due to its complex, diverse, and temporally scattered content. Although video large language models (Video-LLMs) can process videos lasting tens of minutes, applying them to truly long sequences is computationally prohibitive and often leads to unfocused or inconsistent reasoning. A promising solution is to select only the most informative frames, yet existing approaches typically ignore temporal dependencies or rely on unimodal evidence, limiting their ability to provide complete and query-relevant context. We propose a Semantic-Visual Consensus Evidence Selection (SeViCES) framework for effective and reliable long video understanding. SeViCES is training-free and model-agnostic, and introduces two key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module selects frames through (1) a temporal-aware semantic branch that leverages LLM reasoning over captions, and (2) a cluster-guided visual branch that aligns embeddings with semantic scores via mutual information. The Answer Consensus Refinement (ACR) module further resolves inconsistencies between semantic- and visual-based predictions by fusing evidence and constraining the answer space. Extensive experiments on long video understanding benchmarks show that SeViCES consistently outperforms state-of-the-art methods in both accuracy and robustness, demonstrating the importance of consensus-driven evidence selection for Video-LLMs.
<div><strong>Authors:</strong> Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SeViCES, a training‑free, model‑agnostic framework that selects informative video frames by combining a temporal‑aware semantic branch (LLM reasoning over captions) with a cluster‑guided visual branch aligned via mutual information, and refines answers through an answer‑consensus module. Extensive experiments on long video understanding benchmarks demonstrate that this consensus‑driven evidence selection improves accuracy and robustness of Video‑LLMs on lengthy videos.", "summary_cn": "本文提出了 SeViCES 框架，它通过（1）基于 LLM 对字幕进行推理的时序感知语义分支和（2）通过互信息对齐的聚类引导视觉分支，统一进行语义‑视觉证据共识以选择关键帧，并通过答案共识细化模块融合两种预测以消除不一致。实验表明，该方法在长视频理解基准上显著提升了 Video‑LLM 的准确性和鲁棒性。", "keywords": "long video understanding, video-LLM, frame selection, semantic-visual consensus, evidence selection, mutual information, multimodal reasoning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuan Sheng", "Yanbin Hao", "Chenxu Li", "Shuo Wang", "Xiangnan He"]}
]]></acme>

<pubDate>2025-10-23T14:55:28+00:00</pubDate>
</item>
<item>
<title>OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects</title>
<link>https://papers.cool/arxiv/2510.20605</link>
<guid>https://papers.cool/arxiv/2510.20605</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> OnlineSplatter is an online feed-forward framework that reconstructs free-moving objects from monocular video by directly generating object-centric 3D Gaussians without requiring camera pose, depth priors, or bundle adjustment. It uses a dual-key memory module to fuse current frame features with temporally aggregated object states, enabling constant computational cost while progressively refining the reconstruction. Experiments on real-world datasets show significant improvements over pose-free baselines, with performance increasing as more observations are added.<br /><strong>Summary (CN):</strong> OnlineSplatter 是一个在线前馈框架，能够在无需相机位姿、深度先验或束束优化的情况下，仅通过单目视频直接生成以对象为中心的 3D 高斯体，实现自由移动对象的重建。该方法采用双键记忆模块，将当前帧特征与时间聚合的对象状态进行融合，从而在保持计算成本恒定的同时逐步细化重建。真实数据集实验表明，相较于已有的无姿态重建基线，OnlineSplatter 在观测数量增加时持续提升性能。<br /><strong>Keywords:</strong> online 3D reconstruction, pose-free, Gaussian splatting, monocular video, memory module, object-centric representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mark He Huang, Lin Geng Foo, Christian Theobalt, Ying Sun, De Wen Soh</div>
Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.
<div><strong>Authors:</strong> Mark He Huang, Lin Geng Foo, Christian Theobalt, Ying Sun, De Wen Soh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "OnlineSplatter is an online feed-forward framework that reconstructs free-moving objects from monocular video by directly generating object-centric 3D Gaussians without requiring camera pose, depth priors, or bundle adjustment. It uses a dual-key memory module to fuse current frame features with temporally aggregated object states, enabling constant computational cost while progressively refining the reconstruction. Experiments on real-world datasets show significant improvements over pose-free baselines, with performance increasing as more observations are added.", "summary_cn": "OnlineSplatter 是一个在线前馈框架，能够在无需相机位姿、深度先验或束束优化的情况下，仅通过单目视频直接生成以对象为中心的 3D 高斯体，实现自由移动对象的重建。该方法采用双键记忆模块，将当前帧特征与时间聚合的对象状态进行融合，从而在保持计算成本恒定的同时逐步细化重建。真实数据集实验表明，相较于已有的无姿态重建基线，OnlineSplatter 在观测数量增加时持续提升性能。", "keywords": "online 3D reconstruction, pose-free, Gaussian splatting, monocular video, memory module, object-centric representation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mark He Huang", "Lin Geng Foo", "Christian Theobalt", "Ying Sun", "De Wen Soh"]}
]]></acme>

<pubDate>2025-10-23T14:37:25+00:00</pubDate>
</item>
<item>
<title>Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</title>
<link>https://papers.cool/arxiv/2510.20596</link>
<guid>https://papers.cool/arxiv/2510.20596</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes a novel unsupervised domain adaptation framework for cross-modality segmentation that leverages similarity‑based class‑wise prototypes. It learns prototypes in an embedding space with a similarity constraint to ensure intra‑class representativeness and inter‑class separability, and uses a dictionary to store prototypes from multiple images, avoiding class‑missing issues and enabling contrastive prototype learning. Extensive experiments demonstrate superior performance over state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> 本文提出一种基于相似性原型的跨模态分割无监督领域自适应框架。通过在嵌入空间学习类别原型并施加相似性约束，使原型对每个语义类别具有代表性且相互可分，同时使用字典存储不同图像提取的原型以防止类别缺失并实现原型的对比学习，从而提升分割性能。实验表明该方法优于现有最先进技术。<br /><strong>Keywords:</strong> unsupervised domain adaptation, cross-modality segmentation, similarity-based prototypes, contrastive learning, prototype dictionary, class-wise prototypes, domain shift, embedding space, segmentation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang</div>
Deep learning models have achieved great success on various vision challenges, but a well-trained model would face drastic performance degradation when applied to unseen data. Since the model is sensitive to domain shift, unsupervised domain adaptation attempts to reduce the domain gap and avoid costly annotation of unseen domains. This paper proposes a novel framework for cross-modality segmentation via similarity-based prototypes. In specific, we learn class-wise prototypes within an embedding space, then introduce a similarity constraint to make these prototypes representative for each semantic class while separable from different classes. Moreover, we use dictionaries to store prototypes extracted from different images, which prevents the class-missing problem and enables the contrastive learning of prototypes, and further improves performance. Extensive experiments show that our method achieves better results than other state-of-the-art methods.
<div><strong>Authors:</strong> Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes a novel unsupervised domain adaptation framework for cross-modality segmentation that leverages similarity‑based class‑wise prototypes. It learns prototypes in an embedding space with a similarity constraint to ensure intra‑class representativeness and inter‑class separability, and uses a dictionary to store prototypes from multiple images, avoiding class‑missing issues and enabling contrastive prototype learning. Extensive experiments demonstrate superior performance over state‑of‑the‑art methods.", "summary_cn": "本文提出一种基于相似性原型的跨模态分割无监督领域自适应框架。通过在嵌入空间学习类别原型并施加相似性约束，使原型对每个语义类别具有代表性且相互可分，同时使用字典存储不同图像提取的原型以防止类别缺失并实现原型的对比学习，从而提升分割性能。实验表明该方法优于现有最先进技术。", "keywords": "unsupervised domain adaptation, cross-modality segmentation, similarity-based prototypes, contrastive learning, prototype dictionary, class-wise prototypes, domain shift, embedding space, segmentation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ziyu Ye", "Chen Ju", "Chaofan Ma", "Xiaoyun Zhang"]}
]]></acme>

<pubDate>2025-10-23T14:24:12+00:00</pubDate>
</item>
<item>
<title>GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models</title>
<link>https://papers.cool/arxiv/2510.20586</link>
<guid>https://papers.cool/arxiv/2510.20586</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GenColorBench, a benchmark designed to evaluate the color precision of text-to-image generation models using extensive color systems such as ISCC-NBS and CSS3/X11. It provides 44K prompts covering over 400 colors and includes both perceptual and automated assessment methods, revealing performance variations across popular models. The benchmark aims to guide improvements in fine-grained color controllability for applications requiring accurate color representation.<br /><strong>Summary (CN):</strong> 本文提出 GenColorBench 基准，用于评估文本到图像生成模型的颜色精度，采用 ISCC-NBS 和 CSS3/X11 等丰富的颜色体系。基准包含 44K 条涉及 400 多种颜色的提示，并结合感知与自动评估方法，揭示了主流模型在颜色生成上的性能差异。此基准旨在推动在需要精确颜色表现的应用场景中提升模型的颜色可控性。<br /><strong>Keywords:</strong> color generation, text-to-image, benchmark, color controllability, ISCC-NBS, CSS3, perceptual assessment, automated evaluation, prompt engineering, visual fidelity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Muhammad Atif Butt, Alexandra Gomez-Villa, Tao Wu, Javier Vazquez-Corral, Joost Van De Weijer, Kai Wang</div>
Recent years have seen impressive advances in text-to-image generation, with image generative or unified models producing high-quality images from text. Yet these models still struggle with fine-grained color controllability, often failing to accurately match colors specified in text prompts. While existing benchmarks evaluate compositional reasoning and prompt adherence, none systematically assess color precision. Color is fundamental to human visual perception and communication, critical for applications from art to design workflows requiring brand consistency. However, current benchmarks either neglect color or rely on coarse assessments, missing key capabilities such as interpreting RGB values or aligning with human expectations. To this end, we propose GenColorBench, the first comprehensive benchmark for text-to-image color generation, grounded in color systems like ISCC-NBS and CSS3/X11, including numerical colors which are absent elsewhere. With 44K color-focused prompts covering 400+ colors, it reveals models' true capabilities via perceptual and automated assessments. Evaluations of popular text-to-image models using GenColorBench show performance variations, highlighting which color conventions models understand best and identifying failure modes. Our GenColorBench assessments will guide improvements in precise color generation. The benchmark will be made public upon acceptance.
<div><strong>Authors:</strong> Muhammad Atif Butt, Alexandra Gomez-Villa, Tao Wu, Javier Vazquez-Corral, Joost Van De Weijer, Kai Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GenColorBench, a benchmark designed to evaluate the color precision of text-to-image generation models using extensive color systems such as ISCC-NBS and CSS3/X11. It provides 44K prompts covering over 400 colors and includes both perceptual and automated assessment methods, revealing performance variations across popular models. The benchmark aims to guide improvements in fine-grained color controllability for applications requiring accurate color representation.", "summary_cn": "本文提出 GenColorBench 基准，用于评估文本到图像生成模型的颜色精度，采用 ISCC-NBS 和 CSS3/X11 等丰富的颜色体系。基准包含 44K 条涉及 400 多种颜色的提示，并结合感知与自动评估方法，揭示了主流模型在颜色生成上的性能差异。此基准旨在推动在需要精确颜色表现的应用场景中提升模型的颜色可控性。", "keywords": "color generation, text-to-image, benchmark, color controllability, ISCC-NBS, CSS3, perceptual assessment, automated evaluation, prompt engineering, visual fidelity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Muhammad Atif Butt", "Alexandra Gomez-Villa", "Tao Wu", "Javier Vazquez-Corral", "Joost Van De Weijer", "Kai Wang"]}
]]></acme>

<pubDate>2025-10-23T14:12:55+00:00</pubDate>
</item>
<item>
<title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
<link>https://papers.cool/arxiv/2510.20579</link>
<guid>https://papers.cool/arxiv/2510.20579</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Open-o3 Video, a framework that generates video reasoning with explicit spatio-temporal evidence, highlighting key timestamps, objects, and bounding boxes alongside its answers. It curates two high-quality datasets with unified spatio-temporal annotations and employs a cold-start reinforcement learning strategy with rewards for answer accuracy, temporal alignment, and spatial precision, achieving state-of-the-art performance on V-STAR and multiple video understanding benchmarks. Additionally, the generated reasoning traces enable confidence-aware verification and improve answer reliability.<br /><strong>Summary (CN):</strong> 本文提出 Open-o3 Video 框架，能够在视频推理过程中提供显式的时空证据，突出关键时间戳、对象及其边界框并与答案一起展示。作者构建了两套统一时空标注的高质量数据集，并采用冷启动强化学习策略，结合答案准确性、时间对齐和空间精度的奖励，显著提升了 V-STAR 等多项视频理解基准的表现。此外，生成的推理轨迹可用于置信度感知的验证，提升答案可靠性。<br /><strong>Keywords:</strong> video reasoning, spatio-temporal evidence, grounded explanations, reinforcement learning, dataset, Open-o3 Video, visual grounding<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</div>
Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.
<div><strong>Authors:</strong> Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Open-o3 Video, a framework that generates video reasoning with explicit spatio-temporal evidence, highlighting key timestamps, objects, and bounding boxes alongside its answers. It curates two high-quality datasets with unified spatio-temporal annotations and employs a cold-start reinforcement learning strategy with rewards for answer accuracy, temporal alignment, and spatial precision, achieving state-of-the-art performance on V-STAR and multiple video understanding benchmarks. Additionally, the generated reasoning traces enable confidence-aware verification and improve answer reliability.", "summary_cn": "本文提出 Open-o3 Video 框架，能够在视频推理过程中提供显式的时空证据，突出关键时间戳、对象及其边界框并与答案一起展示。作者构建了两套统一时空标注的高质量数据集，并采用冷启动强化学习策略，结合答案准确性、时间对齐和空间精度的奖励，显著提升了 V-STAR 等多项视频理解基准的表现。此外，生成的推理轨迹可用于置信度感知的验证，提升答案可靠性。", "keywords": "video reasoning, spatio-temporal evidence, grounded explanations, reinforcement learning, dataset, Open-o3 Video, visual grounding", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jiahao Meng", "Xiangtai Li", "Haochen Wang", "Yue Tan", "Tao Zhang", "Lingdong Kong", "Yunhai Tong", "Anran Wang", "Zhiyang Teng", "Yujing Wang", "Zhuochen Wang"]}
]]></acme>

<pubDate>2025-10-23T14:05:56+00:00</pubDate>
</item>
<item>
<title>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</title>
<link>https://papers.cool/arxiv/2510.20578</link>
<guid>https://papers.cool/arxiv/2510.20578</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> EmbodiedBrain is a vision-language foundation model (7B/32B) designed for embodied AI agents, featuring an agent-aligned data structure and a novel Step‑Augmented Group Relative Policy Optimization (Step‑GRPO) training pipeline that incorporates preceding steps as guided precursors. The paper introduces a comprehensive reward system with a Generative Reward Model and a three‑part evaluation suite covering general, planning, and end‑to‑end simulation benchmarks, demonstrating state‑of‑the‑art performance on a newly released challenging simulation environment.<br /><strong>Summary (CN):</strong> EmbodiedBrain 是一种面向具身 AI 代理的视觉语言基础模型（7B/32B），采用面向代理的数据结构和新颖的 Step‑Augmented Group Relative Policy Optimization（Step‑GRPO）训练流程，将前置步骤作为引导前体来提升长程任务成功率。论文还引入了包含生成式奖励模型的综合奖励系统，并构建了包括通用、规划和端到端仿真三部分的评估套件，在新发布的挑战性仿真环境中展示了最先进的性能。<br /><strong>Keywords:</strong> Embodied AI, task planning, vision-language model, multimodal LLM, Step-GRPO, Generative Reward Model, simulation benchmark, embodied foundation model<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu</div>
The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at https://zterobot.github.io/EmbodiedBrain.github.io.
<div><strong>Authors:</strong> Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "EmbodiedBrain is a vision-language foundation model (7B/32B) designed for embodied AI agents, featuring an agent-aligned data structure and a novel Step‑Augmented Group Relative Policy Optimization (Step‑GRPO) training pipeline that incorporates preceding steps as guided precursors. The paper introduces a comprehensive reward system with a Generative Reward Model and a three‑part evaluation suite covering general, planning, and end‑to‑end simulation benchmarks, demonstrating state‑of‑the‑art performance on a newly released challenging simulation environment.", "summary_cn": "EmbodiedBrain 是一种面向具身 AI 代理的视觉语言基础模型（7B/32B），采用面向代理的数据结构和新颖的 Step‑Augmented Group Relative Policy Optimization（Step‑GRPO）训练流程，将前置步骤作为引导前体来提升长程任务成功率。论文还引入了包含生成式奖励模型的综合奖励系统，并构建了包括通用、规划和端到端仿真三部分的评估套件，在新发布的挑战性仿真环境中展示了最先进的性能。", "keywords": "Embodied AI, task planning, vision-language model, multimodal LLM, Step-GRPO, Generative Reward Model, simulation benchmark, embodied foundation model", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Ding Zou", "Feifan Wang", "Mengyu Ge", "Siyuan Fan", "Zongbing Zhang", "Wei Chen", "Lingfeng Wang", "Zhongyou Hu", "Wenrui Yan", "Zhengwei Gao", "Hao Wang", "Weizhao Jin", "Yu Zhang", "Hainan Zhao", "Mingliang Zhang", "Xianxian Xi", "Yaru Zhang", "Wenyuan Li", "Zhengguang Gao", "Yurui Zhu"]}
]]></acme>

<pubDate>2025-10-23T14:05:55+00:00</pubDate>
</item>
<item>
<title>From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail</title>
<link>https://papers.cool/arxiv/2510.20558</link>
<guid>https://papers.cool/arxiv/2510.20558</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how users perceive visual quality of crowd character representations at varying levels of detail (LoD) and viewing distances. It compares geometric meshes, image‑based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, providing both qualitative and quantitative insights to inform perceptually optimized LoD strategies for crowd rendering.<br /><strong>Summary (CN):</strong> 本文调查了用户在不同细节层次（LoD）和观看距离下对人群角色表现的视觉质量感知。通过比较几何网格、基于图像的 impostor、NeRF（神经辐射场）和 3D Gaussian 等表示方式，提供了定性和定量结果，为人群渲染的感知优化 LoD 策略提供指导。<br /><strong>Keywords:</strong> crowd rendering, level of detail, LoD, Neural Radiance Fields, 3D Gaussians, impostors, perceptual evaluation, visual fidelity, performance trade-offs<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 1, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaohan Sun, Carol O'Sullivan</div>
In this paper, we investigate how users perceive the visual quality of crowd character representations at different levels of detail (LoD) and viewing distances. Each representation: geometric meshes, image-based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between visual fidelity and computational performance. Our qualitative and quantitative results provide insights to guide the design of perceptually optimized LoD strategies for crowd rendering.
<div><strong>Authors:</strong> Xiaohan Sun, Carol O'Sullivan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how users perceive visual quality of crowd character representations at varying levels of detail (LoD) and viewing distances. It compares geometric meshes, image‑based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, providing both qualitative and quantitative insights to inform perceptually optimized LoD strategies for crowd rendering.", "summary_cn": "本文调查了用户在不同细节层次（LoD）和观看距离下对人群角色表现的视觉质量感知。通过比较几何网格、基于图像的 impostor、NeRF（神经辐射场）和 3D Gaussian 等表示方式，提供了定性和定量结果，为人群渲染的感知优化 LoD 策略提供指导。", "keywords": "crowd rendering, level of detail, LoD, Neural Radiance Fields, 3D Gaussians, impostors, perceptual evaluation, visual fidelity, performance trade-offs", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaohan Sun", "Carol O'Sullivan"]}
]]></acme>

<pubDate>2025-10-23T13:39:18+00:00</pubDate>
</item>
<item>
<title>From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging</title>
<link>https://papers.cool/arxiv/2510.20550</link>
<guid>https://papers.cool/arxiv/2510.20550</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ACamera-Net, a lightweight neural network that predicts optimal exposure (ISO) and white‑balance parameters directly from RAW images to improve image quality under challenging lighting conditions. It consists of two modules—ACamera-Exposure and ACamera-Color—and is designed for real‑time inference on edge devices, showing consistent performance gains over traditional auto‑exposure modes without additional enhancement steps.<br /><strong>Summary (CN):</strong> 本文提出了 ACamera‑Net，一种轻量级神经网络，可直接从 RAW 图像预测最佳曝光 (ISO) 和白平衡参数，以提升在低光、高动态范围等复杂光照下的成像质量。系统包含曝光估计模块和色温/增益预测模块，针对边缘设备实现实时推理，并在无需额外增强模块的情况下，相比传统自动模式显著提升图像质量和感知稳定性。<br /><strong>Keywords:</strong> camera parameter adjustment, exposure estimation, white balance, adaptive imaging, lightweight neural network, RAW image processing, edge inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fuchen Li, Yansong Du, Wenbo Cheng, Xiaoxia Zhou, Sen Yin</div>
Consumer-grade camera systems often struggle to maintain stable image quality under complex illumination conditions such as low light, high dynamic range, and backlighting, as well as spatial color temperature variation. These issues lead to underexposure, color casts, and tonal inconsistency, which degrade the performance of downstream vision tasks. To address this, we propose ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment network that directly predicts optimal exposure and white balance from RAW inputs. The framework consists of two modules: ACamera-Exposure, which estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color, which predicts correlated color temperature and gain factors for improved color consistency. Optimized for real-time inference on edge devices, ACamera-Net can be seamlessly integrated into imaging pipelines. Trained on diverse real-world data with annotated references, the model generalizes well across lighting conditions. Extensive experiments demonstrate that ACamera-Net consistently enhances image quality and stabilizes perception outputs, outperforming conventional auto modes and lightweight baselines without relying on additional image enhancement modules.
<div><strong>Authors:</strong> Fuchen Li, Yansong Du, Wenbo Cheng, Xiaoxia Zhou, Sen Yin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ACamera-Net, a lightweight neural network that predicts optimal exposure (ISO) and white‑balance parameters directly from RAW images to improve image quality under challenging lighting conditions. It consists of two modules—ACamera-Exposure and ACamera-Color—and is designed for real‑time inference on edge devices, showing consistent performance gains over traditional auto‑exposure modes without additional enhancement steps.", "summary_cn": "本文提出了 ACamera‑Net，一种轻量级神经网络，可直接从 RAW 图像预测最佳曝光 (ISO) 和白平衡参数，以提升在低光、高动态范围等复杂光照下的成像质量。系统包含曝光估计模块和色温/增益预测模块，针对边缘设备实现实时推理，并在无需额外增强模块的情况下，相比传统自动模式显著提升图像质量和感知稳定性。", "keywords": "camera parameter adjustment, exposure estimation, white balance, adaptive imaging, lightweight neural network, RAW image processing, edge inference", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fuchen Li", "Yansong Du", "Wenbo Cheng", "Xiaoxia Zhou", "Sen Yin"]}
]]></acme>

<pubDate>2025-10-23T13:35:17+00:00</pubDate>
</item>
<item>
<title>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</title>
<link>https://papers.cool/arxiv/2510.20549</link>
<guid>https://papers.cool/arxiv/2510.20549</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SELM-SLAM3, a deep learning‑enhanced visual SLAM framework that incorporates SuperPoint and LightGlue to improve feature extraction and matching under challenging conditions such as low texture, motion blur, and poor lighting. Experiments on TUM RGB‑D, ICL‑NUIM, and TartanAir datasets show that SELM‑SLAM3 outperforms ORB‑SLAM3 by an average of 87.84% and surpasses state‑of‑the‑art RGB‑D SLAM systems by 36.77%, demonstrating its suitability for reliable navigation assistance for visually impaired users.<br /><strong>Summary (CN):</strong> 本文提出 SELM‑SLAM3，一种融合 SuperPoint 与 LightGlue 的深度学习驱动视觉 SLAM 框架，旨在在低纹理、运动模糊和光照不佳等困难场景下提升特征提取和匹配的鲁棒性。通过在 TUM RGB‑D、ICL‑NUIM 与 TartanAir 数据集上的实验，SELM‑SLAM3 的定位精度比 ORB‑SLAM3 平均提升 87.84%，并超越已有最先进的 RGB‑D SLAM 系统 36.77%，为视障人士的导航辅助提供了更可靠的技术平台。<br /><strong>Keywords:</strong> visual SLAM, deep learning, SuperPoint, LightGlue, assistive navigation, visually impaired, robustness, localization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Marziyeh Bamdad, Hans-Peter Hutter, Alireza Darvishy</div>
Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge. Such conditions are common in applications such as assistive navigation for the visually impaired. These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety. To overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced visual SLAM framework that integrates SuperPoint and LightGlue for robust feature extraction and matching. We evaluated our framework using TUM RGB-D, ICL-NUIM, and TartanAir datasets, which feature diverse and challenging scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of 87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework demonstrates enhanced performance under challenging conditions, such as low-texture scenes and fast motion, providing a reliable platform for developing navigation aids for the visually impaired.
<div><strong>Authors:</strong> Marziyeh Bamdad, Hans-Peter Hutter, Alireza Darvishy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SELM-SLAM3, a deep learning‑enhanced visual SLAM framework that incorporates SuperPoint and LightGlue to improve feature extraction and matching under challenging conditions such as low texture, motion blur, and poor lighting. Experiments on TUM RGB‑D, ICL‑NUIM, and TartanAir datasets show that SELM‑SLAM3 outperforms ORB‑SLAM3 by an average of 87.84% and surpasses state‑of‑the‑art RGB‑D SLAM systems by 36.77%, demonstrating its suitability for reliable navigation assistance for visually impaired users.", "summary_cn": "本文提出 SELM‑SLAM3，一种融合 SuperPoint 与 LightGlue 的深度学习驱动视觉 SLAM 框架，旨在在低纹理、运动模糊和光照不佳等困难场景下提升特征提取和匹配的鲁棒性。通过在 TUM RGB‑D、ICL‑NUIM 与 TartanAir 数据集上的实验，SELM‑SLAM3 的定位精度比 ORB‑SLAM3 平均提升 87.84%，并超越已有最先进的 RGB‑D SLAM 系统 36.77%，为视障人士的导航辅助提供了更可靠的技术平台。", "keywords": "visual SLAM, deep learning, SuperPoint, LightGlue, assistive navigation, visually impaired, robustness, localization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Marziyeh Bamdad", "Hans-Peter Hutter", "Alireza Darvishy"]}
]]></acme>

<pubDate>2025-10-23T13:35:12+00:00</pubDate>
</item>
<item>
<title>Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image</title>
<link>https://papers.cool/arxiv/2510.20539</link>
<guid>https://papers.cool/arxiv/2510.20539</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a deep learning framework that jointly estimates the latent sharp image and the underlying 3D camera motion trajectory from a single motion-blurred photograph. Leveraging a differentiable Projective Motion Blur Model, the system provides interpretability by revealing the blur-causing motion and enables reconstruction of the sequence of sharp frames, with post‑inference trajectory refinement via a reblur loss.<br /><strong>Summary (CN):</strong> 本文提出一个深度学习框架，能够从单张运动模糊图像中同时估计清晰图像和相机的 3D 旋转轨迹。该方法基于可微分的投影运动模糊模型，通过后处理的重模糊损失优化轨迹，实现对模糊成因的可解释化以及序列锐化图像的重建。<br /><strong>Keywords:</strong> blind deblurring, camera motion trajectory, projective motion blur model, differentiable blur module, deep learning, image restoration, motion blur estimation, reblur loss, trajectory reconstruction<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Guillermo Carbajal, Andrés Almansa, Pablo Musé</div>
Motion blur caused by camera shake, particularly under large or rotational movements, remains a major challenge in image restoration. We propose a deep learning framework that jointly estimates the latent sharp image and the underlying camera motion trajectory from a single blurry image. Our method leverages the Projective Motion Blur Model (PMBM), implemented efficiently using a differentiable blur creation module compatible with modern networks. A neural network predicts a full 3D rotation trajectory, which guides a model-based restoration network trained end-to-end. This modular architecture provides interpretability by revealing the camera motion that produced the blur. Moreover, this trajectory enables the reconstruction of the sequence of sharp images that generated the observed blurry image. To further refine results, we optimize the trajectory post-inference via a reblur loss, improving consistency between the blurry input and the restored output. Extensive experiments show that our method achieves state-of-the-art performance on both synthetic and real datasets, particularly in cases with severe or spatially variant blur, where end-to-end deblurring networks struggle. Code and trained models are available at https://github.com/GuillermoCarbajal/Blur2Seq/
<div><strong>Authors:</strong> Guillermo Carbajal, Andrés Almansa, Pablo Musé</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a deep learning framework that jointly estimates the latent sharp image and the underlying 3D camera motion trajectory from a single motion-blurred photograph. Leveraging a differentiable Projective Motion Blur Model, the system provides interpretability by revealing the blur-causing motion and enables reconstruction of the sequence of sharp frames, with post‑inference trajectory refinement via a reblur loss.", "summary_cn": "本文提出一个深度学习框架，能够从单张运动模糊图像中同时估计清晰图像和相机的 3D 旋转轨迹。该方法基于可微分的投影运动模糊模型，通过后处理的重模糊损失优化轨迹，实现对模糊成因的可解释化以及序列锐化图像的重建。", "keywords": "blind deblurring, camera motion trajectory, projective motion blur model, differentiable blur module, deep learning, image restoration, motion blur estimation, reblur loss, trajectory reconstruction", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guillermo Carbajal", "Andrés Almansa", "Pablo Musé"]}
]]></acme>

<pubDate>2025-10-23T13:26:07+00:00</pubDate>
</item>
<item>
<title>Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis</title>
<link>https://papers.cool/arxiv/2510.20531</link>
<guid>https://papers.cool/arxiv/2510.20531</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Fake-in-Facext (FiFa) framework to provide fine‑grained, explainable DeepFake analysis by defining a Facial Image Concept Tree (FICT) for reliable annotation and proposing the Artifact‑Grounding Explanation (AGE) task that outputs textual forgery explanations together with segmentation masks of manipulated artifacts. FiFa‑MLLM, a unified multi‑task multimodal LLM architecture, leverages this annotated data and auxiliary supervision to support diverse multimodal inputs/outputs and achieves state‑of‑the‑art performance on AGE and existing XDFA benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了Fake-in-Facext（FiFa）框架，通过构建面部图像概念树（FICT）实现细粒度、可靠的伪造标注，并引入Artifact‑Grounding Explanation（AGE）任务，使文本伪造解释与操作伪造区域的分割掩码交叉输出。基于此数据，FiFa‑MLLM 采用统一的多任务多模态大语言模型结构，支持丰富的输入输出方式，在AGE任务以及现有可解释深度伪造分析（XDFA）数据集上实现了最先进的性能。<br /><strong>Keywords:</strong> deepfake detection, explainable AI, multimodal large language models, artifact grounding, facial image concept tree, segmentation masks, fine-grained analysis, multi-task learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - interpretability<br /><strong>Authors:</strong> Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu</div>
The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.
<div><strong>Authors:</strong> Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Fake-in-Facext (FiFa) framework to provide fine‑grained, explainable DeepFake analysis by defining a Facial Image Concept Tree (FICT) for reliable annotation and proposing the Artifact‑Grounding Explanation (AGE) task that outputs textual forgery explanations together with segmentation masks of manipulated artifacts. FiFa‑MLLM, a unified multi‑task multimodal LLM architecture, leverages this annotated data and auxiliary supervision to support diverse multimodal inputs/outputs and achieves state‑of‑the‑art performance on AGE and existing XDFA benchmarks.", "summary_cn": "本文提出了Fake-in-Facext（FiFa）框架，通过构建面部图像概念树（FICT）实现细粒度、可靠的伪造标注，并引入Artifact‑Grounding Explanation（AGE）任务，使文本伪造解释与操作伪造区域的分割掩码交叉输出。基于此数据，FiFa‑MLLM 采用统一的多任务多模态大语言模型结构，支持丰富的输入输出方式，在AGE任务以及现有可解释深度伪造分析（XDFA）数据集上实现了最先进的性能。", "keywords": "deepfake detection, explainable AI, multimodal large language models, artifact grounding, facial image concept tree, segmentation masks, fine-grained analysis, multi-task learning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Lixiong Qin", "Yang Zhang", "Mei Wang", "Jiani Hu", "Weihong Deng", "Weiran Xu"]}
]]></acme>

<pubDate>2025-10-23T13:16:12+00:00</pubDate>
</item>
<item>
<title>Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</title>
<link>https://papers.cool/arxiv/2510.20519</link>
<guid>https://papers.cool/arxiv/2510.20519</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Metis-HOME introduces a Hybrid Optimized Mixture-of-Experts framework for multimodal large models, separating a dense model into a thinking branch for complex multi-step reasoning and a non-thinking branch for fast direct inference. A lightweight trainable router dynamically assigns inputs to the appropriate expert, improving both complex reasoning performance and general capabilities while reducing unnecessary computation. Experiments with Qwen2.5-VL-7B show that this hybrid approach mitigates the typical trade‑off between reasoning ability and generalization in multimodal systems.<br /><strong>Summary (CN):</strong> Metis-HOME 提出一种混合优化的专家混合（Mixture‑of‑Experts）框架，将原本的密集模型划分为思考分支（用于复杂多步骤推理）和非思考分支（用于快速直接推断，如通用 VQA 与 OCR）。轻量可训练的路由器能够动态将查询分配至最合适的专家，从而提升复杂推理能力的同时改善模型的通用性能，避免了仅专注推理导致的泛化退化。基于 Qwen2.5-VL-7B 的实验表明，该混合思考范式在多模态系统中有效平衡了推理与通用性的权衡。<br /><strong>Keywords:</strong> multimodal reasoning, mixture of experts, hybrid thinking, dynamic router, Qwen2.5-VL, VQA, OCR, efficient inference, generalization<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma</div>
Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma.
<div><strong>Authors:</strong> Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Metis-HOME introduces a Hybrid Optimized Mixture-of-Experts framework for multimodal large models, separating a dense model into a thinking branch for complex multi-step reasoning and a non-thinking branch for fast direct inference. A lightweight trainable router dynamically assigns inputs to the appropriate expert, improving both complex reasoning performance and general capabilities while reducing unnecessary computation. Experiments with Qwen2.5-VL-7B show that this hybrid approach mitigates the typical trade‑off between reasoning ability and generalization in multimodal systems.", "summary_cn": "Metis-HOME 提出一种混合优化的专家混合（Mixture‑of‑Experts）框架，将原本的密集模型划分为思考分支（用于复杂多步骤推理）和非思考分支（用于快速直接推断，如通用 VQA 与 OCR）。轻量可训练的路由器能够动态将查询分配至最合适的专家，从而提升复杂推理能力的同时改善模型的通用性能，避免了仅专注推理导致的泛化退化。基于 Qwen2.5-VL-7B 的实验表明，该混合思考范式在多模态系统中有效平衡了推理与通用性的权衡。", "keywords": "multimodal reasoning, mixture of experts, hybrid thinking, dynamic router, Qwen2.5-VL, VQA, OCR, efficient inference, generalization", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xiaohan Lan", "Fanfan Liu", "Haibo Qiu", "Siqi Yang", "Delian Ruan", "Peng Shi", "Lin Ma"]}
]]></acme>

<pubDate>2025-10-23T13:02:49+00:00</pubDate>
</item>
<item>
<title>EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization</title>
<link>https://papers.cool/arxiv/2510.20512</link>
<guid>https://papers.cool/arxiv/2510.20512</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces EchoDistill, a bidirectional concept distillation framework that jointly trains a multi-step teacher diffusion model and a one-step student model to enable rapid personalization of text-to-image diffusion models. By sharing the text encoder and employing adversarial and alignment losses, the student learns novel concepts in a single step, while feedback from the student also refines the teacher, achieving superior personalization performance.<br /><strong>Summary (CN):</strong> 本文提出 EchoDistill，一种双向概念蒸馏框架，通过同步训练多步教师扩散模型和一步学生模型，实现文本到图像扩散模型的快速个性化。两者共享文本编码器，并使用对抗损失和对齐损失，使学生在单步生成中学习新概念，同时学生的反馈也优化教师模型，从而在个性化任务中取得显著提升。<br /><strong>Keywords:</strong> diffusion personalization, one-step diffusion, concept distillation, bidirectional distillation, text-to-image generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang</div>
Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.
<div><strong>Authors:</strong> Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces EchoDistill, a bidirectional concept distillation framework that jointly trains a multi-step teacher diffusion model and a one-step student model to enable rapid personalization of text-to-image diffusion models. By sharing the text encoder and employing adversarial and alignment losses, the student learns novel concepts in a single step, while feedback from the student also refines the teacher, achieving superior personalization performance.", "summary_cn": "本文提出 EchoDistill，一种双向概念蒸馏框架，通过同步训练多步教师扩散模型和一步学生模型，实现文本到图像扩散模型的快速个性化。两者共享文本编码器，并使用对抗损失和对齐损失，使学生在单步生成中学习新概念，同时学生的反馈也优化教师模型，从而在个性化任务中取得显著提升。", "keywords": "diffusion personalization, one-step diffusion, concept distillation, bidirectional distillation, text-to-image generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yixiong Yang", "Tao Wu", "Senmao Li", "Shiqi Yang", "Yaxing Wang", "Joost van de Weijer", "Kai Wang"]}
]]></acme>

<pubDate>2025-10-23T12:56:33+00:00</pubDate>
</item>
<item>
<title>Reliable and Reproducible Demographic Inference for Fairness in Face Analysis</title>
<link>https://papers.cool/arxiv/2510.20482</link>
<guid>https://papers.cool/arxiv/2510.20482</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fully reproducible demographic attribute inference pipeline for fairness auditing in face analysis systems, using modular transfer learning with pretrained face recognition encoders and nonlinear classification heads. It defines a robustness metric based on intra‑identity consistency and shows improved accuracy and lower bias, especially for ethnicity inference, across multiple datasets. The authors release code, pretrained models, and evaluation tools to promote transparency and reproducibility.<br /><strong>Summary (CN):</strong> 本文提出了一套可完全复现的人口属性推断流水线，用于人脸分析系统的公平性审计，该流水线采用预训练人脸识别编码器加非线性分类头的模块化迁移学习方法。文中定义了基于同一身份内部一致性的鲁棒性度量，并在多个数据集上展示了尤其在种族属性推断上更高的准确率和更低的偏差。作者公开了数据集元信息、代码、预训练模型以及评估工具，以提升透明度和可复现性。<br /><strong>Keywords:</strong> demographic inference, fairness auditing, face analysis, transfer learning, robustness metric, intra-identity consistency, ethnicity classification, gender classification, reproducibility<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Alexandre Fournier-Montgieux, Hervé Le Borgne, Adrian Popescu, Bertrand Luvison</div>
Fairness evaluation in face analysis systems (FAS) typically depends on automatic demographic attribute inference (DAI), which itself relies on predefined demographic segmentation. However, the validity of fairness auditing hinges on the reliability of the DAI process. We begin by providing a theoretical motivation for this dependency, showing that improved DAI reliability leads to less biased and lower-variance estimates of FAS fairness. To address this, we propose a fully reproducible DAI pipeline that replaces conventional end-to-end training with a modular transfer learning approach. Our design integrates pretrained face recognition encoders with non-linear classification heads. We audit this pipeline across three dimensions: accuracy, fairness, and a newly introduced notion of robustness, defined via intra-identity consistency. The proposed robustness metric is applicable to any demographic segmentation scheme. We benchmark the pipeline on gender and ethnicity inference across multiple datasets and training setups. Our results show that the proposed method outperforms strong baselines, particularly on ethnicity, which is the more challenging attribute. To promote transparency and reproducibility, we will publicly release the training dataset metadata, full codebase, pretrained models, and evaluation toolkit. This work contributes a reliable foundation for demographic inference in fairness auditing.
<div><strong>Authors:</strong> Alexandre Fournier-Montgieux, Hervé Le Borgne, Adrian Popescu, Bertrand Luvison</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fully reproducible demographic attribute inference pipeline for fairness auditing in face analysis systems, using modular transfer learning with pretrained face recognition encoders and nonlinear classification heads. It defines a robustness metric based on intra‑identity consistency and shows improved accuracy and lower bias, especially for ethnicity inference, across multiple datasets. The authors release code, pretrained models, and evaluation tools to promote transparency and reproducibility.", "summary_cn": "本文提出了一套可完全复现的人口属性推断流水线，用于人脸分析系统的公平性审计，该流水线采用预训练人脸识别编码器加非线性分类头的模块化迁移学习方法。文中定义了基于同一身份内部一致性的鲁棒性度量，并在多个数据集上展示了尤其在种族属性推断上更高的准确率和更低的偏差。作者公开了数据集元信息、代码、预训练模型以及评估工具，以提升透明度和可复现性。", "keywords": "demographic inference, fairness auditing, face analysis, transfer learning, robustness metric, intra-identity consistency, ethnicity classification, gender classification, reproducibility", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Alexandre Fournier-Montgieux", "Hervé Le Borgne", "Adrian Popescu", "Bertrand Luvison"]}
]]></acme>

<pubDate>2025-10-23T12:22:02+00:00</pubDate>
</item>
<item>
<title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
<link>https://papers.cool/arxiv/2510.20470</link>
<guid>https://papers.cool/arxiv/2510.20470</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Conan introduces an evidence‑grounded, multi‑step video reasoning framework that jointly learns to identify relevant frames, reason over cross‑frame clues, and decide when to act, using a large Conan‑91K dataset of automatically generated reasoning traces and an Identification‑Reasoning‑Action RLVR training scheme. The progressive cold‑start strategy enables the model to surpass strong baselines by over 10% accuracy on several multi‑step reasoning benchmarks and generalize to long‑video understanding tasks. Experiments demonstrate the system’s scalability and robustness compared with prior text‑only chain or frame‑retrieval methods.<br /><strong>Summary (CN):</strong> Conan 提出一个基于证据的多步骤视频推理框架，能够同时学习识别相关帧、跨帧推理线索以及决定何时采取行动，使用大规模的 Conan‑91K 数据集（包括帧识别、证据推理和行动决策的自动生成推理轨迹），并采用 Identification‑Reasoning‑Action (AIR) 强化学习训练方案。通过渐进式冷启动策略，模型在多个多步骤推理基准上相较于强基线提升超过 10% 的准确率，并在长视频理解任务中表现出良好的可扩展性和鲁棒性。实验表明该系统在证据定位和推理方面优于仅文本链或单帧检索的已有方法。<br /><strong>Keywords:</strong> video reasoning, multimodal LLM, evidence grounding, reinforcement learning, progressive learning, Conan-91K dataset, multi-step deduction, visual evidence, AIR framework, long-video understanding<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun</div>
Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness.
<div><strong>Authors:</strong> Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Conan introduces an evidence‑grounded, multi‑step video reasoning framework that jointly learns to identify relevant frames, reason over cross‑frame clues, and decide when to act, using a large Conan‑91K dataset of automatically generated reasoning traces and an Identification‑Reasoning‑Action RLVR training scheme. The progressive cold‑start strategy enables the model to surpass strong baselines by over 10% accuracy on several multi‑step reasoning benchmarks and generalize to long‑video understanding tasks. Experiments demonstrate the system’s scalability and robustness compared with prior text‑only chain or frame‑retrieval methods.", "summary_cn": "Conan 提出一个基于证据的多步骤视频推理框架，能够同时学习识别相关帧、跨帧推理线索以及决定何时采取行动，使用大规模的 Conan‑91K 数据集（包括帧识别、证据推理和行动决策的自动生成推理轨迹），并采用 Identification‑Reasoning‑Action (AIR) 强化学习训练方案。通过渐进式冷启动策略，模型在多个多步骤推理基准上相较于强基线提升超过 10% 的准确率，并在长视频理解任务中表现出良好的可扩展性和鲁棒性。实验表明该系统在证据定位和推理方面优于仅文本链或单帧检索的已有方法。", "keywords": "video reasoning, multimodal LLM, evidence grounding, reinforcement learning, progressive learning, Conan-91K dataset, multi-step deduction, visual evidence, AIR framework, long-video understanding", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Kun Ouyang", "Yuanxin Liu", "Linli Yao", "Yishuo Cai", "Hao Zhou", "Jie Zhou", "Fandong Meng", "Xu Sun"]}
]]></acme>

<pubDate>2025-10-23T12:11:46+00:00</pubDate>
</item>
<item>
<title>Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment</title>
<link>https://papers.cool/arxiv/2510.20438</link>
<guid>https://papers.cool/arxiv/2510.20438</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FuzzyDistillViT-MobileNet, a knowledge distillation framework that uses fuzzy‑logic‑driven dynamic weighting to transfer knowledge from a Vision Transformer instructor to a MobileNet student for lung cancer classification, combined with pixel‑level image fusion and genetic‑algorithm‑based model selection, achieving over 99% accuracy on histopathology and CT datasets. It emphasizes handling uncertainty by adjusting distillation weights to focus on high‑confidence regions while reducing attention to ambiguous areas, and includes image preprocessing enhancements such as gamma correction and histogram equalization. Experimental results demonstrate high accuracy and robustness across two lung cancer imaging domains.<br /><strong>Summary (CN):</strong> 本文提出 FuzzyDistillViT-MobileNet 框架，利用模糊逻辑驱动的动态权重调整将 Vision Transformer (ViT) 作为教师模型的知识蒸馏到 MobileNet 学生模型，用于肺癌分类，并结合像素级图像融合和遗传算法模型选择，在组织病理学和 CT 数据集上实现超过 99% 的准确率。该方法通过在蒸馏过程中动态调整权重，使学生模型聚焦高置信度区域，降低对模糊区域的关注，同时加入伽马校正和直方图均衡等图像预处理技术。实验显示该方法在两个肺癌影像数据集上具备高准确性和鲁棒性。<br /><strong>Keywords:</strong> dynamic knowledge distillation, fuzzy logic, Vision Transformer, MobileNet, lung cancer detection, medical imaging, genetic algorithm, image fusion<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</div>
This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven knowledge distillation (KD) to address uncertainty and complexity in disease diagnosis. Unlike traditional models that rely on static KD with fixed weights, our method dynamically adjusts the distillation weight using fuzzy logic, enabling the student model to focus on high-confidence regions while reducing attention to ambiguous areas. This dynamic adjustment improves the model ability to handle varying uncertainty levels across different regions of LC images. We employ the Vision Transformer (ViT-B32) as the instructor model, which effectively transfers knowledge to the student model, MobileNet, enhancing the student generalization capabilities. The training process is further optimized using a dynamic wait adjustment mechanism that adapts the training procedure for improved convergence and performance. To enhance image quality, we introduce pixel-level image fusion improvement techniques such as Gamma correction and Histogram Equalization. The processed images (Pix1 and Pix2) are fused using a wavelet-based fusion method to improve image resolution and feature preservation. This fusion method uses the wavedec2 function to standardize images to a 224x224 resolution, decompose them into multi-scale frequency components, and recursively average coefficients at each level for better feature representation. To address computational efficiency, Genetic Algorithm (GA) is used to select the most suitable pre-trained student model from a pool of 12 candidates, balancing model performance with computational cost. The model is evaluated on two datasets, including LC25000 histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images (99.54% accuracy), demonstrating robustness across different imaging domains.
<div><strong>Authors:</strong> Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FuzzyDistillViT-MobileNet, a knowledge distillation framework that uses fuzzy‑logic‑driven dynamic weighting to transfer knowledge from a Vision Transformer instructor to a MobileNet student for lung cancer classification, combined with pixel‑level image fusion and genetic‑algorithm‑based model selection, achieving over 99% accuracy on histopathology and CT datasets. It emphasizes handling uncertainty by adjusting distillation weights to focus on high‑confidence regions while reducing attention to ambiguous areas, and includes image preprocessing enhancements such as gamma correction and histogram equalization. Experimental results demonstrate high accuracy and robustness across two lung cancer imaging domains.", "summary_cn": "本文提出 FuzzyDistillViT-MobileNet 框架，利用模糊逻辑驱动的动态权重调整将 Vision Transformer (ViT) 作为教师模型的知识蒸馏到 MobileNet 学生模型，用于肺癌分类，并结合像素级图像融合和遗传算法模型选择，在组织病理学和 CT 数据集上实现超过 99% 的准确率。该方法通过在蒸馏过程中动态调整权重，使学生模型聚焦高置信度区域，降低对模糊区域的关注，同时加入伽马校正和直方图均衡等图像预处理技术。实验显示该方法在两个肺癌影像数据集上具备高准确性和鲁棒性。", "keywords": "dynamic knowledge distillation, fuzzy logic, Vision Transformer, MobileNet, lung cancer detection, medical imaging, genetic algorithm, image fusion", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"]}
]]></acme>

<pubDate>2025-10-23T11:19:52+00:00</pubDate>
</item>
<item>
<title>Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval</title>
<link>https://papers.cool/arxiv/2510.20393</link>
<guid>https://papers.cool/arxiv/2510.20393</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies a bias in cross‑modal representation learning for image‑to‑recipe retrieval, where visual features dominate and overlook subtle recipe‑specific ingredients and cooking actions, especially in multicultural datasets. It proposes a causal approach that predicts overlooked culinary elements and injects them into the representation learning process, demonstrating improved retrieval performance on both monolingual Recipe1M and a newly curated multilingual cuisine dataset.<br /><strong>Summary (CN):</strong> 本文指出图像到食谱检索的跨模态表示学习存在偏差，即视觉特征主导，忽视了食谱中特有的细微配料和烹饪步骤，尤其在多文化数据集上更为严重。作者提出一种因果方法，预测图像中可能遗漏的烹饪元素并将其注入跨模态表示学习，从而在单语言 Recipe1M 数据集和新构建的多语言文化数据集上显著提升检索性能。<br /><strong>Keywords:</strong> image-to-recipe retrieval, cross-modal representation bias, causal representation learning, multicultural cuisine, ingredient prediction, multimodal retrieval<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim</div>
Existing approaches for image-to-recipe retrieval have the implicit assumption that a food image can fully capture the details textually documented in its recipe. However, a food image only reflects the visual outcome of a cooked dish and not the underlying cooking process. Consequently, learning cross-modal representations to bridge the modality gap between images and recipes tends to ignore subtle, recipe-specific details that are not visually apparent but are crucial for recipe retrieval. Specifically, the representations are biased to capture the dominant visual elements, resulting in difficulty in ranking similar recipes with subtle differences in use of ingredients and cooking methods. The bias in representation learning is expected to be more severe when the training data is mixed of images and recipes sourced from different cuisines. This paper proposes a novel causal approach that predicts the culinary elements potentially overlooked in images, while explicitly injecting these elements into cross-modal representation learning to mitigate biases. Experiments are conducted on the standard monolingual Recipe1M dataset and a newly curated multilingual multicultural cuisine dataset. The results indicate that the proposed causal representation learning is capable of uncovering subtle ingredients and cooking actions and achieves impressive retrieval performance on both monolingual and multilingual multicultural datasets.
<div><strong>Authors:</strong> Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies a bias in cross‑modal representation learning for image‑to‑recipe retrieval, where visual features dominate and overlook subtle recipe‑specific ingredients and cooking actions, especially in multicultural datasets. It proposes a causal approach that predicts overlooked culinary elements and injects them into the representation learning process, demonstrating improved retrieval performance on both monolingual Recipe1M and a newly curated multilingual cuisine dataset.", "summary_cn": "本文指出图像到食谱检索的跨模态表示学习存在偏差，即视觉特征主导，忽视了食谱中特有的细微配料和烹饪步骤，尤其在多文化数据集上更为严重。作者提出一种因果方法，预测图像中可能遗漏的烹饪元素并将其注入跨模态表示学习，从而在单语言 Recipe1M 数据集和新构建的多语言文化数据集上显著提升检索性能。", "keywords": "image-to-recipe retrieval, cross-modal representation bias, causal representation learning, multicultural cuisine, ingredient prediction, multimodal retrieval", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Qing Wang", "Chong-Wah Ngo", "Yu Cao", "Ee-Peng Lim"]}
]]></acme>

<pubDate>2025-10-23T09:43:43+00:00</pubDate>
</item>
<item>
<title>Positional Encoding Field</title>
<link>https://papers.cool/arxiv/2510.20385</link>
<guid>https://papers.cool/arxiv/2510.20385</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Positional Encoding Field (PE-Field), extending traditional 2D positional encodings to a structured 3D field with depth-aware and hierarchical components, allowing diffusion transformers to reason about geometry directly in 3D space. Experiments show that DiTs retain global coherence even when positional encodings are perturbed, suggesting spatial coherence is mainly governed by the encodings, and PE-Field-augmented DiTs achieve state-of-the-art results on novel view synthesis and controllable spatial image editing.<br /><strong>Summary (CN):</strong> 本文提出了位置编码场（PE-Field），将传统的二维位置编码扩展到结构化的三维场，并加入深度感知和层次化编码，使扩散 Transformer 能够直接在三维空间中进行几何推理。实验表明，即使位置编码被扰动，DiT 仍能保持全局一致性，说明空间连贯性主要由编码决定，基于 PE-Field 的 DiT 在单图新视角合成和可控空间图像编辑任务上达到了最新的性能。<br /><strong>Keywords:</strong> positional encoding, diffusion transformer, PE-Field, novel view synthesis, volumetric reasoning, depth-aware encoding, hierarchical encoding, 3D field, spatial image editing<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yunpeng Bai, Haoxiang Li, Qixing Huang</div>
Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation, powering state-of-the-art image and video models. By representing images as patch tokens with positional encodings (PEs), DiTs combine Transformer scalability with spatial and temporal inductive biases. In this work, we revisit how DiTs organize visual content and discover that patch tokens exhibit a surprising degree of independence: even when PEs are perturbed, DiTs still produce globally coherent outputs, indicating that spatial coherence is primarily governed by PEs. Motivated by this finding, we introduce the Positional Encoding Field (PE-Field), which extends positional encodings from the 2D plane to a structured 3D field. PE-Field incorporates depth-aware encodings for volumetric reasoning and hierarchical encodings for fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D space. Our PE-Field-augmented DiT achieves state-of-the-art performance on single-image novel view synthesis and generalizes to controllable spatial image editing.
<div><strong>Authors:</strong> Yunpeng Bai, Haoxiang Li, Qixing Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Positional Encoding Field (PE-Field), extending traditional 2D positional encodings to a structured 3D field with depth-aware and hierarchical components, allowing diffusion transformers to reason about geometry directly in 3D space. Experiments show that DiTs retain global coherence even when positional encodings are perturbed, suggesting spatial coherence is mainly governed by the encodings, and PE-Field-augmented DiTs achieve state-of-the-art results on novel view synthesis and controllable spatial image editing.", "summary_cn": "本文提出了位置编码场（PE-Field），将传统的二维位置编码扩展到结构化的三维场，并加入深度感知和层次化编码，使扩散 Transformer 能够直接在三维空间中进行几何推理。实验表明，即使位置编码被扰动，DiT 仍能保持全局一致性，说明空间连贯性主要由编码决定，基于 PE-Field 的 DiT 在单图新视角合成和可控空间图像编辑任务上达到了最新的性能。", "keywords": "positional encoding, diffusion transformer, PE-Field, novel view synthesis, volumetric reasoning, depth-aware encoding, hierarchical encoding, 3D field, spatial image editing", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yunpeng Bai", "Haoxiang Li", "Qixing Huang"]}
]]></acme>

<pubDate>2025-10-23T09:32:37+00:00</pubDate>
</item>
<item>
<title>AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.20348</link>
<guid>https://papers.cool/arxiv/2510.20348</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AccuQuant is a post‑training quantization technique for diffusion models that explicitly simulates multiple denoising steps during quantization to reduce the accumulation of quantization errors. By minimizing the discrepancy between the full‑precision and quantized model over a few steps and using an O(1) memory objective, it achieves efficient and accurate quantized diffusion sampling.<br /><strong>Summary (CN):</strong> AccuQuant 是一种针对扩散模型的后训练量化方法，通过显式模拟多个去噪步骤来减小量化误差的累计。该方法在少数去噪步骤上最小化全精度模型与量化模型的输出差异，并采用 O(1) 内存复杂度的目标，实现了高效且精确的量化采样。<br /><strong>Keywords:</strong> diffusion models, post-training quantization, AccuQuant, error accumulation, denoising steps, memory efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Seunghoon Lee, Jeongwoo Choi, Byunggwan Son, Jaehyeon Moon, Jeimin Jeon, Bumsub Ham</div>
We present in this paper a novel post-training quantization (PTQ) method, dubbed AccuQuant, for diffusion models. We show analytically and empirically that quantization errors for diffusion models are accumulated over denoising steps in a sampling process. To alleviate the error accumulation problem, AccuQuant minimizes the discrepancies between outputs of a full-precision diffusion model and its quantized version within a couple of denoising steps. That is, it simulates multiple denoising steps of a diffusion sampling process explicitly for quantization, accounting the accumulated errors over multiple denoising steps, which is in contrast to previous approaches to imitating a training process of diffusion models, namely, minimizing the discrepancies independently for each step. We also present an efficient implementation technique for AccuQuant, together with a novel objective, which reduces a memory complexity significantly from $\mathcal{O}(n)$ to $\mathcal{O}(1)$, where $n$ is the number of denoising steps. We demonstrate the efficacy and efficiency of AccuQuant across various tasks and diffusion models on standard benchmarks.
<div><strong>Authors:</strong> Seunghoon Lee, Jeongwoo Choi, Byunggwan Son, Jaehyeon Moon, Jeimin Jeon, Bumsub Ham</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AccuQuant is a post‑training quantization technique for diffusion models that explicitly simulates multiple denoising steps during quantization to reduce the accumulation of quantization errors. By minimizing the discrepancy between the full‑precision and quantized model over a few steps and using an O(1) memory objective, it achieves efficient and accurate quantized diffusion sampling.", "summary_cn": "AccuQuant 是一种针对扩散模型的后训练量化方法，通过显式模拟多个去噪步骤来减小量化误差的累计。该方法在少数去噪步骤上最小化全精度模型与量化模型的输出差异，并采用 O(1) 内存复杂度的目标，实现了高效且精确的量化采样。", "keywords": "diffusion models, post-training quantization, AccuQuant, error accumulation, denoising steps, memory efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Seunghoon Lee", "Jeongwoo Choi", "Byunggwan Son", "Jaehyeon Moon", "Jeimin Jeon", "Bumsub Ham"]}
]]></acme>

<pubDate>2025-10-23T08:48:12+00:00</pubDate>
</item>
<item>
<title>AnyPcc: Compressing Any Point Cloud with a Single Universal Model</title>
<link>https://papers.cool/arxiv/2510.20331</link>
<guid>https://papers.cool/arxiv/2510.20331</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AnyPcc introduces a universal point cloud compression framework that combines a Universal Context Model, leveraging spatial and channel-wise priors, with an Instance-Adaptive Fine-Tuning (IAFT) strategy that fine‑tunes a small subset of weights per instance and encodes them in the bitstream. The approach addresses the lack of robust context modeling and out-of-distribution handling, achieving state‑of‑the‑art compression results across 15 diverse datasets.<br /><strong>Summary (CN):</strong> AnyPcc 提出了一种通用点云压缩框架，结合利用空间和通道先验的通用上下文模型（Universal Context Model）以及实例自适应微调（Instance‑Adaptive Fine‑Tuning, IAFT）策略，对每个实例微调少量网络权重并将其编码进比特流。该方法解决了上下文建模不足和分布外数据处理效率低的问题，在 15 个多样化数据集上实现了最先进的压缩性能。<br /><strong>Keywords:</strong> point cloud compression, universal context model, instance-adaptive fine-tuning, geometry compression, out-of-distribution data, neural compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kangli Wang, Qianxi Yi, Yuqi Ye, Shihao Li, Wei Gao</div>
Generalization remains a critical challenge for deep learning-based point cloud geometry compression. We argue this stems from two key limitations: the lack of robust context models and the inefficient handling of out-of-distribution (OOD) data. To address both, we introduce AnyPcc, a universal point cloud compression framework. AnyPcc first employs a Universal Context Model that leverages priors from both spatial and channel-wise grouping to capture robust contextual dependencies. Second, our novel Instance-Adaptive Fine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit and implicit compression paradigms. It fine-tunes a small subset of network weights for each instance and incorporates them into the bitstream, where the marginal bit cost of the weights is dwarfed by the resulting savings in geometry compression. Extensive experiments on a benchmark of 15 diverse datasets confirm that AnyPcc sets a new state-of-the-art in point cloud compression. Our code and datasets will be released to encourage reproducible research.
<div><strong>Authors:</strong> Kangli Wang, Qianxi Yi, Yuqi Ye, Shihao Li, Wei Gao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AnyPcc introduces a universal point cloud compression framework that combines a Universal Context Model, leveraging spatial and channel-wise priors, with an Instance-Adaptive Fine-Tuning (IAFT) strategy that fine‑tunes a small subset of weights per instance and encodes them in the bitstream. The approach addresses the lack of robust context modeling and out-of-distribution handling, achieving state‑of‑the‑art compression results across 15 diverse datasets.", "summary_cn": "AnyPcc 提出了一种通用点云压缩框架，结合利用空间和通道先验的通用上下文模型（Universal Context Model）以及实例自适应微调（Instance‑Adaptive Fine‑Tuning, IAFT）策略，对每个实例微调少量网络权重并将其编码进比特流。该方法解决了上下文建模不足和分布外数据处理效率低的问题，在 15 个多样化数据集上实现了最先进的压缩性能。", "keywords": "point cloud compression, universal context model, instance-adaptive fine-tuning, geometry compression, out-of-distribution data, neural compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kangli Wang", "Qianxi Yi", "Yuqi Ye", "Shihao Li", "Wei Gao"]}
]]></acme>

<pubDate>2025-10-23T08:28:41+00:00</pubDate>
</item>
<item>
<title>HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20322</link>
<guid>https://papers.cool/arxiv/2510.20322</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HyperET introduces an efficient training paradigm that leverages hyperbolic space to align visual representations with textual counterparts across arbitrary granularity levels in multi-modal large language models. By employing learnable matrices with Möbius multiplication (diagonal, block‑diagonal, and banded configurations) and dynamic hyperbolic radius adjustment, the method improves existing MLLM pre‑training and fine‑tuning with less than 1% additional parameters.<br /><strong>Summary (CN):</strong> HyperET 提出一种高效的训练范式，利用超曲面空间在任意粒度层次上对齐视觉表示与文本对应，从而提升多模态大型语言模型的跨模态对齐。该方法通过可学习矩阵的 Möbius 乘法（对角、块对角、带状）以及动态超曲面半径调节，实现了在几乎不增加参数（<1%）的情况下显著提升现有模型的预训练和微调效果。<br /><strong>Keywords:</strong> hyperbolic embeddings, multi-modal LLM, vision-language alignment, Möbius multiplication, hierarchical representations, efficient training, hyperbolic radius adjustment, parameter-efficient, CLIP, SAM<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen</div>
Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with Möbius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\% additional parameters.
<div><strong>Authors:</strong> Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HyperET introduces an efficient training paradigm that leverages hyperbolic space to align visual representations with textual counterparts across arbitrary granularity levels in multi-modal large language models. By employing learnable matrices with Möbius multiplication (diagonal, block‑diagonal, and banded configurations) and dynamic hyperbolic radius adjustment, the method improves existing MLLM pre‑training and fine‑tuning with less than 1% additional parameters.", "summary_cn": "HyperET 提出一种高效的训练范式，利用超曲面空间在任意粒度层次上对齐视觉表示与文本对应，从而提升多模态大型语言模型的跨模态对齐。该方法通过可学习矩阵的 Möbius 乘法（对角、块对角、带状）以及动态超曲面半径调节，实现了在几乎不增加参数（<1%）的情况下显著提升现有模型的预训练和微调效果。", "keywords": "hyperbolic embeddings, multi-modal LLM, vision-language alignment, Möbius multiplication, hierarchical representations, efficient training, hyperbolic radius adjustment, parameter-efficient, CLIP, SAM", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zelin Peng", "Zhengqin Xu", "Qingyang Liu", "Xiaokang Yang", "Wei Shen"]}
]]></acme>

<pubDate>2025-10-23T08:16:44+00:00</pubDate>
</item>
<item>
<title>A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization</title>
<link>https://papers.cool/arxiv/2510.20291</link>
<guid>https://papers.cool/arxiv/2510.20291</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a parameter-efficient Mixture-of-Experts framework for cross-modal geo-localization, winning the RoboSense 2025 Track 4 competition. It tackles severe inter-platform heterogeneity and domain gaps between generic training captions and platform-specific queries through a domain-aligned preprocessing pipeline and platform-specific experts, using LLM-based caption refinement, BGE-M3 (text) and EVA-CLIP (image) encoders, and a progressive hard-negative mining strategy.<br /><strong>Summary (CN):</strong> 本文提出了一种参数高效的 Mixture-of-Experts 框架用于跨模态地理定位，并在 RoboSense 2025 第四赛道中取得冠军。针对平台间的异构性和通用训练描述与特定测试查询之间的域差距，文中设计了域对齐的预处理流程和平台专属专家，包括基于大语言模型的字幕细化、BGE-M3（文本）与 EVA-CLIP（图像）编码器，以及渐进式硬负例挖掘策略，以提升检索的辨别能力。<br /><strong>Keywords:</strong> cross-modal geo-localization, mixture of experts, domain adaptation, caption refinement, hard-negative mining, satellite imagery, drone navigation, parameter-efficient, multimodal retrieval<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li</div>
We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.
<div><strong>Authors:</strong> LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a parameter-efficient Mixture-of-Experts framework for cross-modal geo-localization, winning the RoboSense 2025 Track 4 competition. It tackles severe inter-platform heterogeneity and domain gaps between generic training captions and platform-specific queries through a domain-aligned preprocessing pipeline and platform-specific experts, using LLM-based caption refinement, BGE-M3 (text) and EVA-CLIP (image) encoders, and a progressive hard-negative mining strategy.", "summary_cn": "本文提出了一种参数高效的 Mixture-of-Experts 框架用于跨模态地理定位，并在 RoboSense 2025 第四赛道中取得冠军。针对平台间的异构性和通用训练描述与特定测试查询之间的域差距，文中设计了域对齐的预处理流程和平台专属专家，包括基于大语言模型的字幕细化、BGE-M3（文本）与 EVA-CLIP（图像）编码器，以及渐进式硬负例挖掘策略，以提升检索的辨别能力。", "keywords": "cross-modal geo-localization, mixture of experts, domain adaptation, caption refinement, hard-negative mining, satellite imagery, drone navigation, parameter-efficient, multimodal retrieval", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["LinFeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li"]}
]]></acme>

<pubDate>2025-10-23T07:23:47+00:00</pubDate>
</item>
<item>
<title>Breakdance Video classification in the age of Generative AI</title>
<link>https://papers.cool/arxiv/2510.20287</link>
<guid>https://papers.cool/arxiv/2510.20287</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the use of modern video foundation models for classifying breakdance videos, a niche but popular dance sport. Experiments compare video encoders and video-language decoders, finding that encoder-only models consistently outperform state-of-the-art video-language models for this task. The authors also offer practical guidance on selecting encoders and analyze the behavior of a fine-tuned decoder.<br /><strong>Summary (CN):</strong> 本文研究了现代视频基础模型在破舞（breakdance）视频分类任务中的适用性。实验比较了视频编码器和视频语言解码器，结果显示仅使用编码器的模型在此任务上始终优于最先进的视频语言模型。作者还提供了选择编码器的实用建议，并深入分析了微调后的解码器的工作方式。<br /><strong>Keywords:</strong> breakdance, video classification, video foundation models, video encoder, video language model, generative AI, fine-tuning, sports AI, multimodal video, model selection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.
<div><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the use of modern video foundation models for classifying breakdance videos, a niche but popular dance sport. Experiments compare video encoders and video-language decoders, finding that encoder-only models consistently outperform state-of-the-art video-language models for this task. The authors also offer practical guidance on selecting encoders and analyze the behavior of a fine-tuned decoder.", "summary_cn": "本文研究了现代视频基础模型在破舞（breakdance）视频分类任务中的适用性。实验比较了视频编码器和视频语言解码器，结果显示仅使用编码器的模型在此任务上始终优于最先进的视频语言模型。作者还提供了选择编码器的实用建议，并深入分析了微调后的解码器的工作方式。", "keywords": "breakdance, video classification, video foundation models, video encoder, video language model, generative AI, fine-tuning, sports AI, multimodal video, model selection", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Michelle Munson"]}
]]></acme>

<pubDate>2025-10-23T07:18:54+00:00</pubDate>
</item>
<item>
<title>UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</title>
<link>https://papers.cool/arxiv/2510.20286</link>
<guid>https://papers.cool/arxiv/2510.20286</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces the Instruction-as-Reasoning paradigm for GUI grounding, treating natural-language instructions as dynamic reasoning pathways and training models to select the most effective one. A two-stage framework (supervised fine-tuning on diverse synthesized instructions followed by reinforcement-learning-based pathway selection) yields UI-Ins-7B/32B models that achieve state-of-the-art accuracy on several UI grounding benchmarks and demonstrate emergent reasoning and strong agentic performance.<br /><strong>Summary (CN):</strong> 本文提出了“指令即推理”(Instruction-as-Reasoning)范式，用于 GUI 定位任务，将自然语言指令视为可动态选择的推理路径，并通过监督微调多样化指令和随后强化学习的两阶段训练，使模型能够在推理时挑选最有效的路径。基于此的 UI-Ins-7B/32B 在多个 GUI 定位基准上取得领先表现，并展现出新颖的推理组合能力和较强的代理潜力。<br /><strong>Keywords:</strong> GUI grounding, instruction-as-reasoning, multi-perspective instructions, reinforcement learning, UI agents, grounding benchmarks<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi</div>
GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.
<div><strong>Authors:</strong> Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces the Instruction-as-Reasoning paradigm for GUI grounding, treating natural-language instructions as dynamic reasoning pathways and training models to select the most effective one. A two-stage framework (supervised fine-tuning on diverse synthesized instructions followed by reinforcement-learning-based pathway selection) yields UI-Ins-7B/32B models that achieve state-of-the-art accuracy on several UI grounding benchmarks and demonstrate emergent reasoning and strong agentic performance.", "summary_cn": "本文提出了“指令即推理”(Instruction-as-Reasoning)范式，用于 GUI 定位任务，将自然语言指令视为可动态选择的推理路径，并通过监督微调多样化指令和随后强化学习的两阶段训练，使模型能够在推理时挑选最有效的路径。基于此的 UI-Ins-7B/32B 在多个 GUI 定位基准上取得领先表现，并展现出新颖的推理组合能力和较强的代理潜力。", "keywords": "GUI grounding, instruction-as-reasoning, multi-perspective instructions, reinforcement learning, UI agents, grounding benchmarks", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liangyu Chen", "Hanzhang Zhou", "Chenglin Cai", "Jianan Zhang", "Panrong Tong", "Quyu Kong", "Xu Zhang", "Chen Liu", "Yuqi Liu", "Wenxuan Wang", "Yue Wang", "Qin Jin", "Steven Hoi"]}
]]></acme>

<pubDate>2025-10-23T07:18:32+00:00</pubDate>
</item>
<item>
<title>DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</title>
<link>https://papers.cool/arxiv/2510.20285</link>
<guid>https://papers.cool/arxiv/2510.20285</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DMC$^3$, a Dual-Modal Counterfactual Contrastive Construction framework for egocentric video question answering. It generates positive and negative counterfactual samples for both text and visual modalities via event description paraphrasing and core interaction mining, and applies contrastive learning to align original and positive samples while separating negatives, achieving state-of-the-art results on EgoTaskQA and QAEGO4D.<br /><strong>Summary (CN):</strong> 本文提出 DMC$^3$（Dual-Modal Counterfactual Contrastive Construction）框架，用于第一人称视频问答（Egocentric VideoQA）。通过事件描述改写和核心交互挖掘分别为文本和视觉模态生成正负对照样本，并在对比学习中使原始样本特征靠近正样本特征、远离负样本特征，从而在 EgoTaskQA 与 QAEGO4D 上实现了最新的性能。<br /><strong>Keywords:</strong> egocentric video QA, counterfactual contrastive learning, dual-modal, event description paraphrasing, hand-object interaction, EgoTaskQA, QAEGO4D, video understanding<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu</div>
Egocentric Video Question Answering (Egocentric VideoQA) plays an important role in egocentric video understanding, which refers to answering questions based on first-person videos. Although existing methods have made progress through the paradigm of pre-training and fine-tuning, they ignore the unique challenges posed by the first-person perspective, such as understanding multiple events and recognizing hand-object interactions. To deal with these challenges, we propose a Dual-Modal Counterfactual Contrastive Construction (DMC$^3$) framework, which contains an egocentric videoqa baseline, a counterfactual sample construction module and a counterfactual sample-involved contrastive optimization. Specifically, We first develop a counterfactual sample construction module to generate positive and negative samples for textual and visual modalities through event description paraphrasing and core interaction mining, respectively. Then, We feed these samples together with the original samples into the baseline. Finally, in the counterfactual sample-involved contrastive optimization module, we apply contrastive loss to minimize the distance between the original sample features and the positive sample features, while maximizing the distance from the negative samples. Experiments show that our method achieve 52.51\% and 46.04\% on the \textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on QAEGO4D, both reaching the state-of-the-art performance.
<div><strong>Authors:</strong> Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DMC$^3$, a Dual-Modal Counterfactual Contrastive Construction framework for egocentric video question answering. It generates positive and negative counterfactual samples for both text and visual modalities via event description paraphrasing and core interaction mining, and applies contrastive learning to align original and positive samples while separating negatives, achieving state-of-the-art results on EgoTaskQA and QAEGO4D.", "summary_cn": "本文提出 DMC$^3$（Dual-Modal Counterfactual Contrastive Construction）框架，用于第一人称视频问答（Egocentric VideoQA）。通过事件描述改写和核心交互挖掘分别为文本和视觉模态生成正负对照样本，并在对比学习中使原始样本特征靠近正样本特征、远离负样本特征，从而在 EgoTaskQA 与 QAEGO4D 上实现了最新的性能。", "keywords": "egocentric video QA, counterfactual contrastive learning, dual-modal, event description paraphrasing, hand-object interaction, EgoTaskQA, QAEGO4D, video understanding", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiayi Zou", "Chaofan Chen", "Bing-Kun Bao", "Changsheng Xu"]}
]]></acme>

<pubDate>2025-10-23T07:15:18+00:00</pubDate>
</item>
<item>
<title>Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition</title>
<link>https://papers.cool/arxiv/2510.20284</link>
<guid>https://papers.cool/arxiv/2510.20284</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Knowledge-Informed Neural Network (KINN), a lightweight architecture for complex-valued SAR image recognition that incorporates physics-guided compression via a dictionary processor and a compression‑aggregation‑compression pipeline, instantiated in both CNN and Vision Transformer variants. Extensive experiments on five SAR benchmarks demonstrate state‑of‑the‑art parameter‑efficient performance, strong generalization under data scarcity and domain shift, and enhanced interpretability through physically grounded representations. The work positions KINN as a solution to the trilemma of generalization, interpretability, and efficiency in SAR image analysis.<br /><strong>Summary (CN):</strong> 本文提出了 Knowledge‑Informed Neural Network (KINN)，一种轻量级的复值 SAR 图像识别架构，利用字典处理器进行物理引导的压缩，并采用压缩‑聚合‑压缩的流程，分别实现了 CNN（0.7M）和 Vision Transformer（0.95M）版本。 在五个 SAR 基准上的大量实验表明，KINN 在参数效率、数据稀缺及分布外场景下的泛化能力以及通过物理特征实现的可解释性方面均达到了最新水平。 此工作旨在解决 SAR 图像分析中泛化、可解释性和效率三者的冲突，实现可信的 AI 应用。<br /><strong>Keywords:</strong> complex-valued SAR, physics-informed neural network, knowledge-informed neural network, compression-aggregation-compression, interpretability, parameter-efficient, out-of-distribution, vision transformer, synthetic aperture radar, dictionary processor<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Haodong Yang, Zhongling Huang, Shaojie Guo, Zhe Zhang, Gong Cheng, Junwei Han</div>
Deep learning models for complex-valued Synthetic Aperture Radar (CV-SAR) image recognition are fundamentally constrained by a representation trilemma under data-limited and domain-shift scenarios: the concurrent, yet conflicting, optimization of generalization, interpretability, and efficiency. Our work is motivated by the premise that the rich electromagnetic scattering features inherent in CV-SAR data hold the key to resolving this trilemma, yet they are insufficiently harnessed by conventional data-driven models. To this end, we introduce the Knowledge-Informed Neural Network (KINN), a lightweight framework built upon a novel "compression-aggregation-compression" architecture. The first stage performs a physics-guided compression, wherein a novel dictionary processor adaptively embeds physical priors, enabling a compact unfolding network to efficiently extract sparse, physically-grounded signatures. A subsequent aggregation module enriches these representations, followed by a final semantic compression stage that utilizes a compact classification head with self-distillation to learn maximally task-relevant and discriminative embeddings. We instantiate KINN in both CNN (0.7M) and Vision Transformer (0.95M) variants. Extensive evaluations on five SAR benchmarks confirm that KINN establishes a state-of-the-art in parameter-efficient recognition, offering exceptional generalization in data-scarce and out-of-distribution scenarios and tangible interpretability, thereby providing an effective solution to the representation trilemma and offering a new path for trustworthy AI in SAR image analysis.
<div><strong>Authors:</strong> Haodong Yang, Zhongling Huang, Shaojie Guo, Zhe Zhang, Gong Cheng, Junwei Han</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Knowledge-Informed Neural Network (KINN), a lightweight architecture for complex-valued SAR image recognition that incorporates physics-guided compression via a dictionary processor and a compression‑aggregation‑compression pipeline, instantiated in both CNN and Vision Transformer variants. Extensive experiments on five SAR benchmarks demonstrate state‑of‑the‑art parameter‑efficient performance, strong generalization under data scarcity and domain shift, and enhanced interpretability through physically grounded representations. The work positions KINN as a solution to the trilemma of generalization, interpretability, and efficiency in SAR image analysis.", "summary_cn": "本文提出了 Knowledge‑Informed Neural Network (KINN)，一种轻量级的复值 SAR 图像识别架构，利用字典处理器进行物理引导的压缩，并采用压缩‑聚合‑压缩的流程，分别实现了 CNN（0.7M）和 Vision Transformer（0.95M）版本。 在五个 SAR 基准上的大量实验表明，KINN 在参数效率、数据稀缺及分布外场景下的泛化能力以及通过物理特征实现的可解释性方面均达到了最新水平。 此工作旨在解决 SAR 图像分析中泛化、可解释性和效率三者的冲突，实现可信的 AI 应用。", "keywords": "complex-valued SAR, physics-informed neural network, knowledge-informed neural network, compression-aggregation-compression, interpretability, parameter-efficient, out-of-distribution, vision transformer, synthetic aperture radar, dictionary processor", "scoring": {"interpretability": 7, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Haodong Yang", "Zhongling Huang", "Shaojie Guo", "Zhe Zhang", "Gong Cheng", "Junwei Han"]}
]]></acme>

<pubDate>2025-10-23T07:12:26+00:00</pubDate>
</item>
<item>
<title>Causal Debiasing for Visual Commonsense Reasoning</title>
<link>https://papers.cool/arxiv/2510.20281</link>
<guid>https://papers.cool/arxiv/2510.20281</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies bias in Visual Commonsense Reasoning (VCR) datasets, identifying co-occurrence and statistical shortcuts in both text and image modalities. It introduces VCR-OOD benchmarks to evaluate out-of-distribution generalization and applies a causal backdoor adjustment using a dictionary of correct answers to remove prediction shortcuts, showing improved performance across datasets.<br /><strong>Summary (CN):</strong> 本文分析了视觉常识推理（VCR）数据集中的共现偏差和统计偏差，指出了文本和视觉两方面的预测捷径。作者构建了 VCR-OOD 基准用于评估模型在两模态上的分布外泛化能力，并采用基于正确答案字典的因果后门调整方法消除预测捷径，实验证明该去偏方法在多个数据集上均有效。<br /><strong>Keywords:</strong> causal debiasing, visual commonsense reasoning, VCR, OOD dataset, backdoor adjustment, dataset bias, multimodal reasoning, shortcut mitigation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Jiayi Zou, Gengyun Jia, Bing-Kun Bao</div>
Visual Commonsense Reasoning (VCR) refers to answering questions and providing explanations based on images. While existing methods achieve high prediction accuracy, they often overlook bias in datasets and lack debiasing strategies. In this paper, our analysis reveals co-occurrence and statistical biases in both textual and visual data. We introduce the VCR-OOD datasets, comprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate the generalization capabilities of models across two modalities. Furthermore, we analyze the causal graphs and prediction shortcuts in VCR and adopt a backdoor adjustment method to remove bias. Specifically, we create a dictionary based on the set of correct answers to eliminate prediction shortcuts. Experiments demonstrate the effectiveness of our debiasing method across different datasets.
<div><strong>Authors:</strong> Jiayi Zou, Gengyun Jia, Bing-Kun Bao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies bias in Visual Commonsense Reasoning (VCR) datasets, identifying co-occurrence and statistical shortcuts in both text and image modalities. It introduces VCR-OOD benchmarks to evaluate out-of-distribution generalization and applies a causal backdoor adjustment using a dictionary of correct answers to remove prediction shortcuts, showing improved performance across datasets.", "summary_cn": "本文分析了视觉常识推理（VCR）数据集中的共现偏差和统计偏差，指出了文本和视觉两方面的预测捷径。作者构建了 VCR-OOD 基准用于评估模型在两模态上的分布外泛化能力，并采用基于正确答案字典的因果后门调整方法消除预测捷径，实验证明该去偏方法在多个数据集上均有效。", "keywords": "causal debiasing, visual commonsense reasoning, VCR, OOD dataset, backdoor adjustment, dataset bias, multimodal reasoning, shortcut mitigation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jiayi Zou", "Gengyun Jia", "Bing-Kun Bao"]}
]]></acme>

<pubDate>2025-10-23T07:10:21+00:00</pubDate>
</item>
<item>
<title>GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection</title>
<link>https://papers.cool/arxiv/2510.20268</link>
<guid>https://papers.cool/arxiv/2510.20268</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes GMFVAD, a method that generates fine-grained multi-modal features by combining visual snippets with caption text to reduce redundant information and improve video anomaly detection. By summarizing the main content of each snippet and enhancing highlighted visual portions with textual cues, the approach achieves state-of-the-art results on four benchmark datasets. Ablation studies confirm that performance gains stem from the redundancy reduction.<br /><strong>Summary (CN):</strong> 本文提出 GMFVAD 方法，通过将视频片段的视觉特征与原始视频字幕文本相结合，生成更细粒度的多模态特征，以降低冗余信息并提升视频异常检测效果。该方法对每个片段的主要内容进行摘要，并利用文本信息加强视觉特征的关键部分，在四个主流数据集上实现了最新的性能。消融实验表明，性能提升来源于冗余信息的减少。<br /><strong>Keywords:</strong> video anomaly detection, multi-modal feature, grained feature, text-caption integration, redundancy reduction, surveillance, spatio-temporal correlation, state-of-the-art<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Guangyu Dai, Dong Chen, Siliang Tang, Yueting Zhuang</div>
Video anomaly detection (VAD) is a challenging task that detects anomalous frames in continuous surveillance videos. Most previous work utilizes the spatio-temporal correlation of visual features to distinguish whether there are abnormalities in video snippets. Recently, some works attempt to introduce multi-modal information, like text feature, to enhance the results of video anomaly detection. However, these works merely incorporate text features into video snippets in a coarse manner, overlooking the significant amount of redundant information that may exist within the video snippets. Therefore, we propose to leverage the diversity among multi-modal information to further refine the extracted features, reducing the redundancy in visual features, and we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD). Specifically, we generate more grained multi-modal feature based on the video snippet, which summarizes the main content, and text features based on the captions of original video will be introduced to further enhance the visual features of highlighted portions. Experiments show that the proposed GMFVAD achieves state-of-the-art performance on four mainly datasets. Ablation experiments also validate that the improvement of GMFVAD is due to the reduction of redundant information.
<div><strong>Authors:</strong> Guangyu Dai, Dong Chen, Siliang Tang, Yueting Zhuang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes GMFVAD, a method that generates fine-grained multi-modal features by combining visual snippets with caption text to reduce redundant information and improve video anomaly detection. By summarizing the main content of each snippet and enhancing highlighted visual portions with textual cues, the approach achieves state-of-the-art results on four benchmark datasets. Ablation studies confirm that performance gains stem from the redundancy reduction.", "summary_cn": "本文提出 GMFVAD 方法，通过将视频片段的视觉特征与原始视频字幕文本相结合，生成更细粒度的多模态特征，以降低冗余信息并提升视频异常检测效果。该方法对每个片段的主要内容进行摘要，并利用文本信息加强视觉特征的关键部分，在四个主流数据集上实现了最新的性能。消融实验表明，性能提升来源于冗余信息的减少。", "keywords": "video anomaly detection, multi-modal feature, grained feature, text-caption integration, redundancy reduction, surveillance, spatio-temporal correlation, state-of-the-art", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guangyu Dai", "Dong Chen", "Siliang Tang", "Yueting Zhuang"]}
]]></acme>

<pubDate>2025-10-23T06:52:53+00:00</pubDate>
</item>
<item>
<title>Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals</title>
<link>https://papers.cool/arxiv/2510.20267</link>
<guid>https://papers.cool/arxiv/2510.20267</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a real-time currency detection system for visually impaired users, employing a YOLOv8‑nano model with a custom detection head enhanced by Squeeze‑and‑Excitation blocks. Trained on 30 classes covering USD, EUR, and BDT notes and coins, the model achieves 97.73% accuracy and provides voice feedback to identify detected currency. The goal is to enable independent handling of money for visually impaired individuals using smartphones.<br /><strong>Summary (CN):</strong> 本文提出了一套用于视障用户的实时货币检测系统，采用 YOLOv8‑nano 模型并加入 Squeeze‑and‑Excitation 块的自定义检测头。模型在涵盖美元、欧元和孟加拉塔卡的 30 类纸币和硬币数据上训练，取得 97.73% 的准确率，并通过语音反馈提示检测到的货币种类，旨在帮助视障人士独立处理金钱。<br /><strong>Keywords:</strong> currency detection, YOLOv8, visual impairment, voice feedback, object detection, assistive technology<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 3, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
Technologies like smartphones have become an essential in our daily lives. It has made accessible to everyone including visually impaired individuals. With the use of smartphone cameras, image capturing and processing have become more convenient. With the use of smartphones and machine learning, the life of visually impaired can be made a little easier. Daily tasks such as handling money without relying on someone can be troublesome for them. For that purpose this paper presents a real-time currency detection system designed to assist visually impaired individuals. The proposed model is trained on a dataset containing 30 classes of notes and coins, representing 3 types of currency: US dollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a YOLOv8 nano model with a custom detection head featuring deep convolutional layers and Squeeze-and-Excitation blocks to enhance feature extraction and detection accuracy. Our model has achieved a higher accuracy of 97.73%, recall of 95.23%, f1-score of 95.85% and a mean Average Precision at IoU=0.5 (mAP50(B)) of 97.21\%. Using the voice feedback after the detection would help the visually impaired to identify the currency. This paper aims to create a practical and efficient currency detection system to empower visually impaired individuals independent in handling money.
<div><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a real-time currency detection system for visually impaired users, employing a YOLOv8‑nano model with a custom detection head enhanced by Squeeze‑and‑Excitation blocks. Trained on 30 classes covering USD, EUR, and BDT notes and coins, the model achieves 97.73% accuracy and provides voice feedback to identify detected currency. The goal is to enable independent handling of money for visually impaired individuals using smartphones.", "summary_cn": "本文提出了一套用于视障用户的实时货币检测系统，采用 YOLOv8‑nano 模型并加入 Squeeze‑and‑Excitation 块的自定义检测头。模型在涵盖美元、欧元和孟加拉塔卡的 30 类纸币和硬币数据上训练，取得 97.73% 的准确率，并通过语音反馈提示检测到的货币种类，旨在帮助视障人士独立处理金钱。", "keywords": "currency detection, YOLOv8, visual impairment, voice feedback, object detection, assistive technology", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"]}
]]></acme>

<pubDate>2025-10-23T06:48:04+00:00</pubDate>
</item>
<item>
<title>Calibrating Multimodal Consensus for Emotion Recognition</title>
<link>https://papers.cool/arxiv/2510.20256</link>
<guid>https://papers.cool/arxiv/2510.20256</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Calibrated Multimodal Consensus (CMC), a framework for multimodal emotion recognition that generates pseudo unimodal labels via a Pseudo Label Generation Module and employs a Parameter‑free Fusion Module together with a Multimodal Consensus Router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four benchmark datasets show that CMC matches or exceeds state‑of‑the‑art performance, especially in scenarios with conflicting cues.<br /><strong>Summary (CN):</strong> 本文提出了“校准多模态共识”(Calibrated Multimodal Consensus, CMC)框架，用伪标签生成模块(Pseudo Label Generation Module)生成单模态伪标签，并通过无参数融合模块(Parameter‑free Fusion Module)和多模态共识路由器(Multimodal Consensus Router)降低文本模态的主导性，缓解跨模态之间的语义不一致。在四个基准数据集上的实验表明，CMC 在整体性能上与最先进方法持平或更优，特别是在存在冲突情感线索的场景中表现突出。<br /><strong>Keywords:</strong> multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, text dominance, multimodal fusion, emotion analysis<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CMC.
<div><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Calibrated Multimodal Consensus (CMC), a framework for multimodal emotion recognition that generates pseudo unimodal labels via a Pseudo Label Generation Module and employs a Parameter‑free Fusion Module together with a Multimodal Consensus Router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four benchmark datasets show that CMC matches or exceeds state‑of‑the‑art performance, especially in scenarios with conflicting cues.", "summary_cn": "本文提出了“校准多模态共识”(Calibrated Multimodal Consensus, CMC)框架，用伪标签生成模块(Pseudo Label Generation Module)生成单模态伪标签，并通过无参数融合模块(Parameter‑free Fusion Module)和多模态共识路由器(Multimodal Consensus Router)降低文本模态的主导性，缓解跨模态之间的语义不一致。在四个基准数据集上的实验表明，CMC 在整体性能上与最先进方法持平或更优，特别是在存在冲突情感线索的场景中表现突出。", "keywords": "multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, text dominance, multimodal fusion, emotion analysis", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"]}
]]></acme>

<pubDate>2025-10-23T06:25:10+00:00</pubDate>
</item>
<item>
<title>Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization</title>
<link>https://papers.cool/arxiv/2510.20247</link>
<guid>https://papers.cool/arxiv/2510.20247</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a mask-driven positional encoding (MPE) that combines spatial coordinates with object silhouettes, and a strip-convolution context enhancement module (CEM) to capture long-range features for cross‑view object geo‑localization. Integrated into the EDGeo framework, these components improve robustness to annotation shifts and enhance discrimination of elongated structures in satellite imagery, achieving state‑of‑the‑art performance on CVOGL and VIGOR‑Building datasets.<br /><strong>Summary (CN):</strong> 本文提出了基于分割掩码的位置信息编码（MPE），将空间坐标与目标轮廓结合，并设计了条形卷积上下文增强模块（CEM），用于捕获卫星图像中长跨度目标的长程特征。将两者集成到 EDGeo 框架后，显著提升了跨视角目标地理定位的鲁棒性和对细长建筑的判别能力，在 CVOGL 与 VIGOR‑Building 数据集上达到了领先的定位精度。<br /><strong>Keywords:</strong> cross-view geo-localization, mask positional encoding, strip convolution, context modeling, satellite imagery, object-aware encoding, deep learning, positional encoding, feature discrimination, EDGeo<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Shuhan Hu, Yiru Li, Yuanyuan Li, Yingying Zhu</div>
Cross-view object geo-localization enables high-precision object localization through cross-view matching, with critical applications in autonomous driving, urban management, and disaster response. However, existing methods rely on keypoint-based positional encoding, which captures only 2D coordinates while neglecting object shape information, resulting in sensitivity to annotation shifts and limited cross-view matching capability. To address these limitations, we propose a mask-based positional encoding scheme that leverages segmentation masks to capture both spatial coordinates and object silhouettes, thereby upgrading the model from "location-aware" to "object-aware." Furthermore, to tackle the challenge of large-span objects (e.g., elongated buildings) in satellite imagery, we design a context enhancement module. This module employs horizontal and vertical strip convolutional kernels to extract long-range contextual features, enhancing feature discrimination among strip-like objects. Integrating MPE and CEM, we present EDGeo, an end-to-end framework for robust cross-view object geo-localization. Extensive experiments on two public datasets (CVOGL and VIGOR-Building) demonstrate that our method achieves state-of-the-art performance, with a 3.39% improvement in localization accuracy under challenging ground-to-satellite scenarios. This work provides a robust positional encoding paradigm and a contextual modeling framework for advancing cross-view geo-localization research.
<div><strong>Authors:</strong> Shuhan Hu, Yiru Li, Yuanyuan Li, Yingying Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a mask-driven positional encoding (MPE) that combines spatial coordinates with object silhouettes, and a strip-convolution context enhancement module (CEM) to capture long-range features for cross‑view object geo‑localization. Integrated into the EDGeo framework, these components improve robustness to annotation shifts and enhance discrimination of elongated structures in satellite imagery, achieving state‑of‑the‑art performance on CVOGL and VIGOR‑Building datasets.", "summary_cn": "本文提出了基于分割掩码的位置信息编码（MPE），将空间坐标与目标轮廓结合，并设计了条形卷积上下文增强模块（CEM），用于捕获卫星图像中长跨度目标的长程特征。将两者集成到 EDGeo 框架后，显著提升了跨视角目标地理定位的鲁棒性和对细长建筑的判别能力，在 CVOGL 与 VIGOR‑Building 数据集上达到了领先的定位精度。", "keywords": "cross-view geo-localization, mask positional encoding, strip convolution, context modeling, satellite imagery, object-aware encoding, deep learning, positional encoding, feature discrimination, EDGeo", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Shuhan Hu", "Yiru Li", "Yuanyuan Li", "Yingying Zhu"]}
]]></acme>

<pubDate>2025-10-23T06:07:07+00:00</pubDate>
</item>
<item>
<title>Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding</title>
<link>https://papers.cool/arxiv/2510.20244</link>
<guid>https://papers.cool/arxiv/2510.20244</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes DualGround, a dual‑branch architecture for video temporal grounding that explicitly separates global sentence‑level semantics (routed through the [EOS] token) from local phrase‑level semantics (clustered word tokens). By employing token‑role aware cross‑modal interaction, the model improves both moment retrieval and highlight detection, achieving state‑of‑the‑art results on QVHighlights and Charades‑STA benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 DualGround 双分支结构，用于视频时序定位，明确区分全局句子层语义（通过 [EOS] 标记）和局部短语层语义（对词汇进行聚类）。通过基于 token 角色的跨模态交互，该模型在 Moment Retrieval 与 Highlight Detection 任务上均实现了领先性能，在 QVHighlights 与 Charades‑STA 基准上取得了最新的成绩。<br /><strong>Keywords:</strong> video temporal grounding, dual-branch architecture, token-role aware cross-modal attention, phrase-level semantics, sentence-level semantics, moment retrieval, highlight detection, CLIP, InternVideo2<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee</div>
Video Temporal Grounding (VTG) aims to localize temporal segments in long, untrimmed videos that align with a given natural language query. This task typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). While recent advances have been progressed by powerful pretrained vision-language models such as CLIP and InternVideo2, existing approaches commonly treat all text tokens uniformly during crossmodal attention, disregarding their distinct semantic roles. To validate the limitations of this approach, we conduct controlled experiments demonstrating that VTG models overly rely on [EOS]-driven global semantics while failing to effectively utilize word-level signals, which limits their ability to achieve fine-grained temporal alignment. Motivated by this limitation, we propose DualGround, a dual-branch architecture that explicitly separates global and local semantics by routing the [EOS] token through a sentence-level path and clustering word tokens into phrase-level units for localized grounding. Our method introduces (1) tokenrole- aware cross modal interaction strategies that align video features with sentence-level and phrase-level semantics in a structurally disentangled manner, and (2) a joint modeling framework that not only improves global sentence-level alignment but also enhances finegrained temporal grounding by leveraging structured phrase-aware context. This design allows the model to capture both coarse and localized semantics, enabling more expressive and context-aware video grounding. DualGround achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection tasks across QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of disentangled semantic modeling in video-language alignment.
<div><strong>Authors:</strong> Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes DualGround, a dual‑branch architecture for video temporal grounding that explicitly separates global sentence‑level semantics (routed through the [EOS] token) from local phrase‑level semantics (clustered word tokens). By employing token‑role aware cross‑modal interaction, the model improves both moment retrieval and highlight detection, achieving state‑of‑the‑art results on QVHighlights and Charades‑STA benchmarks.", "summary_cn": "本文提出 DualGround 双分支结构，用于视频时序定位，明确区分全局句子层语义（通过 [EOS] 标记）和局部短语层语义（对词汇进行聚类）。通过基于 token 角色的跨模态交互，该模型在 Moment Retrieval 与 Highlight Detection 任务上均实现了领先性能，在 QVHighlights 与 Charades‑STA 基准上取得了最新的成绩。", "keywords": "video temporal grounding, dual-branch architecture, token-role aware cross-modal attention, phrase-level semantics, sentence-level semantics, moment retrieval, highlight detection, CLIP, InternVideo2", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Minseok Kang", "Minhyeok Lee", "Minjung Kim", "Donghyeong Kim", "Sangyoun Lee"]}
]]></acme>

<pubDate>2025-10-23T05:53:01+00:00</pubDate>
</item>
<item>
<title>COS3D: Collaborative Open-Vocabulary 3D Segmentation</title>
<link>https://papers.cool/arxiv/2510.20238</link>
<guid>https://papers.cool/arxiv/2510.20238</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> COS3D introduces a collaborative prompt-segmentation framework that jointly leverages an instance field and a language field to improve open-vocabulary 3D segmentation. The method learns an instance-to-language feature mapping with a two-stage training strategy and refines prompts during inference via adaptive language-to-instance refinement, achieving state-of-the-art results on benchmark datasets and enabling applications such as image-based 3D segmentation and robotics.<br /><strong>Summary (CN):</strong> COS3D 提出一种协作式提示分割框架，通过同时利用实例域（instance field）和语言域（language field）来提升开放词汇的 3D 分割效果。该方法在训练阶段学习实例到语言的特征映射，并采用两阶段训练策略；在推理阶段通过自适应语言到实例的提示细化实现高质量分割。实验表明其在多个基准上表现领先，并具备图像驱动 3D 分割、层级分割和机器人等应用潜力。<br /><strong>Keywords:</strong> open-vocabulary 3D segmentation, collaborative field, Gaussian splatting, instance-to-language mapping, prompt refinement, multi-modal segmentation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Runsong Zhu, Ka-Hei Hui, Zhengzhe Liu, Qianyi Wu, Weiliang Tang, Shi Qiu, Pheng-Ann Heng, Chi-Wing Fu</div>
Open-vocabulary 3D segmentation is a fundamental yet challenging task, requiring a mutual understanding of both segmentation and language. However, existing Gaussian-splatting-based methods rely either on a single 3D language field, leading to inferior segmentation, or on pre-computed class-agnostic segmentations, suffering from error accumulation. To address these limitations, we present COS3D, a new collaborative prompt-segmentation framework that contributes to effectively integrating complementary language and segmentation cues throughout its entire pipeline. We first introduce the new concept of collaborative field, comprising an instance field and a language field, as the cornerstone for collaboration. During training, to effectively construct the collaborative field, our key idea is to capture the intrinsic relationship between the instance field and language field, through a novel instance-to-language feature mapping and designing an efficient two-stage training strategy. During inference, to bridge distinct characteristics of the two fields, we further design an adaptive language-to-instance prompt refinement, promoting high-quality prompt-segmentation inference. Extensive experiments not only demonstrate COS3D's leading performance over existing methods on two widely-used benchmarks but also show its high potential to various applications,~\ie, novel image-based 3D segmentation, hierarchical segmentation, and robotics. The code is publicly available at \href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.
<div><strong>Authors:</strong> Runsong Zhu, Ka-Hei Hui, Zhengzhe Liu, Qianyi Wu, Weiliang Tang, Shi Qiu, Pheng-Ann Heng, Chi-Wing Fu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "COS3D introduces a collaborative prompt-segmentation framework that jointly leverages an instance field and a language field to improve open-vocabulary 3D segmentation. The method learns an instance-to-language feature mapping with a two-stage training strategy and refines prompts during inference via adaptive language-to-instance refinement, achieving state-of-the-art results on benchmark datasets and enabling applications such as image-based 3D segmentation and robotics.", "summary_cn": "COS3D 提出一种协作式提示分割框架，通过同时利用实例域（instance field）和语言域（language field）来提升开放词汇的 3D 分割效果。该方法在训练阶段学习实例到语言的特征映射，并采用两阶段训练策略；在推理阶段通过自适应语言到实例的提示细化实现高质量分割。实验表明其在多个基准上表现领先，并具备图像驱动 3D 分割、层级分割和机器人等应用潜力。", "keywords": "open-vocabulary 3D segmentation, collaborative field, Gaussian splatting, instance-to-language mapping, prompt refinement, multi-modal segmentation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Runsong Zhu", "Ka-Hei Hui", "Zhengzhe Liu", "Qianyi Wu", "Weiliang Tang", "Shi Qiu", "Pheng-Ann Heng", "Chi-Wing Fu"]}
]]></acme>

<pubDate>2025-10-23T05:45:15+00:00</pubDate>
</item>
<item>
<title>Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context</title>
<link>https://papers.cool/arxiv/2510.20229</link>
<guid>https://papers.cool/arxiv/2510.20229</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates why Large Vision-Language Models (LVLMs) produce more hallucinations in longer, free-form responses, attributing the issue to increased reliance on context rather than response length itself. It introduces an "induce-detect-suppress" framework that deliberately creates hallucinations via crafted contexts, uses these instances for early detection, and suppresses object-level hallucinations during decoding, achieving consistent improvements across benchmarks. The work provides new insights into hallucination mechanisms and a practical mitigation strategy.<br /><strong>Summary (CN):</strong> 本文探讨了大型视觉语言模型（LVLM）在更长自由式回复中更易产生幻觉的原因，认为关键在于对上下文的依赖增加，而非单纯的长度问题。作者提出了“诱导‑检测‑抑制”框架，通过专门设计的上下文诱导幻觉，用诱导实例进行早期检测，并在实际解码时抑制对象级幻觉，显著提升了多个基准的表现。该研究为理解幻觉机制提供了新视角，并提供了实用的缓解方法。<br /><strong>Keywords:</strong> hallucination, LVLM, vision-language, context, detection, mitigation, safety, alignment<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 8, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel "induce-detect-suppress" framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs' longer responses.
<div><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates why Large Vision-Language Models (LVLMs) produce more hallucinations in longer, free-form responses, attributing the issue to increased reliance on context rather than response length itself. It introduces an \"induce-detect-suppress\" framework that deliberately creates hallucinations via crafted contexts, uses these instances for early detection, and suppresses object-level hallucinations during decoding, achieving consistent improvements across benchmarks. The work provides new insights into hallucination mechanisms and a practical mitigation strategy.", "summary_cn": "本文探讨了大型视觉语言模型（LVLM）在更长自由式回复中更易产生幻觉的原因，认为关键在于对上下文的依赖增加，而非单纯的长度问题。作者提出了“诱导‑检测‑抑制”框架，通过专门设计的上下文诱导幻觉，用诱导实例进行早期检测，并在实际解码时抑制对象级幻觉，显著提升了多个基准的表现。该研究为理解幻觉机制提供了新视角，并提供了实用的缓解方法。", "keywords": "hallucination, LVLM, vision-language, context, detection, mitigation, safety, alignment", "scoring": {"interpretability": 5, "understanding": 8, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"]}
]]></acme>

<pubDate>2025-10-23T05:22:07+00:00</pubDate>
</item>
<item>
<title>EditInfinity: Image Editing with Binary-Quantized Generative Models</title>
<link>https://papers.cool/arxiv/2510.20217</link>
<guid>https://papers.cool/arxiv/2510.20217</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> EditInfinity adapts the binary-quantized generative model Infinity for text-driven image editing by exploiting exact intermediate quantized representations to achieve precise image inversion and style preservation. The method introduces an efficient inversion mechanism with text prompting rectification and a holistic smoothing strategy, resulting in high-fidelity edits across add, change, and delete operations on the PIE-Bench benchmark, outperforming diffusion-based baselines.<br /><strong>Summary (CN):</strong> EditInfinity 利用二值量化生成模型 Infinity 的精确中间量化表征，实现对源图像的精准反演和风格保持，从而进行基于文本的图像编辑。该方法提出了结合文本提示校正的高效反演机制以及全局平滑策略，在 PIE-Bench 基准的添加、修改、删除编辑任务上，显著提升编辑保真度，优于扩散模型基线。<br /><strong>Keywords:</strong> binary-quantized generative model, VQ inversion, text-driven image editing, Infinity, image editing fidelity, quantized representations, PIE-Bench<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei</div>
Adapting pretrained diffusion-based generative models for text-driven image editing with negligible tuning overhead has demonstrated remarkable potential. A classical adaptation paradigm, as followed by these methods, first infers the generative trajectory inversely for a given source image by image inversion, then performs image editing along the inferred trajectory guided by the target text prompts. However, the performance of image editing is heavily limited by the approximation errors introduced during image inversion by diffusion models, which arise from the absence of exact supervision in the intermediate generative steps. To circumvent this issue, we investigate the parameter-efficient adaptation of VQ-based generative models for image editing, and leverage their inherent characteristic that the exact intermediate quantized representations of a source image are attainable, enabling more effective supervision for precise image inversion. Specifically, we propose \emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized generative model, for image editing. We propose an efficient yet effective image inversion mechanism that integrates text prompting rectification and image style preservation, enabling precise image inversion. Furthermore, we devise a holistic smoothing strategy which allows our \emph{EditInfinity} to perform image editing with high fidelity to source images and precise semantic alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark across "add", "change", and "delete" editing operations, demonstrate the superior performance of our model compared to state-of-the-art diffusion-based baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.
<div><strong>Authors:</strong> Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "EditInfinity adapts the binary-quantized generative model Infinity for text-driven image editing by exploiting exact intermediate quantized representations to achieve precise image inversion and style preservation. The method introduces an efficient inversion mechanism with text prompting rectification and a holistic smoothing strategy, resulting in high-fidelity edits across add, change, and delete operations on the PIE-Bench benchmark, outperforming diffusion-based baselines.", "summary_cn": "EditInfinity 利用二值量化生成模型 Infinity 的精确中间量化表征，实现对源图像的精准反演和风格保持，从而进行基于文本的图像编辑。该方法提出了结合文本提示校正的高效反演机制以及全局平滑策略，在 PIE-Bench 基准的添加、修改、删除编辑任务上，显著提升编辑保真度，优于扩散模型基线。", "keywords": "binary-quantized generative model, VQ inversion, text-driven image editing, Infinity, image editing fidelity, quantized representations, PIE-Bench", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiahuan Wang", "Yuxin Chen", "Jun Yu", "Guangming Lu", "Wenjie Pei"]}
]]></acme>

<pubDate>2025-10-23T05:06:24+00:00</pubDate>
</item>
<item>
<title>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</title>
<link>https://papers.cool/arxiv/2510.20214</link>
<guid>https://papers.cool/arxiv/2510.20214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CURL, a self‑supervised contrastive learning framework that learns spatial‑temporal representations from long fetal ultrasound videos to detect fetal movements. By using a dual‑contrastive loss and a task‑specific sampling strategy, the method achieves 78% sensitivity and 81.6% AUROC on a dataset of 92 subjects. The results suggest that contrastive representation learning can provide an objective, automated tool for prenatal monitoring.<br /><strong>Summary (CN):</strong> 本文提出了 CURL，一种自监督对比学习框架，通过学习长时段胎儿超声视频的空间‑时间表征来检测胎儿运动。该方法利用双重对比损失和任务特定抽样策略，在 92 位受试者的数据上实现了 78% 的灵敏度和 81.6% 的 AUROC。实验表明，对比表征学习有望为产前监测提供客观、自动化的工具。<br /><strong>Keywords:</strong> contrastive learning, ultrasound video, fetal movement detection, self-supervised representation, medical imaging, probabilistic fine-tuning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad</div>
Accurate fetal movement (FM) detection is essential for assessing prenatal health, as abnormal movement patterns can indicate underlying complications such as placental dysfunction or fetal distress. Traditional methods, including maternal perception and cardiotocography (CTG), suffer from subjectivity and limited accuracy. To address these challenges, we propose Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised learning framework for FM detection from extended fetal ultrasound video recordings. Our approach leverages a dual-contrastive loss, incorporating both spatial and temporal contrastive learning, to learn robust motion representations. Additionally, we introduce a task-specific sampling strategy, ensuring the effective separation of movement and non-movement segments during self-supervised training, while enabling flexible inference on arbitrarily long ultrasound recordings through a probabilistic fine-tuning approach. Evaluated on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions, CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its potential for reliable and objective FM analysis. These results highlight the potential of self-supervised contrastive learning for fetal movement analysis, paving the way for improved prenatal monitoring and clinical decision-making.
<div><strong>Authors:</strong> Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CURL, a self‑supervised contrastive learning framework that learns spatial‑temporal representations from long fetal ultrasound videos to detect fetal movements. By using a dual‑contrastive loss and a task‑specific sampling strategy, the method achieves 78% sensitivity and 81.6% AUROC on a dataset of 92 subjects. The results suggest that contrastive representation learning can provide an objective, automated tool for prenatal monitoring.", "summary_cn": "本文提出了 CURL，一种自监督对比学习框架，通过学习长时段胎儿超声视频的空间‑时间表征来检测胎儿运动。该方法利用双重对比损失和任务特定抽样策略，在 92 位受试者的数据上实现了 78% 的灵敏度和 81.6% 的 AUROC。实验表明，对比表征学习有望为产前监测提供客观、自动化的工具。", "keywords": "contrastive learning, ultrasound video, fetal movement detection, self-supervised representation, medical imaging, probabilistic fine-tuning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Talha Ilyas", "Duong Nhu", "Allison Thomas", "Arie Levin", "Lim Wei Yap", "Shu Gong", "David Vera Anaya", "Yiwen Jiang", "Deval Mehta", "Ritesh Warty", "Vinayak Smith", "Maya Reddy", "Euan Wallace", "Wenlong Cheng", "Zongyuan Ge", "Faezeh Marzbanrad"]}
]]></acme>

<pubDate>2025-10-23T05:03:23+00:00</pubDate>
</item>
<item>
<title>FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing</title>
<link>https://papers.cool/arxiv/2510.20212</link>
<guid>https://papers.cool/arxiv/2510.20212</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FlowCycle introduces an inversion-free, flow-based image editing framework that learns target-aware intermediate states by optimizing learnable noises through a dual cycle-consistent process. By alternating edits from source to target and back, the method preserves editing-irrelevant content while achieving faithful modifications aligned with the textual prompt, outperforming prior approaches in quality and consistency.<br /><strong>Summary (CN):</strong> FlowCycle 提出一种无需逆向的基于 flow 的图像编辑框架，通过可学习噪声在双向循环一致性过程中的优化，生成针对目标感知的中间状态。该方法在源图像到目标图像以及返回源图像的循环中，保留与编辑无关的内容，实现与文本提示高度匹配的编辑，并在质量和一致性上优于现有方法。<br /><strong>Keywords:</strong> text-to-image, flow models, image editing, cycle-consistency, target-aware editing, inversion-free<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yanghao Wang, Zhen Wang, Long Chen</div>
Recent advances in pre-trained text-to-image flow models have enabled remarkable progress in text-based image editing. Mainstream approaches always adopt a corruption-then-restoration paradigm, where the source image is first corrupted into an ``intermediate state'' and then restored to the target image under the prompt guidance. However, current methods construct this intermediate state in a target-agnostic manner, i.e., they primarily focus on realizing source image reconstruction while neglecting the semantic gaps towards the specific editing target. This design inherently results in limited editability or inconsistency when the desired modifications substantially deviate from the source. In this paper, we argue that the intermediate state should be target-aware, i.e., selectively corrupting editing-relevant contents while preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel inversion-free and flow-based editing framework that parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process. By iteratively editing the source to the target and recovering back to the source with dual consistency constraints, FlowCycle learns to produce a target-aware intermediate state, enabling faithful modifications while preserving source consistency. Extensive ablations have demonstrated that FlowCycle achieves superior editing quality and consistency over state-of-the-art methods.
<div><strong>Authors:</strong> Yanghao Wang, Zhen Wang, Long Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FlowCycle introduces an inversion-free, flow-based image editing framework that learns target-aware intermediate states by optimizing learnable noises through a dual cycle-consistent process. By alternating edits from source to target and back, the method preserves editing-irrelevant content while achieving faithful modifications aligned with the textual prompt, outperforming prior approaches in quality and consistency.", "summary_cn": "FlowCycle 提出一种无需逆向的基于 flow 的图像编辑框架，通过可学习噪声在双向循环一致性过程中的优化，生成针对目标感知的中间状态。该方法在源图像到目标图像以及返回源图像的循环中，保留与编辑无关的内容，实现与文本提示高度匹配的编辑，并在质量和一致性上优于现有方法。", "keywords": "text-to-image, flow models, image editing, cycle-consistency, target-aware editing, inversion-free", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanghao Wang", "Zhen Wang", "Long Chen"]}
]]></acme>

<pubDate>2025-10-23T04:58:29+00:00</pubDate>
</item>
<item>
<title>RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</title>
<link>https://papers.cool/arxiv/2510.20206</link>
<guid>https://papers.cool/arxiv/2510.20206</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> RAPO++ proposes a cross-stage prompt optimization framework for text-to-video diffusion models, combining retrieval-augmented prompt enrichment, test-time iterative refinement using multi-source feedback, and LLM fine-tuning to internalize optimization patterns, thereby improving semantic alignment, compositionality, and temporal coherence without altering the generative backbone.<br /><strong>Summary (CN):</strong> RAPO++ 提出了一种跨阶段提示优化框架，用于文本到视频的扩散模型。该框架先通过检索增强的方式丰富用户提示，使其与训练数据对齐；随后在推理时利用多源反馈（如语义对齐、空间忠实度、时间一致性等）迭代优化提示；最后通过微调大型语言模型，使其在推理前即可生成高质量提示，从而提升生成视频的语义匹配、组合推理和时间一致性。<br /><strong>Keywords:</strong> text-to-video generation, prompt optimization, diffusion models, retrieval-augmented prompts, large language model fine-tuning, temporal coherence, compositional reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Bingjie Gao, Qianli Ma, Xiaoxue Wu, Shuai Yang, Guanzhou Lan, Haonan Zhao, Jiaxuan Chen, Qingyang Liu, Yu Qiao, Xinyuan Chen, Yaohui Wang, Li Niu</div>
Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present \textbf{RAPO++}, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In \textbf{Stage 1}, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. \textbf{Stage 2} introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. \textbf{Stage 3} leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.
<div><strong>Authors:</strong> Bingjie Gao, Qianli Ma, Xiaoxue Wu, Shuai Yang, Guanzhou Lan, Haonan Zhao, Jiaxuan Chen, Qingyang Liu, Yu Qiao, Xinyuan Chen, Yaohui Wang, Li Niu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "RAPO++ proposes a cross-stage prompt optimization framework for text-to-video diffusion models, combining retrieval-augmented prompt enrichment, test-time iterative refinement using multi-source feedback, and LLM fine-tuning to internalize optimization patterns, thereby improving semantic alignment, compositionality, and temporal coherence without altering the generative backbone.", "summary_cn": "RAPO++ 提出了一种跨阶段提示优化框架，用于文本到视频的扩散模型。该框架先通过检索增强的方式丰富用户提示，使其与训练数据对齐；随后在推理时利用多源反馈（如语义对齐、空间忠实度、时间一致性等）迭代优化提示；最后通过微调大型语言模型，使其在推理前即可生成高质量提示，从而提升生成视频的语义匹配、组合推理和时间一致性。", "keywords": "text-to-video generation, prompt optimization, diffusion models, retrieval-augmented prompts, large language model fine-tuning, temporal coherence, compositional reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bingjie Gao", "Qianli Ma", "Xiaoxue Wu", "Shuai Yang", "Guanzhou Lan", "Haonan Zhao", "Jiaxuan Chen", "Qingyang Liu", "Yu Qiao", "Xinyuan Chen", "Yaohui Wang", "Li Niu"]}
]]></acme>

<pubDate>2025-10-23T04:45:09+00:00</pubDate>
</item>
<item>
<title>A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development</title>
<link>https://papers.cool/arxiv/2510.20196</link>
<guid>https://papers.cool/arxiv/2510.20196</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a structured review and quantitative profiling of 54 public brain MRI datasets, analyzing modality composition, disease coverage, scale, and image-level characteristics such as voxel spacing and intensity distributions. It evaluates the impact of common preprocessing steps on voxel statistics and demonstrates residual covariate shift using a 3D DenseNet121, highlighting the limits of preprocessing for harmonizing heterogeneous datasets. The findings underline the need for preprocessing-aware and domain-adaptive strategies when developing generalizable brain MRI foundation models.<br /><strong>Summary (CN):</strong> 本文对 54 个公开的脑部 MRI 数据集进行结构化回顾和定量分析，评估了模态组成、疾病覆盖、规模以及图像层面的体素间距、方向和强度分布等特征。研究进一步量化了常用预处理（如强度归一化、偏场校正、颅骨去除、空间配准和插值）对体素统计和几何形状的影响，并通过 3D DenseNet121 实验显示标准化预处理后仍存在显著的协变量漂移。结果强调在构建通用脑部 MRI 基础模型时，需要采用考虑预处理差异的领域适应策略。<br /><strong>Keywords:</strong> brain MRI, foundation models, dataset heterogeneity, preprocessing, covariate shift, domain adaptation, 3D DenseNet<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Minh Sao Khue Luu, Margaret V. Benedichuk, Ekaterina I. Roppert, Roman M. Kenzhin, Bair N. Tuchinov</div>
The development of foundation models for brain MRI depends critically on the scale, diversity, and consistency of available data, yet systematic assessments of these factors remain scarce. In this study, we analyze 54 publicly accessible brain MRI datasets encompassing over 538,031 to provide a structured, multi-level overview tailored to foundation model development. At the dataset level, we characterize modality composition, disease coverage, and dataset scale, revealing strong imbalances between large healthy cohorts and smaller clinical populations. At the image level, we quantify voxel spacing, orientation, and intensity distributions across 15 representative datasets, demonstrating substantial heterogeneity that can influence representation learning. We then perform a quantitative evaluation of preprocessing variability, examining how intensity normalization, bias field correction, skull stripping, spatial registration, and interpolation alter voxel statistics and geometry. While these steps improve within-dataset consistency, residual differences persist between datasets. Finally, feature-space case study using a 3D DenseNet121 shows measurable residual covariate shift after standardized preprocessing, confirming that harmonization alone cannot eliminate inter-dataset bias. Together, these analyses provide a unified characterization of variability in public brain MRI resources and emphasize the need for preprocessing-aware and domain-adaptive strategies in the design of generalizable brain MRI foundation models.
<div><strong>Authors:</strong> Minh Sao Khue Luu, Margaret V. Benedichuk, Ekaterina I. Roppert, Roman M. Kenzhin, Bair N. Tuchinov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a structured review and quantitative profiling of 54 public brain MRI datasets, analyzing modality composition, disease coverage, scale, and image-level characteristics such as voxel spacing and intensity distributions. It evaluates the impact of common preprocessing steps on voxel statistics and demonstrates residual covariate shift using a 3D DenseNet121, highlighting the limits of preprocessing for harmonizing heterogeneous datasets. The findings underline the need for preprocessing-aware and domain-adaptive strategies when developing generalizable brain MRI foundation models.", "summary_cn": "本文对 54 个公开的脑部 MRI 数据集进行结构化回顾和定量分析，评估了模态组成、疾病覆盖、规模以及图像层面的体素间距、方向和强度分布等特征。研究进一步量化了常用预处理（如强度归一化、偏场校正、颅骨去除、空间配准和插值）对体素统计和几何形状的影响，并通过 3D DenseNet121 实验显示标准化预处理后仍存在显著的协变量漂移。结果强调在构建通用脑部 MRI 基础模型时，需要采用考虑预处理差异的领域适应策略。", "keywords": "brain MRI, foundation models, dataset heterogeneity, preprocessing, covariate shift, domain adaptation, 3D DenseNet", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Minh Sao Khue Luu", "Margaret V. Benedichuk", "Ekaterina I. Roppert", "Roman M. Kenzhin", "Bair N. Tuchinov"]}
]]></acme>

<pubDate>2025-10-23T04:31:09+00:00</pubDate>
</item>
<item>
<title>SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization</title>
<link>https://papers.cool/arxiv/2510.20189</link>
<guid>https://papers.cool/arxiv/2510.20189</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SPAN, a Suspicion Progression Analysis Network that models suspicious intentions in video surveillance as a continuous regression problem rather than discrete classes. By leveraging temporal point process theory, multimodal suspicion coefficient modulation, and concept‑anchored mapping, SPAN captures long‑term dependencies and links actions to intention concepts, achieving lower MSE and higher mAP on the HAI dataset, especially for subtle, low‑frequency behaviors.<br /><strong>Summary (CN):</strong> 本文提出 SPAN（Suspicion Progression Analysis Network），将视频监控中的可疑意图从离散分类转为连续回归建模。利用时间点过程理论、跨模态可疑系数调制以及概念锚定映射，捕获可疑行为的长期依赖并将动作与预定义意图概念关联，在 HAI 数据集上显著降低 MSE 并提升 mAP，尤其在低频微弱行为上表现更佳。<br /><strong>Keywords:</strong> temporal intention localization, suspicion progression, continuous regression, video surveillance, multimodal modulation, concept-anchored mapping, temporal point process, anomaly detection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Xinyi Hu, Yuran Wang, Yue Li, Wenxuan Liu, Zheng Wang</div>
Temporal Intention Localization (TIL) is crucial for video surveillance, focusing on identifying varying levels of suspicious intentions to improve security monitoring. However, existing discrete classification methods fail to capture the continuous nature of suspicious intentions, limiting early intervention and explainability. In this paper, we propose the Suspicion Progression Analysis Network (SPAN), which shifts from discrete classification to continuous regression, enabling the capture of fluctuating and evolving suspicious intentions. We reveal that suspicion exhibits long-term dependencies and cumulative effects, similar to Temporal Point Process (TPP) theory. Based on these insights, we define a suspicion score formula that models continuous changes while accounting for temporal characteristics. We also introduce Suspicion Coefficient Modulation, which adjusts suspicion coefficients using multimodal information to reflect the varying impacts of suspicious actions. Additionally, the Concept-Anchored Mapping method is proposed to link suspicious actions to predefined intention concepts, offering insights into both the actions and their potential underlying intentions. Extensive experiments on the HAI dataset show that SPAN significantly outperforms existing methods, reducing MSE by 19.8% and improving average mAP by 1.78%. Notably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating its superior ability to capture subtle behavioral changes. Compared to discrete classification systems, our continuous suspicion modeling approach enables earlier detection and proactive intervention, greatly enhancing system explainability and practical utility in security applications.
<div><strong>Authors:</strong> Xinyi Hu, Yuran Wang, Yue Li, Wenxuan Liu, Zheng Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SPAN, a Suspicion Progression Analysis Network that models suspicious intentions in video surveillance as a continuous regression problem rather than discrete classes. By leveraging temporal point process theory, multimodal suspicion coefficient modulation, and concept‑anchored mapping, SPAN captures long‑term dependencies and links actions to intention concepts, achieving lower MSE and higher mAP on the HAI dataset, especially for subtle, low‑frequency behaviors.", "summary_cn": "本文提出 SPAN（Suspicion Progression Analysis Network），将视频监控中的可疑意图从离散分类转为连续回归建模。利用时间点过程理论、跨模态可疑系数调制以及概念锚定映射，捕获可疑行为的长期依赖并将动作与预定义意图概念关联，在 HAI 数据集上显著降低 MSE 并提升 mAP，尤其在低频微弱行为上表现更佳。", "keywords": "temporal intention localization, suspicion progression, continuous regression, video surveillance, multimodal modulation, concept-anchored mapping, temporal point process, anomaly detection", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xinyi Hu", "Yuran Wang", "Yue Li", "Wenxuan Liu", "Zheng Wang"]}
]]></acme>

<pubDate>2025-10-23T04:20:07+00:00</pubDate>
</item>
<item>
<title>Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories</title>
<link>https://papers.cool/arxiv/2510.20182</link>
<guid>https://papers.cool/arxiv/2510.20182</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of multi‑person pedestrian dynamics, including a method to reconstruct 2D bird's‑eye view trajectories without known camera parameters. Using start frames from established datasets and a prompt suite for varying pedestrian densities, the authors assess how well leading models capture plausible multi‑agent behavior and identify failure modes such as merging and disappearing agents.<br /><strong>Summary (CN):</strong> 本文提出了一套评估方案，用于基准测试文本到视频（T2V）和图像到视频（I2V）模型在多行人轨迹模拟中的表现，并设计了一种在未知摄像机参数条件下从像素空间重建二维鸟瞰轨迹的方法。通过使用已有数据集的起始帧以及针对不同行人密度的提示集合，作者评估了主流模型在捕捉合理多主体行为方面的能力，并指出了合并和人物消失等失效模式。<br /><strong>Keywords:</strong> video generation, text-to-video, image-to-video, pedestrian trajectory simulation, multi-agent dynamics, evaluation benchmark, trajectory reconstruction, world simulation<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Aaron Appelle, Jerome P. Lynch</div>
Large-scale video generation models have demonstrated high visual realism in diverse contexts, spurring interest in their potential as general-purpose world simulators. Existing benchmarks focus on individual subjects rather than scenes with multiple interacting people. However, the plausibility of multi-agent dynamics in generated videos remains unverified. We propose a rigorous evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of pedestrian dynamics. For I2V, we leverage start frames from established datasets to enable comparison with a ground truth video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian densities and interactions. A key component is a method to reconstruct 2D bird's-eye view trajectories from pixel-space without known camera parameters. Our analysis reveals that leading models have learned surprisingly effective priors for plausible multi-agent behavior. However, failure modes like merging and disappearing people highlight areas for future improvement.
<div><strong>Authors:</strong> Aaron Appelle, Jerome P. Lynch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of multi‑person pedestrian dynamics, including a method to reconstruct 2D bird's‑eye view trajectories without known camera parameters. Using start frames from established datasets and a prompt suite for varying pedestrian densities, the authors assess how well leading models capture plausible multi‑agent behavior and identify failure modes such as merging and disappearing agents.", "summary_cn": "本文提出了一套评估方案，用于基准测试文本到视频（T2V）和图像到视频（I2V）模型在多行人轨迹模拟中的表现，并设计了一种在未知摄像机参数条件下从像素空间重建二维鸟瞰轨迹的方法。通过使用已有数据集的起始帧以及针对不同行人密度的提示集合，作者评估了主流模型在捕捉合理多主体行为方面的能力，并指出了合并和人物消失等失效模式。", "keywords": "video generation, text-to-video, image-to-video, pedestrian trajectory simulation, multi-agent dynamics, evaluation benchmark, trajectory reconstruction, world simulation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Aaron Appelle", "Jerome P. Lynch"]}
]]></acme>

<pubDate>2025-10-23T04:06:58+00:00</pubDate>
</item>
<item>
<title>PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</title>
<link>https://papers.cool/arxiv/2510.20178</link>
<guid>https://papers.cool/arxiv/2510.20178</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PPMStereo, a method that employs a Pick-and-Play Memory (PPM) module to achieve temporally consistent depth estimation in stereo video while remaining computationally efficient. PPM selects the most relevant past frames ('pick') and adaptively weights them ('play') to aggregate spatio‑temporal information, resulting in state‑of‑the‑art accuracy and consistency. Extensive experiments demonstrate significant improvements over prior dynamic stereo methods.<br /><strong>Summary (CN):</strong> 本文提出 PPMStereo，一种利用 Pick-and-Play Memory（PPM）模块实现立体视频时间一致深度估计且计算高效的方法。PPM 通过“pick”步骤挑选最相关的历史帧，再通过“play”步骤对选中的帧进行自适应加权，实现时空信息的聚合，取得了在准确率和一致性上的领先表现。大量实验验证了其相较于现有动态立体匹配方法的显著提升。<br /><strong>Keywords:</strong> dynamic stereo matching, temporally consistent depth estimation, memory buffer, pick-and-play memory, spatio-temporal aggregation, computer vision<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu</div>
Temporally consistent depth estimation from stereo video is critical for real-world applications such as augmented reality, where inconsistent depth estimation disrupts the immersion of users. Despite its importance, this task remains challenging due to the difficulty in modeling long-term temporal consistency in a computationally efficient manner. Previous methods attempt to address this by aggregating spatio-temporal information but face a fundamental trade-off: limited temporal modeling provides only modest gains, whereas capturing long-range dependencies significantly increases computational cost. To address this limitation, we introduce a memory buffer for modeling long-range spatio-temporal consistency while achieving efficient dynamic stereo matching. Inspired by the two-stage decision-making process in humans, we propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM consists of a `pick' process that identifies the most relevant frames and a `play' process that weights the selected frames adaptively for spatio-temporal aggregation. This two-stage collaborative process maintains a compact yet highly informative memory buffer while achieving temporally consistent information aggregation. Extensive experiments validate the effectiveness of PPMStereo, demonstrating state-of-the-art performance in both accuracy and temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer computational costs. Codes are available at \textcolor{blue}{https://github.com/cocowy1/PPMStereo}.
<div><strong>Authors:</strong> Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PPMStereo, a method that employs a Pick-and-Play Memory (PPM) module to achieve temporally consistent depth estimation in stereo video while remaining computationally efficient. PPM selects the most relevant past frames ('pick') and adaptively weights them ('play') to aggregate spatio‑temporal information, resulting in state‑of‑the‑art accuracy and consistency. Extensive experiments demonstrate significant improvements over prior dynamic stereo methods.", "summary_cn": "本文提出 PPMStereo，一种利用 Pick-and-Play Memory（PPM）模块实现立体视频时间一致深度估计且计算高效的方法。PPM 通过“pick”步骤挑选最相关的历史帧，再通过“play”步骤对选中的帧进行自适应加权，实现时空信息的聚合，取得了在准确率和一致性上的领先表现。大量实验验证了其相较于现有动态立体匹配方法的显著提升。", "keywords": "dynamic stereo matching, temporally consistent depth estimation, memory buffer, pick-and-play memory, spatio-temporal aggregation, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yun Wang", "Junjie Hu", "Qiaole Dong", "Yongjian Zhang", "Yanwei Fu", "Tin Lun Lam", "Dapeng Wu"]}
]]></acme>

<pubDate>2025-10-23T03:52:39+00:00</pubDate>
</item>
<item>
<title>IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
<link>https://papers.cool/arxiv/2510.20165</link>
<guid>https://papers.cool/arxiv/2510.20165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces IB‑GAN, a generative adversarial network that incorporates the Information Bottleneck principle to learn disentangled representations. By constraining the mutual information between the input and an intermediate stochastic layer of the generator, the model yields a learnable latent distribution that produces interpretable and diverse samples, achieving competitive disentanglement scores on dSprites and superior FID scores on CelebA and 3D Chairs compared to β‑VAEs and InfoGAN.<br /><strong>Summary (CN):</strong> 本文提出 IB‑GAN，一种将信息瓶颈（Information Bottleneck）原理引入生成对抗网络以实现可分离表征学习的模型。通过在生成器的中间随机层约束输入与输出之间的互信息，模型能够学习可解释的潜在分布，生成的样本在 dSprites 上的可分离性指标与最新 β‑VAE 相当，并在 CelebA 与 3D Chairs 上的 FID 分数超过 InfoGAN 和 β‑VAE。<br /><strong>Keywords:</strong> disentangled representation, Information Bottleneck, GAN, InfoGAN, latent space, mutual information, unsupervised learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim</div>
We propose a new GAN-based unsupervised model for disentangled representation learning. The new model is discovered in an attempt to utilize the Information Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The architecture of IB-GAN is partially similar to that of InfoGAN but has a critical difference; an intermediate layer of the generator is leveraged to constrain the mutual information between the input and the generated output. The intermediate stochastic layer can serve as a learnable latent distribution that is trained with the generator jointly in an end-to-end fashion. As a result, the generator of IB-GAN can harness the latent space in a disentangled and interpretable manner. With the experiments on dSprites and Color-dSprites dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover, the visual quality and the diversity of samples generated by IB-GAN are often better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA and 3D Chairs dataset.
<div><strong>Authors:</strong> Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces IB‑GAN, a generative adversarial network that incorporates the Information Bottleneck principle to learn disentangled representations. By constraining the mutual information between the input and an intermediate stochastic layer of the generator, the model yields a learnable latent distribution that produces interpretable and diverse samples, achieving competitive disentanglement scores on dSprites and superior FID scores on CelebA and 3D Chairs compared to β‑VAEs and InfoGAN.", "summary_cn": "本文提出 IB‑GAN，一种将信息瓶颈（Information Bottleneck）原理引入生成对抗网络以实现可分离表征学习的模型。通过在生成器的中间随机层约束输入与输出之间的互信息，模型能够学习可解释的潜在分布，生成的样本在 dSprites 上的可分离性指标与最新 β‑VAE 相当，并在 CelebA 与 3D Chairs 上的 FID 分数超过 InfoGAN 和 β‑VAE。", "keywords": "disentangled representation, Information Bottleneck, GAN, InfoGAN, latent space, mutual information, unsupervised learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Insu Jeon", "Wonkwang Lee", "Myeongjang Pyeon", "Gunhee Kim"]}
]]></acme>

<pubDate>2025-10-23T03:24:48+00:00</pubDate>
</item>
<item>
<title>TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</title>
<link>https://papers.cool/arxiv/2510.20162</link>
<guid>https://papers.cool/arxiv/2510.20162</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces TOMCAT, a test-time method for compositional zero-shot learning that accumulates visual and textual knowledge from unsupervised data to update multimodal prototypes. It employs an adaptive update weight and a dynamic priority queue of high‑confidence images to mitigate distribution shift, and aligns textual and visual prototypes via collaborative representation learning, achieving state‑of‑the‑art results on multiple benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了 TOMCAT，一种用于组合零样本学习的测试时方法，通过从无监督数据中积累视觉和文本知识来更新多模态原型。该方法使用自适应更新权重和存储高置信度图像的动态优先队列，以缓解标签空间分布转移，并通过多模态协作表示学习对齐文本与视觉原型，在多个基准上实现了最先进的性能。<br /><strong>Keywords:</strong> compositional zero-shot learning, test-time adaptation, multimodal prototypes, knowledge accumulation, dynamic priority queue<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xudong Yan, Songhe Feng</div>
Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at https://github.com/xud-yan/TOMCAT .
<div><strong>Authors:</strong> Xudong Yan, Songhe Feng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces TOMCAT, a test-time method for compositional zero-shot learning that accumulates visual and textual knowledge from unsupervised data to update multimodal prototypes. It employs an adaptive update weight and a dynamic priority queue of high‑confidence images to mitigate distribution shift, and aligns textual and visual prototypes via collaborative representation learning, achieving state‑of‑the‑art results on multiple benchmarks.", "summary_cn": "本文提出了 TOMCAT，一种用于组合零样本学习的测试时方法，通过从无监督数据中积累视觉和文本知识来更新多模态原型。该方法使用自适应更新权重和存储高置信度图像的动态优先队列，以缓解标签空间分布转移，并通过多模态协作表示学习对齐文本与视觉原型，在多个基准上实现了最先进的性能。", "keywords": "compositional zero-shot learning, test-time adaptation, multimodal prototypes, knowledge accumulation, dynamic priority queue", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xudong Yan", "Songhe Feng"]}
]]></acme>

<pubDate>2025-10-23T03:20:29+00:00</pubDate>
</item>
<item>
<title>Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists</title>
<link>https://papers.cool/arxiv/2510.20158</link>
<guid>https://papers.cool/arxiv/2510.20158</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a category-level 8D pose estimation method that predicts the full 3D translation, rotation, and the steering‑handle and pedal angles of articulated bicycles and cyclists from a single RGB image. By jointly estimating 8D pose and 3D keypoints using a mix of synthetic and real data, the approach outperforms state-of-the-art 6D pose estimators on articulated objects. This enables finer-grained travel‑direction inference for vulnerable road users in autonomous driving scenarios.<br /><strong>Summary (CN):</strong> 本文提出一种类别级的 8D 位姿估计方法，能够仅凭单张 RGB 图像预测自行车和骑行者的 3D 平移、旋转以及车把和踏板相对于车体的角度。该模型通过联合估计 8D 位姿和 3D 关键点，并结合合成与真实数据进行训练，在关节化目标上优于现有的 6D 位姿估计器。此技术为自动驾驶中的弱势道路使用者提供更精细的行进方向推断，有助于提升安全性。<br /><strong>Keywords:</strong> monocular pose estimation, 8D pose, articulated bicycles, cyclists, autonomous driving, category-level pose, synthetic training data, keypoint detection, VRU safety<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing</div>
In Autonomous Driving, cyclists belong to the safety-critical class of Vulnerable Road Users (VRU), and accurate estimation of their pose is critical for cyclist crossing intention classification, behavior prediction, and collision avoidance. Unlike rigid objects, articulated bicycles are composed of movable rigid parts linked by joints and constrained by a kinematic structure. 6D pose methods can estimate the 3D rotation and translation of rigid bicycles, but 6D becomes insufficient when the steering/pedals angles of the bicycle vary. That is because: 1) varying the articulated pose of the bicycle causes its 3D bounding box to vary as well, and 2) the 3D box orientation is not necessarily aligned to the orientation of the steering which determines the actual intended travel direction. In this work, we introduce a method for category-level 8D pose estimation for articulated bicycles and cyclists from a single RGB image. Besides being able to estimate the 3D translation and rotation of a bicycle from a single image, our method also estimates the rotations of its steering handles and pedals with respect to the bicycle body frame. These two new parameters enable the estimation of a more fine-grained bicycle pose state and travel direction. Our proposed model jointly estimates the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix of synthetic and real image data to generalize on real images. We include an evaluation section where we evaluate the accuracy of our estimated 8D pose parameters, and our method shows promising results by achieving competitive scores when compared against state-of-the-art category-level 6D pose estimators that use rigid canonical object templates for matching.
<div><strong>Authors:</strong> Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a category-level 8D pose estimation method that predicts the full 3D translation, rotation, and the steering‑handle and pedal angles of articulated bicycles and cyclists from a single RGB image. By jointly estimating 8D pose and 3D keypoints using a mix of synthetic and real data, the approach outperforms state-of-the-art 6D pose estimators on articulated objects. This enables finer-grained travel‑direction inference for vulnerable road users in autonomous driving scenarios.", "summary_cn": "本文提出一种类别级的 8D 位姿估计方法，能够仅凭单张 RGB 图像预测自行车和骑行者的 3D 平移、旋转以及车把和踏板相对于车体的角度。该模型通过联合估计 8D 位姿和 3D 关键点，并结合合成与真实数据进行训练，在关节化目标上优于现有的 6D 位姿估计器。此技术为自动驾驶中的弱势道路使用者提供更精细的行进方向推断，有助于提升安全性。", "keywords": "monocular pose estimation, 8D pose, articulated bicycles, cyclists, autonomous driving, category-level pose, synthetic training data, keypoint detection, VRU safety", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Eduardo R. Corral-Soto", "Yang Liu", "Yuan Ren", "Bai Dongfeng", "Liu Bingbing"]}
]]></acme>

<pubDate>2025-10-23T03:17:22+00:00</pubDate>
</item>
<item>
<title>PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</title>
<link>https://papers.cool/arxiv/2510.20155</link>
<guid>https://papers.cool/arxiv/2510.20155</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PartNeXt, a large-scale dataset of over 23,000 textured 3D models with fine-grained hierarchical part annotations across 50 categories. It evaluates state-of-the-art part segmentation and a new 3D part-centric question-answering benchmark for 3D-LLMs, showing that existing methods struggle with leaf-level parts and that training on PartNeXt improves performance over PartNet. The dataset aims to advance structured 3D understanding by providing scalable, texture-aware annotations and multi-task evaluation.<br /><strong>Summary (CN):</strong> 本文推出 PartNeXt 数据集，包含超过 23,000 个高质量、带纹理的 3D 模型，提供跨 50 类的细粒度层次化部件标签。通过对类别无关部件分割和新设的 3D 部件问答基准进行评估，发现现有方法在细粒度叶层部件上表现不足，并且在 PartNeXt 上训练的 Point‑SAM 相较 PartNet 有显著提升。该数据集通过可扩展的标注、纹理感知标签和多任务评估，推动结构化 3D 理解研究。<br /><strong>Keywords:</strong> PartNeXt, 3D part segmentation, hierarchical annotation, textured 3D models, fine-grained parts, 3D-LLM, point cloud dataset, PartField, SAMPart3D, Point-SAM<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, Jiayuan Gu</div>
Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.
<div><strong>Authors:</strong> Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, Jiayuan Gu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PartNeXt, a large-scale dataset of over 23,000 textured 3D models with fine-grained hierarchical part annotations across 50 categories. It evaluates state-of-the-art part segmentation and a new 3D part-centric question-answering benchmark for 3D-LLMs, showing that existing methods struggle with leaf-level parts and that training on PartNeXt improves performance over PartNet. The dataset aims to advance structured 3D understanding by providing scalable, texture-aware annotations and multi-task evaluation.", "summary_cn": "本文推出 PartNeXt 数据集，包含超过 23,000 个高质量、带纹理的 3D 模型，提供跨 50 类的细粒度层次化部件标签。通过对类别无关部件分割和新设的 3D 部件问答基准进行评估，发现现有方法在细粒度叶层部件上表现不足，并且在 PartNeXt 上训练的 Point‑SAM 相较 PartNet 有显著提升。该数据集通过可扩展的标注、纹理感知标签和多任务评估，推动结构化 3D 理解研究。", "keywords": "PartNeXt, 3D part segmentation, hierarchical annotation, textured 3D models, fine-grained parts, 3D-LLM, point cloud dataset, PartField, SAMPart3D, Point-SAM", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Penghao Wang", "Yiyang He", "Xin Lv", "Yukai Zhou", "Lan Xu", "Jingyi Yu", "Jiayuan Gu"]}
]]></acme>

<pubDate>2025-10-23T03:06:08+00:00</pubDate>
</item>
<item>
<title>Revisiting Logit Distributions for Reliable Out-of-Distribution Detection</title>
<link>https://papers.cool/arxiv/2510.20134</link>
<guid>https://papers.cool/arxiv/2510.20134</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces LogitGap, a post-hoc out-of-distribution detection method that leverages the gap between the maximum logit and the remaining logits to improve separability of in‑distribution and OOD samples. A training‑free strategy automatically selects the most informative subset of logits for scoring, and both theoretical analysis and extensive experiments on vision and vision‑language models show state‑of‑the‑art performance.<br /><strong>Summary (CN):</strong> 本文提出 LogitGap，一种后置 OOD 检测方法，利用最大 logit 与其余 logit 之间的差距增强 ID 与 OOD 样本的可分性。通过训练无关的策略自动挑选最具信息量的 logit 子集进行打分，并提供理论分析与在视觉及视觉‑语言模型上的大量实验，验证了其在各类 OOD 场景中的领先表现。<br /><strong>Keywords:</strong> OOD detection, logits, LogitGap, post-hoc, training-free, reliability, out-of-distribution<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Jiachen Liang, Ruibing Hou, Minyang Hu, Hong Chang, Shiguang Shan, Xilin Chen</div>
Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning models in open-world applications. While post-hoc methods are favored for their efficiency and ease of deployment, existing approaches often underexploit the rich information embedded in the model's logits space. In this paper, we propose LogitGap, a novel post-hoc OOD detection method that explicitly exploits the relationship between the maximum logit and the remaining logits to enhance the separability between in-distribution (ID) and OOD samples. To further improve its effectiveness, we refine LogitGap by focusing on a more compact and informative subset of the logit space. Specifically, we introduce a training-free strategy that automatically identifies the most informative logits for scoring. We provide both theoretical analysis and empirical evidence to validate the effectiveness of our approach. Extensive experiments on both vision-language and vision-only models demonstrate that LogitGap consistently achieves state-of-the-art performance across diverse OOD detection scenarios and benchmarks. Code is available at https://github.com/GIT-LJc/LogitGap.
<div><strong>Authors:</strong> Jiachen Liang, Ruibing Hou, Minyang Hu, Hong Chang, Shiguang Shan, Xilin Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces LogitGap, a post-hoc out-of-distribution detection method that leverages the gap between the maximum logit and the remaining logits to improve separability of in‑distribution and OOD samples. A training‑free strategy automatically selects the most informative subset of logits for scoring, and both theoretical analysis and extensive experiments on vision and vision‑language models show state‑of‑the‑art performance.", "summary_cn": "本文提出 LogitGap，一种后置 OOD 检测方法，利用最大 logit 与其余 logit 之间的差距增强 ID 与 OOD 样本的可分性。通过训练无关的策略自动挑选最具信息量的 logit 子集进行打分，并提供理论分析与在视觉及视觉‑语言模型上的大量实验，验证了其在各类 OOD 场景中的领先表现。", "keywords": "OOD detection, logits, LogitGap, post-hoc, training-free, reliability, out-of-distribution", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Jiachen Liang", "Ruibing Hou", "Minyang Hu", "Hong Chang", "Shiguang Shan", "Xilin Chen"]}
]]></acme>

<pubDate>2025-10-23T02:16:45+00:00</pubDate>
</item>
<item>
<title>Inverse Image-Based Rendering for Light Field Generation from Single Images</title>
<link>https://papers.cool/arxiv/2510.20132</link>
<guid>https://papers.cool/arxiv/2510.20132</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces inverse image-based rendering, a neural rendering pipeline that reconstructs light flows from a single image to synthesize novel views and generate full light fields without additional training or specialized hardware. By storing source ray light flows, using cross-attention to model their relationships, and iteratively updating occluded content, the method outperforms existing novel view synthesis approaches on diverse datasets.<br /><strong>Summary (CN):</strong> 本文提出逆向基于图像的渲染（inverse image-based rendering）方法，利用神经渲染管线从单张输入图像重建光流，从而合成任意视点的光场，无需额外硬件或重新训练。该方法先存储源光线的光流，通过交叉注意力建模光线关系，再预测目标光线颜色，并在迭代过程中更新遮挡区域，实现一致的全光场生成，在多个挑战性数据集上优于现有新视点合成技术。<br /><strong>Keywords:</strong> light field, inverse image-based rendering, neural rendering, novel view synthesis, light flow, cross-attention<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hyunjun Jung, Hae-Gon Jeon</div>
A concept of light-fields computed from multiple view images on regular grids has proven its benefit for scene representations, and supported realistic renderings of novel views and photographic effects such as refocusing and shallow depth of field. In spite of its effectiveness of light flow computations, obtaining light fields requires either computational costs or specialized devices like a bulky camera setup and a specialized microlens array. In an effort to broaden its benefit and applicability, in this paper, we propose a novel view synthesis method for light field generation from only single images, named inverse image-based rendering. Unlike previous attempts to implicitly rebuild 3D geometry or to explicitly represent objective scenes, our method reconstructs light flows in a space from image pixels, which behaves in the opposite way to image-based rendering. To accomplish this, we design a neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our neural renderer first stores the light flow of source rays from the input image, then computes the relationships among them through cross-attention, and finally predicts the color of the target ray based on these relationships. After the rendering pipeline generates the first novel view from a single input image, the generated out-of-view contents are updated to the set of source rays. This procedure is iteratively performed while ensuring the consistent generation of occluded contents. We demonstrate that our inverse image-based rendering works well with various challenging datasets without any retraining or finetuning after once trained on synthetic dataset, and outperforms relevant state-of-the-art novel view synthesis methods.
<div><strong>Authors:</strong> Hyunjun Jung, Hae-Gon Jeon</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces inverse image-based rendering, a neural rendering pipeline that reconstructs light flows from a single image to synthesize novel views and generate full light fields without additional training or specialized hardware. By storing source ray light flows, using cross-attention to model their relationships, and iteratively updating occluded content, the method outperforms existing novel view synthesis approaches on diverse datasets.", "summary_cn": "本文提出逆向基于图像的渲染（inverse image-based rendering）方法，利用神经渲染管线从单张输入图像重建光流，从而合成任意视点的光场，无需额外硬件或重新训练。该方法先存储源光线的光流，通过交叉注意力建模光线关系，再预测目标光线颜色，并在迭代过程中更新遮挡区域，实现一致的全光场生成，在多个挑战性数据集上优于现有新视点合成技术。", "keywords": "light field, inverse image-based rendering, neural rendering, novel view synthesis, light flow, cross-attention", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hyunjun Jung", "Hae-Gon Jeon"]}
]]></acme>

<pubDate>2025-10-23T02:12:45+00:00</pubDate>
</item>
<item>
<title>Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects</title>
<link>https://papers.cool/arxiv/2510.20126</link>
<guid>https://papers.cool/arxiv/2510.20126</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a system that combines deep learning detection with a physics‑based tracking algorithm to robustly detect and track fast‑moving tiny objects using an RGB‑D camera. By integrating kinematic motion equations and an outlier correction module, the approach reduces average displacement error by up to 70% compared to Kalman‑filter trackers on a custom racquetball dataset. The work targets improved robot perception for autonomous platforms.<br /><strong>Summary (CN):</strong> 本文提出了一套将深度学习检测与基于物理的追踪算法相结合的系统，用于在 RGB‑D 相机下鲁棒地检测和跟踪快速移动的微小物体。通过融合运动学方程并加入异常检测与校正模块，该方法在自建的壁球数据集上相比卡尔曼滤波追踪器将平均位移误差降低了最高 70%。该工作旨在提升自主平台机器人的感知能力。<br /><strong>Keywords:</strong> physics-based tracking, 3D tracking, fast moving small objects, RGB-D, outlier detection, kinematics, robot perception<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Prithvi Raj Singh, Raju Gottumukkala, Anthony S. Maida, Alan B. Barhorst, Vijaya Gopu</div>
While computer vision has advanced considerably for general object detection and tracking, the specific problem of fast-moving tiny objects remains underexplored. This paper addresses the significant challenge of detecting and tracking rapidly moving small objects using an RGB-D camera. Our novel system combines deep learning-based detection with physics-based tracking to overcome the limitations of existing approaches. Our contributions include: (1) a comprehensive system design for object detection and tracking of fast-moving small objects in 3D space, (2) an innovative physics-based tracking algorithm that integrates kinematics motion equations to handle outliers and missed detections, and (3) an outlier detection and correction module that significantly improves tracking performance in challenging scenarios such as occlusions and rapid direction changes. We evaluated our proposed system on a custom racquetball dataset. Our evaluation shows our system surpassing kalman filter based trackers with up to 70\% less Average Displacement Error. Our system has significant applications for improving robot perception on autonomous platforms and demonstrates the effectiveness of combining physics-based models with deep learning approaches for real-time 3D detection and tracking of challenging small objects.
<div><strong>Authors:</strong> Prithvi Raj Singh, Raju Gottumukkala, Anthony S. Maida, Alan B. Barhorst, Vijaya Gopu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a system that combines deep learning detection with a physics‑based tracking algorithm to robustly detect and track fast‑moving tiny objects using an RGB‑D camera. By integrating kinematic motion equations and an outlier correction module, the approach reduces average displacement error by up to 70% compared to Kalman‑filter trackers on a custom racquetball dataset. The work targets improved robot perception for autonomous platforms.", "summary_cn": "本文提出了一套将深度学习检测与基于物理的追踪算法相结合的系统，用于在 RGB‑D 相机下鲁棒地检测和跟踪快速移动的微小物体。通过融合运动学方程并加入异常检测与校正模块，该方法在自建的壁球数据集上相比卡尔曼滤波追踪器将平均位移误差降低了最高 70%。该工作旨在提升自主平台机器人的感知能力。", "keywords": "physics-based tracking, 3D tracking, fast moving small objects, RGB-D, outlier detection, kinematics, robot perception", "scoring": {"interpretability": 2, "understanding": 4, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"]}
]]></acme>

<pubDate>2025-10-23T02:00:58+00:00</pubDate>
</item>
<item>
<title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20095</link>
<guid>https://papers.cool/arxiv/2510.20095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BIOCAP, a biological multimodal foundation model that leverages synthetic descriptive captions generated by multimodal large language models to complement image labels. By aligning images and captions in a shared latent morphospace, BIOCAP improves species classification and text‑image retrieval performance. The work demonstrates that domain‑specific caption generation can reduce hallucination and provide instance‑specific supervision for organismal biology.<br /><strong>Summary (CN):</strong> 本文提出 BIOCAP，一种利用多模态大语言模型生成的合成描述性字幕来补充图像标签的生物多模态基础模型。通过在共享的潜在形态空间中对齐图像和字幕，BIOCAP 在物种分类和文本‑图像检索上取得了显著提升。该工作展示了领域特定的字幕生成能够降低幻觉并为生物学实例提供具体监督。<br /><strong>Keywords:</strong> synthetic captions, multimodal foundation models, biological vision, species classification, text-image retrieval, multimodal large language models, BIOCAP, BIOCLIP, latent morphospace<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BIOCAP (i.e., BIOCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models.
<div><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BIOCAP, a biological multimodal foundation model that leverages synthetic descriptive captions generated by multimodal large language models to complement image labels. By aligning images and captions in a shared latent morphospace, BIOCAP improves species classification and text‑image retrieval performance. The work demonstrates that domain‑specific caption generation can reduce hallucination and provide instance‑specific supervision for organismal biology.", "summary_cn": "本文提出 BIOCAP，一种利用多模态大语言模型生成的合成描述性字幕来补充图像标签的生物多模态基础模型。通过在共享的潜在形态空间中对齐图像和字幕，BIOCAP 在物种分类和文本‑图像检索上取得了显著提升。该工作展示了领域特定的字幕生成能够降低幻觉并为生物学实例提供具体监督。", "keywords": "synthetic captions, multimodal foundation models, biological vision, species classification, text-image retrieval, multimodal large language models, BIOCAP, BIOCLIP, latent morphospace", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"]}
]]></acme>

<pubDate>2025-10-23T00:34:21+00:00</pubDate>
</item>
<item>
<title>StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback</title>
<link>https://papers.cool/arxiv/2510.20093</link>
<guid>https://papers.cool/arxiv/2510.20093</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> StableSketcher fine-tunes a diffusion model's variational autoencoder and adds a reinforcement‑learning reward based on visual question answering to improve the fidelity of pixel‑based human sketch generation. The framework achieves better text‑image alignment and releases SketchDUO, a new dataset of sketches paired with captions and QA pairs. Experiments show higher stylistic fidelity compared with the Stable Diffusion baseline.<br /><strong>Summary (CN):</strong> StableSketcher 通过微调扩散模型的变分自编码器并引入基于视觉问答的强化学习奖励函数，以提升像素级手绘草图的生成质量和文本‑图像对齐程度。该框架在此基础上推出 SketchDUO 数据集，包含草图、标题及问答对。实验表明相较于 Stable Diffusion 基准，StableSketcher 在风格忠实度上有显著提升。<br /><strong>Keywords:</strong> diffusion models, sketch generation, visual question answering, reinforcement learning, latent autoencoder, dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</div>
Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance.
<div><strong>Authors:</strong> Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "StableSketcher fine-tunes a diffusion model's variational autoencoder and adds a reinforcement‑learning reward based on visual question answering to improve the fidelity of pixel‑based human sketch generation. The framework achieves better text‑image alignment and releases SketchDUO, a new dataset of sketches paired with captions and QA pairs. Experiments show higher stylistic fidelity compared with the Stable Diffusion baseline.", "summary_cn": "StableSketcher 通过微调扩散模型的变分自编码器并引入基于视觉问答的强化学习奖励函数，以提升像素级手绘草图的生成质量和文本‑图像对齐程度。该框架在此基础上推出 SketchDUO 数据集，包含草图、标题及问答对。实验表明相较于 Stable Diffusion 基准，StableSketcher 在风格忠实度上有显著提升。", "keywords": "diffusion models, sketch generation, visual question answering, reinforcement learning, latent autoencoder, dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"]}
]]></acme>

<pubDate>2025-10-23T00:27:32+00:00</pubDate>
</item>
<item>
<title>Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency</title>
<link>https://papers.cool/arxiv/2510.20092</link>
<guid>https://papers.cool/arxiv/2510.20092</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Attentive Convolution (ATConv), a reformulation of the convolution operator that incorporates adaptive routing and lateral inhibition principles inspired by self‑attention, achieving higher accuracy with only 3×3 kernels. Experiments show ATConv consistently outperforms various self‑attention mechanisms on ImageNet classification and improves diffusion‑based image generation efficiency. The authors also present AttNet, a CNN family attaining 84.4% top‑1 accuracy with 27M parameters.<br /><strong>Summary (CN):</strong> 本文提出了注意卷积（Attentive Convolution，ATConv），一种在卷积算子中引入自注意力（self‑attention）启发的自适应路由和侧抑制机制的重新表述，只使用 3×3 核即可实现更高的准确率。实验表明，ATConv 在 ImageNet 分类等基础视觉任务上持续超越多种自注意力机制，并在基于扩散的图像生成中提升效率。作者进一步构建了 AttNet 系列 CNN，实现了 84.4% 的 Top‑1 准确率（27M 参数）。<br /><strong>Keywords:</strong> Attentive Convolution, self-attention, convolutional neural network, adaptive routing, lateral inhibition, vision backbone, ImageNet, diffusion models, efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hao Yu, Haoyu Chen, Yan Jiang, Wei Peng, Zhaodong Sun, Samuel Kaski, Guoying Zhao</div>
Self-attention (SA) has become the cornerstone of modern vision backbones for its powerful expressivity over traditional Convolutions (Conv). However, its quadratic complexity remains a critical bottleneck for practical applications. Given that Conv offers linear complexity and strong visual priors, continuing efforts have been made to promote the renaissance of Conv. However, a persistent performance chasm remains, highlighting that these modernizations have not yet captured the intrinsic expressivity that defines SA. In this paper, we re-examine the design of the CNNs, directed by a key question: what principles give SA its edge over Conv? As a result, we reveal two fundamental insights that challenge the long-standing design intuitions in prior research (e.g., Receptive field). The two findings are: (1) \textit{Adaptive routing}: SA dynamically regulates positional information flow according to semantic content, whereas Conv employs static kernels uniformly across all positions. (2) \textit{Lateral inhibition}: SA induces score competition among token weighting, effectively suppressing redundancy and sharpening representations, whereas Conv filters lack such inhibitory dynamics and exhibit considerable redundancy. Based on this, we propose \textit{Attentive Convolution} (ATConv), a principled reformulation of the convolutional operator that intrinsically injects these principles. Interestingly, with only $3\times3$ kernels, ATConv consistently outperforms various SA mechanisms in fundamental vision tasks. Building on ATConv, we introduce AttNet, a CNN family that can attain \textbf{84.4\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In diffusion-based image generation, replacing all SA with the proposed $3\times 3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster sampling. Code is available at: github.com/price112/Attentive-Convolution.
<div><strong>Authors:</strong> Hao Yu, Haoyu Chen, Yan Jiang, Wei Peng, Zhaodong Sun, Samuel Kaski, Guoying Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Attentive Convolution (ATConv), a reformulation of the convolution operator that incorporates adaptive routing and lateral inhibition principles inspired by self‑attention, achieving higher accuracy with only 3×3 kernels. Experiments show ATConv consistently outperforms various self‑attention mechanisms on ImageNet classification and improves diffusion‑based image generation efficiency. The authors also present AttNet, a CNN family attaining 84.4% top‑1 accuracy with 27M parameters.", "summary_cn": "本文提出了注意卷积（Attentive Convolution，ATConv），一种在卷积算子中引入自注意力（self‑attention）启发的自适应路由和侧抑制机制的重新表述，只使用 3×3 核即可实现更高的准确率。实验表明，ATConv 在 ImageNet 分类等基础视觉任务上持续超越多种自注意力机制，并在基于扩散的图像生成中提升效率。作者进一步构建了 AttNet 系列 CNN，实现了 84.4% 的 Top‑1 准确率（27M 参数）。", "keywords": "Attentive Convolution, self-attention, convolutional neural network, adaptive routing, lateral inhibition, vision backbone, ImageNet, diffusion models, efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Yu", "Haoyu Chen", "Yan Jiang", "Wei Peng", "Zhaodong Sun", "Samuel Kaski", "Guoying Zhao"]}
]]></acme>

<pubDate>2025-10-23T00:25:17+00:00</pubDate>
</item>
<item>
<title>Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos</title>
<link>https://papers.cool/arxiv/2510.20087</link>
<guid>https://papers.cool/arxiv/2510.20087</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Endoshare, a source‑available, cross‑platform tool for merging, standardizing, and de‑identifying endoscopic videos used in minimally invasive surgery. It follows a user‑centered software development process, incorporates privacy‑by‑design principles, and demonstrates high usability and performance across different hardware configurations. The authors suggest further certification and interoperability testing to make Endoshare a viable alternative to proprietary systems.<br /><strong>Summary (CN):</strong> 本文提出 Endoshare，这是一款开源、跨平台的工具，用于合并、标准化并去标识微创手术的内镜视频。系统遵循以用户为中心的软件开发流程，采用隐私设计原则，并在不同硬件配置下展示出较高的可用性和处理性能。作者指出仍需进行合规认证和更广泛的互操作性验证，以将 Endoshare 发展为专有系统的可部署替代方案。<br /><strong>Keywords:</strong> surgical video, de-identification, privacy-preserving, data standardization, source-available, usability, video management<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Lorenzo Arboit, Dennis N. Schneider, Britty Baby, Vinkle Srivastav, Pietro Mascagni, Nicolas Padoy</div>
Video-based assessment and surgical data science can advance surgical training, research, and quality improvement. However, widespread use remains limited by heterogeneous recording formats and privacy concerns associated with video sharing. We present Endoshare, a source-available, cross-platform application for merging, standardizing, and de-identifying endoscopic videos in minimally invasive surgery. Development followed the software development life cycle with iterative, user-centered feedback. During the analysis phase, an internal survey of clinicians and computer scientists based on ten usability heuristics identified key requirements that guided a privacy-by-design architecture. In the testing phase, an external clinician survey combined the same heuristics with Technology Acceptance Model constructs to assess usability and adoption, complemented by benchmarking across different hardware configurations. Four clinicians and four computer scientists initially tested the prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5), with the lowest score (4.00 +/- 0.93/5) relating to label clarity. After refinement, the testing phase surveyed ten surgeons who reported high perceived usefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic usability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10). Processing time varied with processing mode, video duration (both p <= 0.001), and machine computational power (p = 0.041). Endoshare provides a transparent, user-friendly pipeline for standardized, privacy-preserving surgical video management. Compliance certification and broader interoperability validation are needed to establish it as a deployable alternative to proprietary systems. The software is available at https://camma-public.github.io/Endoshare/
<div><strong>Authors:</strong> Lorenzo Arboit, Dennis N. Schneider, Britty Baby, Vinkle Srivastav, Pietro Mascagni, Nicolas Padoy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Endoshare, a source‑available, cross‑platform tool for merging, standardizing, and de‑identifying endoscopic videos used in minimally invasive surgery. It follows a user‑centered software development process, incorporates privacy‑by‑design principles, and demonstrates high usability and performance across different hardware configurations. The authors suggest further certification and interoperability testing to make Endoshare a viable alternative to proprietary systems.", "summary_cn": "本文提出 Endoshare，这是一款开源、跨平台的工具，用于合并、标准化并去标识微创手术的内镜视频。系统遵循以用户为中心的软件开发流程，采用隐私设计原则，并在不同硬件配置下展示出较高的可用性和处理性能。作者指出仍需进行合规认证和更广泛的互操作性验证，以将 Endoshare 发展为专有系统的可部署替代方案。", "keywords": "surgical video, de-identification, privacy-preserving, data standardization, source-available, usability, video management", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Britty Baby", "Vinkle Srivastav", "Pietro Mascagni", "Nicolas Padoy"]}
]]></acme>

<pubDate>2025-10-23T00:07:58+00:00</pubDate>
</item>
<item>
<title>Data-Adaptive Transformed Bilateral Tensor Low-Rank Representation for Clustering</title>
<link>https://papers.cool/arxiv/2510.20077</link>
<guid>https://papers.cool/arxiv/2510.20077</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes TBTLRR, a transformed bilateral tensor low-rank representation model that learns data-adaptive unitary transforms to define a flexible tensor nuclear norm and captures both global and local correlations for image clustering. It incorporates ℓ_{1/2}-norm and Frobenius regularization to handle complex noise and solves the resulting nonconvex problem with an ADMM-based algorithm, demonstrating superior clustering performance in experiments.<br /><strong>Summary (CN):</strong> 本文提出 TBTLRR 模型，通过学习数据自适应的酉变换构建灵活的张量核范数，从而有效捕获图像聚类中的全局和局部相关性。模型融合 ℓ_{1/2} 范数和 Frobenius 正则，以应对复杂噪声，并采用基于 ADMM 的算法求解非凸问题，在实验中表现出色。<br /><strong>Keywords:</strong> tensor low-rank representation, adaptive unitary transform, bilateral tensor, clustering, l_{1/2} norm, ADMM<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Hui Chen, Xinjie Wang, Xianchao Xiu, Wanquan Liu</div>
Tensor low-rank representation (TLRR) has demonstrated significant success in image clustering. However, most existing methods rely on fixed transformations and suffer from poor robustness to noise. In this paper, we propose a novel transformed bilateral tensor low-rank representation model called TBTLRR, which introduces a data-adaptive tensor nuclear norm by learning arbitrary unitary transforms, allowing for more effective capture of global correlations. In addition, by leveraging the bilateral structure of latent tensor data, TBTLRR is able to exploit local correlations between image samples and features. Furthermore, TBTLRR integrates the $\ell_{1/2}$-norm and Frobenius norm regularization terms for better dealing with complex noise in real-world scenarios. To solve the proposed nonconvex model, we develop an efficient optimization algorithm inspired by the alternating direction method of multipliers (ADMM) and provide theoretical convergence. Extensive experiments validate its superiority over the state-of-the-art methods in clustering. The code will be available at https://github.com/xianchaoxiu/TBTLRR.
<div><strong>Authors:</strong> Hui Chen, Xinjie Wang, Xianchao Xiu, Wanquan Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes TBTLRR, a transformed bilateral tensor low-rank representation model that learns data-adaptive unitary transforms to define a flexible tensor nuclear norm and captures both global and local correlations for image clustering. It incorporates ℓ_{1/2}-norm and Frobenius regularization to handle complex noise and solves the resulting nonconvex problem with an ADMM-based algorithm, demonstrating superior clustering performance in experiments.", "summary_cn": "本文提出 TBTLRR 模型，通过学习数据自适应的酉变换构建灵活的张量核范数，从而有效捕获图像聚类中的全局和局部相关性。模型融合 ℓ_{1/2} 范数和 Frobenius 正则，以应对复杂噪声，并采用基于 ADMM 的算法求解非凸问题，在实验中表现出色。", "keywords": "tensor low-rank representation, adaptive unitary transform, bilateral tensor, clustering, l_{1/2} norm, ADMM", "scoring": {"interpretability": 1, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Hui Chen", "Xinjie Wang", "Xianchao Xiu", "Wanquan Liu"]}
]]></acme>

<pubDate>2025-10-22T23:25:44+00:00</pubDate>
</item>
<item>
<title>Filter-Based Reconstruction of Images from Events</title>
<link>https://papers.cool/arxiv/2510.20071</link>
<guid>https://papers.cool/arxiv/2510.20071</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FIBAR, a filter‑based reconstruction algorithm that converts raw events from a moving event camera into intensity images using a temporal IIR filter and a novel stale‑pixel detection combined with Gaussian blurring. It runs on a standard laptop CPU at tens of millions of events per second and, while noisier and prone to ghosting compared to neural‑network approaches, can support simple tasks such as fiducial marker detection. Code and qualitative comparisons to a neural method (FireNet) are provided.<br /><strong>Summary (CN):</strong> 本文提出了 FIBAR，一种基于滤波的异步重建算法，通过时间数字 IIR 滤波器整合事件相机的强度变化，并使用新颖的陈旧像素检测与高斯模糊来降低噪声。该方法可在普通笔记本 CPU 上实现每秒数千万事件的实时处理，虽比神经网络方法（FireNet）产生更多噪声和幽灵影像，但已足以完成如标记检测等简单任务。文中提供了代码以及与神经网络方法的定性比较。<br /><strong>Keywords:</strong> event camera, asynchronous reconstruction, filter-based method, IIR filter, stale pixel detection, Gaussian blur, CPU implementation, fiducial marker detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Bernd Pfrommer</div>
Reconstructing an intensity image from the events of a moving event camera is a challenging task that is typically approached with neural networks deployed on graphics processing units. This paper presents a much simpler, FIlter Based Asynchronous Reconstruction method (FIBAR). First, intensity changes signaled by events are integrated with a temporal digital IIR filter. To reduce reconstruction noise, stale pixels are detected by a novel algorithm that regulates a window of recently updated pixels. Arguing that for a moving camera, the absence of events at a pixel location likely implies a low image gradient, stale pixels are then blurred with a Gaussian filter. In contrast to most existing methods, FIBAR is asynchronous and permits image read-out at an arbitrary time. It runs on a modern laptop CPU at about 42(140) million events/s with (without) spatial filtering enabled. A few simple qualitative experiments are presented that show the difference in image reconstruction between FIBAR and a neural network-based approach (FireNet). FIBAR's reconstruction is noisier than neural network-based methods and suffers from ghost images. However, it is sufficient for certain tasks such as the detection of fiducial markers. Code is available at https://github.com/ros-event-camera/event_image_reconstruction_fibar
<div><strong>Authors:</strong> Bernd Pfrommer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FIBAR, a filter‑based reconstruction algorithm that converts raw events from a moving event camera into intensity images using a temporal IIR filter and a novel stale‑pixel detection combined with Gaussian blurring. It runs on a standard laptop CPU at tens of millions of events per second and, while noisier and prone to ghosting compared to neural‑network approaches, can support simple tasks such as fiducial marker detection. Code and qualitative comparisons to a neural method (FireNet) are provided.", "summary_cn": "本文提出了 FIBAR，一种基于滤波的异步重建算法，通过时间数字 IIR 滤波器整合事件相机的强度变化，并使用新颖的陈旧像素检测与高斯模糊来降低噪声。该方法可在普通笔记本 CPU 上实现每秒数千万事件的实时处理，虽比神经网络方法（FireNet）产生更多噪声和幽灵影像，但已足以完成如标记检测等简单任务。文中提供了代码以及与神经网络方法的定性比较。", "keywords": "event camera, asynchronous reconstruction, filter-based method, IIR filter, stale pixel detection, Gaussian blur, CPU implementation, fiducial marker detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bernd Pfrommer"]}
]]></acme>

<pubDate>2025-10-22T23:05:38+00:00</pubDate>
</item>
<item>
<title>Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models</title>
<link>https://papers.cool/arxiv/2510.20042</link>
<guid>https://papers.cool/arxiv/2510.20042</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a unified benchmark for evaluating cultural bias in both text-to-image (T2I) generation and image-to-image (I2I) editing across six countries, using a detailed 8‑category schema, era‑aware prompts, automatic metrics, a retrieval‑augmented VQA, and expert human judgments. Experiments on open‑source models reveal that under generic prompts models default to Global‑North, modern depictions, and that iterative I2I edits degrade cultural fidelity despite unchanged conventional metrics. The authors release all data, prompts, and evaluation protocols to enable reproducible tracking of cultural bias in generative image systems.<br /><strong>Summary (CN):</strong> 本文提出一个统一的基准，用于在六个国家范围内评估文本到图像（T2I）生成和图像到图像（I2I）编辑的文化偏见，采用 8 类 36 子类的详细结构、时代感知的提示、自动指标、检索增强的 VQA 以及本土专家的人类评审。对开源模型的实验表明，在通用提示下模型倾向于呈现全球北方、现代化的描绘，且迭代的 I2I 编辑会在传统指标保持不变或提升的情况下削弱文化忠实度。作者公开全部图像数据、提示和评估协议，以实现对生成图像模型文化偏见的可复现诊断与追踪。<br /><strong>Keywords:</strong> cultural bias, generative image models, text-to-image, image-to-image editing, cross-cultural evaluation, fairness, VQA, human evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Huichan Seo, Sieun Choi, Minki Hong, Yi Zhou, Junseo Kim, Lukman Ismaila, Naome Etori, Mehul Agarwal, Zhixuan Liu, Jihie Kim, Jean Oh</div>
Generative image models produce striking visuals yet often misrepresent culture. Prior work has examined cultural bias mainly in text-to-image (T2I) systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap with a unified evaluation across six countries, an 8-category/36-subcategory schema, and era-aware prompts, auditing both T2I generation and I2I editing under a standardized protocol that yields comparable diagnostics. Using open models with fixed settings, we derive cross-country, cross-era, and cross-category evaluations. Our framework combines standard automatic metrics, a culture-aware retrieval-augmented VQA, and expert human judgments collected from native reviewers. To enable reproducibility, we release the complete image corpus, prompts, and configurations. Our study reveals three findings: (1) under country-agnostic prompts, models default to Global-North, modern-leaning depictions that flatten cross-country distinctions; (2) iterative I2I editing erodes cultural fidelity even when conventional metrics remain flat or improve; and (3) I2I models apply superficial cues (palette shifts, generic props) rather than era-consistent, context-aware changes, often retaining source identity for Global-South targets. These results highlight that culture-sensitive edits remain unreliable in current systems. By releasing standardized data, prompts, and human evaluation protocols, we provide a reproducible, culture-centered benchmark for diagnosing and tracking cultural bias in generative image models.
<div><strong>Authors:</strong> Huichan Seo, Sieun Choi, Minki Hong, Yi Zhou, Junseo Kim, Lukman Ismaila, Naome Etori, Mehul Agarwal, Zhixuan Liu, Jihie Kim, Jean Oh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a unified benchmark for evaluating cultural bias in both text-to-image (T2I) generation and image-to-image (I2I) editing across six countries, using a detailed 8‑category schema, era‑aware prompts, automatic metrics, a retrieval‑augmented VQA, and expert human judgments. Experiments on open‑source models reveal that under generic prompts models default to Global‑North, modern depictions, and that iterative I2I edits degrade cultural fidelity despite unchanged conventional metrics. The authors release all data, prompts, and evaluation protocols to enable reproducible tracking of cultural bias in generative image systems.", "summary_cn": "本文提出一个统一的基准，用于在六个国家范围内评估文本到图像（T2I）生成和图像到图像（I2I）编辑的文化偏见，采用 8 类 36 子类的详细结构、时代感知的提示、自动指标、检索增强的 VQA 以及本土专家的人类评审。对开源模型的实验表明，在通用提示下模型倾向于呈现全球北方、现代化的描绘，且迭代的 I2I 编辑会在传统指标保持不变或提升的情况下削弱文化忠实度。作者公开全部图像数据、提示和评估协议，以实现对生成图像模型文化偏见的可复现诊断与追踪。", "keywords": "cultural bias, generative image models, text-to-image, image-to-image editing, cross-cultural evaluation, fairness, VQA, human evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Huichan Seo", "Sieun Choi", "Minki Hong", "Yi Zhou", "Junseo Kim", "Lukman Ismaila", "Naome Etori", "Mehul Agarwal", "Zhixuan Liu", "Jihie Kim", "Jean Oh"]}
]]></acme>

<pubDate>2025-10-22T21:42:59+00:00</pubDate>
</item>
<item>
<title>BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography</title>
<link>https://papers.cool/arxiv/2510.20029</link>
<guid>https://papers.cool/arxiv/2510.20029</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BrainPuzzle is a two-stage hybrid framework that first applies reverse time migration to multi-angle transcranial ultrasound measurements and then fuses the resulting fragments with a transformer‑based super‑resolution encoder‑decoder utilizing a graph‑based attention unit to produce accurate speed‑of‑sound maps of the brain. The method addresses the challenges of skull‑induced attenuation, limited aperture, and low signal‑to‑noise ratio, and experiments on synthetic data show improved reconstruction accuracy and image completeness compared to purely physics‑based or data‑driven approaches.<br /><strong>Summary (CN):</strong> BrainPuzzle 提出了一种两阶段的混合框架：第一阶段使用逆时偏移（时间反演声学）对多角度经颅超声数据进行迁移，得到保留结构细节的片段；第二阶段利用基于 Transformer 的超分辨率编码‑解码网络并结合图注意力单元（GAU）将这些片段融合，生成准确的脑部声速（SoS）图像。该方法克服了颅骨衰减、孔径受限和低信噪比等难题，在合成数据实验中展示了比纯物理或纯数据方法更高的重建精度和图像完整性。<br /><strong>Keywords:</strong> ultrasound tomography, speed-of-sound reconstruction, full-waveform inversion, reverse time migration, transformer, graph attention unit, hybrid physics-data, transcranial imaging<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shengyu Chen, Shihang Feng, Yi Luo, Xiaowei Jia, Youzuo Lin</div>
Ultrasound brain imaging remains challenging due to the large difference in sound speed between the skull and brain tissues and the difficulty of coupling large probes to the skull. This work aims to achieve quantitative transcranial ultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain. Traditional physics-based full-waveform inversion (FWI) is limited by weak signals caused by skull-induced attenuation, mode conversion, and phase aberration, as well as incomplete spatial coverage since full-aperture arrays are clinically impractical. In contrast, purely data-driven methods that learn directly from raw ultrasound data often fail to model the complex nonlinear and nonlocal wave propagation through bone, leading to anatomically plausible but quantitatively biased SoS maps under low signal-to-noise and sparse-aperture conditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage framework that combines physical modeling with machine learning. In the first stage, reverse time migration (time-reversal acoustics) is applied to multi-angle acquisitions to produce migration fragments that preserve structural details even under low SNR. In the second stage, a transformer-based super-resolution encoder-decoder with a graph-based attention unit (GAU) fuses these fragments into a coherent and quantitatively accurate SoS image. A partial-array acquisition strategy using a movable low-count transducer set improves feasibility and coupling, while the hybrid algorithm compensates for the missing aperture. Experiments on two synthetic datasets show that BrainPuzzle achieves superior SoS reconstruction accuracy and image completeness, demonstrating its potential for advancing quantitative ultrasound brain imaging.
<div><strong>Authors:</strong> Shengyu Chen, Shihang Feng, Yi Luo, Xiaowei Jia, Youzuo Lin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BrainPuzzle is a two-stage hybrid framework that first applies reverse time migration to multi-angle transcranial ultrasound measurements and then fuses the resulting fragments with a transformer‑based super‑resolution encoder‑decoder utilizing a graph‑based attention unit to produce accurate speed‑of‑sound maps of the brain. The method addresses the challenges of skull‑induced attenuation, limited aperture, and low signal‑to‑noise ratio, and experiments on synthetic data show improved reconstruction accuracy and image completeness compared to purely physics‑based or data‑driven approaches.", "summary_cn": "BrainPuzzle 提出了一种两阶段的混合框架：第一阶段使用逆时偏移（时间反演声学）对多角度经颅超声数据进行迁移，得到保留结构细节的片段；第二阶段利用基于 Transformer 的超分辨率编码‑解码网络并结合图注意力单元（GAU）将这些片段融合，生成准确的脑部声速（SoS）图像。该方法克服了颅骨衰减、孔径受限和低信噪比等难题，在合成数据实验中展示了比纯物理或纯数据方法更高的重建精度和图像完整性。", "keywords": "ultrasound tomography, speed-of-sound reconstruction, full-waveform inversion, reverse time migration, transformer, graph attention unit, hybrid physics-data, transcranial imaging", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shengyu Chen", "Shihang Feng", "Yi Luo", "Xiaowei Jia", "Youzuo Lin"]}
]]></acme>

<pubDate>2025-10-22T21:15:55+00:00</pubDate>
</item>
<item>
<title>Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses</title>
<link>https://papers.cool/arxiv/2510.20027</link>
<guid>https://papers.cool/arxiv/2510.20027</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a real-time render-aware filtering technique for 3D Gaussian Splatting models that mitigates visual noise when rendering from camera far outside the training distribution. By computing sensitivity scores from intermediate gradients and targeting anisotropic orientation instabilities, the filter improves fidelity, realism, and consistency without requiring retraining, outperforming NeRF-based baselines such as BayesRays.<br /><strong>Summary (CN):</strong> 本文提出一种实时渲染感知的过滤方法，用于在 3D Gaussian Splatting 模型从训练分布之外的相机位姿进行渲染时抑制视觉噪声。该方法利用中间梯度计算的敏感度分数，专注于各向异性方向的不稳定性，从而在无需重新训练的情况下提升图像的真实感和一致性，优于 BayesRays 等 NeRF 基准。<br /><strong>Keywords:</strong> 3D Gaussian Splatting, novel view synthesis, out-of-distribution, render-aware filtering, gradient sensitivity, neural rendering, NeRF, BayesRays, real-time<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Damian Bowness, Charalambos Poullis</div>
When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model. To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints. Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning. Code and results at https://damian-bowness.github.io/EV3DGS
<div><strong>Authors:</strong> Damian Bowness, Charalambos Poullis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a real-time render-aware filtering technique for 3D Gaussian Splatting models that mitigates visual noise when rendering from camera far outside the training distribution. By computing sensitivity scores from intermediate gradients and targeting anisotropic orientation instabilities, the filter improves fidelity, realism, and consistency without requiring retraining, outperforming NeRF-based baselines such as BayesRays.", "summary_cn": "本文提出一种实时渲染感知的过滤方法，用于在 3D Gaussian Splatting 模型从训练分布之外的相机位姿进行渲染时抑制视觉噪声。该方法利用中间梯度计算的敏感度分数，专注于各向异性方向的不稳定性，从而在无需重新训练的情况下提升图像的真实感和一致性，优于 BayesRays 等 NeRF 基准。", "keywords": "3D Gaussian Splatting, novel view synthesis, out-of-distribution, render-aware filtering, gradient sensitivity, neural rendering, NeRF, BayesRays, real-time", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Damian Bowness", "Charalambos Poullis"]}
]]></acme>

<pubDate>2025-10-22T21:09:16+00:00</pubDate>
</item>
<item>
<title>A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance</title>
<link>https://papers.cool/arxiv/2510.20016</link>
<guid>https://papers.cool/arxiv/2510.20016</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a detection framework for fisheye traffic surveillance that uses pre‑ and post‑processing steps and an ensemble of state‑of‑the‑art object detectors to improve performance under severe radial distortion, especially near image boundaries. Experiments on the 2025 AI City Challenge Track 4 show the method achieves an F1 score of 0.6366, ranking 8th out of 62 teams.<br /><strong>Summary (CN):</strong> 本文提出了一种用于鱼眼交通监控的检测框架，结合前处理和后处理步骤以及多种最先进目标检测模型的集成，以提升在强径向畸变尤其是图像边缘区域的检测一致性。对 2025 AI City Challenge Track 4 的实验表明，该方法达到 0.6366 的 F1 分数，在 62 支队伍中排名第 8。<br /><strong>Keywords:</strong> fisheye distortion, object detection, traffic surveillance, ensemble detection, pre-processing, post-processing, AI City Challenge, robust detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Neema Jakisa Owor, Joshua Kofi Asamoah, Tanner Wambui Muturi, Anneliese Jakisa Owor, Blessing Agyei Kyem, Andrews Danyo, Yaw Adu-Gyamfi, Armstrong Aboah</div>
Fisheye cameras offer an efficient solution for wide-area traffic surveillance by capturing large fields of view from a single vantage point. However, the strong radial distortion and nonuniform resolution inherent in fisheye imagery introduce substantial challenges for standard object detectors, particularly near image boundaries where object appearance is severely degraded. In this work, we present a detection framework designed to operate robustly under these conditions. Our approach employs a simple yet effective pre and post processing pipeline that enhances detection consistency across the image, especially in regions affected by severe distortion. We train several state-of-the-art detection models on the fisheye traffic imagery and combine their outputs through an ensemble strategy to improve overall detection accuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City Challenge Track 4, placing 8thoverall out of 62 teams. These results demonstrate the effectiveness of our framework in addressing issues inherent to fisheye imagery.
<div><strong>Authors:</strong> Neema Jakisa Owor, Joshua Kofi Asamoah, Tanner Wambui Muturi, Anneliese Jakisa Owor, Blessing Agyei Kyem, Andrews Danyo, Yaw Adu-Gyamfi, Armstrong Aboah</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a detection framework for fisheye traffic surveillance that uses pre‑ and post‑processing steps and an ensemble of state‑of‑the‑art object detectors to improve performance under severe radial distortion, especially near image boundaries. Experiments on the 2025 AI City Challenge Track 4 show the method achieves an F1 score of 0.6366, ranking 8th out of 62 teams.", "summary_cn": "本文提出了一种用于鱼眼交通监控的检测框架，结合前处理和后处理步骤以及多种最先进目标检测模型的集成，以提升在强径向畸变尤其是图像边缘区域的检测一致性。对 2025 AI City Challenge Track 4 的实验表明，该方法达到 0.6366 的 F1 分数，在 62 支队伍中排名第 8。", "keywords": "fisheye distortion, object detection, traffic surveillance, ensemble detection, pre-processing, post-processing, AI City Challenge, robust detection", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Neema Jakisa Owor", "Joshua Kofi Asamoah", "Tanner Wambui Muturi", "Anneliese Jakisa Owor", "Blessing Agyei Kyem", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"]}
]]></acme>

<pubDate>2025-10-22T20:38:34+00:00</pubDate>
</item>
<item>
<title>Improving Predictive Confidence in Medical Imaging via Online Label Smoothing</title>
<link>https://papers.cool/arxiv/2510.20011</link>
<guid>https://papers.cool/arxiv/2510.20011</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Online Label Smoothing (OLS), a dynamic soft‑labeling technique that adapts to the model's own predictions during training, and evaluates it on the large‑scale RadImageNet medical imaging dataset using ResNet‑50, MobileNetV2, and VGG‑19. OLS consistently improves Top‑1 and Top‑5 accuracy, yields better calibrated predictions, and produces more compact, well‑separated feature embeddings compared to hard labels, conventional label smoothing, and teacher‑free knowledge distillation. These results suggest OLS enhances both predictive performance and reliability for trustworthy AI in healthcare.<br /><strong>Summary (CN):</strong> 本文提出在线标签平滑（Online Label Smoothing，OLS）方法，在训练过程中根据模型自身的预测动态调整软标签，并在大规模医学影像数据集 RadImageNet 上使用 ResNet‑50、MobileNetV2 与 VGG‑19 进行评估。与硬标签、传统标签平滑及无教师知识蒸馏相比，OLS 在 Top‑1 与 Top‑5 准确率上均有提升，并显著改善预测校准，使特征嵌入更紧凑且类别间分离度更高，表明该方法可提升医学影像 AI 系统的性能与可靠性。<br /><strong>Keywords:</strong> online label smoothing, calibration, medical imaging, convolutional neural networks, representation learning, RadImageNet, predictive confidence<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry</div>
Deep learning models, especially convolutional neural networks, have achieved impressive results in medical image classification. However, these models often produce overconfident predictions, which can undermine their reliability in critical healthcare settings. While traditional label smoothing offers a simple way to reduce such overconfidence, it fails to consider relationships between classes by treating all non-target classes equally. In this study, we explore the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft labels throughout training based on the model's own prediction patterns. We evaluate OLS on the large-scale RadImageNet dataset using three widely used architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS consistently improves both Top-1 and Top-5 classification accuracy compared to standard training methods, including hard labels, conventional label smoothing, and teacher-free knowledge distillation. In addition to accuracy gains, OLS leads to more compact and well-separated feature embeddings, indicating improved representation learning. These findings suggest that OLS not only strengthens predictive performance but also enhances calibration, making it a practical and effective solution for developing trustworthy AI systems in the medical imaging domain.
<div><strong>Authors:</strong> Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Online Label Smoothing (OLS), a dynamic soft‑labeling technique that adapts to the model's own predictions during training, and evaluates it on the large‑scale RadImageNet medical imaging dataset using ResNet‑50, MobileNetV2, and VGG‑19. OLS consistently improves Top‑1 and Top‑5 accuracy, yields better calibrated predictions, and produces more compact, well‑separated feature embeddings compared to hard labels, conventional label smoothing, and teacher‑free knowledge distillation. These results suggest OLS enhances both predictive performance and reliability for trustworthy AI in healthcare.", "summary_cn": "本文提出在线标签平滑（Online Label Smoothing，OLS）方法，在训练过程中根据模型自身的预测动态调整软标签，并在大规模医学影像数据集 RadImageNet 上使用 ResNet‑50、MobileNetV2 与 VGG‑19 进行评估。与硬标签、传统标签平滑及无教师知识蒸馏相比，OLS 在 Top‑1 与 Top‑5 准确率上均有提升，并显著改善预测校准，使特征嵌入更紧凑且类别间分离度更高，表明该方法可提升医学影像 AI 系统的性能与可靠性。", "keywords": "online label smoothing, calibration, medical imaging, convolutional neural networks, representation learning, RadImageNet, predictive confidence", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Kushan Choudhury", "Shubhrodeep Roy", "Ankur Chanda", "Shubhajit Biswas", "Somenath Kuiry"]}
]]></acme>

<pubDate>2025-10-22T20:25:14+00:00</pubDate>
</item>
<item>
<title>FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</title>
<link>https://papers.cool/arxiv/2510.19981</link>
<guid>https://papers.cool/arxiv/2510.19981</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FutrTrack introduces a modular camera‑LiDAR fusion framework that enhances 3D multiple object tracking by adding a transformer‑based temporal smoother and a fusion‑driven tracker. The system refines bounding box sequences and propagates identities using multimodal BEV features without an explicit motion model, achieving strong results on nuScenes and KITTI benchmarks. This approach demonstrates that query‑based transformer trackers significantly benefit from multimodal sensor integration.<br /><strong>Summary (CN):</strong> FutrTrack 提出一个模块化的摄像头‑LiDAR 融合框架，通过引入基于 Transformer 的时间平滑器和融合驱动的跟踪器来提升 3D 多目标跟踪。系统在无需显式运动模型的情况下，利用多模态鸟瞰图特征精炼边界框序列并传播目标身份，在 nuScenes 与 KITTI 基准上取得了强劲表现。该方法表明查询式 Transformer 跟踪器在多传感器特征融合后能显著提升性能。<br /><strong>Keywords:</strong> camera-LiDAR fusion, transformer, 3D multi-object tracking, BEV fusion, temporal smoother, nuScenes, KITTI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Martha Teiko Teye, Ori Maoz, Matthias Rottmann</div>
We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.
<div><strong>Authors:</strong> Martha Teiko Teye, Ori Maoz, Matthias Rottmann</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FutrTrack introduces a modular camera‑LiDAR fusion framework that enhances 3D multiple object tracking by adding a transformer‑based temporal smoother and a fusion‑driven tracker. The system refines bounding box sequences and propagates identities using multimodal BEV features without an explicit motion model, achieving strong results on nuScenes and KITTI benchmarks. This approach demonstrates that query‑based transformer trackers significantly benefit from multimodal sensor integration.", "summary_cn": "FutrTrack 提出一个模块化的摄像头‑LiDAR 融合框架，通过引入基于 Transformer 的时间平滑器和融合驱动的跟踪器来提升 3D 多目标跟踪。系统在无需显式运动模型的情况下，利用多模态鸟瞰图特征精炼边界框序列并传播目标身份，在 nuScenes 与 KITTI 基准上取得了强劲表现。该方法表明查询式 Transformer 跟踪器在多传感器特征融合后能显著提升性能。", "keywords": "camera-LiDAR fusion, transformer, 3D multi-object tracking, BEV fusion, temporal smoother, nuScenes, KITTI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"]}
]]></acme>

<pubDate>2025-10-22T19:25:01+00:00</pubDate>
</item>
<item>
<title>Transformed Multi-view 3D Shape Features with Contrastive Learning</title>
<link>https://papers.cool/arxiv/2510.19955</link>
<guid>https://papers.cool/arxiv/2510.19955</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how Vision Transformers combined with contrastive learning objectives improve multi-view 3D shape representation, achieving up to 90.6% accuracy on ModelNet10. Extensive experiments show that ViTs capture global shape semantics while contrastive optimization refines local discriminative features, outperforming traditional CNN-based pipelines.<br /><strong>Summary (CN):</strong> 本文研究了将视觉Transformer与对比学习目标相结合，以提升多视角3D形状特征表示的效果，在ModelNet10上实现了约90.6%的准确率。大量实验表明，ViT能够捕获整体形状语义，而对比学习优化细化局部判别特征，优于传统CNN方法。<br /><strong>Keywords:</strong> 3D shape representation, contrastive learning, vision transformers, multi-view analysis, ModelNet10<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Márcus Vinícius Lobo Costa, Sherlon Almeida da Silva, Bárbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti</div>
This paper addresses the challenges in representation learning of 3D shape features by investigating state-of-the-art backbones paired with both contrastive supervised and self-supervised learning objectives. Computer vision methods struggle with recognizing 3D objects from 2D images, often requiring extensive labeled data and relying on Convolutional Neural Networks (CNNs) that may overlook crucial shape relationships. Our work demonstrates that Vision Transformers (ViTs) based architectures, when paired with modern contrastive objectives, achieve promising results in multi-view 3D analysis on our downstream tasks, unifying contrastive and 3D shape understanding pipelines. For example, supervised contrastive losses reached about 90.6% accuracy on ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability to understand overall shapes and contrastive learning's effectiveness, overcomes the need for extensive labeled data and the limitations of CNNs in capturing crucial shape relationships. The success stems from capturing global shape semantics via ViTs and refining local discriminative features through contrastive optimization. Importantly, our approach is empirical, as it is grounded on extensive experimental evaluation to validate the effectiveness of combining ViTs with contrastive objectives for 3D representation learning.
<div><strong>Authors:</strong> Márcus Vinícius Lobo Costa, Sherlon Almeida da Silva, Bárbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how Vision Transformers combined with contrastive learning objectives improve multi-view 3D shape representation, achieving up to 90.6% accuracy on ModelNet10. Extensive experiments show that ViTs capture global shape semantics while contrastive optimization refines local discriminative features, outperforming traditional CNN-based pipelines.", "summary_cn": "本文研究了将视觉Transformer与对比学习目标相结合，以提升多视角3D形状特征表示的效果，在ModelNet10上实现了约90.6%的准确率。大量实验表明，ViT能够捕获整体形状语义，而对比学习优化细化局部判别特征，优于传统CNN方法。", "keywords": "3D shape representation, contrastive learning, vision transformers, multi-view analysis, ModelNet10", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Márcus Vinícius Lobo Costa", "Sherlon Almeida da Silva", "Bárbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"]}
]]></acme>

<pubDate>2025-10-22T18:29:48+00:00</pubDate>
</item>
<item>
<title>Fourier-Based GAN Fingerprint Detection using ResNet50</title>
<link>https://papers.cool/arxiv/2510.19840</link>
<guid>https://papers.cool/arxiv/2510.19840</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a forensic method that applies a 2‑dimensional Discrete Fourier Transform to images to expose subtle periodic artifacts left by GANs, then trains a ResNet50 classifier on these Fourier‑domain representations to distinguish StyleGAN‑generated images from real ones, achieving 92.8% accuracy and an AUC of 0.95. The results demonstrate that GAN‑produced images possess distinctive frequency‑domain fingerprints, and the approach outperforms models trained on raw pixel data. This highlights the potential of combining signal‑processing techniques with deep learning for industrial digital‑forensics and content‑authenticity assurance.<br /><strong>Summary (CN):</strong> 本文提出一种取证方法，对图像进行二维离散傅里叶变换（2D DFT），以揭示 GAN 生成图像中细微的周期性伪迹，然后在这些频域表示上Net50 分类器，以区分 StyleGAN 生成的图像和真实图像，取得 92.8% 的准确率和 0.95 的 AUC。结果表明，GAN 生成的图像在频域中具有独特的指纹特征，并且该方法显著优于在原始像素上训练的模型。这凸显了将信号处理技术与深度学习相结合用于工业数字取证和内容真实性保障的潜力。<br /><strong>Keywords:</strong> GAN detection, Fourier analysis, frequency fingerprint, ResNet50, image forensics, synthetic image detection, StyleGAN<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Sai Teja Erukude, Viswa Chaitanya Marella, Suhasnadh Reddy Veluru</div>
The rapid rise of photorealistic images produced from Generative Adversarial Networks (GANs) poses a serious challenge for image forensics and industrial systems requiring reliable content authenticity. This paper uses frequency-domain analysis combined with deep learning to solve the problem of distinguishing StyleGAN-generated images from real ones. Specifically, a two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform images into the Fourier domain, where subtle periodic artifacts become detectable. A ResNet50 neural network is trained on these transformed images to differentiate between real and synthetic ones. The experiments demonstrate that the frequency-domain model achieves a 92.8 percent and an AUC of 0.95, significantly outperforming the equivalent model trained on raw spatial-domain images. These results indicate that the GAN-generated images have unique frequency-domain signatures or "fingerprints". The method proposed highlights the industrial potential of combining signal processing techniques and deep learning to enhance digital forensics and strengthen the trustworthiness of industrial AI systems.
<div><strong>Authors:</strong> Sai Teja Erukude, Viswa Chaitanya Marella, Suhasnadh Reddy Veluru</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a forensic method that applies a 2‑dimensional Discrete Fourier Transform to images to expose subtle periodic artifacts left by GANs, then trains a ResNet50 classifier on these Fourier‑domain representations to distinguish StyleGAN‑generated images from real ones, achieving 92.8% accuracy and an AUC of 0.95. The results demonstrate that GAN‑produced images possess distinctive frequency‑domain fingerprints, and the approach outperforms models trained on raw pixel data. This highlights the potential of combining signal‑processing techniques with deep learning for industrial digital‑forensics and content‑authenticity assurance.", "summary_cn": "本文提出一种取证方法，对图像进行二维离散傅里叶变换（2D DFT），以揭示 GAN 生成图像中细微的周期性伪迹，然后在这些频域表示上Net50 分类器，以区分 StyleGAN 生成的图像和真实图像，取得 92.8% 的准确率和 0.95 的 AUC。结果表明，GAN 生成的图像在频域中具有独特的指纹特征，并且该方法显著优于在原始像素上训练的模型。这凸显了将信号处理技术与深度学习相结合用于工业数字取证和内容真实性保障的潜力。", "keywords": "GAN detection, Fourier analysis, frequency fingerprint, ResNet50, image forensics, synthetic image detection, StyleGAN", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Sai Teja Erukude", "Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru"]}
]]></acme>

<pubDate>2025-10-18T14:44:55+00:00</pubDate>
</item>
<item>
<title>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</title>
<link>https://papers.cool/arxiv/2510.20813</link>
<guid>https://papers.cool/arxiv/2510.20813</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> GSWorld is a closed-loop, photo‑realistic simulation suite for robotic manipulation that combines 3D Gaussian Splatting with physics engines via a new Gaussian Scene Description File (GSDF) format. The framework enables zero‑shot sim2real policy learning, automated DAgger data collection, reproducible benchmarking, virtual teleoperation, and visual reinforcement learning without requiring real robots. A curated database of robot embodiments and objects supports diverse manipulation tasks.<br /><strong>Summary (CN):</strong> GSWorld 是一个闭环的、逼真渲染的机器人操作仿真平台，基于 3D Gaussian Splatting 与物理引擎，并引入 Gaussian Scene Description File（GSDF）格式，将高斯‑网格表示与机器人 URDF 等对象融合。该框架实现了零样本 sim2real 像素到动作策略学习、自动 DAgger 数据收集、可复现的仿真基准测试、虚拟远程操作以及视觉强化学习，无需真实机器人。系统提供了多种机器人形态和 40 多个对象的数据库，支持多样化的操作任务。<br /><strong>Keywords:</strong> Gaussian Splatting, photo-realistic simulation, robotic manipulation, sim2real, GSDF, DAgger data collection, virtual teleoperation, reinforcement learning, physics engine, closed-loop evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</div>
This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates "closing the loop" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.
<div><strong>Authors:</strong> Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "GSWorld is a closed-loop, photo‑realistic simulation suite for robotic manipulation that combines 3D Gaussian Splatting with physics engines via a new Gaussian Scene Description File (GSDF) format. The framework enables zero‑shot sim2real policy learning, automated DAgger data collection, reproducible benchmarking, virtual teleoperation, and visual reinforcement learning without requiring real robots. A curated database of robot embodiments and objects supports diverse manipulation tasks.", "summary_cn": "GSWorld 是一个闭环的、逼真渲染的机器人操作仿真平台，基于 3D Gaussian Splatting 与物理引擎，并引入 Gaussian Scene Description File（GSDF）格式，将高斯‑网格表示与机器人 URDF 等对象融合。该框架实现了零样本 sim2real 像素到动作策略学习、自动 DAgger 数据收集、可复现的仿真基准测试、虚拟远程操作以及视觉强化学习，无需真实机器人。系统提供了多种机器人形态和 40 多个对象的数据库，支持多样化的操作任务。", "keywords": "Gaussian Splatting, photo-realistic simulation, robotic manipulation, sim2real, GSDF, DAgger data collection, virtual teleoperation, reinforcement learning, physics engine, closed-loop evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guangqi Jiang", "Haoran Chang", "Ri-Zhao Qiu", "Yutong Liang", "Mazeyu Ji", "Jiyue Zhu", "Zhao Dong", "Xueyan Zou", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:26+00:00</pubDate>
</item>
<item>
<title>Real Deep Research for AI, Robotics and Beyond</title>
<link>https://papers.cool/arxiv/2510.20809</link>
<guid>https://papers.cool/arxiv/2510.20809</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area to identify emerging trends, uncover cross‑domain opportunities, and suggest concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across topics. The work aims to help researchers keep pace with the rapid growth of literature and discover novel research directions.<br /><strong>Summary (CN):</strong> 本文提出了一种通用的流水线（Real Deep Research, RDR），用于系统地分析任何研究领域，识别新兴趋势、跨领域机会，并提供具体的研究起点。作者将该框架应用于 AI 与机器人领域，重点关注基础模型和机器人技术，并在附录中展示了各主题的详尽结果，旨在帮助研究者应对快速增长的文献并发现新方向。<br /><strong>Keywords:</strong> literature analysis, trend detection, cross-domain discovery, AI research mapping, robotics, foundation models, systematic review, meta-research<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.
<div><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area to identify emerging trends, uncover cross‑domain opportunities, and suggest concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across topics. The work aims to help researchers keep pace with the rapid growth of literature and discover novel research directions.", "summary_cn": "本文提出了一种通用的流水线（Real Deep Research, RDR），用于系统地分析任何研究领域，识别新兴趋势、跨领域机会，并提供具体的研究起点。作者将该框架应用于 AI 与机器人领域，重点关注基础模型和机器人技术，并在附录中展示了各主题的详尽结果，旨在帮助研究者应对快速增长的文献并发现新方向。", "keywords": "literature analysis, trend detection, cross-domain discovery, AI research mapping, robotics, foundation models, systematic review, meta-research", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:05+00:00</pubDate>
</item>
<item>
<title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title>
<link>https://papers.cool/arxiv/2510.20800</link>
<guid>https://papers.cool/arxiv/2510.20800</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fast LLM adaptation method that, using a single gradient step on only 100 examples, identifies a small set of weight matrices for low‑rank factor based on singular‑value gradients, eliminating exhaustive layer‑wise search and achieving up to 24.6% accuracy gains without fine‑tuning. It demonstrates that evaluating on a tiny sample set suffices because downstream performance is dominated by prompting style rather than dataset size, resulting in a practical and robust compression‑based adaptation pipeline.<br /><strong>Summary (CN):</strong> 本文提出一种快速的 LLM 适应方法，仅使用 100 条样本进行一次梯度更新，即可通过奇异值梯度识别少数需要进行低秩分解的权重矩阵，省去逐层全面搜索，并在无需微调的情况下实现最高 24.6% 的准确率提升。研究表明，下游任务表现主要受提示风格影响，因而只需少量样本即可评估，从而形成一种实用且鲁棒的基于压缩的适应流程。<br /><strong>Keywords:</strong> LLM adaptation, low-rank factorization, singular value gradient, matrix pruning, LASER, efficient fine-tuning, model compression, single-step adaptation<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.
<div><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fast LLM adaptation method that, using a single gradient step on only 100 examples, identifies a small set of weight matrices for low‑rank factor based on singular‑value gradients, eliminating exhaustive layer‑wise search and achieving up to 24.6% accuracy gains without fine‑tuning. It demonstrates that evaluating on a tiny sample set suffices because downstream performance is dominated by prompting style rather than dataset size, resulting in a practical and robust compression‑based adaptation pipeline.", "summary_cn": "本文提出一种快速的 LLM 适应方法，仅使用 100 条样本进行一次梯度更新，即可通过奇异值梯度识别少数需要进行低秩分解的权重矩阵，省去逐层全面搜索，并在无需微调的情况下实现最高 24.6% 的准确率提升。研究表明，下游任务表现主要受提示风格影响，因而只需少量样本即可评估，从而形成一种实用且鲁棒的基于压缩的适应流程。", "keywords": "LLM adaptation, low-rank factorization, singular value gradient, matrix pruning, LASER, efficient fine-tuning, model compression, single-step adaptation", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}
]]></acme>

<pubDate>2025-10-23T17:58:01+00:00</pubDate>
</item>
<item>
<title>MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs</title>
<link>https://papers.cool/arxiv/2510.20762</link>
<guid>https://papers.cool/arxiv/2510.20762</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MEIcoder is a biologically informed decoding method that uses neuron-specific most exciting inputs (MEIs), a structural similarity index loss, and adversarial training to reconstruct visual stimuli from primary visual cortex activity. It achieves state-of-the-art performance especially on small datasets, requiring as few as 1,000-2,500 neurons and under 1,000 training samples, and introduces a large benchmark for future work.<br /><strong>Summary (CN):</strong> MEIcoder 是一种结合神经元特定的最激发输入（MEI）、结构相似性指数（SSIM）损失以及对抗训练的生物信息解码方法，用于从初级视觉皮层（V1）的神经活动中重建视觉刺激。该方法在小规模数据集表现卓越，仅需约 1,000-2,500 个神经元和不足 1,000 条训练样本即可实现高保真图像重建，并提供了一个包含 160,000 多样本的统一基准以促进后续研究。<br /><strong>Keywords:</strong> visual decoding, neural activity, most exciting inputs, MEIcoder, brain-machine interface, adversarial training, SSIM loss, neuroscience, small dataset, representation learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jan Sobotka, Luca Baroni, Ján Antolík</div>
Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.
<div><strong>Authors:</strong> Jan Sobotka, Luca Baroni, Ján Antolík</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MEIcoder is a biologically informed decoding method that uses neuron-specific most exciting inputs (MEIs), a structural similarity index loss, and adversarial training to reconstruct visual stimuli from primary visual cortex activity. It achieves state-of-the-art performance especially on small datasets, requiring as few as 1,000-2,500 neurons and under 1,000 training samples, and introduces a large benchmark for future work.", "summary_cn": "MEIcoder 是一种结合神经元特定的最激发输入（MEI）、结构相似性指数（SSIM）损失以及对抗训练的生物信息解码方法，用于从初级视觉皮层（V1）的神经活动中重建视觉刺激。该方法在小规模数据集表现卓越，仅需约 1,000-2,500 个神经元和不足 1,000 条训练样本即可实现高保真图像重建，并提供了一个包含 160,000 多样本的统一基准以促进后续研究。", "keywords": "visual decoding, neural activity, most exciting inputs, MEIcoder, brain-machine interface, adversarial training, SSIM loss, neuroscience, small dataset, representation learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jan Sobotka", "Luca Baroni", "Ján Antolík"]}
]]></acme>

<pubDate>2025-10-23T17:35:34+00:00</pubDate>
</item>
<item>
<title>Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models</title>
<link>https://papers.cool/arxiv/2510.20468</link>
<guid>https://papers.cool/arxiv/2510.20468</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a black-box, one-shot watermark forging attack that uses a preference model trained on procedurally generated images to determine watermark presence and then optimizes a target image via backpropagation to embed the stolen watermark, requiring only a single example and no knowledge of the original watermarking scheme. Experiments across several post-hoc image watermarking methods show that the attack can reliably forge watermarks, exposing a significant security weakness in current watermarking approaches.<br /><strong>Summary (CN):</strong> 本文提出一种黑盒单次水印伪造攻击，利用在程序生成图像上训练的偏好模型判断图像是否带有水印，并通过反向传播优化目标图像以嵌入被盗的水印，仅需一张水印图像且无需了解原始水印模型。对多种后置图像水印方法的实验显示，该攻击能够有效伪造水印，揭示了现有水印技术的重大安全缺陷。<br /><strong>Keywords:</strong> watermarking, watermark forging, black-box attack, preference model, post-hoc image watermark, AI-generated content, security, adversarial forgery<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.
<div><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a black-box, one-shot watermark forging attack that uses a preference model trained on procedurally generated images to determine watermark presence and then optimizes a target image via backpropagation to embed the stolen watermark, requiring only a single example and no knowledge of the original watermarking scheme. Experiments across several post-hoc image watermarking methods show that the attack can reliably forge watermarks, exposing a significant security weakness in current watermarking approaches.", "summary_cn": "本文提出一种黑盒单次水印伪造攻击，利用在程序生成图像上训练的偏好模型判断图像是否带有水印，并通过反向传播优化目标图像以嵌入被盗的水印，仅需一张水印图像且无需了解原始水印模型。对多种后置图像水印方法的实验显示，该攻击能够有效伪造水印，揭示了现有水印技术的重大安全缺陷。", "keywords": "watermarking, watermark forging, black-box attack, preference model, post-hoc image watermark, AI-generated content, security, adversarial forgery", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tomáš Souček", "Sylvestre-Alvise Rebuffi", "Pierre Fernandez", "Nikola Jovanović", "Hady Elsahar", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"]}
]]></acme>

<pubDate>2025-10-23T12:06:35+00:00</pubDate>
</item>
<item>
<title>Synthetic Data for Robust Runway Detection</title>
<link>https://papers.cool/arxiv/2510.20349</link>
<guid>https://papers.cool/arxiv/2510.20349</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes generating synthetic runway images using a commercial flight simulator and combining them with a small set of annotated real images to train standard object detection models for accurate runway detection. By controlling the image generation process and applying a customized domain adaptation strategy, the approach improves robustness to adverse conditions such as nighttime scenes that are absent from the real data.<br /><strong>Summary (CN):</strong> 本文利用商业飞行模拟器生成合成跑道图像，并与少量标注的真实图像结合，训练目标检测模型，以实现准确的跑道检测。通过控制合成过程并采用定制的领域适应策略，展示了模型在真实数据中未出现的夜间等不利条件下的鲁棒性提升。<br /><strong>Keywords:</strong> synthetic data, runway detection, domain adaptation, object detection, autonomous landing, robustness, flight simulator, computer vision<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, Thomas Oberlin</div>
Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.
<div><strong>Authors:</strong> Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, Thomas Oberlin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes generating synthetic runway images using a commercial flight simulator and combining them with a small set of annotated real images to train standard object detection models for accurate runway detection. By controlling the image generation process and applying a customized domain adaptation strategy, the approach improves robustness to adverse conditions such as nighttime scenes that are absent from the real data.", "summary_cn": "本文利用商业飞行模拟器生成合成跑道图像，并与少量标注的真实图像结合，训练目标检测模型，以实现准确的跑道检测。通过控制合成过程并采用定制的领域适应策略，展示了模型在真实数据中未出现的夜间等不利条件下的鲁棒性提升。", "keywords": "synthetic data, runway detection, domain adaptation, object detection, autonomous landing, robustness, flight simulator, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Fabrice Jimenez", "Thomas Oberlin"]}
]]></acme>

<pubDate>2025-10-23T08:48:37+00:00</pubDate>
</item>
<item>
<title>Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking</title>
<link>https://papers.cool/arxiv/2510.20335</link>
<guid>https://papers.cool/arxiv/2510.20335</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Dino-Diffusion Parking (DDP), a domain‑agnostic autonomous parking pipeline that leverages visual foundation models and diffusion‑based planning to achieve robust perception and motion planning under distribution shifts. Trained only in a standard CARLA setting, DDP transfers zero‑shot to adverse weather and lighting conditions, attaining >90 % success across multiple out‑of‑distribution scenarios, and shows promising sim‑to‑real performance in a 3D Gaussian splatting reconstruction of a real parking lot.<br /><strong>Summary (CN):</strong> 本文提出 Dino‑Diffusion Parking (DDP) 系统，将视觉基础模型与基于扩散的规划相结合，实现跨域鲁棒的自动泊车。模型仅在标准 CARLA 环境下训练，却能在零样本条件下适应恶劣天气、光照等分布转移，在所有测试的 OOD 场景中停车成功率超过 90%，并在基于真实停车场的 3D Gaussian splatting 环境中展示了有前景的模拟到真实转移能力。<br /><strong>Keywords:</strong> autonomous parking, domain generalization, diffusion planning, visual foundation models, zero-shot transfer, sim-to-real, robustness, 3D Gaussian splatting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren</div>
Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.
<div><strong>Authors:</strong> Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Dino-Diffusion Parking (DDP), a domain‑agnostic autonomous parking pipeline that leverages visual foundation models and diffusion‑based planning to achieve robust perception and motion planning under distribution shifts. Trained only in a standard CARLA setting, DDP transfers zero‑shot to adverse weather and lighting conditions, attaining >90 % success across multiple out‑of‑distribution scenarios, and shows promising sim‑to‑real performance in a 3D Gaussian splatting reconstruction of a real parking lot.", "summary_cn": "本文提出 Dino‑Diffusion Parking (DDP) 系统，将视觉基础模型与基于扩散的规划相结合，实现跨域鲁棒的自动泊车。模型仅在标准 CARLA 环境下训练，却能在零样本条件下适应恶劣天气、光照等分布转移，在所有测试的 OOD 场景中停车成功率超过 90%，并在基于真实停车场的 3D Gaussian splatting 环境中展示了有前景的模拟到真实转移能力。", "keywords": "autonomous parking, domain generalization, diffusion planning, visual foundation models, zero-shot transfer, sim-to-real, robustness, 3D Gaussian splatting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Zixuan Wu", "Hengyuan Zhang", "Ting-Hsuan Chen", "Yuliang Guo", "David Paz", "Xinyu Huang", "Liu Ren"]}
]]></acme>

<pubDate>2025-10-23T08:35:50+00:00</pubDate>
</item>
<item>
<title>GUSL-Dehaze: A Green U-Shaped Learning Approach to Image Dehazing</title>
<link>https://papers.cool/arxiv/2510.20266</link>
<guid>https://papers.cool/arxiv/2510.20266</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GUSL-Dehaze, a lightweight, non‑learning image dehazing method that combines a modified Dark Channel Prior with a green learning pipeline implemented via a U‑shaped architecture. The approach uses unsupervised representation learning, feature‑engineering techniques such as Relevant Feature Test and Least‑Squares Normal Transform, and a transparent supervised step to produce dehazed images while keeping parameter count low and maintaining mathematical interpretability.<br /><strong>Summary (CN):</strong> 本文提出 GUSL-Dehaze，一种轻量级、完全不使用深度学习的图像去雾方法，先利用改进的暗通道先验进行初步去雾，再通过 U 形结构的绿色学习管道进行无监督特征提取，并结合 Relevant Feature Test (RFT) 与 Least‑Squares Normal Transform (LNT) 等特征工程技术，最后采用透明的监督学习得到去雾图像，实现参数量低且具有数学可解释性。<br /><strong>Keywords:</strong> image dehazing, dark channel prior, green learning, U-shaped architecture, unsupervised representation learning, feature engineering, lightweight model, mathematical interpretability<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mahtab Movaheddrad, Laurence Palmer, C. -C. Jay Kuo</div>
Image dehazing is a restoration task that aims to recover a clear image from a single hazy input. Traditional approaches rely on statistical priors and the physics-based atmospheric scattering model to reconstruct the haze-free image. While recent state-of-the-art methods are predominantly based on deep learning architectures, these models often involve high computational costs and large parameter sizes, making them unsuitable for resource-constrained devices. In this work, we propose GUSL-Dehaze, a Green U-Shaped Learning approach to image dehazing. Our method integrates a physics-based model with a green learning (GL) framework, offering a lightweight, transparent alternative to conventional deep learning techniques. Unlike neural network-based solutions, GUSL-Dehaze completely avoids deep learning. Instead, we begin with an initial dehazing step using a modified Dark Channel Prior (DCP), which is followed by a green learning pipeline implemented through a U-shaped architecture. This architecture employs unsupervised representation learning for effective feature extraction, together with feature-engineering techniques such as the Relevant Feature Test (RFT) and the Least-Squares Normal Transform (LNT) to maintain a compact model size. Finally, the dehazed image is obtained via a transparent supervised learning strategy. GUSL-Dehaze significantly reduces parameter count while ensuring mathematical interpretability and achieving performance on par with state-of-the-art deep learning models.
<div><strong>Authors:</strong> Mahtab Movaheddrad, Laurence Palmer, C. -C. Jay Kuo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GUSL-Dehaze, a lightweight, non‑learning image dehazing method that combines a modified Dark Channel Prior with a green learning pipeline implemented via a U‑shaped architecture. The approach uses unsupervised representation learning, feature‑engineering techniques such as Relevant Feature Test and Least‑Squares Normal Transform, and a transparent supervised step to produce dehazed images while keeping parameter count low and maintaining mathematical interpretability.", "summary_cn": "本文提出 GUSL-Dehaze，一种轻量级、完全不使用深度学习的图像去雾方法，先利用改进的暗通道先验进行初步去雾，再通过 U 形结构的绿色学习管道进行无监督特征提取，并结合 Relevant Feature Test (RFT) 与 Least‑Squares Normal Transform (LNT) 等特征工程技术，最后采用透明的监督学习得到去雾图像，实现参数量低且具有数学可解释性。", "keywords": "image dehazing, dark channel prior, green learning, U-shaped architecture, unsupervised representation learning, feature engineering, lightweight model, mathematical interpretability", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahtab Movaheddrad", "Laurence Palmer", "C. -C. Jay Kuo"]}
]]></acme>

<pubDate>2025-10-23T06:46:22+00:00</pubDate>
</item>
<item>
<title>Kinaema: a recurrent sequence model for memory and pose in motion</title>
<link>https://papers.cool/arxiv/2510.20261</link>
<guid>https://papers.cool/arxiv/2510.20261</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Kinaema, a recurrent sequence model that maintains an implicit latent memory via a transformer to integrate visual observations for continuous navigation tasks. By compressing observation history into a compact representation, the model can answer pose queries and navigate to goals observed before an episode, demonstrating efficiency compared to classic attention‑based transformers in the newly proposed Mem‑Nav task.<br /><strong>Summary (CN):</strong> 本文提出了 Kinaema，一种递归序列模型，通过 transformer 以隐式潜在记忆方式整合测，从而在连续导航任务中实现对历史信息的压缩表示。该模型能够在请求时对查询图像进行位姿预测，并在实际任务开始前已观测到的目标位置进行导航，展示了相较于传统注意力 transformer 在新任务 Mem‑Nav 中的计算效率和有效性。<br /><strong>Keywords:</strong> recurrent transformer, latent memory, visual navigation, pose estimation, Mem-Nav, scene representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Mert Bulent Sariyildiz, Philippe Weinzaepfel, Guillaume Bono, Gianluca Monaci, Christian Wolf</div>
One key aspect of spatially aware robots is the ability to "find their bearings", ie. to correctly situate themselves in previously seen spaces. In this work, we focus on this particular scenario of continuous robotics operations, where information observed before an actual episode start is exploited to optimize efficiency. We introduce a new model, Kinaema, and agent, capable of integrating a stream of visual observations while moving in a potentially large scene, and upon request, processing a query image and predicting the relative position of the shown space with respect to its current position. Our model does not explicitly store an observation history, therefore does not have hard constraints on context length. It maintains an implicit latent memory, which is updated by a transformer in a recurrent way, compressing the history of sensor readings into a compact representation. We evaluate the impact of this model in a new downstream task we call "Mem-Nav". We show that our large-capacity recurrent model maintains a useful representation of the scene, navigates to goals observed before the actual episode start, and is computationally efficient, in particular compared to classical transformers with attention over an observation history.
<div><strong>Authors:</strong> Mert Bulent Sariyildiz, Philippe Weinzaepfel, Guillaume Bono, Gianluca Monaci, Christian Wolf</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Kinaema, a recurrent sequence model that maintains an implicit latent memory via a transformer to integrate visual observations for continuous navigation tasks. By compressing observation history into a compact representation, the model can answer pose queries and navigate to goals observed before an episode, demonstrating efficiency compared to classic attention‑based transformers in the newly proposed Mem‑Nav task.", "summary_cn": "本文提出了 Kinaema，一种递归序列模型，通过 transformer 以隐式潜在记忆方式整合测，从而在连续导航任务中实现对历史信息的压缩表示。该模型能够在请求时对查询图像进行位姿预测，并在实际任务开始前已观测到的目标位置进行导航，展示了相较于传统注意力 transformer 在新任务 Mem‑Nav 中的计算效率和有效性。", "keywords": "recurrent transformer, latent memory, visual navigation, pose estimation, Mem-Nav, scene representation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Mert Bulent Sariyildiz", "Philippe Weinzaepfel", "Guillaume Bono", "Gianluca Monaci", "Christian Wolf"]}
]]></acme>

<pubDate>2025-10-23T06:34:53+00:00</pubDate>
</item>
<item>
<title>Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</title>
<link>https://papers.cool/arxiv/2510.20193</link>
<guid>https://papers.cool/arxiv/2510.20193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This survey reviews recent advancements in question answering systems that incorporate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. It categorizes approaches by retrieval methods, fusion techniques, and answer generation strategies, and discusses benchmarks, evaluation protocols, and challenges such as cross-modal alignment and latency‑accuracy tradeoffs.<br /><strong>Summary (CN):</strong> 本文综述了多媒体检索集成到问答系统中的进展，重点关注将视觉、语言和音频模态与用户查询对齐的架构。文章按检索方法、融合技术和答案生成策略进行分类，并讨论了基准数据集、评估协议以及跨模态对齐、延迟‑准确性权衡等关键挑战。<br /><strong>Keywords:</strong> multimodal QA, retrieval-augmented QA, cross-modal reasoning, vision-language fusion, audio-visual retrieval, benchmark datasets, semantic grounding, latency-accuracy tradeoff<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.
<div><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This survey reviews recent advancements in question answering systems that incorporate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. It categorizes approaches by retrieval methods, fusion techniques, and answer generation strategies, and discusses benchmarks, evaluation protocols, and challenges such as cross-modal alignment and latency‑accuracy tradeoffs.", "summary_cn": "本文综述了多媒体检索集成到问答系统中的进展，重点关注将视觉、语言和音频模态与用户查询对齐的架构。文章按检索方法、融合技术和答案生成策略进行分类，并讨论了基准数据集、评估协议以及跨模态对齐、延迟‑准确性权衡等关键挑战。", "keywords": "multimodal QA, retrieval-augmented QA, cross-modal reasoning, vision-language fusion, audio-visual retrieval, benchmark datasets, semantic grounding, latency-accuracy tradeoff", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rahul Raja", "Arpita Vats"]}
]]></acme>

<pubDate>2025-10-23T04:25:44+00:00</pubDate>
</item>
<item>
<title>Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning</title>
<link>https://papers.cool/arxiv/2510.20108</link>
<guid>https://papers.cool/arxiv/2510.20108</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies partial prototype collapse as a common failure in prototypical self‑supervised learning, where many prototypes converge to nearly identical vectors, reducing target diversity. By analyzing the joint optimization of encoders and prototypes, the authors propose a fully decoupled training scheme that updates prototypes via an online EM‑style Gaussian‑mixture model independent of the encoder loss, eliminating collapse and improving downstream performance.<br /><strong>Summary (CN):</strong> 本文指出原型自监督学习中常出现的部分原型坍缩现象，即多个原型向几乎相同的表示收敛，导致目标多样性下降。通过分析编码器与原型的联合优化导致的快捷学习，作者提出一种完全解耦的训练策略，使用在线 EM 式的高斯混合模型独立更新原型，从而消除坍缩并提升下游任务表现。<br /><strong>Keywords:</strong> prototypical self-supervised learning, prototype collapse, Gaussian mixture, EM algorithm, decoupled training, representation diversity<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Gabriel Y. Arteaga, Marius Aasan, Rwiddhi Chakraborty, Martine Hjelkrem-Tan, Thalles Silva, Michael Kampffmeyer, Adín Ramírez Rivera</div>
Prototypical self-supervised learning methods consistently suffer from partial prototype collapse, where multiple prototypes converge to nearly identical representations. This undermines their central purpose -- providing diverse and informative targets to guide encoders toward rich representations -- and has led practitioners to over-parameterize prototype sets or add ad-hoc regularizers, which mitigate symptoms rather than address the root cause. We empirically trace the collapse to the joint optimization of encoders and prototypes, which encourages a type of shortcut learning: early in training prototypes drift toward redundant representations that minimize loss without necessarily enhancing representation diversity. To break the joint optimization, we introduce a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Concretely, we model prototypes as a Gaussian mixture updated with an online EM-style procedure, independent of the encoder's loss. This simple yet principled decoupling eliminates prototype collapse without explicit regularization and yields consistently diverse prototypes and stronger downstream performance.
<div><strong>Authors:</strong> Gabriel Y. Arteaga, Marius Aasan, Rwiddhi Chakraborty, Martine Hjelkrem-Tan, Thalles Silva, Michael Kampffmeyer, Adín Ramírez Rivera</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies partial prototype collapse as a common failure in prototypical self‑supervised learning, where many prototypes converge to nearly identical vectors, reducing target diversity. By analyzing the joint optimization of encoders and prototypes, the authors propose a fully decoupled training scheme that updates prototypes via an online EM‑style Gaussian‑mixture model independent of the encoder loss, eliminating collapse and improving downstream performance.", "summary_cn": "本文指出原型自监督学习中常出现的部分原型坍缩现象，即多个原型向几乎相同的表示收敛，导致目标多样性下降。通过分析编码器与原型的联合优化导致的快捷学习，作者提出一种完全解耦的训练策略，使用在线 EM 式的高斯混合模型独立更新原型，从而消除坍缩并提升下游任务表现。", "keywords": "prototypical self-supervised learning, prototype collapse, Gaussian mixture, EM algorithm, decoupled training, representation diversity", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gabriel Y. Arteaga", "Marius Aasan", "Rwiddhi Chakraborty", "Martine Hjelkrem-Tan", "Thalles Silva", "Michael Kampffmeyer", "Adín Ramírez Rivera"]}
]]></acme>

<pubDate>2025-10-23T01:25:10+00:00</pubDate>
</item>
<item>
<title>AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training</title>
<link>https://papers.cool/arxiv/2510.20012</link>
<guid>https://papers.cool/arxiv/2510.20012</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents an AI-driven pipeline for pose estimation that quantifies joint-angle trajectories and derived kinematic metrics during resistance‑training exercises, comparing lengthened partial (pROM) and full range‑of‑motion (fROM) protocols across multiple participants. Using a random‑effects meta‑analytic model, the study finds that pROM sets exhibit reduced ROM and shorter eccentric phases, with participant variability driving most of the observed differences, and introduces the %ROM metric to standardize partial‑range definitions. The results highlight how AI‑based motion analysis can deepen biomechanical insight and inform training prescription.<br /><strong>Summary (CN):</strong> 本文提出了一套基于 AI 的姿态估计流水线，用于提取阻力训练中关节角度轨迹并计算运动学指标，比较了加长的部分范围 (pROM) 与完整范围 (fROM) 的训练方式。通过随机效应元分析模型，研究发现 pROM 训练的运动范围更小且离心相位更短，且参与者之间的差异是主要变异来源，并引入 %ROM 指标统一部分范围的定义。结果表明 AI 运动分析能够提升对生物力学的理解并改进训练处方。<br /><strong>Keywords:</strong> AI pose estimation, kinematic profiling, range-of-motion, resistance training, meta-analysis, video analysis, %ROM, biomechanics, partial ROM, movement dynamics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Adam Diamant</div>
This study develops an AI-based pose estimation pipeline to enable precise quantification of movement kinematics in resistance training. Using video data from Wolf et al. (2025), which compared lengthened partial (pROM) and full range-of-motion (fROM) training across eight upper-body exercises in 26 participants, 280 recordings were processed to extract frame-level joint-angle trajectories. After filtering and smoothing, per-set metrics were derived, including range of motion (ROM), tempo, and concentric/eccentric phase durations. A random-effects meta-analytic model was applied to account for within-participant and between-exercise variability. Results show that pROM repetitions were performed with a smaller ROM and shorter overall durations, particularly during the eccentric phase of movement. Variance analyses revealed that participant-level differences, rather than exercise-specific factors, were the primary driver of variation, although there is substantial evidence of heterogeneous treatment effects. We then introduce a novel metric, \%ROM, which is the proportion of full ROM achieved during pROM, and demonstrate that this definition of lengthened partials remains relatively consistent across exercises. Overall, these findings suggest that lengthened partials differ from full ROM training not only in ROM, but also in execution dynamics and consistency, highlighting the potential of AI-based methods for advancing research and improving resistance training prescription.
<div><strong>Authors:</strong> Adam Diamant</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents an AI-driven pipeline for pose estimation that quantifies joint-angle trajectories and derived kinematic metrics during resistance‑training exercises, comparing lengthened partial (pROM) and full range‑of‑motion (fROM) protocols across multiple participants. Using a random‑effects meta‑analytic model, the study finds that pROM sets exhibit reduced ROM and shorter eccentric phases, with participant variability driving most of the observed differences, and introduces the %ROM metric to standardize partial‑range definitions. The results highlight how AI‑based motion analysis can deepen biomechanical insight and inform training prescription.", "summary_cn": "本文提出了一套基于 AI 的姿态估计流水线，用于提取阻力训练中关节角度轨迹并计算运动学指标，比较了加长的部分范围 (pROM) 与完整范围 (fROM) 的训练方式。通过随机效应元分析模型，研究发现 pROM 训练的运动范围更小且离心相位更短，且参与者之间的差异是主要变异来源，并引入 %ROM 指标统一部分范围的定义。结果表明 AI 运动分析能够提升对生物力学的理解并改进训练处方。", "keywords": "AI pose estimation, kinematic profiling, range-of-motion, resistance training, meta-analysis, video analysis, %ROM, biomechanics, partial ROM, movement dynamics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Adam Diamant"]}
]]></acme>

<pubDate>2025-10-22T20:27:45+00:00</pubDate>
</item>
<item>
<title>Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts</title>
<link>https://papers.cool/arxiv/2510.19986</link>
<guid>https://papers.cool/arxiv/2510.19986</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a methodology that combines Large Language Models with Retrieval‑Augmented Generation and vector‑based search to automatically assign Iconclass codes to early‑modern religious woodcut illustrations. By generating full‑page textual descriptions that integrate visual and textual cues and then matching them via hybrid vector search, the system attains 87 % and 92 % precision at five‑ and four‑level classification, surpassing conventional image‑ and keyword‑based approaches.<br /><strong>Summary (CN):</strong> 本文提出一种将大型语言模型（LLM）与检索增强生成（RAG）以及向量搜索相结合的方法，用于自动为早期现代宗教木刻插图分配 Iconclass 编码。通过生成融合视觉和文本信息的整页描述，并通过混合向量检索匹配相应编码，系统在五级和四级分类上分别实现 87% 与 92% 的精度，显著优于传统的图像和关键词检索方法。<br /><strong>Keywords:</strong> Iconclass, LLM, Retrieval-Augmented Generation, vector search, image classification, digital humanities, religious woodcuts, multimodal description<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Drew B. Thomas</div>
This paper presents a novel methodology for classifying early modern religious images by using Large Language Models (LLMs) and vector databases in combination with Retrieval-Augmented Generation (RAG). The approach leverages the full-page context of book illustrations from the Holy Roman Empire, allowing the LLM to generate detailed descriptions that incorporate both visual and textual elements. These descriptions are then matched to relevant Iconclass codes through a hybrid vector search. This method achieves 87% and 92% precision at five and four levels of classification, significantly outperforming traditional image and keyword-based searches. By employing full-page descriptions and RAG, the system enhances classification accuracy, offering a powerful tool for large-scale analysis of early modern visual archives. This interdisciplinary approach demonstrates the growing potential of LLMs and RAG in advancing research within art history and digital humanities.
<div><strong>Authors:</strong> Drew B. Thomas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a methodology that combines Large Language Models with Retrieval‑Augmented Generation and vector‑based search to automatically assign Iconclass codes to early‑modern religious woodcut illustrations. By generating full‑page textual descriptions that integrate visual and textual cues and then matching them via hybrid vector search, the system attains 87 % and 92 % precision at five‑ and four‑level classification, surpassing conventional image‑ and keyword‑based approaches.", "summary_cn": "本文提出一种将大型语言模型（LLM）与检索增强生成（RAG）以及向量搜索相结合的方法，用于自动为早期现代宗教木刻插图分配 Iconclass 编码。通过生成融合视觉和文本信息的整页描述，并通过混合向量检索匹配相应编码，系统在五级和四级分类上分别实现 87% 与 92% 的精度，显著优于传统的图像和关键词检索方法。", "keywords": "Iconclass, LLM, Retrieval-Augmented Generation, vector search, image classification, digital humanities, religious woodcuts, multimodal description", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Drew B. Thomas"]}
]]></acme>

<pubDate>2025-10-22T19:34:19+00:00</pubDate>
</item>
<item>
<title>FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals</title>
<link>https://papers.cool/arxiv/2510.19917</link>
<guid>https://papers.cool/arxiv/2510.19917</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents FINDER, a framework that treats datasets as realizations of random fields and extracts stochastic features via the Kosambi‑Karhunen‑Loéve expansion, enabling classification in low‑signal‑to‑noise regimes by analyzing eigenspace residuals. It demonstrates state‑of‑the‑art performance on tasks such as Alzheimer's disease staging and deforestation detection, and discusses the method's expected advantages, failure modes, and limitations.<br /><strong>Summary (CN):</strong> 本文提出 FINDER 框架，将数据视为随机场的实现，通过 Kosambi‑Karhunen‑Loéve 展开提取随机特征，并利用特征的特征空间残差进行分类，尤其在信噪比低、样本稀少的情形下表现优秀。作者在阿尔茨海默病分期和森林砍伐遥感检测等数据匮乏的科学任务上实现了领先水平，并讨论了该方法的适用条件、失败模式及局限性。<br /><strong>Keywords:</strong> noisy data classification, stochastic features, Karhunen–Loève expansion, eigen-decomposition, feature inference, robustness, Hilbert space mapping, data-deficient domains<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Trajan Murphy, Akshunna S. Dogra, Hanfeng Gu, Caleb Meredith, Mark Kon, Julio Enrique Castrillion-Candas</div>
''Noisy'' datasets (regimes with low signal to noise ratios, small sample sizes, faulty data collection, etc) remain a key research frontier for classification methods with both theoretical and practical implications. We introduce FINDER, a rigorous framework for analyzing generic classification problems, with tailored algorithms for noisy datasets. FINDER incorporates fundamental stochastic analysis ideas into the feature learning and inference stages to optimally account for the randomness inherent to all empirical datasets. We construct ''stochastic features'' by first viewing empirical datasets as realizations from an underlying random field (without assumptions on its exact distribution) and then mapping them to appropriate Hilbert spaces. The Kosambi-Karhunen-Loéve expansion (KLE) breaks these stochastic features into computable irreducible components, which allow classification over noisy datasets via an eigen-decomposition: data from different classes resides in distinct regions, identified by analyzing the spectrum of the associated operators. We validate FINDER on several challenging, data-deficient scientific domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease stage classification, (ii) Remote sensing detection of deforestation. We end with a discussion on when FINDER is expected to outperform existing methods, its failure modes, and other limitations.
<div><strong>Authors:</strong> Trajan Murphy, Akshunna S. Dogra, Hanfeng Gu, Caleb Meredith, Mark Kon, Julio Enrique Castrillion-Candas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents FINDER, a framework that treats datasets as realizations of random fields and extracts stochastic features via the Kosambi‑Karhunen‑Loéve expansion, enabling classification in low‑signal‑to‑noise regimes by analyzing eigenspace residuals. It demonstrates state‑of‑the‑art performance on tasks such as Alzheimer's disease staging and deforestation detection, and discusses the method's expected advantages, failure modes, and limitations.", "summary_cn": "本文提出 FINDER 框架，将数据视为随机场的实现，通过 Kosambi‑Karhunen‑Loéve 展开提取随机特征，并利用特征的特征空间残差进行分类，尤其在信噪比低、样本稀少的情形下表现优秀。作者在阿尔茨海默病分期和森林砍伐遥感检测等数据匮乏的科学任务上实现了领先水平，并讨论了该方法的适用条件、失败模式及局限性。", "keywords": "noisy data classification, stochastic features, Karhunen–Loève expansion, eigen-decomposition, feature inference, robustness, Hilbert space mapping, data-deficient domains", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Trajan Murphy", "Akshunna S. Dogra", "Hanfeng Gu", "Caleb Meredith", "Mark Kon", "Julio Enrique Castrillion-Candas"]}
]]></acme>

<pubDate>2025-10-22T18:00:03+00:00</pubDate>
</item>
<item>
<title>Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18876</link>
<guid>https://papers.cool/arxiv/2510.18876</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Grasp Any Region (GAR), a region-level multimodal LLM architecture that uses a RoI‑aligned feature replay technique to incorporate necessary global context and model interactions among multiple prompts, enabling precise perception and advanced compositional reasoning for any image region. GAR also provides a new benchmark, GAR‑Bench, to evaluate single‑region comprehension as well as multi‑region interaction and reasoning. Experiments demonstrate that GAR‑1B attains state‑of‑the‑art captioning and VQA performance, and the zero‑shot GAR‑8B surpasses specialized video models on video‑referential benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了 Grasp Any Region (GAR) 框架，这是一种基于 RoI 对齐特征回放的多模态大语言模型，用于在理解任意图像区域时引入全局上下文并建模多个提示之间的交互，从而实现精确感知和高级组合推理。GAR 同时提供了新基准 GAR‑Bench，用于评估单区域理解以及多区域交互与推理能力。实验表明，GAR‑1B 在字幕生成和视觉问答任务上达到业界领先水平，零样本的 GAR‑8B 在视频指代基准上亦超越专用视频模型。<br /><strong>Keywords:</strong> multimodal LLM, region-level visual understanding, RoI-aligned feature replay, compositional reasoning, GAR-Bench, fine-grained perception, visual grounding, zero-shot transfer, VQA<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.
<div><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Grasp Any Region (GAR), a region-level multimodal LLM architecture that uses a RoI‑aligned feature replay technique to incorporate necessary global context and model interactions among multiple prompts, enabling precise perception and advanced compositional reasoning for any image region. GAR also provides a new benchmark, GAR‑Bench, to evaluate single‑region comprehension as well as multi‑region interaction and reasoning. Experiments demonstrate that GAR‑1B attains state‑of‑the‑art captioning and VQA performance, and the zero‑shot GAR‑8B surpasses specialized video models on video‑referential benchmarks.", "summary_cn": "本文提出了 Grasp Any Region (GAR) 框架，这是一种基于 RoI 对齐特征回放的多模态大语言模型，用于在理解任意图像区域时引入全局上下文并建模多个提示之间的交互，从而实现精确感知和高级组合推理。GAR 同时提供了新基准 GAR‑Bench，用于评估单区域理解以及多区域交互与推理能力。实验表明，GAR‑1B 在字幕生成和视觉问答任务上达到业界领先水平，零样本的 GAR‑8B 在视频指代基准上亦超越专用视频模型。", "keywords": "multimodal LLM, region-level visual understanding, RoI-aligned feature replay, compositional reasoning, GAR-Bench, fine-grained perception, visual grounding, zero-shot transfer, VQA", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:59:59+00:00</pubDate>
</item>
<item>
<title>DSI-Bench: A Benchmark for Dynamic Spatial Intelligence</title>
<link>https://papers.cool/arxiv/2510.18873</link>
<guid>https://papers.cool/arxiv/2510.18873</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DSI-Bench introduces a benchmark for evaluating dynamic spatial intelligence, comprising nearly 1,000 videos and 1,700 annotated questions covering nine distinct motion patterns of observers and objects. Experiments on 14 vision‑language and expert models reveal systematic shortcomings such as conflating observer and object motion, semantic biases, and inaccurate relative relationship inference. The benchmark highlights current limitations and guides future development of models with robust dynamic spatial reasoning.<br /><strong>Summary (CN):</strong> DSI-Bench 提出了用于评估动态空间智能的基准，包含约 1,000 条视频和 1,700 条人工标注的问题，覆盖观察者和对象的九种独立运动模式。对 14 种视觉‑语言模型和专业模型的实验显示，它们常将观察者与对象的运动混淆、存在语义偏差，并且在动态场景中难以准确推断相对关系。该基准揭示了现有模型的局限，为未来具备稳健动态空间推理能力的模型研发提供指引。<br /><strong>Keywords:</strong> dynamic spatial intelligence, benchmark, vision-language models, motion reasoning, spatial-temporal reasoning, dataset, observer motion, object motion, model evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziang Zhang, Zehan Wang, Guanghao Zhang, Weilong Dai, Yan Xia, Ziang Yan, Minjie Hong, Zhou Zhao</div>
Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.
<div><strong>Authors:</strong> Ziang Zhang, Zehan Wang, Guanghao Zhang, Weilong Dai, Yan Xia, Ziang Yan, Minjie Hong, Zhou Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DSI-Bench introduces a benchmark for evaluating dynamic spatial intelligence, comprising nearly 1,000 videos and 1,700 annotated questions covering nine distinct motion patterns of observers and objects. Experiments on 14 vision‑language and expert models reveal systematic shortcomings such as conflating observer and object motion, semantic biases, and inaccurate relative relationship inference. The benchmark highlights current limitations and guides future development of models with robust dynamic spatial reasoning.", "summary_cn": "DSI-Bench 提出了用于评估动态空间智能的基准，包含约 1,000 条视频和 1,700 条人工标注的问题，覆盖观察者和对象的九种独立运动模式。对 14 种视觉‑语言模型和专业模型的实验显示，它们常将观察者与对象的运动混淆、存在语义偏差，并且在动态场景中难以准确推断相对关系。该基准揭示了现有模型的局限，为未来具备稳健动态空间推理能力的模型研发提供指引。", "keywords": "dynamic spatial intelligence, benchmark, vision-language models, motion reasoning, spatial-temporal reasoning, dataset, observer motion, object motion, model evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziang Zhang", "Zehan Wang", "Guanghao Zhang", "Weilong Dai", "Yan Xia", "Ziang Yan", "Minjie Hong", "Zhou Zhao"]}
]]></acme>

<pubDate>2025-10-21T17:59:36+00:00</pubDate>
</item>
<item>
<title>DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</title>
<link>https://papers.cool/arxiv/2510.18851</link>
<guid>https://papers.cool/arxiv/2510.18851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DP²O‑SR introduces a Direct Perceptual Preference Optimization framework for real‑world image super‑resolution that leverages pretrained text‑to‑image diffusion models. By constructing a hybrid reward from full‑reference and no‑reference IQA models and forming multiple preference pairs with hierarchical weighting, the method improves perceptual quality without human annotations and adapts to model capacity.<br /><strong>Summary (CN):</strong> DP²O‑SR 提出一种直接感知偏好优化框架，用于实际图像超分辨率，利用预训练的文本到图像扩散模型。通过结合全参考与无参考图像质量评估模型构建混合奖励，并从同一模型的输出构造多对偏好样本，采用分层加权方式，实现无需人工标注的感知质量提升，并根据模型容量自适应优化。<br /><strong>Keywords:</strong> real-world super-resolution, diffusion models, perceptual preference optimization, image quality assessment, hierarchical preference weighting, generative models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</div>
Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.
<div><strong>Authors:</strong> Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DP²O‑SR introduces a Direct Perceptual Preference Optimization framework for real‑world image super‑resolution that leverages pretrained text‑to‑image diffusion models. By constructing a hybrid reward from full‑reference and no‑reference IQA models and forming multiple preference pairs with hierarchical weighting, the method improves perceptual quality without human annotations and adapts to model capacity.", "summary_cn": "DP²O‑SR 提出一种直接感知偏好优化框架，用于实际图像超分辨率，利用预训练的文本到图像扩散模型。通过结合全参考与无参考图像质量评估模型构建混合奖励，并从同一模型的输出构造多对偏好样本，采用分层加权方式，实现无需人工标注的感知质量提升，并根据模型容量自适应优化。", "keywords": "real-world super-resolution, diffusion models, perceptual preference optimization, image quality assessment, hierarchical preference weighting, generative models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rongyuan Wu", "Lingchen Sun", "Zhengqiang Zhang", "Shihao Wang", "Tianhe Wu", "Qiaosi Yi", "Shuai Li", "Lei Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:43:23+00:00</pubDate>
</item>
<item>
<title>See the Text: From Tokenization to Visual Reading</title>
<link>https://papers.cool/arxiv/2510.18840</link>
<guid>https://papers.cool/arxiv/2510.18840</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SeeTok, a vision-centric tokenization approach that renders text as images and feeds them to pretrained multimodal large language models, leveraging OCR and text‑vision alignment capabilities. Experiments across three language tasks show that SeeTok matches or outperforms traditional subword tokenizers while using 4.43× fewer tokens and reducing FLOPs by 70.5%, also improving cross‑lingual generalization and robustness to typographic noise. The work argues for shifting from symbolic tokenization toward human‑like visual reading for more natural language modeling.<br /><strong>Summary (CN):</strong> 本文提出 SeeTok，一种视觉化的文本标记方法，将文字渲染为图像后交由预训练的多模态大语言模型处理，利用 OCR 和文本‑视觉对齐能力。实验在三项语言任务上显示，SeeTok 在匹配或超越传统子词标记器的同时，仅使用 4.43 倍更少的 token，计算量降低 70.5%，并提升跨语言泛化能力和对排版噪声的鲁棒性。该工作主张从符号化标记转向类似人类视觉阅读的自然语言模型。<br /><strong>Keywords:</strong> visual tokenization, multimodal LLM, OCR, cross-lingual, token efficiency, visual-text, SeeTok<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.
<div><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SeeTok, a vision-centric tokenization approach that renders text as images and feeds them to pretrained multimodal large language models, leveraging OCR and text‑vision alignment capabilities. Experiments across three language tasks show that SeeTok matches or outperforms traditional subword tokenizers while using 4.43× fewer tokens and reducing FLOPs by 70.5%, also improving cross‑lingual generalization and robustness to typographic noise. The work argues for shifting from symbolic tokenization toward human‑like visual reading for more natural language modeling.", "summary_cn": "本文提出 SeeTok，一种视觉化的文本标记方法，将文字渲染为图像后交由预训练的多模态大语言模型处理，利用 OCR 和文本‑视觉对齐能力。实验在三项语言任务上显示，SeeTok 在匹配或超越传统子词标记器的同时，仅使用 4.43 倍更少的 token，计算量降低 70.5%，并提升跨语言泛化能力和对排版噪声的鲁棒性。该工作主张从符号化标记转向类似人类视觉阅读的自然语言模型。", "keywords": "visual tokenization, multimodal LLM, OCR, cross-lingual, token efficiency, visual-text, SeeTok", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"]}
]]></acme>

<pubDate>2025-10-21T17:34:48+00:00</pubDate>
</item>
<item>
<title>FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning</title>
<link>https://papers.cool/arxiv/2510.18837</link>
<guid>https://papers.cool/arxiv/2510.18837</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FedDEAP, an adaptive federated prompt tuning framework that enhances CLIP's zero-shot classification across multiple client domains by disentangling semantic and domain-specific features, employing a dual-prompt design (global semantic prompt and local domain prompt), and aligning transformed visual and textual representations. Experiments on four datasets demonstrate improved generalization in federated image recognition under domain shift and label heterogeneity.<br /><strong>Summary (CN):</strong> 本文提出 FedDEAP——一种自适应的联邦提示调优框架，通过语义与领域特征解耦、全局语义提示与本地领域提示的双提示设计，以及在两种变换下对齐视觉与文本表示，提升 CLIP 在多域联邦图像分类中的零样本泛化能力。实验在四个数据集上显示，该方法在面对域迁移和标签异构时显著提升了全局模型的泛化表现。<br /><strong>Keywords:</strong> federated learning, CLIP, prompt tuning, domain adaptation, dual prompt, multi-domain, vision-language, label heterogeneity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yubin Zheng, Pak-Hei Yeung, Jing Xia, Tianjie Ju, Peng Tang, Weidong Qiu, Jagath C. Rajapakse</div>
Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.
<div><strong>Authors:</strong> Yubin Zheng, Pak-Hei Yeung, Jing Xia, Tianjie Ju, Peng Tang, Weidong Qiu, Jagath C. Rajapakse</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FedDEAP, an adaptive federated prompt tuning framework that enhances CLIP's zero-shot classification across multiple client domains by disentangling semantic and domain-specific features, employing a dual-prompt design (global semantic prompt and local domain prompt), and aligning transformed visual and textual representations. Experiments on four datasets demonstrate improved generalization in federated image recognition under domain shift and label heterogeneity.", "summary_cn": "本文提出 FedDEAP——一种自适应的联邦提示调优框架，通过语义与领域特征解耦、全局语义提示与本地领域提示的双提示设计，以及在两种变换下对齐视觉与文本表示，提升 CLIP 在多域联邦图像分类中的零样本泛化能力。实验在四个数据集上显示，该方法在面对域迁移和标签异构时显著提升了全局模型的泛化表现。", "keywords": "federated learning, CLIP, prompt tuning, domain adaptation, dual prompt, multi-domain, vision-language, label heterogeneity", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yubin Zheng", "Pak-Hei Yeung", "Jing Xia", "Tianjie Ju", "Peng Tang", "Weidong Qiu", "Jagath C. Rajapakse"]}
]]></acme>

<pubDate>2025-10-21T17:32:44+00:00</pubDate>
</item>
<item>
<title>Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework</title>
<link>https://papers.cool/arxiv/2510.18825</link>
<guid>https://papers.cool/arxiv/2510.18825</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified hierarchical mask framework that shows how graph transformer architectures can be expressed as attention mask constructions, introducing design principles linking receptive field size and label consistency to classification accuracy. Building on this framework, the authors develop M3Dphormer, a Mixture-of-Experts graph transformer with multi-level masking and a dual attention computation that adapts between dense and sparse modes, achieving state-of-the-art results on several benchmarks.<br /><strong>Summary (CN):</strong> 本文提出统一的层次掩码框架，展示图 Transformer 架构可以等价地视为注意力掩码的构造，并提出有效掩码应兼顾足够的感受野和标签一致性这一设计原则。在此基础上，作者构建 M3Dphormer——一种基于专家混合的图 Transformer，采用多层掩码和双模式注意力计算，根据局部掩码稀疏度在密集和稀疏模式间切换，在多个基准上实现了最新性能。<br /><strong>Keywords:</strong> graph transformers, hierarchical masks, attention mask, mixture-of-experts, dual attention, receptive field, label consistency, graph representation learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi</div>
Graph Transformers (GTs) have emerged as a powerful paradigm for graph representation learning due to their ability to model diverse node interactions. However, existing GTs often rely on intricate architectural designs tailored to specific interactions, limiting their flexibility. To address this, we propose a unified hierarchical mask framework that reveals an underlying equivalence between model architecture and attention mask construction. This framework enables a consistent modeling paradigm by capturing diverse interactions through carefully designed attention masks. Theoretical analysis under this framework demonstrates that the probability of correct classification positively correlates with the receptive field size and label consistency, leading to a fundamental design principle: an effective attention mask should ensure both a sufficiently large receptive field and a high level of label consistency. While no single existing mask satisfies this principle across all scenarios, our analysis reveals that hierarchical masks offer complementary strengths, motivating their effective integration. Then, we introduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with Multi-Level Masking and Dual Attention Computation. M3Dphormer incorporates three theoretically grounded hierarchical masks and employs a bi-level expert routing mechanism to adaptively integrate multi-level interaction information. To ensure scalability, we further introduce a dual attention computation scheme that dynamically switches between dense and sparse modes based on local mask sparsity. Extensive experiments across multiple benchmarks demonstrate that M3Dphormer achieves state-of-the-art performance, validating the effectiveness of our unified framework and model design.
<div><strong>Authors:</strong> Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified hierarchical mask framework that shows how graph transformer architectures can be expressed as attention mask constructions, introducing design principles linking receptive field size and label consistency to classification accuracy. Building on this framework, the authors develop M3Dphormer, a Mixture-of-Experts graph transformer with multi-level masking and a dual attention computation that adapts between dense and sparse modes, achieving state-of-the-art results on several benchmarks.", "summary_cn": "本文提出统一的层次掩码框架，展示图 Transformer 架构可以等价地视为注意力掩码的构造，并提出有效掩码应兼顾足够的感受野和标签一致性这一设计原则。在此基础上，作者构建 M3Dphormer——一种基于专家混合的图 Transformer，采用多层掩码和双模式注意力计算，根据局部掩码稀疏度在密集和稀疏模式间切换，在多个基准上实现了最新性能。", "keywords": "graph transformers, hierarchical masks, attention mask, mixture-of-experts, dual attention, receptive field, label consistency, graph representation learning", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yujie Xing", "Xiao Wang", "Bin Wu", "Hai Huang", "Chuan Shi"]}
]]></acme>

<pubDate>2025-10-21T17:22:32+00:00</pubDate>
</item>
<item>
<title>SAM 2++: Tracking Anything at Any Granularity</title>
<link>https://papers.cool/arxiv/2510.18822</link>
<guid>https://papers.cool/arxiv/2510.18822</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SAM 2++ proposes a unified video tracking architecture that can handle targets at any granularity—masks, boxes, or points—by using task-specific prompts and a shared decoder. It introduces a task-adaptive memory mechanism to maintain consistent tracking across granularities and provides a large, multi-granularity dataset (Tracking-Any-Granularity) for training and evaluation. Experiments show state-of-the-art performance on diverse benchmarks, demonstrating the model's versatility and robustness across tracking tasks.<br /><strong>Summary (CN):</strong> SAM 2++ 提出了一种统一的视频跟踪框架，能够通过任务特定的提示和通用解码器在掩码、边框或点等任意粒度上进行目标跟踪。该模型引入任务自适应记忆机制，以在不同粒度之间保持一致的匹配，并提供了大规模的多粒度数据集（Tracking-Any-Granularity）用于训练和评估。实验表明，其在多个基准上实现了最新的性能，展示了在多任务跟踪中的通用性和鲁棒性。<br /><strong>Keywords:</strong> video tracking, unified model, multi-granularity, mask tracking, box tracking, point tracking, task-adaptive memory, prompt embeddings, dataset, multi-task benchmarking<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiaming Zhang, Cheng Liang, Yichun Yang, Chenkai Zeng, Yutao Cui, Xinwen Zhang, Xin Zhou, Kai Ma, Gangshan Wu, Limin Wang</div>
Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.
<div><strong>Authors:</strong> Jiaming Zhang, Cheng Liang, Yichun Yang, Chenkai Zeng, Yutao Cui, Xinwen Zhang, Xin Zhou, Kai Ma, Gangshan Wu, Limin Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SAM 2++ proposes a unified video tracking architecture that can handle targets at any granularity—masks, boxes, or points—by using task-specific prompts and a shared decoder. It introduces a task-adaptive memory mechanism to maintain consistent tracking across granularities and provides a large, multi-granularity dataset (Tracking-Any-Granularity) for training and evaluation. Experiments show state-of-the-art performance on diverse benchmarks, demonstrating the model's versatility and robustness across tracking tasks.", "summary_cn": "SAM 2++ 提出了一种统一的视频跟踪框架，能够通过任务特定的提示和通用解码器在掩码、边框或点等任意粒度上进行目标跟踪。该模型引入任务自适应记忆机制，以在不同粒度之间保持一致的匹配，并提供了大规模的多粒度数据集（Tracking-Any-Granularity）用于训练和评估。实验表明，其在多个基准上实现了最新的性能，展示了在多任务跟踪中的通用性和鲁棒性。", "keywords": "video tracking, unified model, multi-granularity, mask tracking, box tracking, point tracking, task-adaptive memory, prompt embeddings, dataset, multi-task benchmarking", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiaming Zhang", "Cheng Liang", "Yichun Yang", "Chenkai Zeng", "Yutao Cui", "Xinwen Zhang", "Xin Zhou", "Kai Ma", "Gangshan Wu", "Limin Wang"]}
]]></acme>

<pubDate>2025-10-21T17:20:15+00:00</pubDate>
</item>
<item>
<title>An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection</title>
<link>https://papers.cool/arxiv/2510.18819</link>
<guid>https://papers.cool/arxiv/2510.18819</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a teacher–student hybrid AI framework that combines supervised and self-supervised heads to detect tuberculosis, COVID-19, and associated symptoms from chest X‑rays. It achieves 98.85% accuracy for disease classification and a macro‑F1 of 90.09% for multilabel symptom detection, and provides explainability assessments showing predictions are based on relevant anatomical features.<br /><strong>Summary (CN):</strong> 本文提出了一种教师‑学生混合 AI 框架，融合监督头和自监督头，对胸部 X 光图像进行结核病、COVID‑19 以及相关症状的检测。该模型在疾病分类上达到 98.85% 的准确率，在多标签症状检测上实现 90.09% 的宏观 F1 分数，并通过可解释性评估表明预测依据的是相关的解剖特征。<br /><strong>Keywords:</strong> tuberculosis detection, chest X-ray, teacher-student framework, hybrid AI, explainable AI, symptom detection, multi-label classification, self-supervised learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Neel Patel, Alexander Wong, Ashkan Ebadi</div>
Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.
<div><strong>Authors:</strong> Neel Patel, Alexander Wong, Ashkan Ebadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a teacher–student hybrid AI framework that combines supervised and self-supervised heads to detect tuberculosis, COVID-19, and associated symptoms from chest X‑rays. It achieves 98.85% accuracy for disease classification and a macro‑F1 of 90.09% for multilabel symptom detection, and provides explainability assessments showing predictions are based on relevant anatomical features.", "summary_cn": "本文提出了一种教师‑学生混合 AI 框架，融合监督头和自监督头，对胸部 X 光图像进行结核病、COVID‑19 以及相关症状的检测。该模型在疾病分类上达到 98.85% 的准确率，在多标签症状检测上实现 90.09% 的宏观 F1 分数，并通过可解释性评估表明预测依据的是相关的解剖特征。", "keywords": "tuberculosis detection, chest X-ray, teacher-student framework, hybrid AI, explainable AI, symptom detection, multi-label classification, self-supervised learning", "scoring": {"interpretability": 6, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Neel Patel", "Alexander Wong", "Ashkan Ebadi"]}
]]></acme>

<pubDate>2025-10-21T17:18:55+00:00</pubDate>
</item>
<item>
<title>A Geometric Approach to Steerable Convolutions</title>
<link>https://papers.cool/arxiv/2510.18813</link>
<guid>https://papers.cool/arxiv/2510.18813</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents an intuitive geometric derivation of steerable convolutional neural networks in any dimension, explaining the emergence of the Clebsch–Gordan decomposition and spherical harmonic bases from pattern‑matching principles. It proposes a novel construction of steerable layers using interpolation kernels, which improves implementation efficiency and provides greater robustness to noisy data.<br /><strong>Summary (CN):</strong> 本文提供了一种直观的几何推导方法，解释了在任意维度下可转向卷积神经网络中为何会出现 Clebsch–Gordan 分解和球面调和基函数（spherical harmonic），并基于模式匹配原理阐释其原理。作者进一步提出使用插值核（interpolation kernels）构建可转向卷积层的创新方案，提升了实现效率并增强了对噪声数据的鲁棒性。<br /><strong>Keywords:</strong> steerable convolutions, geometric derivation, Clebsch–Gordan decomposition, spherical harmonics, interpolation kernels, equivariant neural networks, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Soumyabrata Kundu, Risi Kondor</div>
In contrast to the somewhat abstract, group theoretical approach adopted by many papers, our work provides a new and more intuitive derivation of steerable convolutional neural networks in $d$ dimensions. This derivation is based on geometric arguments and fundamental principles of pattern matching. We offer an intuitive explanation for the appearance of the Clebsch--Gordan decomposition and spherical harmonic basis functions. Furthermore, we suggest a novel way to construct steerable convolution layers using interpolation kernels that improve upon existing implementation, and offer greater robustness to noisy data.
<div><strong>Authors:</strong> Soumyabrata Kundu, Risi Kondor</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents an intuitive geometric derivation of steerable convolutional neural networks in any dimension, explaining the emergence of the Clebsch–Gordan decomposition and spherical harmonic bases from pattern‑matching principles. It proposes a novel construction of steerable layers using interpolation kernels, which improves implementation efficiency and provides greater robustness to noisy data.", "summary_cn": "本文提供了一种直观的几何推导方法，解释了在任意维度下可转向卷积神经网络中为何会出现 Clebsch–Gordan 分解和球面调和基函数（spherical harmonic），并基于模式匹配原理阐释其原理。作者进一步提出使用插值核（interpolation kernels）构建可转向卷积层的创新方案，提升了实现效率并增强了对噪声数据的鲁棒性。", "keywords": "steerable convolutions, geometric derivation, Clebsch–Gordan decomposition, spherical harmonics, interpolation kernels, equivariant neural networks, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Soumyabrata Kundu", "Risi Kondor"]}
]]></acme>

<pubDate>2025-10-21T17:10:48+00:00</pubDate>
</item>
<item>
<title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
<link>https://papers.cool/arxiv/2510.18795</link>
<guid>https://papers.cool/arxiv/2510.18795</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ProCLIP introduces a curriculum learning framework that progressively aligns CLIP's image encoder with a large language model (LLM) based text embedder, addressing CLIP's limitations on token length and multilingual support. The method first distills knowledge from CLIP's original text encoder into the LLM embedder, then fine‑tunes the image encoder using contrastive learning with self‑distillation regularization and additional alignment losses. Experiments demonstrate that the progressive alignment preserves CLIP's pretrained vision-language knowledge while enabling long‑text and multilingual capabilities.<br /><strong>Summary (CN):</strong> ProCLIP 提出一种基于 curriculum learning 的渐进式对齐框架，将 CLIP 的图像编码器与大型语言模型（LLM）文本嵌入器对齐，以克服 CLIP 在 77 token 限制和多语言支持方面的不足。该方法首先将 CLIP 原始文本编码器的知识蒸馏到 LLM 嵌入器中建立初始对齐，然后通过图像‑文本对比学习、使用自蒸馏正则化以及实例语义对齐损失和嵌入结构对齐损失，实现对图像编码器的进一步对齐。实验表明该渐进式对齐能够保留 CLIP 预训练的视觉‑语言知识，同时提升对长文本和多语言的处理能力。<br /><strong>Keywords:</strong> vision-language, CLIP, LLM embedder, curriculum learning, contrastive learning, knowledge distillation, multilingual, fine-grained semantics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</div>
The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP
<div><strong>Authors:</strong> Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ProCLIP introduces a curriculum learning framework that progressively aligns CLIP's image encoder with a large language model (LLM) based text embedder, addressing CLIP's limitations on token length and multilingual support. The method first distills knowledge from CLIP's original text encoder into the LLM embedder, then fine‑tunes the image encoder using contrastive learning with self‑distillation regularization and additional alignment losses. Experiments demonstrate that the progressive alignment preserves CLIP's pretrained vision-language knowledge while enabling long‑text and multilingual capabilities.", "summary_cn": "ProCLIP 提出一种基于 curriculum learning 的渐进式对齐框架，将 CLIP 的图像编码器与大型语言模型（LLM）文本嵌入器对齐，以克服 CLIP 在 77 token 限制和多语言支持方面的不足。该方法首先将 CLIP 原始文本编码器的知识蒸馏到 LLM 嵌入器中建立初始对齐，然后通过图像‑文本对比学习、使用自蒸馏正则化以及实例语义对齐损失和嵌入结构对齐损失，实现对图像编码器的进一步对齐。实验表明该渐进式对齐能够保留 CLIP 预训练的视觉‑语言知识，同时提升对长文本和多语言的处理能力。", "keywords": "vision-language, CLIP, LLM embedder, curriculum learning, contrastive learning, knowledge distillation, multilingual, fine-grained semantics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaoxing Hu", "Kaicheng Yang", "Ziyong Feng", "Qi Ming", "Zonghao Guo", "Xiang An", "Ziyong Feng", "Junchi Yan", "Xue Yang"]}
]]></acme>

<pubDate>2025-10-21T16:48:49+00:00</pubDate>
</item>
<item>
<title>Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection</title>
<link>https://papers.cool/arxiv/2510.18781</link>
<guid>https://papers.cool/arxiv/2510.18781</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a "Rebellious Student" framework that jointly enhances spectral and spatial background features for hyperspectral anomaly detection. The spatial branch is deliberately trained to diverge from a spectral teacher using decorrelation losses, enabling complementary feature learning without per-scene retraining. Experiments on the HAD100 benchmark demonstrate significant performance gains over existing methods with minimal computational overhead.<br /><strong>Summary (CN):</strong> 本文提出“Rebellious Student”（叛逆学生）框架，通过有意让空间分支与光谱教师分支背离并使用去相关损失，学习互补的空间特征，从而在高光谱异常检测中增强光谱和空间背景特征。该方法无需针对每个场景重新训练或调参，可与常规检测器结合实现参数自由的异常检测。实验在 HAD100 基准上显示出相较于多个基线的显著提升且计算开销极低。<br /><strong>Keywords:</strong> hyperspectral anomaly detection, spectral-spatial feature enhancement, rebellious student, teacher-student paradigm, reverse distillation, decorrelation loss, background modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Wenping Jin, Yuyang Tang, Li Zhu, Fei Guo</div>
A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel "Rebellious Student" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at https://github.com/xjpp2016/FERS.
<div><strong>Authors:</strong> Wenping Jin, Yuyang Tang, Li Zhu, Fei Guo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a \"Rebellious Student\" framework that jointly enhances spectral and spatial background features for hyperspectral anomaly detection. The spatial branch is deliberately trained to diverge from a spectral teacher using decorrelation losses, enabling complementary feature learning without per-scene retraining. Experiments on the HAD100 benchmark demonstrate significant performance gains over existing methods with minimal computational overhead.", "summary_cn": "本文提出“Rebellious Student”（叛逆学生）框架，通过有意让空间分支与光谱教师分支背离并使用去相关损失，学习互补的空间特征，从而在高光谱异常检测中增强光谱和空间背景特征。该方法无需针对每个场景重新训练或调参，可与常规检测器结合实现参数自由的异常检测。实验在 HAD100 基准上显示出相较于多个基线的显著提升且计算开销极低。", "keywords": "hyperspectral anomaly detection, spectral-spatial feature enhancement, rebellious student, teacher-student paradigm, reverse distillation, decorrelation loss, background modeling", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Wenping Jin", "Yuyang Tang", "Li Zhu", "Fei Guo"]}
]]></acme>

<pubDate>2025-10-21T16:31:56+00:00</pubDate>
</item>
<item>
<title>UltraGen: High-Resolution Video Generation with Hierarchical Attention</title>
<link>https://papers.cool/arxiv/2510.18775</link>
<guid>https://papers.cool/arxiv/2510.18775</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> UltraGen introduces a hierarchical dual‑branch attention architecture that decomposes full attention into a local branch for detailed regional content and a global branch for semantic consistency, enabling native high‑resolution (1080p‑4K) video generation with diffusion transformers. The paper presents a spatially compressed global modeling strategy and a hierarchical cross‑window local attention mechanism, allowing efficient scaling of pre‑trained low‑resolution models to high resolutions while outperforming existing methods and two‑stage super‑resolution pipelines. Extensive experiments demonstrate superior qualitative and quantitative results across resolutions.<br /><strong>Summary (CN):</strong> UltraGen 提出了一种层级双分支注意力架构，将完整注意力分解为局部分支（负责高保真区域内容）和全局分支（确保语义一致性），从而实现原生高分辨率（1080p‑4K）视频生成（使用 diffusion transformer）。论文又引入空间压缩全局部建模策略和层级交叉窗口局部注意力机制，使得预训练的低分辨率模型能够高效扩展至高分辨率，并在质量上超越现有方法及两阶段超分辨率流水线。大量实验显示在定性和定量评估上均取得显著提升。<br /><strong>Keywords:</strong> high-resolution video generation, diffusion models, hierarchical attention, global-local attention, spatial compression, cross-window attention, video synthesis, Super-resolution<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi</div>
Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.
<div><strong>Authors:</strong> Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "UltraGen introduces a hierarchical dual‑branch attention architecture that decomposes full attention into a local branch for detailed regional content and a global branch for semantic consistency, enabling native high‑resolution (1080p‑4K) video generation with diffusion transformers. The paper presents a spatially compressed global modeling strategy and a hierarchical cross‑window local attention mechanism, allowing efficient scaling of pre‑trained low‑resolution models to high resolutions while outperforming existing methods and two‑stage super‑resolution pipelines. Extensive experiments demonstrate superior qualitative and quantitative results across resolutions.", "summary_cn": "UltraGen 提出了一种层级双分支注意力架构，将完整注意力分解为局部分支（负责高保真区域内容）和全局分支（确保语义一致性），从而实现原生高分辨率（1080p‑4K）视频生成（使用 diffusion transformer）。论文又引入空间压缩全局部建模策略和层级交叉窗口局部注意力机制，使得预训练的低分辨率模型能够高效扩展至高分辨率，并在质量上超越现有方法及两阶段超分辨率流水线。大量实验显示在定性和定量评估上均取得显著提升。", "keywords": "high-resolution video generation,diffusion models,hierarchical attention,global-local attention,spatial compression,cross-window attention,video synthesis,Super-resolution", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Teng Hu", "Jiangning Zhang", "Zihan Su", "Ran Yi"]}
]]></acme>

<pubDate>2025-10-21T16:23:21+00:00</pubDate>
</item>
<item>
<title>Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction</title>
<link>https://papers.cool/arxiv/2510.18773</link>
<guid>https://papers.cool/arxiv/2510.18773</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a fine‑tuned geospatial foundation model to predict and simulate urban heat island (UHI) patterns, using limited ground‑truth temperature data to benchmark performance. It demonstrates the model's ability to forecast land‑surface temperatures under future climate scenarios and visualizes mitigation impacts via an inpainting simulation. The results suggest that foundation models can aid climate‑resilient planning in data‑scarce urban areas.<br /><strong>Summary (CN):</strong> 本文利用微调后的地理空间基础模型对城市热岛（UHI）进行检测与模拟，采用有限的地面温度真实数据进行基准评估。研究展示了模型在未来气候情景下预测地表温度的能力，并通过修补（inpainting）模拟展示了缓解措施的效果。结果表明，基础模型可在数据稀缺的城市地区支持气候韧性规划。<br /><strong>Keywords:</strong> urban heat island, geospatial foundation model, microclimate prediction, land surface temperature, climate mitigation, fine-tuning, inpainting simulation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Jannis Fleckenstein, David Kreismann, Tamara Rosemary Govindasamy, Thomas Brunschwiler, Etienne Vos, Mattia Rigotti</div>
As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.
<div><strong>Authors:</strong> Jannis Fleckenstein, David Kreismann, Tamara Rosemary Govindasamy, Thomas Brunschwiler, Etienne Vos, Mattia Rigotti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a fine‑tuned geospatial foundation model to predict and simulate urban heat island (UHI) patterns, using limited ground‑truth temperature data to benchmark performance. It demonstrates the model's ability to forecast land‑surface temperatures under future climate scenarios and visualizes mitigation impacts via an inpainting simulation. The results suggest that foundation models can aid climate‑resilient planning in data‑scarce urban areas.", "summary_cn": "本文利用微调后的地理空间基础模型对城市热岛（UHI）进行检测与模拟，采用有限的地面温度真实数据进行基准评估。研究展示了模型在未来气候情景下预测地表温度的能力，并通过修补（inpainting）模拟展示了缓解措施的效果。结果表明，基础模型可在数据稀缺的城市地区支持气候韧性规划。", "keywords": "urban heat island, geospatial foundation model, microclimate prediction, land surface temperature, climate mitigation, fine-tuning, inpainting simulation", "scoring": {"interpretability": 2, "understanding": 3, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jannis Fleckenstein", "David Kreismann", "Tamara Rosemary Govindasamy", "Thomas Brunschwiler", "Etienne Vos", "Mattia Rigotti"]}
]]></acme>

<pubDate>2025-10-21T16:21:15+00:00</pubDate>
</item>
<item>
<title>SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery</title>
<link>https://papers.cool/arxiv/2510.18740</link>
<guid>https://papers.cool/arxiv/2510.18740</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SEAL, a Semantic-Aware Hierarchical Learning framework for Generalized Category Discovery, which leverages naturally occurring hierarchical structures to improve classification of both known and unknown classes. It proposes a hierarchical semantic-guided soft contrastive learning loss that generates soft negatives based on hierarchical similarity, and a Cross-Granularity Consistency module to align predictions across different granularity levels, achieving state-of-the-art results on several fine-grained and coarse-grained benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 SEAL（语义感知层次学习）框架用于广义类别发现（Generalized Category Discovery），利用自然存在的层次结构提升对已知和未知类别的分类效果。框架中设计了层次语义引导的软对比学习损失（Hierarchical Semantic-Guided Soft Contrastive Learning），通过层次相似性生成软负例，并引入跨粒度一致性（Cross-Granularity Consistency）模块对不同粒度的预测进行对齐，在多个细粒度和粗粒度基准上实现了最先进的性能。<br /><strong>Keywords:</strong> Generalized Category Discovery, hierarchical learning, semantic-aware, contrastive learning, soft negatives, cross-granularity consistency, computer vision<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenqi He, Yuanpei Liu, Kai Han</div>
This paper investigates the problem of Generalized Category Discovery (GCD). Given a partially labelled dataset, GCD aims to categorize all unlabelled images, regardless of whether they belong to known or unknown classes. Existing approaches typically depend on either single-level semantics or manually designed abstract hierarchies, which limit their generalizability and scalability. To address these limitations, we introduce a SEmantic-aware hierArchical Learning framework (SEAL), guided by naturally occurring and easily accessible hierarchical structures. Within SEAL, we propose a Hierarchical Semantic-Guided Soft Contrastive Learning approach that exploits hierarchical similarity to generate informative soft negatives, addressing the limitations of conventional contrastive losses that treat all negatives equally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed to align the predictions from different levels of granularity. SEAL consistently achieves state-of-the-art performance on fine-grained benchmarks, including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and further demonstrates generalization on coarse-grained datasets. Project page: https://visual-ai.github.io/seal/
<div><strong>Authors:</strong> Zhenqi He, Yuanpei Liu, Kai Han</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SEAL, a Semantic-Aware Hierarchical Learning framework for Generalized Category Discovery, which leverages naturally occurring hierarchical structures to improve classification of both known and unknown classes. It proposes a hierarchical semantic-guided soft contrastive learning loss that generates soft negatives based on hierarchical similarity, and a Cross-Granularity Consistency module to align predictions across different granularity levels, achieving state-of-the-art results on several fine-grained and coarse-grained benchmarks.", "summary_cn": "本文提出 SEAL（语义感知层次学习）框架用于广义类别发现（Generalized Category Discovery），利用自然存在的层次结构提升对已知和未知类别的分类效果。框架中设计了层次语义引导的软对比学习损失（Hierarchical Semantic-Guided Soft Contrastive Learning），通过层次相似性生成软负例，并引入跨粒度一致性（Cross-Granularity Consistency）模块对不同粒度的预测进行对齐，在多个细粒度和粗粒度基准上实现了最先进的性能。", "keywords": "Generalized Category Discovery, hierarchical learning, semantic-aware, contrastive learning, soft negatives, cross-granularity consistency, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenqi He", "Yuanpei Liu", "Kai Han"]}
]]></acme>

<pubDate>2025-10-21T15:44:47+00:00</pubDate>
</item>
<item>
<title>Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting</title>
<link>https://papers.cool/arxiv/2510.18739</link>
<guid>https://papers.cool/arxiv/2510.18739</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ColIAGS, an enhanced 3D Gaussian Splatting framework for colonoscopy that explicitly models illumination attenuation using improved appearance and geometry modeling techniques, enabling more accurate novel view synthesis and geometric reconstruction despite dynamic lighting conditions.<br /><strong>Summary (CN):</strong> 本文提出了 ColIAGS，一种针对结肠镜检查的改进版 3D 高斯点绘框架，通过改进的外观建模和几何建模显式考虑光照衰减，使得在光源和摄像头动态变化的情况下仍能实现高质量的新视角合成和几何重建。<br /><strong>Keywords:</strong> 3D Gaussian Splatting, colonoscopy, illumination attenuation, view synthesis, geometry modeling, medical imaging<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hao Wang, Ying Zhou, Haoyu Zhao, Rui Wang, Qiang Hu, Xing Zhang, Qiang Li, Zhiwei Wang</div>
3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.
<div><strong>Authors:</strong> Hao Wang, Ying Zhou, Haoyu Zhao, Rui Wang, Qiang Hu, Xing Zhang, Qiang Li, Zhiwei Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ColIAGS, an enhanced 3D Gaussian Splatting framework for colonoscopy that explicitly models illumination attenuation using improved appearance and geometry modeling techniques, enabling more accurate novel view synthesis and geometric reconstruction despite dynamic lighting conditions.", "summary_cn": "本文提出了 ColIAGS，一种针对结肠镜检查的改进版 3D 高斯点绘框架，通过改进的外观建模和几何建模显式考虑光照衰减，使得在光源和摄像头动态变化的情况下仍能实现高质量的新视角合成和几何重建。", "keywords": "3D Gaussian Splatting, colonoscopy, illumination attenuation, view synthesis, geometry modeling, medical imaging", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Wang", "Ying Zhou", "Haoyu Zhao", "Rui Wang", "Qiang Hu", "Xing Zhang", "Qiang Li", "Zhiwei Wang"]}
]]></acme>

<pubDate>2025-10-21T15:44:23+00:00</pubDate>
</item>
<item>
<title>IF-VidCap: Can Video Caption Models Follow Instructions?</title>
<link>https://papers.cool/arxiv/2510.18726</link>
<guid>https://papers.cool/arxiv/2510.18726</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces IF‑VidCap, a new benchmark designed to evaluate how well video captioning models can follow user instructions, assessing both format and content correctness across 1,400 curated samples. Extensive experiments on over 20 state‑of‑the‑art models show that while proprietary systems still lead, open‑source models are rapidly closing the gap, and specialized dense‑captioning models lag behind general‑purpose MLLMs on complex instructions.<br /><strong>Summary (CN):</strong> 本文提出 IF‑VidCap 基准，用于评估视频字幕模型的指令遵循能力，包含 1,400 条高质量样本，并从格式正确性和内容正确性两维度进行评价。对 20 多种主流模型的全面实验表明，尽管专有模型仍占优势，但开源模型正在快速逼近；而针对密集字幕的专门模型在复杂指令下表现不如通用多模态大语言模型（MLLM）。<br /><strong>Keywords:</strong> controllable video captioning, instruction-following, benchmark, multimodal large language models, video caption evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, Jiaheng Liu</div>
Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.
<div><strong>Authors:</strong> Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, Jiaheng Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces IF‑VidCap, a new benchmark designed to evaluate how well video captioning models can follow user instructions, assessing both format and content correctness across 1,400 curated samples. Extensive experiments on over 20 state‑of‑the‑art models show that while proprietary systems still lead, open‑source models are rapidly closing the gap, and specialized dense‑captioning models lag behind general‑purpose MLLMs on complex instructions.", "summary_cn": "本文提出 IF‑VidCap 基准，用于评估视频字幕模型的指令遵循能力，包含 1,400 条高质量样本，并从格式正确性和内容正确性两维度进行评价。对 20 多种主流模型的全面实验表明，尽管专有模型仍占优势，但开源模型正在快速逼近；而针对密集字幕的专门模型在复杂指令下表现不如通用多模态大语言模型（MLLM）。", "keywords": "controllable video captioning, instruction-following, benchmark, multimodal large language models, video caption evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shihao Li", "Yuanxing Zhang", "Jiangtao Wu", "Zhide Lei", "Yiwen He", "Runzhe Wen", "Chenxi Liao", "Chengkang Jiang", "An Ping", "Shuo Gao", "Suhan Wang", "Zhaozhou Bian", "Zijun Zhou", "Jingyi Xie", "Jiayi Zhou", "Jing Wang", "Yifan Yao", "Weihao Xie", "Yingshui Tan", "Yanghai Wang", "Qianqian Xie", "Zhaoxiang Zhang", "Jiaheng Liu"]}
]]></acme>

<pubDate>2025-10-21T15:25:08+00:00</pubDate>
</item>
<item>
<title>SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation</title>
<link>https://papers.cool/arxiv/2510.18716</link>
<guid>https://papers.cool/arxiv/2510.18716</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a KV cache compression framework for autoregressive image generation that decouples attention heads into spatial-locality and semantic-sink types, maintaining a short recent token window for the former and a compact set of highly‑attended tokens for the latter. This approach yields a 5× reduction in memory usage and a 6.6× speedup in throughput with minimal loss of visual quality, enabling efficient generation on limited hardware.<br /><strong>Summary (CN):</strong> 本文提出了一种针对自回归图像生成的 KV 缓存压缩框架，将注意力头解耦为空间局部头和语义汇聚头。对空间局部头保留最近的短窗口 token，对语义汇聚头则保留少量高度被关注的 token。实验表明，该方法在几乎不影响图像质量的情况下实现了 5 倍内存缩减和 6.6 倍吞吐提升，使资源受限设备上进行高效原生自回归图像生成成为可能。<br /><strong>Keywords:</strong> autoregressive image generation, KV cache compression, spatial locality, semantic sink, token pruning, memory reduction, speedup, efficient inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Siyong Jian, Huan Wang</div>
Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens. While KV cache compression has been extensively studied in language modeling, it still remains largely unexplored for the image generation domain. In this work, we begin by identifying a distinct and prominent attention phenomenon, which we term spatial locality and emergent semantic sink. To leverage this key insight, we introduce a novel KV cache compression framework. Specifically, we compress the KV cache for all visual tokens by adaptively decoupling attention heads into two separate types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it strategically preserves a compact set of highly-attended tokens. Our extensive experiments demonstrate that the proposed method achieves a 5$\times$ reduction in memory usage and a notable 6.6$\times$ speedup in overall throughput with only minimal visual quality loss, thereby enabling highly efficient native autoregressive image generation on resource-constrained hardware.
<div><strong>Authors:</strong> Siyong Jian, Huan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a KV cache compression framework for autoregressive image generation that decouples attention heads into spatial-locality and semantic-sink types, maintaining a short recent token window for the former and a compact set of highly‑attended tokens for the latter. This approach yields a 5× reduction in memory usage and a 6.6× speedup in throughput with minimal loss of visual quality, enabling efficient generation on limited hardware.", "summary_cn": "本文提出了一种针对自回归图像生成的 KV 缓存压缩框架，将注意力头解耦为空间局部头和语义汇聚头。对空间局部头保留最近的短窗口 token，对语义汇聚头则保留少量高度被关注的 token。实验表明，该方法在几乎不影响图像质量的情况下实现了 5 倍内存缩减和 6.6 倍吞吐提升，使资源受限设备上进行高效原生自回归图像生成成为可能。", "keywords": "autoregressive image generation, KV cache compression, spatial locality, semantic sink, token pruning, memory reduction, speedup, efficient inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Siyong Jian", "Huan Wang"]}
]]></acme>

<pubDate>2025-10-21T15:17:37+00:00</pubDate>
</item>
<item>
<title>PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting</title>
<link>https://papers.cool/arxiv/2510.18714</link>
<guid>https://papers.cool/arxiv/2510.18714</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> PLANA3R proposes a pose‑free, feed‑forward framework for metric planar 3D reconstruction of indoor scenes using only unposed two‑view images. It leverages Vision Transformers to predict sparse planar primitives, estimate relative camera poses, and supervise geometry through planar splatting with high‑resolution depth and normal maps, training without explicit plane annotations. Experiments show strong metric accuracy, generalization to out‑of‑domain environments, and accurate plane segmentation.<br /><strong>Summary (CN):</strong> PLANA3R 提出一种无需相机姿态、端到端的度量平面 3D 重建框架，仅使用未配准的两幅图像。该方法利用 Vision Transformer 预测稀疏平面原语、估计相对相机位姿，并通过平面投影将高分辨率深度和法线图的梯度传播到几何学习，实现无需显式平面标注的训练。实验表明该模型在度量精度、跨域泛化以及平面分割方面表现出色。<br /><strong>Keywords:</strong> planar 3D reconstruction, metric reconstruction, vision transformers, planar splatting, zero-shot, indoor scenes, depth estimation, pose estimation, plane segmentation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Changkun Liu, Bin Tan, Zeran Ke, Shangzhan Zhang, Jiachen Liu, Ming Qian, Nan Xue, Yujun Shen, Tristan Braud</div>
This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at https://lck666666.github.io/plana3r
<div><strong>Authors:</strong> Changkun Liu, Bin Tan, Zeran Ke, Shangzhan Zhang, Jiachen Liu, Ming Qian, Nan Xue, Yujun Shen, Tristan Braud</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "PLANA3R proposes a pose‑free, feed‑forward framework for metric planar 3D reconstruction of indoor scenes using only unposed two‑view images. It leverages Vision Transformers to predict sparse planar primitives, estimate relative camera poses, and supervise geometry through planar splatting with high‑resolution depth and normal maps, training without explicit plane annotations. Experiments show strong metric accuracy, generalization to out‑of‑domain environments, and accurate plane segmentation.", "summary_cn": "PLANA3R 提出一种无需相机姿态、端到端的度量平面 3D 重建框架，仅使用未配准的两幅图像。该方法利用 Vision Transformer 预测稀疏平面原语、估计相对相机位姿，并通过平面投影将高分辨率深度和法线图的梯度传播到几何学习，实现无需显式平面标注的训练。实验表明该模型在度量精度、跨域泛化以及平面分割方面表现出色。", "keywords": "planar 3D reconstruction, metric reconstruction, vision transformers, planar splatting, zero-shot, indoor scenes, depth estimation, pose estimation, plane segmentation", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Changkun Liu", "Bin Tan", "Zeran Ke", "Shangzhan Zhang", "Jiachen Liu", "Ming Qian", "Nan Xue", "Yujun Shen", "Tristan Braud"]}
]]></acme>

<pubDate>2025-10-21T15:15:33+00:00</pubDate>
</item>
<item>
<title>A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition</title>
<link>https://papers.cool/arxiv/2510.18705</link>
<guid>https://papers.cool/arxiv/2510.18705</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Explicit Motion Information Mining (EMIM) module that integrates cost‑volume‑style affinity matrices into video transformers to better capture motion. EMIM samples key tokens from neighboring regions in the next frame, using the resulting matrix for both appearance aggregation and motion feature extraction, leading to state‑of‑the‑art results on motion‑sensitive benchmarks such as Something‑Something V1/V2.<br /><strong>Summary (CN):</strong> 本文提出显式运动信息挖掘 (EMIM) 模块，将传统动作识别中的代价体（cost volume）式亲和矩阵融合进视频 Transformer，以更好地捕获运动信息。EMIM 在下一帧的相邻区域中采样关键 token，构建亲和矩阵用于外观特征聚合并转化为运动特征，在 Something‑Something V1/V2 等运动敏感数据集上实现了最先进的性能。<br /><strong>Keywords:</strong> Explicit Motion Information Mining, transformer, cost volume, action recognition, motion modeling, self-attention, affinity matrix, video understanding, Something-Something<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang</div>
Recently, action recognition has been dominated by transformer-based methods, thanks to their spatiotemporal contextual aggregation capacities. However, despite the significant progress achieved on scene-related datasets, they do not perform well on motion-sensitive datasets due to the lack of elaborate motion modeling designs. Meanwhile, we observe that the widely-used cost volume in traditional action recognition is highly similar to the affinity matrix defined in self-attention, but equipped with powerful motion modeling capacities. In light of this, we propose to integrate those effective motion modeling properties into the existing transformer in a unified and neat way, with the proposal of the Explicit Motion Information Mining module (EMIM). In EMIM, we propose to construct the desirable affinity matrix in a cost volume style, where the set of key candidate tokens is sampled from the query-based neighboring area in the next frame in a sliding-window manner. Then, the constructed affinity matrix is used to aggregate contextual information for appearance modeling and is converted into motion features for motion modeling as well. We validate the motion modeling capacities of our method on four widely-used datasets, and our method performs better than existing state-of-the-art approaches, especially on motion-sensitive datasets, i.e., Something-Something V1 & V2.
<div><strong>Authors:</strong> Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Explicit Motion Information Mining (EMIM) module that integrates cost‑volume‑style affinity matrices into video transformers to better capture motion. EMIM samples key tokens from neighboring regions in the next frame, using the resulting matrix for both appearance aggregation and motion feature extraction, leading to state‑of‑the‑art results on motion‑sensitive benchmarks such as Something‑Something V1/V2.", "summary_cn": "本文提出显式运动信息挖掘 (EMIM) 模块，将传统动作识别中的代价体（cost volume）式亲和矩阵融合进视频 Transformer，以更好地捕获运动信息。EMIM 在下一帧的相邻区域中采样关键 token，构建亲和矩阵用于外观特征聚合并转化为运动特征，在 Something‑Something V1/V2 等运动敏感数据集上实现了最先进的性能。", "keywords": "Explicit Motion Information Mining, transformer, cost volume, action recognition, motion modeling, self-attention, affinity matrix, video understanding, Something-Something", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Peiqin Zhuang", "Lei Bai", "Yichao Wu", "Ding Liang", "Luping Zhou", "Yali Wang", "Wanli Ouyang"]}
]]></acme>

<pubDate>2025-10-21T15:01:48+00:00</pubDate>
</item>
<item>
<title>Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</title>
<link>https://papers.cool/arxiv/2510.18703</link>
<guid>https://papers.cool/arxiv/2510.18703</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision-Centric Contrastive Learning (VC2L), a framework that treats text, images, and their combinations as rendered images processed by a single vision transformer, eliminating OCR and tokenization. By using snippet-level contrastive objectives on consecutive multimodal segments, VC2L captures cross‑modal relationships in web documents and achieves competitive performance on new retrieval benchmarks and existing datasets. The work demonstrates that unified vision‑centric approaches can effectively learn representations from complex web‑based multimodal data.<br /><strong>Summary (CN):</strong> 本文提出了 Vision‑Centric Contrastive Learning（VC2L）框架，将文本、图像及其组合渲染为图像，由单一视觉 Transformer 处理，从而无需 OCR、分词或模态融合。通过对连续多模态片段的片段级对比学习，VC2L 捕获网页文档中的跨模态关系，并在新建检索基准及现有数据集上取得竞争或更佳的表现。该工作展示了统一的视觉中心方法在复杂网页多模态数据上学习表示的潜力。<br /><strong>Keywords:</strong> vision transformer, contrastive learning, multimodal web documents, pixel-space encoding, cross-modal retrieval, snippet-level contrastive, VC2L<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</div>
Contrastive vision-language models such as CLIP have demonstrated strong performance across a wide range of multimodal tasks by learning from aligned image-text pairs. However, their ability to handle complex, real-world web documents remains limited, particularly in scenarios where text and images are interleaved, loosely aligned, or embedded in visual form. To address these challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified framework that models text, images, and their combinations using a single vision transformer. VC2L operates entirely in pixel space by rendering all inputs, whether textual, visual, or combined, as images, thus eliminating the need for OCR, text tokenization, or modality fusion strategy. To capture complex cross-modal relationships in multimodal web documents, VC2L employs a snippet-level contrastive learning objective that aligns consecutive multimodal segments, leveraging the inherent coherence of documents without requiring explicitly paired image-text data. To assess the effectiveness of this approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR, designed to evaluate cross-modal retrieval, fine-grained sequential understanding, and generalization to unseen data, respectively. Empirical results show that VC2L achieves competitive or superior performance compared to CLIP-style models on both the proposed benchmarks and established datasets such as M-BEIR and MTEB. These findings underscore the potential of multimodal web data as a valuable training resource for contrastive learning and illustrate the scalability of a unified, vision-centric approach for multimodal representation learning. Code and models are available at: https://github.com/showlab/VC2L.
<div><strong>Authors:</strong> Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision-Centric Contrastive Learning (VC2L), a framework that treats text, images, and their combinations as rendered images processed by a single vision transformer, eliminating OCR and tokenization. By using snippet-level contrastive objectives on consecutive multimodal segments, VC2L captures cross‑modal relationships in web documents and achieves competitive performance on new retrieval benchmarks and existing datasets. The work demonstrates that unified vision‑centric approaches can effectively learn representations from complex web‑based multimodal data.", "summary_cn": "本文提出了 Vision‑Centric Contrastive Learning（VC2L）框架，将文本、图像及其组合渲染为图像，由单一视觉 Transformer 处理，从而无需 OCR、分词或模态融合。通过对连续多模态片段的片段级对比学习，VC2L 捕获网页文档中的跨模态关系，并在新建检索基准及现有数据集上取得竞争或更佳的表现。该工作展示了统一的视觉中心方法在复杂网页多模态数据上学习表示的潜力。", "keywords": "vision transformer, contrastive learning, multimodal web documents, pixel-space encoding, cross-modal retrieval, snippet-level contrastive, VC2L", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yiqi Lin", "Alex Jinpeng Wang", "Linjie Li", "Zhengyuan Yang", "Mike Zheng Shou"]}
]]></acme>

<pubDate>2025-10-21T14:59:29+00:00</pubDate>
</item>
<item>
<title>UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</title>
<link>https://papers.cool/arxiv/2510.18701</link>
<guid>https://papers.cool/arxiv/2510.18701</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces UniGenBench++, a comprehensive benchmark for assessing the semantic consistency of text-to-image generation models across diverse real-world scenarios and multiple languages. It offers 600 hierarchical prompts in English and Chinese, evaluates models on 10 primary and 27 sub‑dimensions, and provides a trained offline evaluation model for efficient assessment. Benchmark results reveal strengths and weaknesses of various open‑ and closed‑source T2I systems.<br /><strong>Summary (CN):</strong> 本文提出 UniGenBench++，一个用于评估文本到图像生成模型语义一致性的综合基准，覆盖多种真实场景并提供中英双语提示。基准包含 600 条层级化提示，涵盖 10 个主要及 27 个子评估维度，并配备离线评估模型，以便高效衡量模型表现。实验展示了不同开源和闭源 T2I 模型的优势与不足。<br /><strong>Keywords:</strong> text-to-image generation, semantic evaluation, multilingual benchmark, prompt diversity, robustness, evaluation metrics, UniGenBench++<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</div>
Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.
<div><strong>Authors:</strong> Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces UniGenBench++, a comprehensive benchmark for assessing the semantic consistency of text-to-image generation models across diverse real-world scenarios and multiple languages. It offers 600 hierarchical prompts in English and Chinese, evaluates models on 10 primary and 27 sub‑dimensions, and provides a trained offline evaluation model for efficient assessment. Benchmark results reveal strengths and weaknesses of various open‑ and closed‑source T2I systems.", "summary_cn": "本文提出 UniGenBench++，一个用于评估文本到图像生成模型语义一致性的综合基准，覆盖多种真实场景并提供中英双语提示。基准包含 600 条层级化提示，涵盖 10 个主要及 27 个子评估维度，并配备离线评估模型，以便高效衡量模型表现。实验展示了不同开源和闭源 T2I 模型的优势与不足。", "keywords": "text-to-image generation, semantic evaluation, multilingual benchmark, prompt diversity, robustness, evaluation metrics, UniGenBench++", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Jiazi Bu", "Yujie Zhou", "Yi Xin", "Junjun He", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"]}
]]></acme>

<pubDate>2025-10-21T14:56:46+00:00</pubDate>
</item>
<item>
<title>MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation</title>
<link>https://papers.cool/arxiv/2510.18692</link>
<guid>https://papers.cool/arxiv/2510.18692</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Mixture-of-Groups Attention (MoGA), a learnable token routing mechanism that provides precise sparse attention without blockwise estimation, enabling efficient long-range interactions in Diffusion Transformers for video generation. Using MoGA, the authors build a model that can generate minute‑level, multi‑shot videos at 480p/24 fps with a context length of about 580k frames, demonstrating strong performance on multiple video generation benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 Mixture-of-Groups Attention（MoGA），一种可学习的 token 路由器，实现了无需块级估计的精确稀疏注意力，从而在 Diffusion Transformer 中支持高效的长程交互。基于 MoGA，作者构建了能够端到端生成分钟级、多镜头、480p/24 fps 视频（上下文长度约 58 万帧）的模型，并在多项视频生成任务上验证了其有效性。<br /><strong>Keywords:</strong> Mixture-of-Groups Attention, sparse attention, diffusion transformer, long video generation, token routing, FlashAttention, sequence parallelism, efficient attention, video diffusion models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</div>
Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.
<div><strong>Authors:</strong> Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Mixture-of-Groups Attention (MoGA), a learnable token routing mechanism that provides precise sparse attention without blockwise estimation, enabling efficient long-range interactions in Diffusion Transformers for video generation. Using MoGA, the authors build a model that can generate minute‑level, multi‑shot videos at 480p/24 fps with a context length of about 580k frames, demonstrating strong performance on multiple video generation benchmarks.", "summary_cn": "本文提出 Mixture-of-Groups Attention（MoGA），一种可学习的 token 路由器，实现了无需块级估计的精确稀疏注意力，从而在 Diffusion Transformer 中支持高效的长程交互。基于 MoGA，作者构建了能够端到端生成分钟级、多镜头、480p/24 fps 视频（上下文长度约 58 万帧）的模型，并在多项视频生成任务上验证了其有效性。", "keywords": "Mixture-of-Groups Attention, sparse attention, diffusion transformer, long video generation, token routing, FlashAttention, sequence parallelism, efficient attention, video diffusion models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Weinan Jia", "Yuning Lu", "Mengqi Huang", "Hualiang Wang", "Binyuan Huang", "Nan Chen", "Mu Liu", "Jidong Jiang", "Zhendong Mao"]}
]]></acme>

<pubDate>2025-10-21T14:50:42+00:00</pubDate>
</item>
<item>
<title>Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning for Historical Writer Identification</title>
<link>https://papers.cool/arxiv/2510.18671</link>
<guid>https://papers.cool/arxiv/2510.18671</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates how different pre‑processing methods, backbone architectures, and post‑processing strategies affect end‑to‑end deep learning models for historical writer identification, highlighting challenges such as weak low‑level feature capture and sensitivity to content noise. Experiments reveal that most configurations perform poorly in realistic, document‑level and zero‑shot settings, though one simpler setup reaches performance comparable to state‑of‑the‑art systems.<br /><strong>Summary (CN):</strong> 本文评估了不同的预处理方法、主干网络和后处理策略对历史手写作者识别端到端深度学习模型的影响，指出了低层视觉特征捕获不足和对内容噪声高度敏感等挑战。实验表明，大多数配置在真实文档级和零样本情境下表现不佳，但一种更简洁的设置能够达到与最先进系统相当的性能。<br /><strong>Keywords:</strong> historical writer identification, deep learning, end-to-end pipeline, feature aggregation, patch sampling, handwriting recognition<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hanif Rasyidi, Moshiur Farazi</div>
This paper investigates various factors that influence the performance of end-to-end deep learning approaches for historical writer identification (HWI), a task that remains challenging due to the diversity of handwriting styles, document degradation, and the limited number of labelled samples per writer. These conditions often make accurate recognition difficult, even for human experts. Traditional HWI methods typically rely on handcrafted image processing and clustering techniques, which tend to perform well on small and carefully curated datasets. In contrast, end-to-end pipelines aim to automate the process by learning features directly from document images. However, our experiments show that many of these models struggle to generalise in more realistic, document-level settings, especially under zero-shot scenarios where writers in the test set are not present in the training data. We explore different combinations of pre-processing methods, backbone architectures, and post-processing strategies, including text segmentation, patch sampling, and feature aggregation. The results suggest that most configurations perform poorly due to weak capture of low-level visual features, inconsistent patch representations, and high sensitivity to content noise. Still, we identify one end-to-end setup that achieves results comparable to the top-performing system, despite using a simpler design. These findings point to key challenges in building robust end-to-end systems and offer insight into design choices that improve performance in historical document writer identification.
<div><strong>Authors:</strong> Hanif Rasyidi, Moshiur Farazi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates how different pre‑processing methods, backbone architectures, and post‑processing strategies affect end‑to‑end deep learning models for historical writer identification, highlighting challenges such as weak low‑level feature capture and sensitivity to content noise. Experiments reveal that most configurations perform poorly in realistic, document‑level and zero‑shot settings, though one simpler setup reaches performance comparable to state‑of‑the‑art systems.", "summary_cn": "本文评估了不同的预处理方法、主干网络和后处理策略对历史手写作者识别端到端深度学习模型的影响，指出了低层视觉特征捕获不足和对内容噪声高度敏感等挑战。实验表明，大多数配置在真实文档级和零样本情境下表现不佳，但一种更简洁的设置能够达到与最先进系统相当的性能。", "keywords": "historical writer identification, deep learning, end-to-end pipeline, feature aggregation, patch sampling, handwriting recognition", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hanif Rasyidi", "Moshiur Farazi"]}
]]></acme>

<pubDate>2025-10-21T14:25:16+00:00</pubDate>
</item>
<item>
<title>Image augmentation with invertible networks in interactive satellite image change detection</title>
<link>https://papers.cool/arxiv/2510.18660</link>
<guid>https://papers.cool/arxiv/2510.18660</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an interactive satellite image change detection framework that uses active learning with a question‑answer loop to query a user for labels on a small display set. A novel invertible network maps these displays to a latent space where linear augmentations are applied, then maps the augmented data back for retraining the change detector in successive iterations. Experiments show that this approach outperforms existing methods on benchmark datasets.<br /><strong>Summary (CN):</strong> 本文提出了一种交互式卫星图像变化检测框架，采用主动学习的问答循环让用户为少量显示集标注标签。利用新颖的可逆网络将显示图像映射到潜在空间，在该空间进行线性增强后再映射回原始空间，用于后续迭代中重新训练检测模型。实验表明，该方法在基准数据集上优于现有技术。<br /><strong>Keywords:</strong> active learning, satellite image change detection, invertible networks, data augmentation, interactive learning, remote sensing, latent space transformation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hichem Sahbi</div>
This paper devises a novel interactive satellite image change detection algorithm based on active learning. Our framework employs an iterative process that leverages a question-and-answer model. This model queries the oracle (user) about the labels of a small subset of images (dubbed as display), and based on the oracle's responses, change detection model is dynamically updated. The main contribution of our framework resides in a novel invertible network that allows augmenting displays, by mapping them from highly nonlinear input spaces to latent ones, where augmentation transformations become linear and more tractable. The resulting augmented data are afterwards mapped back to the input space, and used to retrain more effective change detection criteria in the subsequent iterations of active learning. Experimental results demonstrate superior performance of our proposed method compared to the related work.
<div><strong>Authors:</strong> Hichem Sahbi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an interactive satellite image change detection framework that uses active learning with a question‑answer loop to query a user for labels on a small display set. A novel invertible network maps these displays to a latent space where linear augmentations are applied, then maps the augmented data back for retraining the change detector in successive iterations. Experiments show that this approach outperforms existing methods on benchmark datasets.", "summary_cn": "本文提出了一种交互式卫星图像变化检测框架，采用主动学习的问答循环让用户为少量显示集标注标签。利用新颖的可逆网络将显示图像映射到潜在空间，在该空间进行线性增强后再映射回原始空间，用于后续迭代中重新训练检测模型。实验表明，该方法在基准数据集上优于现有技术。", "keywords": "active learning, satellite image change detection, invertible networks, data augmentation, interactive learning, remote sensing, latent space transformation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hichem Sahbi"]}
]]></acme>

<pubDate>2025-10-21T14:11:22+00:00</pubDate>
</item>
<item>
<title>Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression</title>
<link>https://papers.cool/arxiv/2510.18650</link>
<guid>https://papers.cool/arxiv/2510.18650</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that uses binary quadratic expressions instead of traditional linear binary bases. Experiments on a matrix compression benchmark and post‑training quantization of Vision Transformer models show that BQQ achieves better memory‑efficiency versus reconstruction error trade‑offs and improves PTQ performance, surpassing state‑of‑the‑art methods by up to 2.2% (calibration) and 59.1% (data‑free) at 2‑bit quantization.<br /><strong>Summary (CN):</strong> 本文提出二次二进制量化 (Binary Quadratic Quantization, BQQ) 方法，用二进制二次表达式替代传统的线性二进制基进行矩阵压缩。实验在矩阵压缩基准和 Vision Transformer 的后训练量化任务上显示，BQQ 在内存效率与重构误差之间取得更佳平衡，并在 2 位量化下分别比最先进的校准式和无数据式 PTQ 方法（PTQ）提升 2.2% 和 59.1%。<br /><strong>Keywords:</strong> binary quadratic quantization, matrix compression, post-training quantization, vision transformer, low-bit quantization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
This paper proposes a novel matrix quantization method, Binary Quadratic Quantization (BQQ). In contrast to conventional first-order quantization approaches, such as uniform quantization and binary coding quantization, that approximate real-valued matrices via linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. We validate our approach with two experiments: a matrix compression benchmark and post-training quantization (PTQ) on pretrained Vision Transformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error than conventional methods for compressing diverse matrix data. It also delivers strong PTQ performance, even though we neither target state-of-the-art PTQ accuracy under tight memory constraints nor rely on PTQ-specific binary matrix optimization. For example, our proposed method outperforms the state-of-the-art PTQ method by up to 2.2\% and 59.1% on the ImageNet dataset under the calibration-based and data-free scenarios, respectively, with quantization equivalent to 2 bits. These findings highlight the surprising effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression.
<div><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that uses binary quadratic expressions instead of traditional linear binary bases. Experiments on a matrix compression benchmark and post‑training quantization of Vision Transformer models show that BQQ achieves better memory‑efficiency versus reconstruction error trade‑offs and improves PTQ performance, surpassing state‑of‑the‑art methods by up to 2.2% (calibration) and 59.1% (data‑free) at 2‑bit quantization.", "summary_cn": "本文提出二次二进制量化 (Binary Quadratic Quantization, BQQ) 方法，用二进制二次表达式替代传统的线性二进制基进行矩阵压缩。实验在矩阵压缩基准和 Vision Transformer 的后训练量化任务上显示，BQQ 在内存效率与重构误差之间取得更佳平衡，并在 2 位量化下分别比最先进的校准式和无数据式 PTQ 方法（PTQ）提升 2.2% 和 59.1%。", "keywords": "binary quadratic quantization, matrix compression, post-training quantization, vision transformer, low-bit quantization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kyo Kuroki", "Yasuyuki Okoshi", "Thiem Van Chu", "Kazushi Kawamura", "Masato Motomura"]}
]]></acme>

<pubDate>2025-10-21T13:58:46+00:00</pubDate>
</item>
<item>
<title>ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
<link>https://papers.cool/arxiv/2510.18637</link>
<guid>https://papers.cool/arxiv/2510.18637</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ε-Seg, a sparsely supervised semantic segmentation method for electron microscopy and fluorescence microscopy images that leverages hierarchical variational autoencoders, center-region masking, contrastive learning, and a Gaussian mixture model prior to learn robust latent embeddings and directly predict class labels. By requiring only a tiny fraction of pixel-level annotations (as low as 0.05%), ε-Seg achieves competitive segmentation performance on dense biological imaging datasets. Experiments demonstrate its effectiveness compared to baseline approaches across multiple microscopy modalities.<br /><strong>Summary (CN):</strong> 本文提出 ε-Seg，一种针对电子显微镜和荧光显微镜图像的稀疏监督语义分割方法，使用层次变分自编码器 (HVAE)、中心区域遮罩、对比学习和高斯混合模型先验来学习鲁棒的潜在嵌入，并直接预测类别标签。该方法仅需极少的像素标注（低至 0.05%），即可在密集的生物成像数据集上实现竞争性的分割效果。实验表明 ε-Seg 相较于基线在多种显微镜模态上表现出色。<br /><strong>Keywords:</strong> sparse supervision, semantic segmentation, hierarchical variational autoencoder, contrastive learning, Gaussian mixture model, microscopy imaging, EM data, label-efficient learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.
<div><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ε-Seg, a sparsely supervised semantic segmentation method for electron microscopy and fluorescence microscopy images that leverages hierarchical variational autoencoders, center-region masking, contrastive learning, and a Gaussian mixture model prior to learn robust latent embeddings and directly predict class labels. By requiring only a tiny fraction of pixel-level annotations (as low as 0.05%), ε-Seg achieves competitive segmentation performance on dense biological imaging datasets. Experiments demonstrate its effectiveness compared to baseline approaches across multiple microscopy modalities.", "summary_cn": "本文提出 ε-Seg，一种针对电子显微镜和荧光显微镜图像的稀疏监督语义分割方法，使用层次变分自编码器 (HVAE)、中心区域遮罩、对比学习和高斯混合模型先验来学习鲁棒的潜在嵌入，并直接预测类别标签。该方法仅需极少的像素标注（低至 0.05%），即可在密集的生物成像数据集上实现竞争性的分割效果。实验表明 ε-Seg 相较于基线在多种显微镜模态上表现出色。", "keywords": "sparse supervision, semantic segmentation, hierarchical variational autoencoder, contrastive learning, Gaussian mixture model, microscopy imaging, EM data, label-efficient learning", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sheida Rahnamai Kordasiabi", "Damian Dalle Nogare", "Florian Jug"]}
]]></acme>

<pubDate>2025-10-21T13:41:07+00:00</pubDate>
</item>
<item>
<title>C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression</title>
<link>https://papers.cool/arxiv/2510.18636</link>
<guid>https://papers.cool/arxiv/2510.18636</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes C-SWAP, a one-shot structured pruning framework that incorporates explainability by using causal relationships between model predictions and structural components to guide pruning. By progressively removing structures deemed non-essential, the method achieves substantial model size reductions on CNN and vision transformer baselines without fine‑tuning, outperforming existing one-shot approaches. Experimental results demonstrate minimal performance loss while offering a superior trade‑off between compression and accuracy.<br /><strong>Summary (CN):</strong> 本文提出 C-SWAP，一种在一次性结构化剪枝过程中引入可解释性的框架，通过利用模型预测与结构之间的因果关系（cause‑effect relations）指导剪枝，逐步移除对性能影响不大的网络结构。该方法在卷积神经网络和视觉变换器（vision transformer）基准上实现了显著的模型体积缩减且无需微调，性能下降极小，整体压缩效果优于现有一次性剪枝方法。<br /><strong>Keywords:</strong> structured pruning, explainability, causal pruning, one-shot pruning, model compression, CNN, vision transformer<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.
<div><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes C-SWAP, a one-shot structured pruning framework that incorporates explainability by using causal relationships between model predictions and structural components to guide pruning. By progressively removing structures deemed non-essential, the method achieves substantial model size reductions on CNN and vision transformer baselines without fine‑tuning, outperforming existing one-shot approaches. Experimental results demonstrate minimal performance loss while offering a superior trade‑off between compression and accuracy.", "summary_cn": "本文提出 C-SWAP，一种在一次性结构化剪枝过程中引入可解释性的框架，通过利用模型预测与结构之间的因果关系（cause‑effect relations）指导剪枝，逐步移除对性能影响不大的网络结构。该方法在卷积神经网络和视觉变换器（vision transformer）基准上实现了显著的模型体积缩减且无需微调，性能下降极小，整体压缩效果优于现有一次性剪枝方法。", "keywords": "structured pruning, explainability, causal pruning, one-shot pruning, model compression, CNN, vision transformer", "scoring": {"interpretability": 6, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baptiste Bauvin", "Loïc Baret", "Ola Ahmad"]}
]]></acme>

<pubDate>2025-10-21T13:40:11+00:00</pubDate>
</item>
<item>
<title>Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</title>
<link>https://papers.cool/arxiv/2510.18632</link>
<guid>https://papers.cool/arxiv/2510.18632</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces 3DThinker, a framework that enables vision-language models to perform geometric imagination and spatial reasoning from limited 2D views without any explicit 3D supervision. It aligns the model's internal 3D latent representation with that of a 3D foundation model during supervised training, then refines the reasoning trajectory using outcome-based optimization. Experiments on several benchmarks demonstrate consistent performance gains over strong baselines, showcasing a new way to integrate 3D representations into multimodal reasoning.<br /><strong>Summary (CN):</strong> 本文提出 3DThinker 框架，使视觉语言模型能够在仅有有限 2D 视角且无显式 3D 标注的情况下进行几何想象和空间推理。通过在监督阶段将模型生成的 3D 潜在空间与 3D 基础模型对齐，随后仅依据结果信号优化整个推理过程，从而提升内部 3D 思维能力。大量基准实验显示该方法相较于强基线有持续的性能提升，为将 3D 表征统一到多模态推理中提供了新视角。<br /><strong>Keywords:</strong> 3D reasoning, geometric imagination, vision-language models, multimodal reasoning, latent alignment, spatial understanding, 3D foundation model, limited view, mental imagery<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</div>
Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.
<div><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces 3DThinker, a framework that enables vision-language models to perform geometric imagination and spatial reasoning from limited 2D views without any explicit 3D supervision. It aligns the model's internal 3D latent representation with that of a 3D foundation model during supervised training, then refines the reasoning trajectory using outcome-based optimization. Experiments on several benchmarks demonstrate consistent performance gains over strong baselines, showcasing a new way to integrate 3D representations into multimodal reasoning.", "summary_cn": "本文提出 3DThinker 框架，使视觉语言模型能够在仅有有限 2D 视角且无显式 3D 标注的情况下进行几何想象和空间推理。通过在监督阶段将模型生成的 3D 潜在空间与 3D 基础模型对齐，随后仅依据结果信号优化整个推理过程，从而提升内部 3D 思维能力。大量基准实验显示该方法相较于强基线有持续的性能提升，为将 3D 表征统一到多模态推理中提供了新视角。", "keywords": "3D reasoning, geometric imagination, vision-language models, multimodal reasoning, latent alignment, spatial understanding, 3D foundation model, limited view, mental imagery", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhangquan Chen", "Manyuan Zhang", "Xinlei Yu", "Xufang Luo", "Mingze Sun", "Zihao Pan", "Yan Feng", "Peng Pei", "Xunliang Cai", "Ruqi Huang"]}
]]></acme>

<pubDate>2025-10-21T13:36:58+00:00</pubDate>
</item>
<item>
<title>CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder</title>
<link>https://papers.cool/arxiv/2510.18583</link>
<guid>https://papers.cool/arxiv/2510.18583</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CovMatch introduces a multimodal dataset distillation framework that aligns the cross‑covariance of real and synthetic image‑text features while regularizing each modality's feature distribution, enabling joint optimization of both vision and text encoders. This approach yields stronger cross‑modal alignment and achieves up to 6.8% absolute improvement in retrieval accuracy on Flickr30K and COCO using only 500 synthetic pairs. The method addresses scalability challenges of multimodal contrastive learning by moving beyond frozen text encoders used in prior work.<br /><strong>Summary (CN):</strong> CovMatch 提出一种多模态数据集蒸馏框架，通过对齐真实与合成图像‑文本特征的交叉协方差并对每个模态的特征分布进行正则化，实现视觉编码器和可训练文本编码器的联合优化。该方法显著提升跨模态对齐，在 Flickr30K 与 COCO 数据集上仅使用 500 对合成样本即可实现检索准确率最高提升 6.8%。相较于以往冻结文本编码器的做法，CovMatch 解决了多模态对比学习的可扩展性问题。<br /><strong>Keywords:</strong> multimodal dataset distillation, cross-covariance alignment, vision-language models, synthetic image-text pairs, retrieval accuracy, Flickr30K, COCO, trainable text encoder, contrastive learning, scalability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yongmin Lee, Hye Won Chung</div>
Multimodal dataset distillation aims to synthesize a small set of image-text pairs that enables efficient training of large-scale vision-language models. While dataset distillation has shown promise in unimodal tasks, extending it to multimodal contrastive learning presents key challenges: learning cross-modal alignment and managing the high computational cost of large encoders. Prior approaches address scalability by freezing the text encoder and update only the image encoder and text projection layer. However, we find this severely limits semantic alignment and becomes a bottleneck for performance scaling. We propose CovMatch, a scalable dataset distillation framework that aligns the cross-covariance of real and synthetic features while regularizing feature distributions within each modality. Unlike prior approaches, CovMatch enables joint optimization of both encoders, leading to stronger cross-modal alignment and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms state-of-the-art multimodal distillation methods and achieves up to 6.8% absolute gains in retrieval accuracy using only 500 synthetic pairs.
<div><strong>Authors:</strong> Yongmin Lee, Hye Won Chung</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CovMatch introduces a multimodal dataset distillation framework that aligns the cross‑covariance of real and synthetic image‑text features while regularizing each modality's feature distribution, enabling joint optimization of both vision and text encoders. This approach yields stronger cross‑modal alignment and achieves up to 6.8% absolute improvement in retrieval accuracy on Flickr30K and COCO using only 500 synthetic pairs. The method addresses scalability challenges of multimodal contrastive learning by moving beyond frozen text encoders used in prior work.", "summary_cn": "CovMatch 提出一种多模态数据集蒸馏框架，通过对齐真实与合成图像‑文本特征的交叉协方差并对每个模态的特征分布进行正则化，实现视觉编码器和可训练文本编码器的联合优化。该方法显著提升跨模态对齐，在 Flickr30K 与 COCO 数据集上仅使用 500 对合成样本即可实现检索准确率最高提升 6.8%。相较于以往冻结文本编码器的做法，CovMatch 解决了多模态对比学习的可扩展性问题。", "keywords": "multimodal dataset distillation, cross-covariance alignment, vision-language models, synthetic image-text pairs, retrieval accuracy, Flickr30K, COCO, trainable text encoder, contrastive learning, scalability", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yongmin Lee", "Hye Won Chung"]}
]]></acme>

<pubDate>2025-10-21T12:36:25+00:00</pubDate>
</item>
<item>
<title>Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model</title>
<link>https://papers.cool/arxiv/2510.18573</link>
<guid>https://papers.cool/arxiv/2510.18573</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Kaleido is a subject-to-video (S2V) generation framework that synthesizes videos consistent with multiple reference images of target subjects. The paper addresses limitations of existing S2V models by introducing a data construction pipeline with low‑quality filtering and diverse synthesis, as well as a Reference Rotary Positional Encoding (R‑RoPE) to integrate multiple references more reliably, achieving superior consistency, fidelity, and generalization across benchmarks.<br /><strong>Summary (CN):</strong> Kaleido 是一种面向主体‑到‑视频（S2V）的生成框架，能够基于多个目标主体的参考图像合成主体一致的视频。论文通过构建数据管线（包括低质量样本过滤和多样化合成）以及引入参考旋转位置编码（R‑RoPE）来更稳健地融合多图像，从而在一致性、保真度和泛化能力上显著超越现有方法。<br /><strong>Keywords:</strong> subject-to-video, multi-subject video generation, reference video synthesis, Reference Rotary Positional Encoding, data pipeline, video consistency, generative models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenxing Zhang, Jiayan Teng, Zhuoyi Yang, Tiankun Cao, Cheng Wang, Xiaotao Gu, Jie Tang, Dan Guo, Meng Wang</div>
We present Kaleido, a subject-to-video~(S2V) generation framework, which aims to synthesize subject-consistent videos conditioned on multiple reference images of target subjects. Despite recent progress in S2V generation models, existing approaches remain inadequate at maintaining multi-subject consistency and at handling background disentanglement, often resulting in lower reference fidelity and semantic drift under multi-image conditioning. These shortcomings can be attributed to several factors. Primarily, the training dataset suffers from a lack of diversity and high-quality samples, as well as cross-paired data, i.e., paired samples whose components originate from different instances. In addition, the current mechanism for integrating multiple reference images is suboptimal, potentially resulting in the confusion of multiple subjects. To overcome these limitations, we propose a dedicated data construction pipeline, incorporating low-quality sample filtering and diverse data synthesis, to produce consistency-preserving training data. Moreover, we introduce Reference Rotary Positional Encoding (R-RoPE) to process reference images, enabling stable and precise multi-image integration. Extensive experiments across numerous benchmarks demonstrate that Kaleido significantly outperforms previous methods in consistency, fidelity, and generalization, marking an advance in S2V generation.
<div><strong>Authors:</strong> Zhenxing Zhang, Jiayan Teng, Zhuoyi Yang, Tiankun Cao, Cheng Wang, Xiaotao Gu, Jie Tang, Dan Guo, Meng Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Kaleido is a subject-to-video (S2V) generation framework that synthesizes videos consistent with multiple reference images of target subjects. The paper addresses limitations of existing S2V models by introducing a data construction pipeline with low‑quality filtering and diverse synthesis, as well as a Reference Rotary Positional Encoding (R‑RoPE) to integrate multiple references more reliably, achieving superior consistency, fidelity, and generalization across benchmarks.", "summary_cn": "Kaleido 是一种面向主体‑到‑视频（S2V）的生成框架，能够基于多个目标主体的参考图像合成主体一致的视频。论文通过构建数据管线（包括低质量样本过滤和多样化合成）以及引入参考旋转位置编码（R‑RoPE）来更稳健地融合多图像，从而在一致性、保真度和泛化能力上显著超越现有方法。", "keywords": "subject-to-video, multi-subject video generation, reference video synthesis, Reference Rotary Positional Encoding, data pipeline, video consistency, generative models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenxing Zhang", "Jiayan Teng", "Zhuoyi Yang", "Tiankun Cao", "Cheng Wang", "Xiaotao Gu", "Jie Tang", "Dan Guo", "Meng Wang"]}
]]></acme>

<pubDate>2025-10-21T12:28:14+00:00</pubDate>
</item>
<item>
<title>Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</title>
<link>https://papers.cool/arxiv/2510.18552</link>
<guid>https://papers.cool/arxiv/2510.18552</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Occluded nuScenes, an extension of the nuScenes benchmark that provides controlled, parameterised occlusions for camera, radar, and LiDAR sensors. It releases four camera occlusion types and scripted degradations for radar and LiDAR, enabling reproducible evaluation of perception and fusion models under partial sensor failures. The dataset aims to facilitate research on robust sensor fusion and safety‑critical perception in automated driving.<br /><strong>Summary (CN):</strong> 本文推出了 Occluded nuScenes 数据集，这是 nuScenes 基准的扩展，提供对相机、雷达和 LiDAR 传感器的受控、可参数化的遮挡。数据集发布了四种相机遮挡类型，并提供雷达和 LiDAR 的遮挡脚本，以实现对部分传感器失效情况下感知与融合模型的可重复评估。该资源旨在推动自动驾驶中鲁棒传感器融合和安全关键感知的研究。<br /><strong>Keywords:</strong> occlusion dataset, sensor fusion, perception robustness, autonomous driving, multi-sensor, nuScenes, radar occlusion, LiDAR occlusion, safety-critical perception<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</div>
Robust perception in automated driving requires reliable performance under adverse conditions, where sensors may be affected by partial failures or environmental occlusions. Although existing autonomous driving datasets inherently contain sensor noise and environmental variability, very few enable controlled, parameterised, and reproducible degradations across multiple sensing modalities. This gap limits the ability to systematically evaluate how perception and fusion architectures perform under well-defined adverse conditions. To address this limitation, we introduce the Occluded nuScenes Dataset, a novel extension of the widely used nuScenes benchmark. For the camera modality, we release both the full and mini versions with four types of occlusions, two adapted from public implementations and two newly designed. For radar and LiDAR, we provide parameterised occlusion scripts that implement three types of degradations each, enabling flexible and repeatable generation of occluded data. This resource supports consistent, reproducible evaluation of perception models under partial sensor failures and environmental interference. By releasing the first multi-sensor occlusion dataset with controlled and reproducible degradations, we aim to advance research on robust sensor fusion, resilience analysis, and safety-critical perception in automated driving.
<div><strong>Authors:</strong> Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Occluded nuScenes, an extension of the nuScenes benchmark that provides controlled, parameterised occlusions for camera, radar, and LiDAR sensors. It releases four camera occlusion types and scripted degradations for radar and LiDAR, enabling reproducible evaluation of perception and fusion models under partial sensor failures. The dataset aims to facilitate research on robust sensor fusion and safety‑critical perception in automated driving.", "summary_cn": "本文推出了 Occluded nuScenes 数据集，这是 nuScenes 基准的扩展，提供对相机、雷达和 LiDAR 传感器的受控、可参数化的遮挡。数据集发布了四种相机遮挡类型，并提供雷达和 LiDAR 的遮挡脚本，以实现对部分传感器失效情况下感知与融合模型的可重复评估。该资源旨在推动自动驾驶中鲁棒传感器融合和安全关键感知的研究。", "keywords": "occlusion dataset, sensor fusion, perception robustness, autonomous driving, multi-sensor, nuScenes, radar occlusion, LiDAR occlusion, safety-critical perception", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Sanjay Kumar", "Tim Brophy", "Reenu Mohandas", "Eoin Martino Grua", "Ganesh Sistu", "Valentina Donzella", "Ciaran Eising"]}
]]></acme>

<pubDate>2025-10-21T12:02:26+00:00</pubDate>
</item>
<item>
<title>GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization</title>
<link>https://papers.cool/arxiv/2510.18539</link>
<guid>https://papers.cool/arxiv/2510.18539</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GBlobs, a local point cloud feature descriptor designed to mitigate the geometric shortcut that causes LiDAR detectors to over‑rely on absolute object positions. By using GBlobs as inputs, the method forces networks to learn object‑centric representations, substantially improving generalization across varied sensor placements and achieving top performance in the RoboSense 2025 Track 3 challenge.<br /><strong>Summary (CN):</strong> 本文提出 GBlobs，一种局部点云特征描述子，用于消除 LiDAR 检测器对绝对位置的几何捷径依赖。通过将 GBlobs 作为网络输入，模型被迫学习以物体为中心的表示，从而在不同传感器摆放情况下显著提升泛化能力，并在 RoboSense 2025 第三赛道中取得最佳成绩。<br /><strong>Keywords:</strong> GBlobs, LiDAR, 3D object detection, sensor placement, geometric shortcut, local point cloud descriptor, generalization, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Dušan Malić, Christian Fruhwirth-Reisinger, Alexander Prutsch, Wei Lin, Samuel Schulter, Horst Possegger</div>
This technical report outlines the top-ranking solution for RoboSense 2025: Track 3, achieving state-of-the-art performance on 3D object detection under various sensor placements. Our submission utilizes GBlobs, a local point cloud feature descriptor specifically designed to enhance model generalization across diverse LiDAR configurations. Current LiDAR-based 3D detectors often suffer from a \enquote{geometric shortcut} when trained on conventional global features (\ie, absolute Cartesian coordinates). This introduces a position bias that causes models to primarily rely on absolute object position rather than distinguishing shape and appearance characteristics. Although effective for in-domain data, this shortcut severely limits generalization when encountering different point distributions, such as those resulting from varying sensor placements. By using GBlobs as network input features, we effectively circumvent this geometric shortcut, compelling the network to learn robust, object-centric representations. This approach significantly enhances the model's ability to generalize, resulting in the exceptional performance demonstrated in this challenge.
<div><strong>Authors:</strong> Dušan Malić, Christian Fruhwirth-Reisinger, Alexander Prutsch, Wei Lin, Samuel Schulter, Horst Possegger</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GBlobs, a local point cloud feature descriptor designed to mitigate the geometric shortcut that causes LiDAR detectors to over‑rely on absolute object positions. By using GBlobs as inputs, the method forces networks to learn object‑centric representations, substantially improving generalization across varied sensor placements and achieving top performance in the RoboSense 2025 Track 3 challenge.", "summary_cn": "本文提出 GBlobs，一种局部点云特征描述子，用于消除 LiDAR 检测器对绝对位置的几何捷径依赖。通过将 GBlobs 作为网络输入，模型被迫学习以物体为中心的表示，从而在不同传感器摆放情况下显著提升泛化能力，并在 RoboSense 2025 第三赛道中取得最佳成绩。", "keywords": "GBlobs, LiDAR, 3D object detection, sensor placement, geometric shortcut, local point cloud descriptor, generalization, robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Dušan Malić", "Christian Fruhwirth-Reisinger", "Alexander Prutsch", "Wei Lin", "Samuel Schulter", "Horst Possegger"]}
]]></acme>

<pubDate>2025-10-21T11:35:58+00:00</pubDate>
</item>
<item>
<title>RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation</title>
<link>https://papers.cool/arxiv/2510.18521</link>
<guid>https://papers.cool/arxiv/2510.18521</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> RayPose reformulates template-based 6D object pose estimation as a ray alignment problem and trains a diffusion transformer to align a query image with multiple posed template views. The method reparameterizes rotation using object-centered camera rays and extends scale‑invariant translation estimation to dense translation offsets, employing a coarse‑to‑fine training strategy that samples narrowed template sets. Experiments on several benchmarks demonstrate competitive performance on unseen object pose estimation.<br /><strong>Summary (CN):</strong> RayPose 将基于模板的 6D 目标姿态估计重新表述为光线对齐问题，并利用扩散 Transformer 将查询图像与多个已定位的模板视图对齐。该方法使用以对象为中心的相机光线对旋转进行重新参数化，并通过稠密平移偏移扩展尺度不变的平移估计，同时采用从粗到细的训练策略对模板采样进行收窄。实验在多个基准数据集上显示了在未见对象姿态估计方面的竞争性表现。<br /><strong>Keywords:</strong> 6D object pose estimation, diffusion transformer, ray bundling, template views, unseen objects, geometric priors<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Junwen Huang, Shishir Reddy Vutukur, Peter KT Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam</div>
Typical template-based object pose pipelines estimate the pose by retrieving the closest matching template and aligning it with the observed image. However, failure to retrieve the correct template often leads to inaccurate pose predictions. To address this, we reformulate template-based object pose estimation as a ray alignment problem, where the viewing directions from multiple posed template images are learned to align with a non-posed query image. Inspired by recent progress in diffusion-based camera pose estimation, we embed this formulation into a diffusion transformer architecture that aligns a query image with a set of posed templates. We reparameterize object rotation using object-centered camera rays and model object translation by extending scale-invariant translation estimation to dense translation offsets. Our model leverages geometric priors from the templates to guide accurate query pose inference. A coarse-to-fine training strategy based on narrowed template sampling improves performance without modifying the network architecture. Extensive experiments across multiple benchmark datasets show competitive results of our method compared to state-of-the-art approaches in unseen object pose estimation.
<div><strong>Authors:</strong> Junwen Huang, Shishir Reddy Vutukur, Peter KT Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "RayPose reformulates template-based 6D object pose estimation as a ray alignment problem and trains a diffusion transformer to align a query image with multiple posed template views. The method reparameterizes rotation using object-centered camera rays and extends scale‑invariant translation estimation to dense translation offsets, employing a coarse‑to‑fine training strategy that samples narrowed template sets. Experiments on several benchmarks demonstrate competitive performance on unseen object pose estimation.", "summary_cn": "RayPose 将基于模板的 6D 目标姿态估计重新表述为光线对齐问题，并利用扩散 Transformer 将查询图像与多个已定位的模板视图对齐。该方法使用以对象为中心的相机光线对旋转进行重新参数化，并通过稠密平移偏移扩展尺度不变的平移估计，同时采用从粗到细的训练策略对模板采样进行收窄。实验在多个基准数据集上显示了在未见对象姿态估计方面的竞争性表现。", "keywords": "6D object pose estimation, diffusion transformer, ray bundling, template views, unseen objects, geometric priors", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junwen Huang", "Shishir Reddy Vutukur", "Peter KT Yu", "Nassir Navab", "Slobodan Ilic", "Benjamin Busam"]}
]]></acme>

<pubDate>2025-10-21T11:01:20+00:00</pubDate>
</item>
<item>
<title>DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices</title>
<link>https://papers.cool/arxiv/2510.18513</link>
<guid>https://papers.cool/arxiv/2510.18513</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DWaste, a computer‑vision platform for real‑time waste sorting on resource‑constrained smartphones and edge devices, emphasizing low latency, small model size, and reduced carbon emissions. It benchmarks several image classification and object detection models, showing trade‑offs between accuracy and resource consumption, and demonstrates that quantization can further improve efficiency.<br /><strong>Summary (CN):</strong> 本文提出 DWaste 系统，在资源受限的手机和边缘设备上实现实时垃圾分类，重点降低延迟、模型体积和碳排放。通过对多种分类和检测模型的基准测试，展示了准确率与资源消耗之间的权衡，并说明量化技术可显著提升效率。<br /><strong>Keywords:</strong> waste sorting, computer vision, edge devices, model quantization, EfficientNetV2, YOLO, green AI, low-power inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Suman Kunwar</div>
The rise of convenience packaging has led to generation of enormous waste, making efficient waste sorting crucial for sustainable waste management. To address this, we developed DWaste, a computer vision-powered platform designed for real-time waste sorting on resource-constrained smartphones and edge devices, including offline functionality. We benchmarked various image classification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object detection (YOLOv8n, YOLOv11n) using a subset of our own waste data set and annotated it using the custom tool Annotated Lab. We found a clear trade-off between accuracy and resource consumption: the best classifier, EfficientNetV2S, achieved high accuracy (~ 96%) but suffered from high latency (~ 0.22s) and elevated carbon emissions. In contrast, lightweight object detection models delivered strong performance (up to 77% mAP) with ultra-fast inference (~ 0.03s) and significantly smaller model sizes (< 7MB), making them ideal for real-time, low-power use. Model quantization further maximized efficiency, substantially reducing model size and VRAM usage by up to 75%. Our work demonstrates the successful implementation of "Greener AI" models to support real-time, sustainable waste sorting on edge devices.
<div><strong>Authors:</strong> Suman Kunwar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DWaste, a computer‑vision platform for real‑time waste sorting on resource‑constrained smartphones and edge devices, emphasizing low latency, small model size, and reduced carbon emissions. It benchmarks several image classification and object detection models, showing trade‑offs between accuracy and resource consumption, and demonstrates that quantization can further improve efficiency.", "summary_cn": "本文提出 DWaste 系统，在资源受限的手机和边缘设备上实现实时垃圾分类，重点降低延迟、模型体积和碳排放。通过对多种分类和检测模型的基准测试，展示了准确率与资源消耗之间的权衡，并说明量化技术可显著提升效率。", "keywords": "waste sorting, computer vision, edge devices, model quantization, EfficientNetV2, YOLO, green AI, low-power inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Suman Kunwar"]}
]]></acme>

<pubDate>2025-10-21T10:55:32+00:00</pubDate>
</item>
<item>
<title>Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18502</link>
<guid>https://papers.cool/arxiv/2510.18502</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a zero‑shot vehicle make and model recognition pipeline that combines a vision‑language model with retrieval‑augmented generation. The VLM describes the vehicle image in text, retrieves matching entries from a database of vehicle attributes, and prompts a language model to infer the exact make and model, achieving about a 20% improvement over a CLIP baseline without any image‑specific finetuning.<br /><strong>Summary (CN):</strong> 本文提出一种零样本车辆品牌车型识别流水线，将视觉语言模型生成的图像描述与基于文本的检索增强生成相结合。系统从车辆属性数据库中检索相关条目，并将其与描述一起提示语言模型输出具体品牌和车型，实现相较于 CLIP 基线约 20% 的性能提升，无需对图像进行专门微调。<br /><strong>Keywords:</strong> vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, smart city<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.
<div><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a zero‑shot vehicle make and model recognition pipeline that combines a vision‑language model with retrieval‑augmented generation. The VLM describes the vehicle image in text, retrieves matching entries from a database of vehicle attributes, and prompts a language model to infer the exact make and model, achieving about a 20% improvement over a CLIP baseline without any image‑specific finetuning.", "summary_cn": "本文提出一种零样本车辆品牌车型识别流水线，将视觉语言模型生成的图像描述与基于文本的检索增强生成相结合。系统从车辆属性数据库中检索相关条目，并将其与描述一起提示语言模型输出具体品牌和车型，实现相较于 CLIP 基线约 20% 的性能提升，无需对图像进行专门微调。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, smart city", "scoring": {"interpretability": 3, "understanding": 4, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}
]]></acme>

<pubDate>2025-10-21T10:39:39+00:00</pubDate>
</item>
<item>
<title>Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos</title>
<link>https://papers.cool/arxiv/2510.18489</link>
<guid>https://papers.cool/arxiv/2510.18489</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Mono4DGS-HDR is the first system that reconstructs renderable four‑dimensional high‑dynamic‑range (HDR) scenes from unposed monocular low‑dynamic‑range (LDR) videos captured with alternating exposures, using a two‑stage Gaussian splatting optimization. The first stage learns an HDR Gaussian representation in an orthographic frame without camera poses, while the second stage refines world‑space Gaussians and estimates camera poses, aided by a temporal luminance regularization for consistency. Experiments on a newly built benchmark show substantial improvements over adapted state‑of‑the‑art methods in both quality and speed.<br /><strong>Summary (CN):</strong> Mono4DGS-HDR 是首个从交替曝光的单目 LDR 视频中重建可渲染的四维 HDR 场景的系统，采用两阶段 Gaussian Splatting 优化。第一阶段在正交相机坐标系中学习 HDR 高斯表示，无需相机位姿；第二阶段将高斯转换到世界坐标并联合优化位姿，同时使用时序亮度正则化提升外观一致性。实验在新建的 HDR 视频重建基准上显示出在渲染质量和速度上显著优于现有方法。<br /><strong>Keywords:</strong> HDR video reconstruction, Gaussian splatting, monocular video, alternating exposure, temporal luminance regularization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinfeng Liu, Lingtong Kong, Mi Zhou, Jinwen Chen, Dan Xu</div>
We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.
<div><strong>Authors:</strong> Jinfeng Liu, Lingtong Kong, Mi Zhou, Jinwen Chen, Dan Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Mono4DGS-HDR is the first system that reconstructs renderable four‑dimensional high‑dynamic‑range (HDR) scenes from unposed monocular low‑dynamic‑range (LDR) videos captured with alternating exposures, using a two‑stage Gaussian splatting optimization. The first stage learns an HDR Gaussian representation in an orthographic frame without camera poses, while the second stage refines world‑space Gaussians and estimates camera poses, aided by a temporal luminance regularization for consistency. Experiments on a newly built benchmark show substantial improvements over adapted state‑of‑the‑art methods in both quality and speed.", "summary_cn": "Mono4DGS-HDR 是首个从交替曝光的单目 LDR 视频中重建可渲染的四维 HDR 场景的系统，采用两阶段 Gaussian Splatting 优化。第一阶段在正交相机坐标系中学习 HDR 高斯表示，无需相机位姿；第二阶段将高斯转换到世界坐标并联合优化位姿，同时使用时序亮度正则化提升外观一致性。实验在新建的 HDR 视频重建基准上显示出在渲染质量和速度上显著优于现有方法。", "keywords": "HDR video reconstruction, Gaussian splatting, monocular video, alternating exposure, temporal luminance regularization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinfeng Liu", "Lingtong Kong", "Mi Zhou", "Jinwen Chen", "Dan Xu"]}
]]></acme>

<pubDate>2025-10-21T10:14:33+00:00</pubDate>
</item>
<item>
<title>Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.18457</link>
<guid>https://papers.cool/arxiv/2510.18457</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Foundation Model Variational Autoencoder (VFM‑VAE) as a direct visual tokenizer for latent diffusion models, avoiding distillation and using a decoder with multi‑scale latent fusion and progressive resolution reconstruction to preserve semantic fidelity while achieving high‑quality pixel reconstruction. It also proposes the SE‑CKNNA metric to analyze representation dynamics during diffusion training and a joint tokenizer‑diffusion alignment strategy that accelerates convergence, achieving gFID 2.20 in 80 epochs and 1.62 after 640 epochs.<br /><strong>Summary (CN):</strong> 本文提出 Vision Foundation Model 变分自编码器 (VFM‑VAE) 作为潜在扩散模型的直接视觉分词器，摆脱蒸馏，并通过多尺度潜在融合与渐进分辨率重建解码器在保持语义聚焦的同时实现高质量像素重建。文中还引入 SE‑CKNNA 指标用于分析扩散训练期间的表征动力学，并提出联合分词器‑扩散对齐策略，大幅加速收敛，使模型在 80 轮后达到 gFID 2.20，640 轮后达到 1.62。<br /><strong>Keywords:</strong> vision foundation models, latent diffusion, tokenizer, VFM‑VAE, multi-scale latent fusion, SE‑CKNNA, gFID, diffusion alignment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.
<div><strong>Authors:</strong> Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Foundation Model Variational Autoencoder (VFM‑VAE) as a direct visual tokenizer for latent diffusion models, avoiding distillation and using a decoder with multi‑scale latent fusion and progressive resolution reconstruction to preserve semantic fidelity while achieving high‑quality pixel reconstruction. It also proposes the SE‑CKNNA metric to analyze representation dynamics during diffusion training and a joint tokenizer‑diffusion alignment strategy that accelerates convergence, achieving gFID 2.20 in 80 epochs and 1.62 after 640 epochs.", "summary_cn": "本文提出 Vision Foundation Model 变分自编码器 (VFM‑VAE) 作为潜在扩散模型的直接视觉分词器，摆脱蒸馏，并通过多尺度潜在融合与渐进分辨率重建解码器在保持语义聚焦的同时实现高质量像素重建。文中还引入 SE‑CKNNA 指标用于分析扩散训练期间的表征动力学，并提出联合分词器‑扩散对齐策略，大幅加速收敛，使模型在 80 轮后达到 gFID 2.20，640 轮后达到 1.62。", "keywords": "vision foundation models, latent diffusion, tokenizer, VFM‑VAE, multi-scale latent fusion, SE‑CKNNA, gFID, diffusion alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tianci Bi", "Xiaoyi Zhang", "Yan Lu", "Nanning Zheng"]}
]]></acme>

<pubDate>2025-10-21T09:30:45+00:00</pubDate>
</item>
<item>
<title>LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance</title>
<link>https://papers.cool/arxiv/2510.18446</link>
<guid>https://papers.cool/arxiv/2510.18446</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents LAND, a latent diffusion model that synthesizes high‑resolution 3D chest CT volumes (256³ at 1 mm isotropic) conditioned on anatomical masks of lungs and nodules. By incorporating global lung structure alongside nodule masks, the method produces anatomically plausible scans and can generate diverse examples with or without nodules, enabling low‑cost data creation for AI training. Experiments show that conditioning only on nodule masks leads to unrealistic anatomy, emphasizing the need for comprehensive anatomical guidance.<br /><strong>Summary (CN):</strong> 本文提出了 LAND，一种基于潜在扩散的模型，可在肺部和结节的三维解剖掩码条件下生成高分辨率 3D 胸部 CT（256³，1 mm 等距）体积图像。通过结合全局肺部结构与结节掩码，模型能够生成解剖上合理且多样化的扫描，支持生成有或无结节的 CT，帮助 AI 模型或医护人员的训练。实验表明，仅使用结节掩码会产生解剖错误的结果，凸显完整解剖引导的重要性。<br /><strong>Keywords:</strong> latent diffusion, 3D chest CT synthesis, anatomical guidance, lung nodule generation, synthetic medical imaging, volumetric diffusion, mask-conditioned generation, high-resolution CT, data augmentation, AI training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Anna Oliveras, Roger Marí, Rafael Redondo, Oriol Guardià, Ana Tost, Bhalaji Nagarajan, Carolina Migliorelli, Vicent Ribas, Petia Radeva</div>
This work introduces a new latent diffusion model to generate high-quality 3D chest CT scans conditioned on 3D anatomical masks. The method synthesizes volumetric images of size 256x256x256 at 1 mm isotropic resolution using a single mid-range GPU, significantly lowering the computational cost compared to existing approaches. The conditioning masks delineate lung and nodule regions, enabling precise control over the output anatomical features. Experimental results demonstrate that conditioning solely on nodule masks leads to anatomically incorrect outputs, highlighting the importance of incorporating global lung structure for accurate conditional synthesis. The proposed approach supports the generation of diverse CT volumes with and without lung nodules of varying attributes, providing a valuable tool for training AI models or healthcare professionals.
<div><strong>Authors:</strong> Anna Oliveras, Roger Marí, Rafael Redondo, Oriol Guardià, Ana Tost, Bhalaji Nagarajan, Carolina Migliorelli, Vicent Ribas, Petia Radeva</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents LAND, a latent diffusion model that synthesizes high‑resolution 3D chest CT volumes (256³ at 1 mm isotropic) conditioned on anatomical masks of lungs and nodules. By incorporating global lung structure alongside nodule masks, the method produces anatomically plausible scans and can generate diverse examples with or without nodules, enabling low‑cost data creation for AI training. Experiments show that conditioning only on nodule masks leads to unrealistic anatomy, emphasizing the need for comprehensive anatomical guidance.", "summary_cn": "本文提出了 LAND，一种基于潜在扩散的模型，可在肺部和结节的三维解剖掩码条件下生成高分辨率 3D 胸部 CT（256³，1 mm 等距）体积图像。通过结合全局肺部结构与结节掩码，模型能够生成解剖上合理且多样化的扫描，支持生成有或无结节的 CT，帮助 AI 模型或医护人员的训练。实验表明，仅使用结节掩码会产生解剖错误的结果，凸显完整解剖引导的重要性。", "keywords": "latent diffusion, 3D chest CT synthesis, anatomical guidance, lung nodule generation, synthetic medical imaging, volumetric diffusion, mask-conditioned generation, high-resolution CT, data augmentation, AI training", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Anna Oliveras", "Roger Marí", "Rafael Redondo", "Oriol Guardià", "Ana Tost", "Bhalaji Nagarajan", "Carolina Migliorelli", "Vicent Ribas", "Petia Radeva"]}
]]></acme>

<pubDate>2025-10-21T09:20:22+00:00</pubDate>
</item>
<item>
<title>Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</title>
<link>https://papers.cool/arxiv/2510.18437</link>
<guid>https://papers.cool/arxiv/2510.18437</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RISE, a retrieval self-augmented framework for unsupervised camouflaged object detection that leverages the entire training dataset to build prototype libraries and generate pseudo-labels via clustering-then-retrieval and multi-view KNN retrieval. By constructing environment and object prototypes without any ground-truth annotations, RISE produces high-quality pseudo-masks that improve detection performance over existing unsupervised and prompt-based methods.<br /><strong>Summary (CN):</strong> 本文提出 RISE，一种检索自增强的无监督伪装目标检测框架，利用整个训练数据集构建环境和目标原型库，并通过聚类后检索及多视角 KNN 检索生成伪标签。该方法在无需任何标注的情况下产生高质量伪掩码，在实验中超越了现有的无监督和基于提示的方法。<br /><strong>Keywords:</strong> camouflaged object detection, unsupervised learning, prototype library, KNN retrieval, clustering, multi-view retrieval, pseudo-labels<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ji Du, Xin Wang, Fangwei Hao, Mingyang Yu, Chunyuan Chen, Jiesheng Wu, Bin Wang, Jing Xu, Ping Li</div>
At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings. Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization. Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented paradigm that exploits the entire training dataset to generate pseudo-labels for single images, which could be used to train COD models. RISE begins by constructing prototype libraries for environments and camouflaged objects using training images (without ground truth), followed by K-Nearest Neighbor (KNN) retrieval to generate pseudo-masks for each image based on these libraries. It is important to recognize that using only training images without annotations exerts a pronounced challenge in crafting high-quality prototype libraries. In this light, we introduce a Clustering-then-Retrieval (CR) strategy, where coarse masks are first generated through clustering, facilitating subsequent histogram-based image filtering and cross-category retrieval to produce high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which integrates retrieval results from diverse views to produce more robust and precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms state-of-the-art unsupervised and prompt-based methods. Code is available at https://github.com/xiaohainku/RISE.
<div><strong>Authors:</strong> Ji Du, Xin Wang, Fangwei Hao, Mingyang Yu, Chunyuan Chen, Jiesheng Wu, Bin Wang, Jing Xu, Ping Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RISE, a retrieval self-augmented framework for unsupervised camouflaged object detection that leverages the entire training dataset to build prototype libraries and generate pseudo-labels via clustering-then-retrieval and multi-view KNN retrieval. By constructing environment and object prototypes without any ground-truth annotations, RISE produces high-quality pseudo-masks that improve detection performance over existing unsupervised and prompt-based methods.", "summary_cn": "本文提出 RISE，一种检索自增强的无监督伪装目标检测框架，利用整个训练数据集构建环境和目标原型库，并通过聚类后检索及多视角 KNN 检索生成伪标签。该方法在无需任何标注的情况下产生高质量伪掩码，在实验中超越了现有的无监督和基于提示的方法。", "keywords": "camouflaged object detection, unsupervised learning, prototype library, KNN retrieval, clustering, multi-view retrieval, pseudo-labels", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ji Du", "Xin Wang", "Fangwei Hao", "Mingyang Yu", "Chunyuan Chen", "Jiesheng Wu", "Bin Wang", "Jing Xu", "Ping Li"]}
]]></acme>

<pubDate>2025-10-21T09:12:26+00:00</pubDate>
</item>
<item>
<title>ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</title>
<link>https://papers.cool/arxiv/2510.18433</link>
<guid>https://papers.cool/arxiv/2510.18433</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents ImageGem, a large-scale in-the-wild dataset containing fine-grained user preference annotations for generative image models, including 57K users, 242K customized LoRAs, 3M prompts, and 5M generated images. Using these annotations, the authors train improved preference alignment models and evaluate personalized image retrieval and diffusion model recommendation, also introducing an end-to-end framework for editing diffusion models in latent weight space to match individual preferences. The results demonstrate that ImageGem enables a new paradigm for generative model personalization.<br /><strong>Summary (CN):</strong> 本文推出 ImageGem 数据集，收集了 57 万用户的真实交互数据，包含 242K 定制 LoRA、300 万文本提示和 500 万生成图像，以细粒度用户偏好标注为核心。基于这些标注，作者训练了更好的偏好对齐模型，并在个性化图像检索、生成模型推荐以及在潜在权重空间编辑扩散模型以匹配用户偏好方面进行实验，展示了该数据集为生成模型个性化提供了全新范式。<br /><strong>Keywords:</strong> generative model personalization, preference alignment, LoRA, diffusion models, image retrieval, vision-language, fine-grained user preferences, dataset<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</div>
We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.
<div><strong>Authors:</strong> Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents ImageGem, a large-scale in-the-wild dataset containing fine-grained user preference annotations for generative image models, including 57K users, 242K customized LoRAs, 3M prompts, and 5M generated images. Using these annotations, the authors train improved preference alignment models and evaluate personalized image retrieval and diffusion model recommendation, also introducing an end-to-end framework for editing diffusion models in latent weight space to match individual preferences. The results demonstrate that ImageGem enables a new paradigm for generative model personalization.", "summary_cn": "本文推出 ImageGem 数据集，收集了 57 万用户的真实交互数据，包含 242K 定制 LoRA、300 万文本提示和 500 万生成图像，以细粒度用户偏好标注为核心。基于这些标注，作者训练了更好的偏好对齐模型，并在个性化图像检索、生成模型推荐以及在潜在权重空间编辑扩散模型以匹配用户偏好方面进行实验，展示了该数据集为生成模型个性化提供了全新范式。", "keywords": "generative model personalization, preference alignment, LoRA, diffusion models, image retrieval, vision-language, fine-grained user preferences, dataset", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Yuanhe Guo", "Linxi Xie", "Zhuoran Chen", "Kangrui Yu", "Ryan Po", "Guandao Yang", "Gordon Wetztein", "Hongyi Wen"]}
]]></acme>

<pubDate>2025-10-21T09:08:01+00:00</pubDate>
</item>
<item>
<title>ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters</title>
<link>https://papers.cool/arxiv/2510.18431</link>
<guid>https://papers.cool/arxiv/2510.18431</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ScaleNet proposes an efficient method for scaling pretrained vision transformers by inserting additional layers that share weights with the original model and adding small parallel adapter modules as adjustment parameters. This allows depth expansion with negligible parameter increase and faster training, achieving a 7.42% accuracy gain on ImageNet-1K with only a third of the epochs compared to training from scratch. The approach also shows promise for downstream tasks such as object detection.<br /><strong>Summary (CN):</strong> ScaleNet 提出了一种高效的 Vision Transformer（ViT）扩展方法，通过在预训练模型中插入共享权重的额外层，并为每层添加小规模并行适配器（adapter）作为调整参数，实现了增量参数的深度扩展。该方法在保持参数几乎不增的情况下，仅使用三分之一的训练轮数即可在 ImageNet-1K 上实现 7.42% 的准确率提升，并在目标检测等下游任务中展现出潜力。<br /><strong>Keywords:</strong> ScaleNet, vision transformer scaling, incremental parameters, weight sharing, adapter modules, efficient training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</div>
Recent advancements in vision transformers (ViTs) have demonstrated that larger models often achieve superior performance. However, training these models remains computationally intensive and costly. To address this challenge, we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike conventional training from scratch, ScaleNet facilitates rapid model expansion with negligible increases in parameters, building on existing pretrained models. This offers a cost-effective solution for scaling up ViTs. Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. Each added layer shares its parameter tensor with a corresponding layer from the pretrained model. To mitigate potential performance degradation due to shared weights, ScaleNet introduces a small set of adjustment parameters for each layer. These adjustment parameters are implemented through parallel adapter modules, ensuring that each instance of the shared parameter tensor remains distinct and optimized for its specific function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet enables efficient expansion of ViT models. With a 2$\times$ depth-scaled DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs, highlighting its efficiency in scaling ViTs. Beyond image classification, our method shows significant potential for application in downstream vision areas, as evidenced by the validation in object detection task.
<div><strong>Authors:</strong> Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ScaleNet proposes an efficient method for scaling pretrained vision transformers by inserting additional layers that share weights with the original model and adding small parallel adapter modules as adjustment parameters. This allows depth expansion with negligible parameter increase and faster training, achieving a 7.42% accuracy gain on ImageNet-1K with only a third of the epochs compared to training from scratch. The approach also shows promise for downstream tasks such as object detection.", "summary_cn": "ScaleNet 提出了一种高效的 Vision Transformer（ViT）扩展方法，通过在预训练模型中插入共享权重的额外层，并为每层添加小规模并行适配器（adapter）作为调整参数，实现了增量参数的深度扩展。该方法在保持参数几乎不增的情况下，仅使用三分之一的训练轮数即可在 ImageNet-1K 上实现 7.42% 的准确率提升，并在目标检测等下游任务中展现出潜力。", "keywords": "ScaleNet, vision transformer scaling, incremental parameters, weight sharing, adapter modules, efficient training", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhiwei Hao", "Jianyuan Guo", "Li Shen", "Kai Han", "Yehui Tang", "Han Hu", "Yunhe Wang"]}
]]></acme>

<pubDate>2025-10-21T09:07:25+00:00</pubDate>
</item>
<item>
<title>Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</title>
<link>https://papers.cool/arxiv/2510.18405</link>
<guid>https://papers.cool/arxiv/2510.18405</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an automated system for analyzing cricket videos that combines YOLOv8-based pitch and ball detection with OCR-driven scorecard extraction to identify wicket‑taking deliveries. It includes extensive image preprocessing and achieves high detection performance (99.5% mAP50 for pitch, 99.18% mAP50 for ball) and uses trajectory modeling to reveal batting weaknesses, aiming to support coaching and strategic decisions.<br /><strong>Summary (CN):</strong> 本文提出一种自动化的板球视频分析系统，结合基于 YOLOv8 的投球区和球体检测以及 OCR 驱动的记分卡提取，以识别夺门（wicket）投球。系统通过灰度变换、幂变换和形态学操作等预处理，实现了高检测精度（投球区 99.5% mAP50，球体 99.18% mAP50），并利用轨迹建模提供击球弱点分析，以辅助教练和战术决策。<br /><strong>Keywords:</strong> cricket video analysis, YOLOv8, OCR, trajectory modeling, deep learning, object detection, sports analytics, ball detection, pitch detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 2, Safety: 1, Technicality: 6, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe</div>
This paper presents an automated system for cricket video analysis that leverages deep learning techniques to extract wicket-taking deliveries, detect cricket balls, and model ball trajectories. The system employs the YOLOv8 architecture for pitch and ball detection, combined with optical character recognition (OCR) for scorecard extraction to identify wicket-taking moments. Through comprehensive image preprocessing, including grayscale transformation, power transformation, and morphological operations, the system achieves robust text extraction from video frames. The pitch detection model achieved 99.5% mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while the ball detection model using transfer learning attained 99.18% mAP50 with 0.968 precision and 0.978 recall. The system enables trajectory modeling on detected pitches, providing data-driven insights for identifying batting weaknesses. Experimental results on multiple cricket match videos demonstrate the effectiveness of this approach for automated cricket analytics, offering significant potential for coaching and strategic decision-making.
<div><strong>Authors:</strong> Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an automated system for analyzing cricket videos that combines YOLOv8-based pitch and ball detection with OCR-driven scorecard extraction to identify wicket‑taking deliveries. It includes extensive image preprocessing and achieves high detection performance (99.5% mAP50 for pitch, 99.18% mAP50 for ball) and uses trajectory modeling to reveal batting weaknesses, aiming to support coaching and strategic decisions.", "summary_cn": "本文提出一种自动化的板球视频分析系统，结合基于 YOLOv8 的投球区和球体检测以及 OCR 驱动的记分卡提取，以识别夺门（wicket）投球。系统通过灰度变换、幂变换和形态学操作等预处理，实现了高检测精度（投球区 99.5% mAP50，球体 99.18% mAP50），并利用轨迹建模提供击球弱点分析，以辅助教练和战术决策。", "keywords": "cricket video analysis, YOLOv8, OCR, trajectory modeling, deep learning, object detection, sports analytics, ball detection, pitch detection", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mst Jannatun Ferdous", "Masum Billah", "Joy Karmoker", "Mohd Ruhul Ameen", "Akif Islam", "Md. Omar Faruqe"]}
]]></acme>

<pubDate>2025-10-21T08:27:23+00:00</pubDate>
</item>
<item>
<title>Bayesian Fully-Connected Tensor Network for Hyperspectral-Multispectral Image Fusion</title>
<link>https://papers.cool/arxiv/2510.18400</link>
<guid>https://papers.cool/arxiv/2510.18400</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Bayesian Fully-Connected Tensor Network (BFCTN) for hyperspectral‑multispectral image fusion. By imposing a hierarchical sparse prior within a probabilistic framework and employing variational Bayesian inference combined with EM, the method preserves spatial‑spectral structures, reduces manual tuning, and improves robustness to noise. Experiments show state‑of‑the‑art accuracy and practical applicability.<br /><strong>Summary (CN):</strong> 本文提出了用于高光谱‑多光谱图像融合的贝叶斯全连接张量网络（BFCTN）。通过在概率框架中引入层次稀疏先验，并结合变分贝叶斯推断和 EM 算法，能够保留空间‑光谱结构、降低手动参数调节并提升对噪声的鲁棒性。实验表明该方法达到了先进的融合精度并具备实际应用潜力。<br /><strong>Keywords:</strong> hyperspectral image fusion, multispectral fusion, Bayesian tensor network, Fully-Connected Tensor Network, variational Bayesian inference, EM algorithm, tensor decomposition, sparse prior<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Linsong Shan, Zecan Yang, Laurence T. Yang, Changlong Li, Honglu Zhao, Xin Nie</div>
Tensor decomposition is a powerful tool for data analysis and has been extensively employed in the field of hyperspectral-multispectral image fusion (HMF). Existing tensor decomposition-based fusion methods typically rely on disruptive data vectorization/reshaping or impose rigid constraints on the arrangement of factor tensors, hindering the preservation of spatial-spectral structures and the modeling of cross-dimensional correlations. Although recent advances utilizing the Fully-Connected Tensor Network (FCTN) decomposition have partially alleviated these limitations, the process of reorganizing data into higher-order tensors still disrupts the intrinsic spatial-spectral structure. Furthermore, these methods necessitate extensive manual parameter tuning and exhibit limited robustness against noise and spatial degradation. To alleviate these issues, we propose the Bayesian FCTN (BFCTN) method. Within this probabilistic framework, a hierarchical sparse prior that characterizing the sparsity of physical elements, establishes connections between the factor tensors. This framework explicitly models the intrinsic physical coupling among spatial structures, spectral signatures, and local scene homogeneity. For model learning, we develop a parameter estimation method based on Variational Bayesian inference (VB) and the Expectation-Maximization (EM) algorithm, which significantly reduces the need for manual parameter tuning. Extensive experiments demonstrate that BFCTN not only achieves state-of-the-art fusion accuracy and strong robustness but also exhibits practical applicability in complex real-world scenarios.
<div><strong>Authors:</strong> Linsong Shan, Zecan Yang, Laurence T. Yang, Changlong Li, Honglu Zhao, Xin Nie</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Bayesian Fully-Connected Tensor Network (BFCTN) for hyperspectral‑multispectral image fusion. By imposing a hierarchical sparse prior within a probabilistic framework and employing variational Bayesian inference combined with EM, the method preserves spatial‑spectral structures, reduces manual tuning, and improves robustness to noise. Experiments show state‑of‑the‑art accuracy and practical applicability.", "summary_cn": "本文提出了用于高光谱‑多光谱图像融合的贝叶斯全连接张量网络（BFCTN）。通过在概率框架中引入层次稀疏先验，并结合变分贝叶斯推断和 EM 算法，能够保留空间‑光谱结构、降低手动参数调节并提升对噪声的鲁棒性。实验表明该方法达到了先进的融合精度并具备实际应用潜力。", "keywords": "hyperspectral image fusion, multispectral fusion, Bayesian tensor network, Fully-Connected Tensor Network, variational Bayesian inference, EM algorithm, tensor decomposition, sparse prior", "scoring": {"interpretability": 2, "understanding": 7, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Linsong Shan", "Zecan Yang", "Laurence T. Yang", "Changlong Li", "Honglu Zhao", "Xin Nie"]}
]]></acme>

<pubDate>2025-10-21T08:19:54+00:00</pubDate>
</item>
<item>
<title>Entropy-Enhanced Conformal Features from Ricci Flow for Robust Alzheimer's Disease Classification</title>
<link>https://papers.cool/arxiv/2510.18396</link>
<guid>https://papers.cool/arxiv/2510.18396</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes a novel method that uses Ricci flow to compute area distortion and conformal factors on cortical surface meshes, then applies Shannon entropy to the resulting geometric features to create compact descriptors for classifying Alzheimer's disease from MRI scans. The approach achieves high accuracy (≈98.6%) with several classifiers, notably MLP and Logistic Regression, and demonstrates statistical significance over other methods.<br /><strong>Summary (CN):</strong> 本文采用 Ricci 流算法对皮层表面网格进行面积畸变和共形因子计算，并结合 Shannon 熵对几何特征进行压缩，进而用于 Alzheimer's AD 患者的 MRI 诊断分类，表现出约98.6%的高准确率。<br /><strong>Keywords:</strong> Alzheimer's disease, cortical morphometry, Ricci flow, conformal features, Shannon entropy, MRI classification, robust machine learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> F. Ahmadi, B. Bidabad, H. Nasiri</div>
Background and Objective: In brain imaging, geometric surface models are essential for analyzing the 3D shapes of anatomical structures. Alzheimer's disease (AD) is associated with significant cortical atrophy, making such shape analysis a valuable diagnostic tool. The objective of this study is to introduce and validate a novel local surface representation method for the automated and accurate diagnosis of AD. Methods: The study utilizes T1-weighted MRI scans from 160 participants (80 AD patients and 80 healthy controls) from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Cortical surface models were reconstructed from the MRI data using Freesurfer. Key geometric attributes were computed from the 3D meshes. Area distortion and conformal factor were derived using Ricci flow for conformal parameterization, while Gaussian curvature was calculated directly from the mesh geometry. Shannon entropy was applied to these three features to create compact and informative feature vectors. The feature vectors were used to train and evaluate a suite of classifiers (e.g. XGBoost, MLP, Logistic Regression, etc.). Results: Statistical significance of performance differences between classifiers was evaluated using paired Welch's t-test. The method proved highly effective in distinguishing AD patients from healthy controls. The Multi-Layer Perceptron (MLP) and Logistic Regression classifiers outperformed all others, achieving an accuracy and F$_1$ Score of 98.62%. Conclusions: This study confirms that the entropy of conformally-derived geometric features provides a powerful and robust metric for cortical morphometry. The high classification accuracy underscores the method's potential to enhance the study and diagnosis of Alzheimer's disease, offering a straightforward yet powerful tool for clinical research applications.
<div><strong>Authors:</strong> F. Ahmadi, B. Bidabad, H. Nasiri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes a novel method that uses Ricci flow to compute area distortion and conformal factors on cortical surface meshes, then applies Shannon entropy to the resulting geometric features to create compact descriptors for classifying Alzheimer's disease from MRI scans. The approach achieves high accuracy (≈98.6%) with several classifiers, notably MLP and Logistic Regression, and demonstrates statistical significance over other methods.", "summary_cn": "本文采用 Ricci 流算法对皮层表面网格进行面积畸变和共形因子计算，并结合 Shannon 熵对几何特征进行压缩，进而用于 Alzheimer's AD 患者的 MRI 诊断分类，表现出约98.6%的高准确率。", "keywords": "Alzheimer's disease, cortical morphometry, Ricci flow, conformal features, Shannon entropy, MRI classification, robust machine learning", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["F. Ahmadi", "B. Bidabad", "H. Nasiri"]}
]]></acme>

<pubDate>2025-10-21T08:16:45+00:00</pubDate>
</item>
<item>
<title>S2AP: Score-space Sharpness Minimization for Adversarial Pruning</title>
<link>https://papers.cool/arxiv/2510.18381</link>
<guid>https://papers.cool/arxiv/2510.18381</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug‑in technique that reduces sharpness in the importance‑score landscape during mask selection, thereby stabilizing binary pruning masks and improving the adversarial robustness of compressed neural networks. Experiments on multiple datasets, architectures, and sparsity levels show that S2AP consistently enhances robustness compared to existing adversarial pruning pipelines.<br /><strong>Summary (CN):</strong> 本文提出了“Score-space Sharpness-aware Adversarial Pruning (S2AP)” 方法，通过在掩码搜索阶段对重要性分数进行扰动并最小化对应的鲁棒损失，降低分数空间的尖锐度，从而稳定二值化剪枝掩码并提升压缩后模型的对抗鲁棒性。大量实验在不同数据集、模型和稀疏度下验证了 S2AP 相较于传统对抗剪枝方法在鲁棒性方面的显著提升。<br /><strong>Keywords:</strong> adversarial pruning, score-space sharpness, robustness, model compression, mask selection, sharpness minimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
Adversarial pruning methods have emerged as a powerful tool for compressing neural networks while preserving robustness against adversarial attacks. These methods typically follow a three-step pipeline: (i) pretrain a robust model, (ii) select a binary mask for weight pruning, and (iii) finetune the pruned model. To select the binary mask, these methods minimize a robust loss by assigning an importance score to each weight, and then keep the weights with the highest scores. However, this score-space optimization can lead to sharp local minima in the robust loss landscape and, in turn, to an unstable mask selection, reducing the robustness of adversarial pruning methods. To overcome this issue, we propose a novel plug-in method for adversarial pruning, termed Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we introduce the concept of score-space sharpness minimization, which operates during the mask search by perturbing importance scores and minimizing the corresponding robust loss. Extensive experiments across various datasets, models, and sparsity levels demonstrate that S2AP effectively minimizes sharpness in score space, stabilizing the mask selection, and ultimately improving the robustness of adversarial pruning methods.
<div><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug‑in technique that reduces sharpness in the importance‑score landscape during mask selection, thereby stabilizing binary pruning masks and improving the adversarial robustness of compressed neural networks. Experiments on multiple datasets, architectures, and sparsity levels show that S2AP consistently enhances robustness compared to existing adversarial pruning pipelines.", "summary_cn": "本文提出了“Score-space Sharpness-aware Adversarial Pruning (S2AP)” 方法，通过在掩码搜索阶段对重要性分数进行扰动并最小化对应的鲁棒损失，降低分数空间的尖锐度，从而稳定二值化剪枝掩码并提升压缩后模型的对抗鲁棒性。大量实验在不同数据集、模型和稀疏度下验证了 S2AP 相较于传统对抗剪枝方法在鲁棒性方面的显著提升。", "keywords": "adversarial pruning, score-space sharpness, robustness, model compression, mask selection, sharpness minimization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Giorgio Piras", "Qi Zhao", "Fabio Brau", "Maura Pintor", "Christian Wressnegger", "Battista Biggio"]}
]]></acme>

<pubDate>2025-10-21T07:55:31+00:00</pubDate>
</item>
<item>
<title>Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</title>
<link>https://papers.cool/arxiv/2510.18377</link>
<guid>https://papers.cool/arxiv/2510.18377</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Cross-Modal Scene Semantic Alignment (CM-SSA), a method that leverages image‑text semantic alignment to improve image complexity assessment. CM-SSA consists of a complexity regression branch guided by a scene semantic alignment branch that learns pairwise alignment between images and textual scene descriptions. Experiments on multiple ICA datasets show that CM-SSA outperforms existing state‑of‑the‑art approaches.<br /><strong>Summary (CN):</strong> 本文提出了跨模态场景语义对齐 (CM-SSA) 方法，利用图像与文本描述之间的语义对齐来提升图像复杂度评估的效果。CM-SSA 包含一个在场景语义对齐分支指导下的复杂度回归分支，该分支通过成对学习将图像与富含场景信息的文本提示对齐。大量实验表明，该方法在多个 ICA 数据集上显著优于现有的最先进技术。<br /><strong>Keywords:</strong> image complexity assessment, cross-modal alignment, scene semantics, multimodal learning, regression, text prompts<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuqing Luo, Yixiao Li, Jiang Liu, Jun Fu, Hadi Amirpour, Guanghui Yue, Baoquan Zhao, Padraig Corcoran, Hantao Liu, Wei Zhou</div>
Image complexity assessment (ICA) is a challenging task in perceptual evaluation due to the subjective nature of human perception and the inherent semantic diversity in real-world images. Existing ICA methods predominantly rely on hand-crafted or shallow convolutional neural network-based features of a single visual modality, which are insufficient to fully capture the perceived representations closely related to image complexity. Recently, cross-modal scene semantic information has been shown to play a crucial role in various computer vision tasks, particularly those involving perceptual understanding. However, the exploration of cross-modal scene semantic information in the context of ICA remains unaddressed. Therefore, in this paper, we propose a novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which leverages scene semantic alignment from a cross-modal perspective to enhance ICA performance, enabling complexity predictions to be more consistent with subjective human perception. Specifically, the proposed CM-SSA consists of a complexity regression branch and a scene semantic alignment branch. The complexity regression branch estimates image complexity levels under the guidance of the scene semantic alignment branch, while the scene semantic alignment branch is used to align images with corresponding text prompts that convey rich scene semantic information by pair-wise learning. Extensive experiments on several ICA datasets demonstrate that the proposed CM-SSA significantly outperforms state-of-the-art approaches. Codes are available at https://github.com/XQ2K/First-Cross-Model-ICA.
<div><strong>Authors:</strong> Yuqing Luo, Yixiao Li, Jiang Liu, Jun Fu, Hadi Amirpour, Guanghui Yue, Baoquan Zhao, Padraig Corcoran, Hantao Liu, Wei Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Cross-Modal Scene Semantic Alignment (CM-SSA), a method that leverages image‑text semantic alignment to improve image complexity assessment. CM-SSA consists of a complexity regression branch guided by a scene semantic alignment branch that learns pairwise alignment between images and textual scene descriptions. Experiments on multiple ICA datasets show that CM-SSA outperforms existing state‑of‑the‑art approaches.", "summary_cn": "本文提出了跨模态场景语义对齐 (CM-SSA) 方法，利用图像与文本描述之间的语义对齐来提升图像复杂度评估的效果。CM-SSA 包含一个在场景语义对齐分支指导下的复杂度回归分支，该分支通过成对学习将图像与富含场景信息的文本提示对齐。大量实验表明，该方法在多个 ICA 数据集上显著优于现有的最先进技术。", "keywords": "image complexity assessment, cross-modal alignment, scene semantics, multimodal learning, regression, text prompts", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuqing Luo", "Yixiao Li", "Jiang Liu", "Jun Fu", "Hadi Amirpour", "Guanghui Yue", "Baoquan Zhao", "Padraig Corcoran", "Hantao Liu", "Wei Zhou"]}
]]></acme>

<pubDate>2025-10-21T07:52:40+00:00</pubDate>
</item>
<item>
<title>FeatureFool: Zero-Query Fooling of Video Models via Feature Map</title>
<link>https://papers.cool/arxiv/2510.18362</link>
<guid>https://papers.cool/arxiv/2510.18362</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FeatureFool, a zero‑query black‑box adversarial attack for video models that leverages extracted feature maps to shift the feature space of clean videos without any queries to the target model. Experiments demonstrate over 70% success against traditional video classifiers and effective transfer to Video‑LLMs while maintaining high visual quality. This approach reveals a new vulnerability in video‑based AI systems.<br /><strong>Summary (CN):</strong> 本文提出 FeatureFool，一种零查询（zero‑query）黑盒对抗攻击，利用深度网络提取的特征图（feature map）直接改变干净视频的特征空间，无需向目标模型发送查询。实验显示对传统视频分类器的攻击成功率超过 70%，且可迁移至 Video‑LLM，同时保持高视觉质量。该方法揭示了视频模型在安全性方面的新脆弱性。<br /><strong>Keywords:</strong> zero-query attack, video adversarial attack, feature map, black-box attack, video classifiers, Video-LLM, robustness, adversarial perturbation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Duoxun Tang, Xi Xiao, Guangwu Hu, Kangkang Sun, Xiao Yang, Dongyang Chen, Qing Li, Yongjie Yin, Jiyao Wang</div>
The vulnerability of deep neural networks (DNNs) has been preliminarily verified. Existing black-box adversarial attacks usually require multi-round interaction with the model and consume numerous queries, which is impractical in the real-world and hard to scale to recently emerged Video-LLMs. Moreover, no attack in the video domain directly leverages feature maps to shift the clean-video feature space. We therefore propose FeatureFool, a stealthy, video-domain, zero-query black-box attack that utilizes information extracted from a DNN to alter the feature space of clean videos. Unlike query-based methods that rely on iterative interaction, FeatureFool performs a zero-query attack by directly exploiting DNN-extracted information. This efficient approach is unprecedented in the video domain. Experiments show that FeatureFool achieves an attack success rate above 70\% against traditional video classifiers without any queries. Benefiting from the transferability of the feature map, it can also craft harmful content and bypass Video-LLM recognition. Additionally, adversarial videos generated by FeatureFool exhibit high quality in terms of SSIM, PSNR, and Temporal-Inconsistency, making the attack barely perceptible. This paper may contain violent or explicit content.
<div><strong>Authors:</strong> Duoxun Tang, Xi Xiao, Guangwu Hu, Kangkang Sun, Xiao Yang, Dongyang Chen, Qing Li, Yongjie Yin, Jiyao Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FeatureFool, a zero‑query black‑box adversarial attack for video models that leverages extracted feature maps to shift the feature space of clean videos without any queries to the target model. Experiments demonstrate over 70% success against traditional video classifiers and effective transfer to Video‑LLMs while maintaining high visual quality. This approach reveals a new vulnerability in video‑based AI systems.", "summary_cn": "本文提出 FeatureFool，一种零查询（zero‑query）黑盒对抗攻击，利用深度网络提取的特征图（feature map）直接改变干净视频的特征空间，无需向目标模型发送查询。实验显示对传统视频分类器的攻击成功率超过 70%，且可迁移至 Video‑LLM，同时保持高视觉质量。该方法揭示了视频模型在安全性方面的新脆弱性。", "keywords": "zero-query attack, video adversarial attack, feature map, black-box attack, video classifiers, Video-LLM, robustness, adversarial perturbation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Duoxun Tang", "Xi Xiao", "Guangwu Hu", "Kangkang Sun", "Xiao Yang", "Dongyang Chen", "Qing Li", "Yongjie Yin", "Jiyao Wang"]}
]]></acme>

<pubDate>2025-10-21T07:33:35+00:00</pubDate>
</item>
<item>
<title>Learning Human-Object Interaction as Groups</title>
<link>https://papers.cool/arxiv/2510.18357</link>
<guid>https://papers.cool/arxiv/2510.18357</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GroupHOI, a framework for Human-Object Interaction detection that groups humans and objects based on geometric proximity and semantic similarity, using a learnable proximity estimator and self‑attention within each group to aggregate contextual cues. It enhances a transformer‑based interaction decoder with local HO‑pair features and demonstrates state‑of‑the‑art performance on HICO‑DET, V‑COCO, and the challenging Nonverbal Interaction Detection task. By shifting from pairwise to group‑wise relation modeling, the work addresses higher‑order interactions in real‑world scenes.<br /><strong>Summary (CN):</strong> 本文提出 GroupHOI 框架，用于人‑物交互检测，通过基于空间特征的可学习接近度估计器将人和物体按几何接近度与语义相似性分组，并在每个组内使用自注意力聚合上下文信息。该方法在 Transformer 交互解码器中加入局部 HO‑pair 特征，显著提升了 HICO‑DET、V‑COCO 以及更具挑战性的非语言交互检测（NVI‑DET）任务的性能。通过从点对关系转向组级建模，论文解决了真实场景中更高阶的交互行为。<br /><strong>Keywords:</strong> human-object interaction, group modeling, transformer, proximity estimator, HOI detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiajun Hong, Jianan Wei, Wenguan Wang</div>
Human-Object Interaction Detection (HOI-DET) aims to localize human-object pairs and identify their interactive relationships. To aggregate contextual cues, existing methods typically propagate information across all detected entities via self-attention mechanisms, or establish message passing between humans and objects with bipartite graphs. However, they primarily focus on pairwise relationships, overlooking that interactions in real-world scenarios often emerge from collective behaviors (multiple humans and objects engaging in joint activities). In light of this, we revisit relation modeling from a group view and propose GroupHOI, a framework that propagates contextual information in terms of geometric proximity and semantic similarity. To exploit the geometric proximity, humans and objects are grouped into distinct clusters using a learnable proximity estimator based on spatial features derived from bounding boxes. In each group, a soft correspondence is computed via self-attention to aggregate and dispatch contextual cues. To incorporate the semantic similarity, we enhance the vanilla transformer-based interaction decoder with local contextual cues from HO-pair features. Extensive experiments on HICO-DET and V-COCO benchmarks demonstrate the superiority of GroupHOI over the state-of-the-art methods. It also exhibits leading performance on the more challenging Nonverbal Interaction Detection (NVI-DET) task, which involves varied forms of higher-order interactions within groups.
<div><strong>Authors:</strong> Jiajun Hong, Jianan Wei, Wenguan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GroupHOI, a framework for Human-Object Interaction detection that groups humans and objects based on geometric proximity and semantic similarity, using a learnable proximity estimator and self‑attention within each group to aggregate contextual cues. It enhances a transformer‑based interaction decoder with local HO‑pair features and demonstrates state‑of‑the‑art performance on HICO‑DET, V‑COCO, and the challenging Nonverbal Interaction Detection task. By shifting from pairwise to group‑wise relation modeling, the work addresses higher‑order interactions in real‑world scenes.", "summary_cn": "本文提出 GroupHOI 框架，用于人‑物交互检测，通过基于空间特征的可学习接近度估计器将人和物体按几何接近度与语义相似性分组，并在每个组内使用自注意力聚合上下文信息。该方法在 Transformer 交互解码器中加入局部 HO‑pair 特征，显著提升了 HICO‑DET、V‑COCO 以及更具挑战性的非语言交互检测（NVI‑DET）任务的性能。通过从点对关系转向组级建模，论文解决了真实场景中更高阶的交互行为。", "keywords": "human-object interaction, group modeling, transformer, proximity estimator, HOI detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiajun Hong", "Jianan Wei", "Wenguan Wang"]}
]]></acme>

<pubDate>2025-10-21T07:25:10+00:00</pubDate>
</item>
<item>
<title>Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</title>
<link>https://papers.cool/arxiv/2510.18353</link>
<guid>https://papers.cool/arxiv/2510.18353</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Diffusion Denoising Ranking Optimization (Diffusion-DRO), a preference learning framework for text-to-image diffusion models that casts human feedback as a ranking problem, removing the need for a reward model and integrating offline expert demonstrations with online policy-generated negative samples. This denoising-based objective simplifies training and mitigates the non-linear probability estimation issues of prior DPO methods. Experiments demonstrate improved generation quality across diverse and unseen prompts, outperforming state-of-the-art baselines in quantitative metrics and user studies.<br /><strong>Summary (CN):</strong> 本文提出 Diffusion Denoising Ranking Optimization（Diffusion-DRO），一种将人类偏好视为排序问题的扩散模型偏好学习框架，摆脱了奖励模型的依赖，并将离线专家示范与在线策略生成的负样本结合。该去噪式目标简化了训练并解决了之前 DPO 方法中的非线性概率估计问题。实验显示在各种挑战性和未见提示上，Diffusion-DRO 能提升生成质量，超越了最新基准的量化指标和用户研究结果。<br /><strong>Keywords:</strong> diffusion models, preference learning, ranking optimization, inverse reinforcement learning, DPO, alignment, offline demonstrations, denoising, human feedback, generative AI<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</div>
Direct preference optimization (DPO) methods have shown strong potential in aligning text-to-image diffusion models with human preferences by training on paired comparisons. These methods improve training stability by avoiding the REINFORCE algorithm but still struggle with challenges such as accurately estimating image probabilities due to the non-linear nature of the sigmoid function and the limited diversity of offline datasets. In this paper, we introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new preference learning framework grounded in inverse reinforcement learning. Diffusion-DRO removes the dependency on a reward model by casting preference learning as a ranking problem, thereby simplifying the training objective into a denoising formulation and overcoming the non-linear estimation issues found in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert demonstrations with online policy-generated negative samples, enabling it to effectively capture human preferences while addressing the limitations of offline data. Comprehensive experiments show that Diffusion-DRO delivers improved generation quality across a range of challenging and unseen prompts, outperforming state-of-the-art baselines in both both quantitative metrics and user studies. Our source code and pre-trained models are available at https://github.com/basiclab/DiffusionDRO.
<div><strong>Authors:</strong> Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Diffusion Denoising Ranking Optimization (Diffusion-DRO), a preference learning framework for text-to-image diffusion models that casts human feedback as a ranking problem, removing the need for a reward model and integrating offline expert demonstrations with online policy-generated negative samples. This denoising-based objective simplifies training and mitigates the non-linear probability estimation issues of prior DPO methods. Experiments demonstrate improved generation quality across diverse and unseen prompts, outperforming state-of-the-art baselines in quantitative metrics and user studies.", "summary_cn": "本文提出 Diffusion Denoising Ranking Optimization（Diffusion-DRO），一种将人类偏好视为排序问题的扩散模型偏好学习框架，摆脱了奖励模型的依赖，并将离线专家示范与在线策略生成的负样本结合。该去噪式目标简化了训练并解决了之前 DPO 方法中的非线性概率估计问题。实验显示在各种挑战性和未见提示上，Diffusion-DRO 能提升生成质量，超越了最新基准的量化指标和用户研究结果。", "keywords": "diffusion models, preference learning, ranking optimization, inverse reinforcement learning, DPO, alignment, offline demonstrations, denoising, human feedback, generative AI", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yi-Lun Wu", "Bo-Kai Ruan", "Chiang Tseng", "Hong-Han Shuai"]}
]]></acme>

<pubDate>2025-10-21T07:22:34+00:00</pubDate>
</item>
<item>
<title>AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering</title>
<link>https://papers.cool/arxiv/2510.18346</link>
<guid>https://papers.cool/arxiv/2510.18346</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AV-Master introduces a dual-path framework for audio-visual question answering that dynamically adapts temporal sampling and modality preference to focus on question-relevant segments, using a dynamic adaptive focus sampling mechanism and a preference-aware strategy. A dual-path contrastive loss further enforces consistency across temporal and modality dimensions, yielding state-of-the-art performance on multiple benchmarks, especially for complex reasoning tasks.<br /><strong>Summary (CN):</strong> AV-Master 提出了一个双路径框架用于音视听问答，通过动态自适应聚焦采样机制在时间维度上聚焦与问题相关的音视频片段，并在模态维度上采用偏好感知策略独立建模每种模态的贡献，从而实现关键特征的选择激活。双路径对比损失进一步强化时间和模态维度间的一致性和互补性，使模型在多个大型基准上，尤其是复杂推理任务中显著超越现有方法。<br /><strong>Keywords:</strong> audio-visual question answering, dynamic adaptive focus sampling, modality preference awareness, dual-path contrastive loss, cross-modal representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiayu Zhang, Qilang Ye, Shuo Ye, Xun Lin, Zihan Song, Zitong Yu</div>
Audio-Visual Question Answering (AVQA) requires models to effectively utilize both visual and auditory modalities to answer complex and diverse questions about audio-visual scenes. However, existing methods lack sufficient flexibility and dynamic adaptability in temporal sampling and modality preference awareness, making it difficult to focus on key information based on the question. This limits their reasoning capability in complex scenarios. To address these challenges, we propose a novel framework named AV-Master. It enhances the model's ability to extract key information from complex audio-visual scenes with substantial redundant content by dynamically modeling both temporal and modality dimensions. In the temporal dimension, we introduce a dynamic adaptive focus sampling mechanism that progressively focuses on audio-visual segments most relevant to the question, effectively mitigating redundancy and segment fragmentation in traditional sampling methods. In the modality dimension, we propose a preference-aware strategy that models each modality's contribution independently, enabling selective activation of critical features. Furthermore, we introduce a dual-path contrastive loss to reinforce consistency and complementarity across temporal and modality dimensions, guiding the model to learn question-specific cross-modal collaborative representations. Experiments on four large-scale benchmarks show that AV-Master significantly outperforms existing methods, especially in complex reasoning tasks.
<div><strong>Authors:</strong> Jiayu Zhang, Qilang Ye, Shuo Ye, Xun Lin, Zihan Song, Zitong Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AV-Master introduces a dual-path framework for audio-visual question answering that dynamically adapts temporal sampling and modality preference to focus on question-relevant segments, using a dynamic adaptive focus sampling mechanism and a preference-aware strategy. A dual-path contrastive loss further enforces consistency across temporal and modality dimensions, yielding state-of-the-art performance on multiple benchmarks, especially for complex reasoning tasks.", "summary_cn": "AV-Master 提出了一个双路径框架用于音视听问答，通过动态自适应聚焦采样机制在时间维度上聚焦与问题相关的音视频片段，并在模态维度上采用偏好感知策略独立建模每种模态的贡献，从而实现关键特征的选择激活。双路径对比损失进一步强化时间和模态维度间的一致性和互补性，使模型在多个大型基准上，尤其是复杂推理任务中显著超越现有方法。", "keywords": "audio-visual question answering, dynamic adaptive focus sampling, modality preference awareness, dual-path contrastive loss, cross-modal representation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiayu Zhang", "Qilang Ye", "Shuo Ye", "Xun Lin", "Zihan Song", "Zitong Yu"]}
]]></acme>

<pubDate>2025-10-21T06:58:34+00:00</pubDate>
</item>
<item>
<title>GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data</title>
<link>https://papers.cool/arxiv/2510.18345</link>
<guid>https://papers.cool/arxiv/2510.18345</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GPTFace, a generative pre‑training framework that jointly learns facial visual and linguistic representations from large‑scale weakly paired web images and text using span masking and image‑text matching objectives. The pretrained model can be used for various downstream facial tasks such as attribute classification, expression recognition, and diverse face‑editing operations, achieving performance comparable to state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> 本文提出 GPTFace，一种生成式预训练框架，通过跨度掩码和图文匹配任务，从大规模弱相关的网络人脸图像与文本中学习面部视觉‑语言表征。该模型在属性分类、表情识别以及多种人脸编辑任务上表现与最先进方法相当。<br /><strong>Keywords:</strong> facial representation, multimodal pretraining, span masking, image-text matching, generative transformer, face editing<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</div>
Compared to the prosperity of pre-training models in natural image understanding, the research on large-scale pre-training models for facial knowledge learning is still limited. Current approaches mainly rely on manually assembled and annotated face datasets for training, but labeling such datasets is labor-intensive and the trained models have limited scalability beyond the training data. To address these limitations, we present a generative pre-training model for facial knowledge learning that leverages large-scale web-built data for training. We use texts and images containing human faces crawled from the internet and conduct pre-training on self-supervised tasks, including masked image/language modeling (MILM) and image-text matching (ITM). During the generation stage, we further utilize the image-text matching loss to pull the generation distribution towards the control signal for controllable image/text generation. Experimental results demonstrate that our model achieves comparable performance to state-of-the-art pre-training models for various facial downstream tasks, such as attribution classification and expression recognition. Furthermore, our approach is also applicable to a wide range of face editing tasks, including face attribute editing, expression manipulation, mask removal, and photo inpainting.
<div><strong>Authors:</strong> Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GPTFace, a generative pre‑training framework that jointly learns facial visual and linguistic representations from large‑scale weakly paired web images and text using span masking and image‑text matching objectives. The pretrained model can be used for various downstream facial tasks such as attribute classification, expression recognition, and diverse face‑editing operations, achieving performance comparable to state‑of‑the‑art methods.", "summary_cn": "本文提出 GPTFace，一种生成式预训练框架，通过跨度掩码和图文匹配任务，从大规模弱相关的网络人脸图像与文本中学习面部视觉‑语言表征。该模型在属性分类、表情识别以及多种人脸编辑任务上表现与最先进方法相当。", "keywords": "facial representation, multimodal pretraining, span masking, image-text matching, generative transformer, face editing", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yudong Li", "Hao Li", "Xianxu Hou", "Linlin Shen"]}
]]></acme>

<pubDate>2025-10-21T06:55:44+00:00</pubDate>
</item>
<item>
<title>ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</title>
<link>https://papers.cool/arxiv/2510.18341</link>
<guid>https://papers.cool/arxiv/2510.18341</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ViSE, a four‑stage pipeline for realistic vision‑only street‑view extrapolation aimed at closed‑loop autonomous‑driving simulation. It combines data‑driven pseudo‑LiDAR initialization, a dimension‑reduced 2D‑SDF road surface model, generative priors for pseudo ground‑truth views, and an adaptation network to remove temporal artifacts, achieving state‑of‑the‑art performance on the RealADSim‑NVS benchmark.<br /><strong>Summary (CN):</strong> 本文提出 ViSE 系统，采用四阶段管线实现仅视觉的街景外推，用于自动驾驶闭环仿真。方法包括数据驱动的伪 LiDAR 初始化、用于道路表面的 2D‑SDF 几何先验、生成式伪真值视图以及去除时间伪影的自适应网络，在 RealADSim‑NVS 基准上取得领先.<br /><strong>Keywords:</strong> vision-only extrapolation, novel view synthesis, pseudo-LiDAR, 2D-SDF, generative prior, autonomous driving simulation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</div>
Realistic view extrapolation is critical for closed-loop simulation in autonomous driving, yet it remains a significant challenge for current Novel View Synthesis (NVS) methods, which often produce distorted and inconsistent images beyond the original trajectory. This report presents our winning solution which ctook first place in the RealADSim Workshop NVS track at ICCV 2025. To address the core challenges of street view extrapolation, we introduce a comprehensive four-stage pipeline. First, we employ a data-driven initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding local minima. Second, we inject strong geometric priors by modeling the road surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a generative prior to create pseudo ground truth for extrapolated viewpoints, providing auxilary supervision. Finally, a data-driven adaptation network removes time-specific artifacts. On the RealADSim-NVS benchmark, our method achieves a final score of 0.441, ranking first among all participants.
<div><strong>Authors:</strong> Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ViSE, a four‑stage pipeline for realistic vision‑only street‑view extrapolation aimed at closed‑loop autonomous‑driving simulation. It combines data‑driven pseudo‑LiDAR initialization, a dimension‑reduced 2D‑SDF road surface model, generative priors for pseudo ground‑truth views, and an adaptation network to remove temporal artifacts, achieving state‑of‑the‑art performance on the RealADSim‑NVS benchmark.", "summary_cn": "本文提出 ViSE 系统，采用四阶段管线实现仅视觉的街景外推，用于自动驾驶闭环仿真。方法包括数据驱动的伪 LiDAR 初始化、用于道路表面的 2D‑SDF 几何先验、生成式伪真值视图以及去除时间伪影的自适应网络，在 RealADSim‑NVS 基准上取得领先.", "keywords": "vision-only extrapolation, novel view synthesis, pseudo-LiDAR, 2D-SDF, generative prior, autonomous driving simulation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kaiyuan Tan", "Yingying Shen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye"]}
]]></acme>

<pubDate>2025-10-21T06:50:20+00:00</pubDate>
</item>
<item>
<title>Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</title>
<link>https://papers.cool/arxiv/2510.18326</link>
<guid>https://papers.cool/arxiv/2510.18326</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net) for few-shot image classification, especially targeting benchmark and disaster datasets. By linearly combining the Bhattacharyya coefficient and Hellinger distance, the method aggregates feature probability distributions to build robust class prototypes and introduces a distribution-based contrastive loss alongside cross-entropy. Experiments on four few-shot benchmarks and two disaster image collections show that ATTBHFA-Net outperforms existing approaches.<br /><strong>Summary (CN):</strong> 本文提出基于注意力的 Bhattacharyya-Hellinger 特征聚合网络 (ATTBHFA-Net) 用于少样本图像分类，特别针对基准数据集和灾害图像。该方法通过线性结合 Bhattacharyya 系数和 Hellinger 距离来比较并聚合特征概率分布，以构建稳健的原型，并引入基于分布的对比损失与交叉熵共同训练。实验在四个少样本基准和两个灾害图像数据集上验证了其相较于现有方法的显著提升。<br /><strong>Keywords:</strong> few-shot learning, disaster imagery, Bhattacharyya coefficient, Hellinger distance, feature aggregation, attention network, contrastive loss, computer vision, prototype formation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-aplicable; Primary focus - other<br /><strong>Authors:</strong> Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong</div>
The increasing frequency of natural and human-induced disasters necessitates advanced visual recognition techniques capable of analyzing critical photographic data. With progress in artificial intelligence and resilient computational systems, rapid and accurate disaster classification has become crucial for efficient rescue operations. However, visual recognition in disaster contexts faces significant challenges due to limited and diverse data from the difficulties in collecting and curating comprehensive, high-quality disaster imagery. Few-Shot Learning (FSL) provides a promising approach to data scarcity, yet current FSL research mainly relies on generic benchmark datasets lacking remote-sensing disaster imagery, limiting its practical effectiveness. Moreover, disaster images exhibit high intra-class variation and inter-class similarity, hindering the performance of conventional metric-based FSL methods. To address these issues, this paper introduces the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), which linearly combines the Bhattacharyya coefficient and Hellinger distances to compare and aggregate feature probability distributions for robust prototype formation. The Bhattacharyya coefficient serves as a contrastive margin that enhances inter-class separability, while the Hellinger distance regularizes same-class alignment. This framework parallels contrastive learning but operates over probability distributions rather than embedded feature points. Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss is proposed as a distributional counterpart to cosine similarity loss, used jointly with categorical cross-entropy to significantly improve FSL performance. Experiments on four FSL benchmarks and two disaster image datasets demonstrate the superior effectiveness and generalization of ATTBHFA-Net compared to existing approaches.
<div><strong>Authors:</strong> Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net) for few-shot image classification, especially targeting benchmark and disaster datasets. By linearly combining the Bhattacharyya coefficient and Hellinger distance, the method aggregates feature probability distributions to build robust class prototypes and introduces a distribution-based contrastive loss alongside cross-entropy. Experiments on four few-shot benchmarks and two disaster image collections show that ATTBHFA-Net outperforms existing approaches.", "summary_cn": "本文提出基于注意力的 Bhattacharyya-Hellinger 特征聚合网络 (ATTBHFA-Net) 用于少样本图像分类，特别针对基准数据集和灾害图像。该方法通过线性结合 Bhattacharyya 系数和 Hellinger 距离来比较并聚合特征概率分布，以构建稳健的原型，并引入基于分布的对比损失与交叉熵共同训练。实验在四个少样本基准和两个灾害图像数据集上验证了其相较于现有方法的显著提升。", "keywords": "few-shot learning, disaster imagery, Bhattacharyya coefficient, Hellinger distance, feature aggregation, attention network, contrastive loss, computer vision, prototype formation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-aplicable", "primary_focus": "other"}, "authors": ["Gao Yu Lee", "Tanmoy Dam", "Md Meftahul Ferdaus", "Daniel Puiu Poenar", "Vu Duong"]}
]]></acme>

<pubDate>2025-10-21T06:24:42+00:00</pubDate>
</item>
<item>
<title>Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</title>
<link>https://papers.cool/arxiv/2510.18321</link>
<guid>https://papers.cool/arxiv/2510.18321</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Token Ensemble Decoding (ATED), a training-free inference framework that reduces object hallucinations in large vision-language models by aggregating predictions from multiple models with uncertainty-based token-level weighting and diverse decoding paths. Experiments show that ATED markedly lowers hallucination rates on benchmark datasets without sacrificing fluency or relevance, offering a scalable and model-agnostic way to improve LVLM robustness for high‑stakes applications.<br /><strong>Summary (CN):</strong> 本文提出了一种无需训练的自适应令牌集成解码方法（ATED），通过对多个大规模视觉语言模型的预测进行基于不确定性的令牌级加权并结合多样的解码路径，显著降低了对象幻觉的产生。实验表明，在标准幻觉检测基准上，ATED 在不损失流畅性和相关性的前提下，大幅降低了幻觉率，为高风险场景中的 LVLM 稳健性提供了可扩展、模型无关的解决方案。<br /><strong>Keywords:</strong> hallucination mitigation, vision-language models, adaptive token ensemble, uncertainty weighting, multimodal robustness, ensemble decoding, LVLM safety<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</div>
Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering. However, they remain prone to object hallucination -- generating descriptions of nonexistent or misidentified objects. Prior work has partially mitigated this via auxiliary training objectives or external modules, but challenges remain in terms of scalability, adaptability, and model independence. To address these limitations, we propose Adaptive Token Ensemble Decoding (ATED), a training-free, token-level ensemble framework that mitigates hallucination by aggregating predictions from multiple LVLMs during inference. ATED dynamically computes uncertainty-based weights for each model, reflecting their reliability at each decoding step. It also integrates diverse decoding paths to improve contextual grounding and semantic consistency. Experiments on standard hallucination detection benchmarks demonstrate that ATED significantly outperforms state-of-the-art methods, reducing hallucination without compromising fluency or relevance. Our findings highlight the benefits of adaptive ensembling and point to a promising direction for improving LVLM robustness in high-stakes applications. The code is available at https://github.com/jinlin2021/ATED.
<div><strong>Authors:</strong> Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Token Ensemble Decoding (ATED), a training-free inference framework that reduces object hallucinations in large vision-language models by aggregating predictions from multiple models with uncertainty-based token-level weighting and diverse decoding paths. Experiments show that ATED markedly lowers hallucination rates on benchmark datasets without sacrificing fluency or relevance, offering a scalable and model-agnostic way to improve LVLM robustness for high‑stakes applications.", "summary_cn": "本文提出了一种无需训练的自适应令牌集成解码方法（ATED），通过对多个大规模视觉语言模型的预测进行基于不确定性的令牌级加权并结合多样的解码路径，显著降低了对象幻觉的产生。实验表明，在标准幻觉检测基准上，ATED 在不损失流畅性和相关性的前提下，大幅降低了幻觉率，为高风险场景中的 LVLM 稳健性提供了可扩展、模型无关的解决方案。", "keywords": "hallucination mitigation, vision-language models, adaptive token ensemble, uncertainty weighting, multimodal robustness, ensemble decoding, LVLM safety", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jinlin Li", "Yuran Wang", "Yifei Yuan", "Xiao Zhou", "Yingying Zhang", "Xixian Yong", "Yefeng Zheng", "Xian Wu"]}
]]></acme>

<pubDate>2025-10-21T06:11:24+00:00</pubDate>
</item>
<item>
<title>OmniNWM: Omniscient Driving Navigation World Models</title>
<link>https://papers.cool/arxiv/2510.18313</link>
<guid>https://papers.cool/arxiv/2510.18313</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> OmniNWM introduces an omniscient panoramic navigation world model for autonomous driving that jointly generates RGB, semantic, depth, and 3D occupancy video streams, and employs a normalized panoramic Plucker ray‑map representation for precise long‑horizon control. The model also defines rule‑based dense rewards directly from the generated 3D occupancy to enforce driving compliance and safety, achieving state‑of‑the‑art performance in video generation, control accuracy, and stability.<br /><strong>Summary (CN):</strong> OmniNWM 提出了一种全视角全景导航世界模型，用于自动驾驶场景，能够同时生成 RGB、语义、深度和 3D 占用的全景视频，并通过归一化的全景 Plucker ray‑map 表示实现对全景视频的高精度长时控制。该模型利用生成的 3D 占用直接定义基于规则的密集奖励，以确保驾驶合规与安全，展现了视频生成、控制精度和长时稳定性的最先进表现。<br /><strong>Keywords:</strong> OmniNWM, panoramic world model, Plucker ray-map, 3D occupancy, rule-based dense rewards, autonomous driving, long-horizon control, video generation, state-action-reward modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Bohan Li, Zhuang Ma, Dalong Du, Baorui Peng, Zhujin Liang, Zhenqiang Liu, Chao Ma, Yueming Jin, Hao Zhao, Wenjun Zeng, Xin Jin</div>
Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.
<div><strong>Authors:</strong> Bohan Li, Zhuang Ma, Dalong Du, Baorui Peng, Zhujin Liang, Zhenqiang Liu, Chao Ma, Yueming Jin, Hao Zhao, Wenjun Zeng, Xin Jin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "OmniNWM introduces an omniscient panoramic navigation world model for autonomous driving that jointly generates RGB, semantic, depth, and 3D occupancy video streams, and employs a normalized panoramic Plucker ray‑map representation for precise long‑horizon control. The model also defines rule‑based dense rewards directly from the generated 3D occupancy to enforce driving compliance and safety, achieving state‑of‑the‑art performance in video generation, control accuracy, and stability.", "summary_cn": "OmniNWM 提出了一种全视角全景导航世界模型，用于自动驾驶场景，能够同时生成 RGB、语义、深度和 3D 占用的全景视频，并通过归一化的全景 Plucker ray‑map 表示实现对全景视频的高精度长时控制。该模型利用生成的 3D 占用直接定义基于规则的密集奖励，以确保驾驶合规与安全，展现了视频生成、控制精度和长时稳定性的最先进表现。", "keywords": "OmniNWM, panoramic world model, Plucker ray-map, 3D occupancy, rule-based dense rewards, autonomous driving, long-horizon control, video generation, state-action-reward modeling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Bohan Li", "Zhuang Ma", "Dalong Du", "Baorui Peng", "Zhujin Liang", "Zhenqiang Liu", "Chao Ma", "Yueming Jin", "Hao Zhao", "Wenjun Zeng", "Xin Jin"]}
]]></acme>

<pubDate>2025-10-21T05:49:01+00:00</pubDate>
</item>
<item>
<title>The Impact of Image Resolution on Biomedical Multimodal Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18304</link>
<guid>https://papers.cool/arxiv/2510.18304</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how image resolution influences the performance of biomedical multimodal large language models (MLLMs). It shows that training and inference on native-resolution images markedly improve task performance, while mismatched resolutions cause severe degradation, and proposes mixed-resolution training as a compromise between computational cost and accuracy.<br /><strong>Summary (CN):</strong> 本文研究了图像分辨率对生物医学多模态大语言模型（MLLM）性能的影响。研究表明，在原始分辨率上进行训练和推理能够显著提升任务表现，而训练‑推理分辨率不匹配会导致性能大幅下降，并提出混合分辨率训练以在计算资源和性能之间取得平衡。<br /><strong>Keywords:</strong> biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, robustness, performance scaling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.
<div><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how image resolution influences the performance of biomedical multimodal large language models (MLLMs). It shows that training and inference on native-resolution images markedly improve task performance, while mismatched resolutions cause severe degradation, and proposes mixed-resolution training as a compromise between computational cost and accuracy.", "summary_cn": "本文研究了图像分辨率对生物医学多模态大语言模型（MLLM）性能的影响。研究表明，在原始分辨率上进行训练和推理能够显著提升任务表现，而训练‑推理分辨率不匹配会导致性能大幅下降，并提出混合分辨率训练以在计算资源和性能之间取得平衡。", "keywords": "biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, robustness, performance scaling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"]}
]]></acme>

<pubDate>2025-10-21T05:19:43+00:00</pubDate>
</item>
<item>
<title>Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18303</link>
<guid>https://papers.cool/arxiv/2510.18303</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Med-RwR, a multimodal medical reasoning-with-retrieval framework that enables large language models to actively retrieve external medical knowledge based on visual and textual cues during diagnosis. A two-stage reinforcement learning approach with specialized rewards encourages the model to combine visual findings and clinical text for effective retrieval, and a confidence‑driven image re‑retrieval mechanism adapts at test time when uncertainty is high. Experiments on several medical benchmarks, including a new echocardiography dataset, show significant performance gains and improved generalization to under‑represented domains.<br /><strong>Summary (CN):</strong> 本文提出了 Med‑RwR，一个多模态医学推理‑检索框架，使大语言模型在诊断过程中能够主动依据视觉诊断信息和文本临床信息检索外部医学知识。通过两阶段强化学习并设计专属奖励，模型被激励有效利用视觉和文本线索进行检索，同时在测试时加入置信度驱动的图像再检索，以应对低信心预测。多项医学基准实验，包括新建的超声心动图基准，显示该方法显著提升性能并在陌生领域具有良好泛化能力。<br /><strong>Keywords:</strong> multimodal LLM, medical reasoning, retrieval-augmented generation, reinforcement learning, confidence-driven re-retrieval, hallucination reduction, external knowledge integration, echocardiography benchmark<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li</div>
Incentivizing the reasoning ability of Multimodal Large Language Models (MLLMs) is essential for medical applications to transparently analyze medical scans and provide reliable diagnosis. However, existing medical MLLMs rely solely on internal knowledge during reasoning, leading to hallucinated reasoning and factual inaccuracies when encountering cases beyond their training scope. Although recent Agentic Retrieval-Augmented Generation (RAG) methods elicit the medical model's proactive retrieval ability during reasoning, they are confined to unimodal LLMs, neglecting the crucial visual information during reasoning and retrieval. Consequently, we propose the first Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively retrieves external knowledge by querying observed symptoms or domain-specific medical concepts during reasoning. Specifically, we design a two-stage reinforcement learning strategy with tailored rewards that stimulate the model to leverage both visual diagnostic findings and textual clinical information for effective retrieval. Building on this foundation, we further propose a Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when low prediction confidence is detected. Evaluation on various public medical benchmarks demonstrates Med-RwR's significant improvements over baseline models, proving the effectiveness of enhancing reasoning capabilities with external knowledge integration. Furthermore, Med-RwR demonstrates remarkable generalizability to unfamiliar domains, evidenced by 8.8% performance gain on our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of echocardiography data in the training corpus. Our data, model, and codes will be made publicly available at https://github.com/xmed-lab/Med-RwR.
<div><strong>Authors:</strong> Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Med-RwR, a multimodal medical reasoning-with-retrieval framework that enables large language models to actively retrieve external medical knowledge based on visual and textual cues during diagnosis. A two-stage reinforcement learning approach with specialized rewards encourages the model to combine visual findings and clinical text for effective retrieval, and a confidence‑driven image re‑retrieval mechanism adapts at test time when uncertainty is high. Experiments on several medical benchmarks, including a new echocardiography dataset, show significant performance gains and improved generalization to under‑represented domains.", "summary_cn": "本文提出了 Med‑RwR，一个多模态医学推理‑检索框架，使大语言模型在诊断过程中能够主动依据视觉诊断信息和文本临床信息检索外部医学知识。通过两阶段强化学习并设计专属奖励，模型被激励有效利用视觉和文本线索进行检索，同时在测试时加入置信度驱动的图像再检索，以应对低信心预测。多项医学基准实验，包括新建的超声心动图基准，显示该方法显著提升性能并在陌生领域具有良好泛化能力。", "keywords": "multimodal LLM, medical reasoning, retrieval-augmented generation, reinforcement learning, confidence-driven re-retrieval, hallucination reduction, external knowledge integration, echocardiography benchmark", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Lehan Wang", "Yi Qin", "Honglong Yang", "Xiaomeng Li"]}
]]></acme>

<pubDate>2025-10-21T05:18:18+00:00</pubDate>
</item>
<item>
<title>GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation</title>
<link>https://papers.cool/arxiv/2510.18291</link>
<guid>https://papers.cool/arxiv/2510.18291</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes GeoDiff, a training-free framework that augments pretrained diffusion-based monocular depth estimation models with stereo vision geometry to resolve scale ambiguity and produce metric depth. By treating depth estimation as an inverse problem and leveraging latent diffusion conditioned on RGB images together with stereo-derived scale and shift constraints, the method achieves state-of-the-art performance on diverse indoor and outdoor scenes without retraining.<br /><strong>Summary (CN):</strong> 本文提出 GeoDiff 框架，在预训练的基于扩散的单目深度估计模型上加入立体视觉几何约束，以解决尺度歧义并实现度量深度估计。通过将深度估计重新表述为逆问题，并结合以 RGB 图像为条件的潜在扩散模型与立体产生的尺度和位移约束，该方法在无需重新训练的情况下，在多种室内外场景中达到或超过最新水平。<br /><strong>Keywords:</strong> metric depth estimation, diffusion models, latent diffusion, stereo guidance, geometry constraints, monocular depth, inverse problem<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tuan Pham, Thanh-Tung Le, Xiaohui Xie, Stephan Mandt</div>
We introduce a novel framework for metric depth estimation that enhances pretrained diffusion-based monocular depth estimation (DB-MDE) models with stereo vision guidance. While existing DB-MDE methods excel at predicting relative depth, estimating absolute metric depth remains challenging due to scale ambiguities in single-image scenarios. To address this, we reframe depth estimation as an inverse problem, leveraging pretrained latent diffusion models (LDMs) conditioned on RGB images, combined with stereo-based geometric constraints, to learn scale and shift for accurate depth recovery. Our training-free solution seamlessly integrates into existing DB-MDE frameworks and generalizes across indoor, outdoor, and complex environments. Extensive experiments demonstrate that our approach matches or surpasses state-of-the-art methods, particularly in challenging scenarios involving translucent and specular surfaces, all without requiring retraining.
<div><strong>Authors:</strong> Tuan Pham, Thanh-Tung Le, Xiaohui Xie, Stephan Mandt</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes GeoDiff, a training-free framework that augments pretrained diffusion-based monocular depth estimation models with stereo vision geometry to resolve scale ambiguity and produce metric depth. By treating depth estimation as an inverse problem and leveraging latent diffusion conditioned on RGB images together with stereo-derived scale and shift constraints, the method achieves state-of-the-art performance on diverse indoor and outdoor scenes without retraining.", "summary_cn": "本文提出 GeoDiff 框架，在预训练的基于扩散的单目深度估计模型上加入立体视觉几何约束，以解决尺度歧义并实现度量深度估计。通过将深度估计重新表述为逆问题，并结合以 RGB 图像为条件的潜在扩散模型与立体产生的尺度和位移约束，该方法在无需重新训练的情况下，在多种室内外场景中达到或超过最新水平。", "keywords": "metric depth estimation, diffusion models, latent diffusion, stereo guidance, geometry constraints, monocular depth, inverse problem", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tuan Pham", "Thanh-Tung Le", "Xiaohui Xie", "Stephan Mandt"]}
]]></acme>

<pubDate>2025-10-21T04:47:36+00:00</pubDate>
</item>
<item>
<title>Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models</title>
<link>https://papers.cool/arxiv/2510.18287</link>
<guid>https://papers.cool/arxiv/2510.18287</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an efficient few-shot method for identity-preserving attribute editing in 3D-aware generative models. By estimating latent space directions from ten or fewer labelled images per attribute, it achieves photorealistic, view-consistent edits such as illumination changes, eyeglass addition, aging, and hairstyle modifications while maintaining the subject's identity. Experiments also explore the linearity of edits and continuous style manipulation using the Attribute Style Manipulation (ASM) technique.<br /><strong>Summary (CN):</strong> 本文提出一种高效的少样本方法，用于在 3D 感知生成模型中实现保持身份的人脸属性编辑。通过仅使用十张或更少的带标签图像来估计潜在空间方向，实现光照、眼镜、老化、发型等属性的真实感、跨视角一致的编辑，并保持原始身份。实验还考察了编辑的线性可叠加性，并使用属性风格操作 (ASM) 技术研究连续风格流形。<br /><strong>Keywords:</strong> few-shot editing, 3D-aware generative models, identity preservation, latent space directions, attribute editing, style manipulation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vishal Vinod</div>
Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving face aging. Code and results are available at: https://vishal-vinod.github.io/gmpi-edit/
<div><strong>Authors:</strong> Vishal Vinod</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an efficient few-shot method for identity-preserving attribute editing in 3D-aware generative models. By estimating latent space directions from ten or fewer labelled images per attribute, it achieves photorealistic, view-consistent edits such as illumination changes, eyeglass addition, aging, and hairstyle modifications while maintaining the subject's identity. Experiments also explore the linearity of edits and continuous style manipulation using the Attribute Style Manipulation (ASM) technique.", "summary_cn": "本文提出一种高效的少样本方法，用于在 3D 感知生成模型中实现保持身份的人脸属性编辑。通过仅使用十张或更少的带标签图像来估计潜在空间方向，实现光照、眼镜、老化、发型等属性的真实感、跨视角一致的编辑，并保持原始身份。实验还考察了编辑的线性可叠加性，并使用属性风格操作 (ASM) 技术研究连续风格流形。", "keywords": "few-shot editing, 3D-aware generative models, identity preservation, latent space directions, attribute editing, style manipulation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vishal Vinod"]}
]]></acme>

<pubDate>2025-10-21T04:27:46+00:00</pubDate>
</item>
<item>
<title>StreamingTOM: Streaming Token Compression for Efficient Video Understanding</title>
<link>https://papers.cool/arxiv/2510.18269</link>
<guid>https://papers.cool/arxiv/2510.18269</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> StreamingTOM proposes a training-free, plug‑and‑play two‑stage framework for streaming video vision‑language models that tackles both pre‑LLM token prefill and post‑LLM KV‑cache bottlenecks. Causal Temporal Reduction limits per‑frame token budgets by selecting salient tokens based on frame changes, while Online Quantized Memory stores tokens in 4‑bit format and retrieves them on demand, keeping the active KV‑cache. Experiments show up to 15.7× KV‑cache compression, reduced peak memory, and faster time‑to‑first‑token while retaining competitive accuracy.<br /><strong>Summary (CN):</strong> StreamingTOM 提出一种无需训练、即插即用的两阶段框架，用于流式视频视觉语言模型，解决了 LLM 前置的 token 预填充和后置 KV‑cache 两个瓶颈。因果时间压缩通过基于相邻帧变化和 token 显著性选择有限数量的视觉 token，显著降低每帧的预填充成本；在线量化记忆将 token 存储为 4 位格式，按需检索并解量化，使得 KV‑cache 大小随流长度保持有限。实验表明该方法实现 15.7 倍的 KV‑cache 压缩、降低峰值内存并加快响应速度，同时保持与离线基准接近的准确率。<br /><strong>Keywords:</strong> video understanding, token compression, streaming vision-language, causal temporal reduction, quantized memory, KV-cache compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</div>
Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, existing approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage framework that addresses both pre-LLM and post-LLM bottlenecks with predictable latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects tokens based on adjacent-frame changes and token saliency, drastically reducing per-frame prefill cost by processing only a compact subset of visual tokens per frame instead of all visual tokens. Online Quantized Memory stores tokens in 4-bit format, retrieves relevant groups on demand, and dequantizes them, keeping the active kv-cache bounded regardless of stream length. Experiments demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$ lower peak memory and $2\times$ faster TTFT compared to prior SOTA. StreamingTOM maintains state-of-the-art accuracy among training-free methods with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS. These results highlight the practical benefits of our two-stage approach for efficient streaming video understanding with bounded growth.
<div><strong>Authors:</strong> Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "StreamingTOM proposes a training-free, plug‑and‑play two‑stage framework for streaming video vision‑language models that tackles both pre‑LLM token prefill and post‑LLM KV‑cache bottlenecks. Causal Temporal Reduction limits per‑frame token budgets by selecting salient tokens based on frame changes, while Online Quantized Memory stores tokens in 4‑bit format and retrieves them on demand, keeping the active KV‑cache. Experiments show up to 15.7× KV‑cache compression, reduced peak memory, and faster time‑to‑first‑token while retaining competitive accuracy.", "summary_cn": "StreamingTOM 提出一种无需训练、即插即用的两阶段框架，用于流式视频视觉语言模型，解决了 LLM 前置的 token 预填充和后置 KV‑cache 两个瓶颈。因果时间压缩通过基于相邻帧变化和 token 显著性选择有限数量的视觉 token，显著降低每帧的预填充成本；在线量化记忆将 token 存储为 4 位格式，按需检索并解量化，使得 KV‑cache 大小随流长度保持有限。实验表明该方法实现 15.7 倍的 KV‑cache 压缩、降低峰值内存并加快响应速度，同时保持与离线基准接近的准确率。", "keywords": "video understanding, token compression, streaming vision-language, causal temporal reduction, quantized memory, KV-cache compression", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyi Chen", "Keda Tao", "Kele Shao", "Huan Wang"]}
]]></acme>

<pubDate>2025-10-21T03:39:41+00:00</pubDate>
</item>
<item>
<title>TreeFedDG: Alleviating Global Drift in Federated Domain Generalization for Medical Image Segmentation</title>
<link>https://papers.cool/arxiv/2510.18268</link>
<guid>https://papers.cool/arxiv/2510.18268</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies the Global Drift problem in federated domain generalization for medical image segmentation and proposes TreeFedDG, a tree‑structured hierarchical aggregation framework combined with a parameter‑difference style mixing (FedStyle) and progressive personalized fusion to mitigate drift and improve cross‑domain performance. During inference, feature similarity guides the selection of the most relevant model chain from the tree for ensemble decision making.<br /><strong>Summary (CN):</strong> 本文指出在联邦域泛化医学图像分割任务中出现的全局漂移（Global Drift）问题，并提出 TreeFedDG——基于树形拓扑的层次参数聚合框架，结合基于参数差异的风格混合（FedStyle）与渐进式个性化融合，以抑制全局模型偏移并提升跨域分割性能。推理阶段利用特征相似度检索最相关的模型链进行集成决策。<br /><strong>Keywords:</strong> federated learning, domain generalization, medical image segmentation, global drift, tree topology, FedStyle, hierarchical aggregation, personalized fusion, cross-domain robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yucheng Song, Chenxi Li, Haokang Ding, Zhining Liao, Zhifang Liao</div>
In medical image segmentation tasks, Domain Generalization (DG) under the Federated Learning (FL) framework is crucial for addressing challenges related to privacy protection and data heterogeneity. However, traditional federated learning methods fail to account for the imbalance in information aggregation across clients in cross-domain scenarios, leading to the Global Drift (GD) problem and a consequent decline in model generalization performance. This motivates us to delve deeper and define a new critical issue: global drift in federated domain generalization for medical imaging (FedDG-GD). In this paper, we propose a novel tree topology framework called TreeFedDG. First, starting from the distributed characteristics of medical images, we design a hierarchical parameter aggregation method based on a tree-structured topology to suppress deviations in the global model direction. Second, we introduce a parameter difference-based style mixing method (FedStyle), which enforces mixing among clients with maximum parameter differences to enhance robustness against drift. Third, we develop a a progressive personalized fusion strategy during model distribution, ensuring a balance between knowledge transfer and personalized features. Finally, during the inference phase, we use feature similarity to guide the retrieval of the most relevant model chain from the tree structure for ensemble decision-making, thereby fully leveraging the advantages of hierarchical knowledge. We conducted extensive experiments on two publicly available datasets. The results demonstrate that our method outperforms other state-of-the-art domain generalization approaches in these challenging tasks and achieves better balance in cross-domain performance.
<div><strong>Authors:</strong> Yucheng Song, Chenxi Li, Haokang Ding, Zhining Liao, Zhifang Liao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies the Global Drift problem in federated domain generalization for medical image segmentation and proposes TreeFedDG, a tree‑structured hierarchical aggregation framework combined with a parameter‑difference style mixing (FedStyle) and progressive personalized fusion to mitigate drift and improve cross‑domain performance. During inference, feature similarity guides the selection of the most relevant model chain from the tree for ensemble decision making.", "summary_cn": "本文指出在联邦域泛化医学图像分割任务中出现的全局漂移（Global Drift）问题，并提出 TreeFedDG——基于树形拓扑的层次参数聚合框架，结合基于参数差异的风格混合（FedStyle）与渐进式个性化融合，以抑制全局模型偏移并提升跨域分割性能。推理阶段利用特征相似度检索最相关的模型链进行集成决策。", "keywords": "federated learning, domain generalization, medical image segmentation, global drift, tree topology, FedStyle, hierarchical aggregation, personalized fusion, cross-domain robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yucheng Song", "Chenxi Li", "Haokang Ding", "Zhining Liao", "Zhifang Liao"]}
]]></acme>

<pubDate>2025-10-21T03:38:05+00:00</pubDate>
</item>
<item>
<title>Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization</title>
<link>https://papers.cool/arxiv/2510.18267</link>
<guid>https://papers.cool/arxiv/2510.18267</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a two-stage network for 3D human mesh recovery that first extracts global and local latent information from image features in a hybrid latent frequency domain, and then uses low-dimensional mesh-pose interaction with parallel optimization to refine pose and shape. This approach aims to reduce computational cost while improving reconstruction accuracy compared to state-of-the-art methods.<br /><strong>Summary (CN):</strong> 本文提出了一种两阶段网络用于 3D 人体网格恢复。第一阶段从图像特征的低频和高频成分中提取全局（如整体形状对齐）和局部（如纹理细节）潜在信息，形成混合潜在频域特征；第二阶段在该特征的帮助下，通过降维并行优化的低维网格姿态交互模块，提升 3D 姿态与形状的估计，并显著降低计算成本。<br /><strong>Keywords:</strong> human mesh recovery, latent information, low-dimensional learning, 3D pose estimation, parallel optimization, attention mechanisms, shape alignment, texture detail, computer vision<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiang Zhang, Suping Wu, Sheng Yang</div>
Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.
<div><strong>Authors:</strong> Xiang Zhang, Suping Wu, Sheng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a two-stage network for 3D human mesh recovery that first extracts global and local latent information from image features in a hybrid latent frequency domain, and then uses low-dimensional mesh-pose interaction with parallel optimization to refine pose and shape. This approach aims to reduce computational cost while improving reconstruction accuracy compared to state-of-the-art methods.", "summary_cn": "本文提出了一种两阶段网络用于 3D 人体网格恢复。第一阶段从图像特征的低频和高频成分中提取全局（如整体形状对齐）和局部（如纹理细节）潜在信息，形成混合潜在频域特征；第二阶段在该特征的帮助下，通过降维并行优化的低维网格姿态交互模块，提升 3D 姿态与形状的估计，并显著降低计算成本。", "keywords": "human mesh recovery, latent information, low-dimensional learning, 3D pose estimation, parallel optimization, attention mechanisms, shape alignment, texture detail, computer vision", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Sheng Yang"]}
]]></acme>

<pubDate>2025-10-21T03:35:12+00:00</pubDate>
</item>
<item>
<title>UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding</title>
<link>https://papers.cool/arxiv/2510.18262</link>
<guid>https://papers.cool/arxiv/2510.18262</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces UWBench, a large-scale vision-language benchmark specifically designed for underwater understanding, containing over 15,000 high‑resolution images with detailed object referring expressions and 125,000 question‑answer pairs covering ecological reasoning. It defines three evaluation tasks—image captioning, visual grounding, and visual question answering—and shows that current state‑of‑the‑art VLMs struggle with the unique challenges of underwater imagery. The benchmark aims to advance research in marine science applications and autonomous underwater exploration.<br /><strong>Summary (CN):</strong> 本文提出了 UWBench，一个面向水下场景的规模化视觉‑语言基准，收录 15,000 多张高分辨率水下图像，配有人类验证的 15,281 条目标指涉描述和 124,983 条问答对，涵盖生态关系推理等多种能力。该基准设定了图像描述、视觉定位和视觉问答三项任务，并展示了现有最先进 VLM 在光照衰减、颜色失真和浊度等水下独特挑战下的性能不足。该资源旨在推动海洋科学、生态监测和水下自主探索等领域的视觉‑语言研究。<br /><strong>Keywords:</strong> underwater vision-language, benchmark, VLM, marine ecology, visual grounding, image captioning, visual question answering, dataset, underwater imaging, multimodal reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</div>
Large vision-language models (VLMs) have achieved remarkable success in natural scene understanding, yet their application to underwater environments remains largely unexplored. Underwater imagery presents unique challenges including severe light attenuation, color distortion, and suspended particle scattering, while requiring specialized knowledge of marine ecosystems and organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive benchmark specifically designed for underwater vision-language understanding. UWBench comprises 15,003 high-resolution underwater images captured across diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea habitats. Each image is enriched with human-verified annotations including 15,281 object referring expressions that precisely describe marine organisms and underwater structures, and 124,983 question-answer pairs covering diverse reasoning capabilities from object recognition to ecological relationship understanding. The dataset captures rich variations in visibility, lighting conditions, and water turbidity, providing a realistic testbed for model evaluation. Based on UWBench, we establish three comprehensive benchmarks: detailed image captioning for generating ecologically informed scene descriptions, visual grounding for precise localization of marine organisms, and visual question answering for multimodal reasoning about underwater environments. Extensive experiments on state-of-the-art VLMs demonstrate that underwater understanding remains challenging, with substantial room for improvement. Our benchmark provides essential resources for advancing vision-language research in underwater contexts and supporting applications in marine science, ecological monitoring, and autonomous underwater exploration. Our code and benchmark will be available.
<div><strong>Authors:</strong> Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces UWBench, a large-scale vision-language benchmark specifically designed for underwater understanding, containing over 15,000 high‑resolution images with detailed object referring expressions and 125,000 question‑answer pairs covering ecological reasoning. It defines three evaluation tasks—image captioning, visual grounding, and visual question answering—and shows that current state‑of‑the‑art VLMs struggle with the unique challenges of underwater imagery. The benchmark aims to advance research in marine science applications and autonomous underwater exploration.", "summary_cn": "本文提出了 UWBench，一个面向水下场景的规模化视觉‑语言基准，收录 15,000 多张高分辨率水下图像，配有人类验证的 15,281 条目标指涉描述和 124,983 条问答对，涵盖生态关系推理等多种能力。该基准设定了图像描述、视觉定位和视觉问答三项任务，并展示了现有最先进 VLM 在光照衰减、颜色失真和浊度等水下独特挑战下的性能不足。该资源旨在推动海洋科学、生态监测和水下自主探索等领域的视觉‑语言研究。", "keywords": "underwater vision-language, benchmark, VLM, marine ecology, visual grounding, image captioning, visual question answering, dataset, underwater imaging, multimodal reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Da Zhang", "Chenggang Rong", "Bingyu Li", "Feiyu Wang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"]}
]]></acme>

<pubDate>2025-10-21T03:32:15+00:00</pubDate>
</item>
<item>
<title>Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery</title>
<link>https://papers.cool/arxiv/2510.18256</link>
<guid>https://papers.cool/arxiv/2510.18256</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a hyperbolic space learning framework that leverages temporal motion priors to improve video‑based 3D human mesh recovery. By extracting motion features from pose and image sequences and optimizing mesh representations in hyperbolic space with a dedicated loss, the method captures the hierarchical structure of the human body and yields smoother, more accurate meshes compared to Euclidean baselines.<br /><strong>Summary (CN):</strong> 本文提出一种利用时间运动先验的双曲空间学习框架，以提升基于视频的 3D 人体网格恢复。该方法从姿态序列和图像特征序列中提取运动特征，在双曲空间中对网格进行优化，并引入双曲网格优化损失，从而更好地捕捉人体的层次结构，实现更平滑、更加精确的网格重建。<br /><strong>Keywords:</strong> hyperbolic learning, temporal motion prior, 3D human mesh recovery, video-based reconstruction, hierarchical representation, non-Euclidean space, mesh optimization loss, pose estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang</div>
3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.
<div><strong>Authors:</strong> Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a hyperbolic space learning framework that leverages temporal motion priors to improve video‑based 3D human mesh recovery. By extracting motion features from pose and image sequences and optimizing mesh representations in hyperbolic space with a dedicated loss, the method captures the hierarchical structure of the human body and yields smoother, more accurate meshes compared to Euclidean baselines.", "summary_cn": "本文提出一种利用时间运动先验的双曲空间学习框架，以提升基于视频的 3D 人体网格恢复。该方法从姿态序列和图像特征序列中提取运动特征，在双曲空间中对网格进行优化，并引入双曲网格优化损失，从而更好地捕捉人体的层次结构，实现更平滑、更加精确的网格重建。", "keywords": "hyperbolic learning, temporal motion prior, 3D human mesh recovery, video-based reconstruction, hierarchical representation, non-Euclidean space, mesh optimization loss, pose estimation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Weibin Qiu", "Zhaocheng Jin", "Sheng Yang"]}
]]></acme>

<pubDate>2025-10-21T03:26:27+00:00</pubDate>
</item>
<item>
<title>OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion</title>
<link>https://papers.cool/arxiv/2510.18253</link>
<guid>https://papers.cool/arxiv/2510.18253</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> OpenInsGaussian introduces an open-vocabulary instance segmentation framework for 3D Gaussian representations that leverages context-aware feature extraction and attention-driven cross‑view fusion to improve semantic consistency and detail retention across multiple views. Extensive experiments show state‑of‑the‑art performance on benchmark datasets, highlighting its robustness for autonomous driving, robotics, and AR applications.<br /><strong>Summary (CN):</strong> OpenInsGaussian 提出了一种基于 open‑vocabulary 的 3D Gaussian 实例分割框架，通过上下文感知特征提取和注意力驱动的跨视融合来提升多视角语义一致性和细节保留。大量实验表明其在基准数据集上取得了最先进的性能，展示了在自动驾驶、机器人和增强现实等场景中的鲁棒性。<br /><strong>Keywords:</strong> open-vocabulary, instance segmentation, Gaussian splatting, context-aware fusion, attention-driven aggregation, 3D scene understanding<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu</div>
Understanding 3D scenes is pivotal for autonomous driving, robotics, and augmented reality. Recent semantic Gaussian Splatting approaches leverage large-scale 2D vision models to project 2D semantic features onto 3D scenes. However, they suffer from two major limitations: (1) insufficient contextual cues for individual masks during preprocessing and (2) inconsistencies and missing details when fusing multi-view features from these 2D models. In this paper, we introduce \textbf{OpenInsGaussian}, an \textbf{Open}-vocabulary \textbf{Ins}tance \textbf{Gaussian} segmentation framework with Context-aware Cross-view Fusion. Our method consists of two modules: Context-Aware Feature Extraction, which augments each mask with rich semantic context, and Attention-Driven Feature Aggregation, which selectively fuses multi-view features to mitigate alignment errors and incompleteness. Through extensive experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art results in open-vocabulary 3D Gaussian segmentation, outperforming existing baselines by a large margin. These findings underscore the robustness and generality of our proposed approach, marking a significant step forward in 3D scene understanding and its practical deployment across diverse real-world scenarios.
<div><strong>Authors:</strong> Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "OpenInsGaussian introduces an open-vocabulary instance segmentation framework for 3D Gaussian representations that leverages context-aware feature extraction and attention-driven cross‑view fusion to improve semantic consistency and detail retention across multiple views. Extensive experiments show state‑of‑the‑art performance on benchmark datasets, highlighting its robustness for autonomous driving, robotics, and AR applications.", "summary_cn": "OpenInsGaussian 提出了一种基于 open‑vocabulary 的 3D Gaussian 实例分割框架，通过上下文感知特征提取和注意力驱动的跨视融合来提升多视角语义一致性和细节保留。大量实验表明其在基准数据集上取得了最先进的性能，展示了在自动驾驶、机器人和增强现实等场景中的鲁棒性。", "keywords": "open-vocabulary, instance segmentation, Gaussian splatting, context-aware fusion, attention-driven aggregation, 3D scene understanding", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tianyu Huang", "Runnan Chen", "Dongting Hu", "Fengming Huang", "Mingming Gong", "Tongliang Liu"]}
]]></acme>

<pubDate>2025-10-21T03:24:12+00:00</pubDate>
</item>
<item>
<title>BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</title>
<link>https://papers.cool/arxiv/2510.18244</link>
<guid>https://papers.cool/arxiv/2510.18244</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BlendCLIP proposes a multimodal pretraining framework that combines synthetic CAD data with real-world LiDAR scans via a curriculum-based data mixing strategy to bridge the synthetic-to-real domain gap for zero-shot 3D object classification. By mining object-level triplets (point cloud, image, text) from real driving data and progressively adapting the model, the method achieves large accuracy gains on benchmarks such as nuScenes with only a small fraction of real samples. The approach demonstrates that targeted domain adaptation can replace extensive real-world annotation for robust open-vocabulary 3D perception.<br /><strong>Summary (CN):</strong> BlendCLIP 提出了一种多模态预训练框架，通过课程式数据混合策略，将合成 CAD 数据与真实 LiDAR 扫描相结合，以缩小合成到真实的域差距，实现零样本 3D 物体分类。该方法从真实驾驶数据中挖掘点云、图像和文本三元组，并逐步适应模型，在仅使用极少真实样本的情况下显著提升了 nuScenes 等基准的准确率。研究表明，有针对性的域适应能够取代大规模真实标注，实现鲁棒的开放词汇 3D 感知。<br /><strong>Keywords:</strong> zero-shot 3D classification, multimodal pretraining, domain adaptation, synthetic-to-real, curriculum mixing, LiDAR, CLIP, open-vocabulary perception<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt</div>
Zero-shot 3D object classification is crucial for real-world applications like autonomous driving, however it is often hindered by a significant domain gap between the synthetic data used for training and the sparse, noisy LiDAR scans encountered in the real-world. Current methods trained solely on synthetic data fail to generalize to outdoor scenes, while those trained only on real data lack the semantic diversity to recognize rare or unseen objects. We introduce BlendCLIP, a multimodal pretraining framework that bridges this synthetic-to-real gap by strategically combining the strengths of both domains. We first propose a pipeline to generate a large-scale dataset of object-level triplets -- consisting of a point cloud, image, and text description -- mined directly from real-world driving data and human annotated 3D boxes. Our core contribution is a curriculum-based data mixing strategy that first grounds the model in the semantically rich synthetic CAD data before progressively adapting it to the specific characteristics of real-world scans. Our experiments show that our approach is highly label-efficient: introducing as few as 1.5\% real-world samples per batch into training boosts zero-shot accuracy on the nuScenes benchmark by 27\%. Consequently, our final model achieves state-of-the-art performance on challenging outdoor datasets like nuScenes and TruckScenes, improving over the best prior method by 19.3\% on nuScenes, while maintaining strong generalization on diverse synthetic benchmarks. Our findings demonstrate that effective domain adaptation, not full-scale real-world annotation, is the key to unlocking robust open-vocabulary 3D perception. Our code and dataset will be released upon acceptance on https://github.com/kesu1/BlendCLIP.
<div><strong>Authors:</strong> Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BlendCLIP proposes a multimodal pretraining framework that combines synthetic CAD data with real-world LiDAR scans via a curriculum-based data mixing strategy to bridge the synthetic-to-real domain gap for zero-shot 3D object classification. By mining object-level triplets (point cloud, image, text) from real driving data and progressively adapting the model, the method achieves large accuracy gains on benchmarks such as nuScenes with only a small fraction of real samples. The approach demonstrates that targeted domain adaptation can replace extensive real-world annotation for robust open-vocabulary 3D perception.", "summary_cn": "BlendCLIP 提出了一种多模态预训练框架，通过课程式数据混合策略，将合成 CAD 数据与真实 LiDAR 扫描相结合，以缩小合成到真实的域差距，实现零样本 3D 物体分类。该方法从真实驾驶数据中挖掘点云、图像和文本三元组，并逐步适应模型，在仅使用极少真实样本的情况下显著提升了 nuScenes 等基准的准确率。研究表明，有针对性的域适应能够取代大规模真实标注，实现鲁棒的开放词汇 3D 感知。", "keywords": "zero-shot 3D classification, multimodal pretraining, domain adaptation, synthetic-to-real, curriculum mixing, LiDAR, CLIP, open-vocabulary perception", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ajinkya Khoche", "Gergő László Nagy", "Maciej Wozniak", "Thomas Gustafsson", "Patric Jensfelt"]}
]]></acme>

<pubDate>2025-10-21T03:08:27+00:00</pubDate>
</item>
<item>
<title>DeepSeek-OCR: Contexts Optical Compression</title>
<link>https://papers.cool/arxiv/2510.18234</link>
<guid>https://papers.cool/arxiv/2510.18234</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeepSeek-OCR, a system that compresses long textual contexts into a compact 2D optical representation using a DeepEncoder and a large MoE decoder (DeepSeek3B-MoE-A570M). Experiments show that with compression ratios below 10x the OCR accuracy remains around 97%, while even at 20x the accuracy stays near 60%, demonstrating promise for applications like historical document digitization and memory forgetting mechanisms in LLMs. The method outperforms prior OCR systems on benchmarks while using far fewer vision tokens, and can generate large-scale training data efficiently.<br /><strong>Summary (CN):</strong> 本文提出 DeepSeek-OCR 系统，通过 DeepEncoder 将长文本上下文压缩为二维光学映射，并使用 DeepSeek3B-MoE-A570M（MoE 解码器）进行 OCR 解码。实验表明，在压缩比例低 10 倍时，OCR 精度约为 97%，即使在 20 倍压缩下仍保持约 60% 的准确率，展示了其在历史长文本压缩和 LLM 的记忆遗忘机制等研究方向的潜力。该方法在 OmniDocBench 基准上使用更少的视觉 token 超越了现有 OCR 系统，并能以单卡 A100‑40G 的速度每日生成超过 20 万页的训练数据。<br /><strong>Keywords:</strong> optical compression, OCR, vision tokens, MoE decoder, long-context compression, DeepEncoder<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoran Wei, Yaofeng Sun, Yukun Li</div>
We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR.
<div><strong>Authors:</strong> Haoran Wei, Yaofeng Sun, Yukun Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeepSeek-OCR, a system that compresses long textual contexts into a compact 2D optical representation using a DeepEncoder and a large MoE decoder (DeepSeek3B-MoE-A570M). Experiments show that with compression ratios below 10x the OCR accuracy remains around 97%, while even at 20x the accuracy stays near 60%, demonstrating promise for applications like historical document digitization and memory forgetting mechanisms in LLMs. The method outperforms prior OCR systems on benchmarks while using far fewer vision tokens, and can generate large-scale training data efficiently.", "summary_cn": "本文提出 DeepSeek-OCR 系统，通过 DeepEncoder 将长文本上下文压缩为二维光学映射，并使用 DeepSeek3B-MoE-A570M（MoE 解码器）进行 OCR 解码。实验表明，在压缩比例低 10 倍时，OCR 精度约为 97%，即使在 20 倍压缩下仍保持约 60% 的准确率，展示了其在历史长文本压缩和 LLM 的记忆遗忘机制等研究方向的潜力。该方法在 OmniDocBench 基准上使用更少的视觉 token 超越了现有 OCR 系统，并能以单卡 A100‑40G 的速度每日生成超过 20 万页的训练数据。", "keywords": "optical compression, OCR, vision tokens, MoE decoder, long-context compression, DeepEncoder", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoran Wei", "Yaofeng Sun", "Yukun Li"]}
]]></acme>

<pubDate>2025-10-21T02:41:44+00:00</pubDate>
</item>
<item>
<title>Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis</title>
<link>https://papers.cool/arxiv/2510.18229</link>
<guid>https://papers.cool/arxiv/2510.18229</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a generation-based debiasing framework for object detection that uses a representation score to identify gaps beyond class frequency and employs precise visual blueprints with a generative alignment strategy to create high‑fidelity, unbiased training images. Experiments show notable improvements in mean average precision for underrepresented object groups, narrowing performance gaps and outperforming prior layout‑to‑image synthesis methods.<br /><strong>Summary (CN):</strong> 本文提出一种基于生成的目标检测去偏框架，利用“表示分数”(representation score) 诊断超越类别频次的表征缺口，并通过精确的视觉蓝图和生成对齐策略合成高质量、无偏的训练图像。实验表明，对稀疏对象组的 mAP 明显提升，缩小了性能差距，并超越了之前的布局到图像合成方法。<br /><strong>Keywords:</strong> object detection, debiasing, generative augmentation, representation score, blueprint prompt, layout-to-image synthesis, data imbalance, fairness, mAP<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Xinhao Cai, Liulei Li, Gensheng Pei, Tao Chen, Jinshan Pan, Yazhou Yao, Wenguan Wang</div>
This paper presents a generation-based debiasing framework for object detection. Prior debiasing methods are often limited by the representation diversity of samples, while naive generative augmentation often preserves the biases it aims to solve. Moreover, our analysis reveals that simply generating more data for rare classes is suboptimal due to two core issues: i) instance frequency is an incomplete proxy for the true data needs of a model, and ii) current layout-to-image synthesis lacks the fidelity and control to generate high-quality, complex scenes. To overcome this, we introduce the representation score (RS) to diagnose representational gaps beyond mere frequency, guiding the creation of new, unbiased layouts. To ensure high-quality synthesis, we replace ambiguous text prompts with a precise visual blueprint and employ a generative alignment strategy, which fosters communication between the detector and generator. Our method significantly narrows the performance gap for underrepresented object groups, \eg, improving large/rare instances by 4.4/3.6 mAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP for layout accuracy in generated images.
<div><strong>Authors:</strong> Xinhao Cai, Liulei Li, Gensheng Pei, Tao Chen, Jinshan Pan, Yazhou Yao, Wenguan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a generation-based debiasing framework for object detection that uses a representation score to identify gaps beyond class frequency and employs precise visual blueprints with a generative alignment strategy to create high‑fidelity, unbiased training images. Experiments show notable improvements in mean average precision for underrepresented object groups, narrowing performance gaps and outperforming prior layout‑to‑image synthesis methods.", "summary_cn": "本文提出一种基于生成的目标检测去偏框架，利用“表示分数”(representation score) 诊断超越类别频次的表征缺口，并通过精确的视觉蓝图和生成对齐策略合成高质量、无偏的训练图像。实验表明，对稀疏对象组的 mAP 明显提升，缩小了性能差距，并超越了之前的布局到图像合成方法。", "keywords": "object detection, debiasing, generative augmentation, representation score, blueprint prompt, layout-to-image synthesis, data imbalance, fairness, mAP", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Xinhao Cai", "Liulei Li", "Gensheng Pei", "Tao Chen", "Jinshan Pan", "Yazhou Yao", "Wenguan Wang"]}
]]></acme>

<pubDate>2025-10-21T02:19:12+00:00</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18214</link>
<guid>https://papers.cool/arxiv/2510.18214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Language Safety Understanding (VU), a framework that evaluates multimodal foundation models through fine‑grained severity categories and combinatorial analysis of 17 safety patterns, constructing a benchmark of 8,187 image‑text samples across 15 harm types. Experiments on eleven state‑of‑the‑art models reveal severe joint image‑text reasoning failures: while unimodal safety signals are classified with >90% accuracy, joint understanding drops to 20‑55%, and a large share of errors occur despite correct unimodal predictions. The study also quantifies the trade‑off between over‑blocking borderline content and under‑refusing unsafe content, highlighting critical alignment gaps in current models.<br /><strong>Summary (CN):</strong> 本文提出了“视觉语言安全理解（VLSU）”框架，通过细粒度的严重程度分类和 17 种安全模式的组合分析，对多模态基础模型进行系统评估，并构建了包含 8,187 条跨 15 类危害的图文样本基准。对十一种先进模型的实验表明，虽然单模态安全信号的准确率超过 90%，但在需要联合图文推理时准确率骤降至 20%‑55%，且约 34% 的错误发生在单模态已正确分类的情况下，显示出模型缺乏组合推理能力。研究还揭示了在降低对边缘内容的过度拦截与减少对危险内容的低拒绝率之间的权衡，突显当前模型在对齐方面的显著缺口。<br /><strong>Keywords:</strong> multimodal safety, vision-language models, compositional reasoning, safety benchmark, alignment, joint understanding, severity classification, over-blocking, under-refusal<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
<div><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Language Safety Understanding (VU), a framework that evaluates multimodal foundation models through fine‑grained severity categories and combinatorial analysis of 17 safety patterns, constructing a benchmark of 8,187 image‑text samples across 15 harm types. Experiments on eleven state‑of‑the‑art models reveal severe joint image‑text reasoning failures: while unimodal safety signals are classified with >90% accuracy, joint understanding drops to 20‑55%, and a large share of errors occur despite correct unimodal predictions. The study also quantifies the trade‑off between over‑blocking borderline content and under‑refusing unsafe content, highlighting critical alignment gaps in current models.", "summary_cn": "本文提出了“视觉语言安全理解（VLSU）”框架，通过细粒度的严重程度分类和 17 种安全模式的组合分析，对多模态基础模型进行系统评估，并构建了包含 8,187 条跨 15 类危害的图文样本基准。对十一种先进模型的实验表明，虽然单模态安全信号的准确率超过 90%，但在需要联合图文推理时准确率骤降至 20%‑55%，且约 34% 的错误发生在单模态已正确分类的情况下，显示出模型缺乏组合推理能力。研究还揭示了在降低对边缘内容的过度拦截与减少对危险内容的低拒绝率之间的权衡，突显当前模型在对齐方面的显著缺口。", "keywords": "multimodal safety, vision-language models, compositional reasoning, safety benchmark, alignment, joint understanding, severity classification, over-blocking, under-refusal", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}
]]></acme>

<pubDate>2025-10-21T01:30:31+00:00</pubDate>
</item>
<item>
<title>EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation</title>
<link>https://papers.cool/arxiv/2510.18213</link>
<guid>https://papers.cool/arxiv/2510.18213</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes EMA-SAM, a lightweight extension of Segment Anything Model 2 that adds a confidence-weighted exponential moving average pointer to maintain a stable latent prototype of papillary thyroid microcarcinoma lesions across ultrasound video frames. This approach improves temporal coherence and segmentation performance on a curated PTMC-RFA dataset and external video benchmarks while adding negligible computational overhead. The method demonstrates real-time applicability for interventional ultrasound with consistent Dice score gains over the baseline SAM-2.<br /><strong>Summary (CN):</strong> 本文提出 EMA‑SAM，这是一种在 Segment Anything Model 2 基础上加入置信度加权指数移动平均指针的轻量化扩展，用于在超声视频帧间保持甲状腺微癌病灶的稳定潜在原型，实现时间连贯性。该方法在自建 PTMC‑RFA 数据集及外部视频基准上提升了分割性能，且计算开销极小，保持约 30 FPS 的实时推理。<br /><strong>Keywords:</strong> exponential moving average, SAM-2, ultrasound segmentation, PTMC, temporal coherence, medical video segmentation, real-time inference<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon</div>
Papillary thyroid microcarcinoma (PTMC) is increasingly managed with radio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasound videos remains difficult due to low contrast, probe-induced motion, and heat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizes well to static images, but its frame-independent design yields unstable predictions and temporal drift in interventional ultrasound. We introduce \textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates a confidence-weighted exponential moving average pointer into the memory bank, providing a stable latent prototype of the tumour across frames. This design preserves temporal coherence through probe pressure and bubble occlusion while rapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset (124 minutes, 13 patients), EMA-SAM improves \emph{maxDice} from 0.82 (SAM-2) to 0.86 and \emph{maxIoU} from 0.72 to 0.76, while reducing false positives by 29\%. On external benchmarks, including VTUS and colonoscopy video polyp datasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2. Importantly, the EMA pointer adds \textless0.1\% FLOPs, preserving real-time throughput of $\sim$30\,FPS on a single A100 GPU. These results establish EMA-SAM as a robust and efficient framework for stable tumour tracking, bridging the gap between foundation models and the stringent demands of interventional ultrasound. Codes are available here \hyperref[code {https://github.com/mdialameh/EMA-SAM}.
<div><strong>Authors:</strong> Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes EMA-SAM, a lightweight extension of Segment Anything Model 2 that adds a confidence-weighted exponential moving average pointer to maintain a stable latent prototype of papillary thyroid microcarcinoma lesions across ultrasound video frames. This approach improves temporal coherence and segmentation performance on a curated PTMC-RFA dataset and external video benchmarks while adding negligible computational overhead. The method demonstrates real-time applicability for interventional ultrasound with consistent Dice score gains over the baseline SAM-2.", "summary_cn": "本文提出 EMA‑SAM，这是一种在 Segment Anything Model 2 基础上加入置信度加权指数移动平均指针的轻量化扩展，用于在超声视频帧间保持甲状腺微癌病灶的稳定潜在原型，实现时间连贯性。该方法在自建 PTMC‑RFA 数据集及外部视频基准上提升了分割性能，且计算开销极小，保持约 30 FPS 的实时推理。", "keywords": "exponential moving average, SAM-2, ultrasound segmentation, PTMC, temporal coherence, medical video segmentation, real-time inference", "scoring": {"interpretability": 3, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Maryam Dialameh", "Hossein Rajabzadeh", "Jung Suk Sim", "Hyock Ju Kwon"]}
]]></acme>

<pubDate>2025-10-21T01:30:27+00:00</pubDate>
</item>
<item>
<title>RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</title>
<link>https://papers.cool/arxiv/2510.18188</link>
<guid>https://papers.cool/arxiv/2510.18188</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RadDiagSeg-D, a new dataset that unifies abnormality detection, diagnosis, and multi-target segmentation for radiology, and presents RadDiagSeg-M, a vision‑language model that can jointly generate diagnostic text and corresponding segmentation masks. Experiments demonstrate that RadDiagSeg-M achieves strong performance across all components of this combined task, providing a competitive baseline for future research.<br /><strong>Summary (CN):</strong> 本文推出了 RadDiagSeg-D 数据集，将异常检测、诊断和多目标分割统一为放射学中的层次任务，并提出了能够同步生成诊断文本和对应分割掩码的视觉语言模型 RadDiagSeg-M。实验表明，该模型在文本与掩码联合生成的各子任务上表现优异，为后续研究提供了稳健的基准。<br /><strong>Keywords:</strong> vision-language model, radiology, joint diagnosis, multi-target segmentation, medical imaging, dataset, abnormality detection, text-and-mask generation, RadDiagSeg-M<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</div>
Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.
<div><strong>Authors:</strong> Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RadDiagSeg-D, a new dataset that unifies abnormality detection, diagnosis, and multi-target segmentation for radiology, and presents RadDiagSeg-M, a vision‑language model that can jointly generate diagnostic text and corresponding segmentation masks. Experiments demonstrate that RadDiagSeg-M achieves strong performance across all components of this combined task, providing a competitive baseline for future research.", "summary_cn": "本文推出了 RadDiagSeg-D 数据集，将异常检测、诊断和多目标分割统一为放射学中的层次任务，并提出了能够同步生成诊断文本和对应分割掩码的视觉语言模型 RadDiagSeg-M。实验表明，该模型在文本与掩码联合生成的各子任务上表现优异，为后续研究提供了稳健的基准。", "keywords": "vision-language model, radiology, joint diagnosis, multi-target segmentation, medical imaging, dataset, abnormality detection, text-and-mask generation, RadDiagSeg-M", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chengrun Li", "Corentin Royer", "Haozhe Luo", "Bastian Wittmann", "Xia Li", "Ibrahim Hamamci", "Sezgin Er", "Anjany Sekuboyina", "Bjoern Menze"]}
]]></acme>

<pubDate>2025-10-21T00:28:13+00:00</pubDate>
</item>
<item>
<title>VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis</title>
<link>https://papers.cool/arxiv/2510.18187</link>
<guid>https://papers.cool/arxiv/2510.18187</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> VelocityNet proposes a dual‑pipeline system that combines head detection with dense optical‑flow analysis to compute individual pedestrian velocities in crowded scenes, then clusters these velocities into semantic motion categories and applies a percentile‑based scoring to flag deviations as anomalies. Experiments show it can operate in real time and detect varied abnormal motion patterns despite occlusions and density changes.<br /><strong>Summary (CN):</strong> VelocityNet 提出一种双管道方案，将人头检测与密集光流相结合，提取每个人的运动速度，再通过层次聚类将速度划分为停顿、缓慢、正常和快速等语义类别，并使用基于百分位的异常评分衡量与学习的正常模式的偏差。实验表明该方法在高密度拥挤环境中能够实时检测多样的异常运动。<br /><strong>Keywords:</strong> crowd anomaly detection, person-specific velocity, optical flow, hierarchical clustering, real-time monitoring<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Fatima AlGhamdi, Omar Alharbi, Abdullah Aldwyish, Raied Aljadaany, Muhammad Kamran J Khan, Huda Alamri</div>
Detecting anomalies in crowded scenes is challenging due to severe inter-person occlusions and highly dynamic, context-dependent motion patterns. Existing approaches often struggle to adapt to varying crowd densities and lack interpretable anomaly indicators. To address these limitations, we introduce VelocityNet, a dual-pipeline framework that combines head detection and dense optical flow to extract person-specific velocities. Hierarchical clustering categorizes these velocities into semantic motion classes (halt, slow, normal, and fast), and a percentile-based anomaly scoring system measures deviations from learned normal patterns. Experiments demonstrate the effectiveness of our framework in real-time detection of diverse anomalous motion patterns within densely crowded environments.
<div><strong>Authors:</strong> Fatima AlGhamdi, Omar Alharbi, Abdullah Aldwyish, Raied Aljadaany, Muhammad Kamran J Khan, Huda Alamri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "VelocityNet proposes a dual‑pipeline system that combines head detection with dense optical‑flow analysis to compute individual pedestrian velocities in crowded scenes, then clusters these velocities into semantic motion categories and applies a percentile‑based scoring to flag deviations as anomalies. Experiments show it can operate in real time and detect varied abnormal motion patterns despite occlusions and density changes.", "summary_cn": "VelocityNet 提出一种双管道方案，将人头检测与密集光流相结合，提取每个人的运动速度，再通过层次聚类将速度划分为停顿、缓慢、正常和快速等语义类别，并使用基于百分位的异常评分衡量与学习的正常模式的偏差。实验表明该方法在高密度拥挤环境中能够实时检测多样的异常运动。", "keywords": "crowd anomaly detection, person-specific velocity, optical flow, hierarchical clustering, real-time monitoring", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Fatima AlGhamdi", "Omar Alharbi", "Abdullah Aldwyish", "Raied Aljadaany", "Muhammad Kamran J Khan", "Huda Alamri"]}
]]></acme>

<pubDate>2025-10-21T00:26:54+00:00</pubDate>
</item>
<item>
<title>Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset</title>
<link>https://papers.cool/arxiv/2510.18172</link>
<guid>https://papers.cool/arxiv/2510.18172</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents LunarStereo, a novel photorealistic stereo dataset of the Moon created via ray tracing of high‑resolution topography and reflectance models, covering diverse altitudes, lighting, and viewing angles near the lunar South Pole. Using this dataset, the authors fine‑tune the MASt3R model for lunar 3D surface reconstruction and relative pose estimation, demonstrating significant performance gains over zero‑shot baselines on both synthetic and real lunar data.<br /><strong>Summary (CN):</strong> 本文推出 LunarStereo 数据集，通过光线追踪技术基于高分辨率月球地形和反射模型生成逼真的月球立体图像，涵盖不同高度、光照以及南极附近的视角。作者在此数据集上对 MASt3R 模型进行微调，实现月球三维表面重建和相对姿态估计，并在合成和真实月球数据上显著超越零样本基线。<br /><strong>Keywords:</strong> stereo vision, lunar surface reconstruction, photorealistic dataset, MASt3R, pose estimation, deep learning, planetary imagery, cross-scale generalization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Clementine Grethen, Simone Gasparini, Geraldine Morin, Jeremy Lebreton, Lucas Marti, Manuel Sanchez-Gestido</div>
Accurate 3D reconstruction of lunar surfaces is essential for space exploration. However, existing stereo vision reconstruction methods struggle in this context due to the Moon's lack of texture, difficult lighting variations, and atypical orbital trajectories. State-of-the-art deep learning models, trained on human-scale datasets, have rarely been tested on planetary imagery and cannot be transferred directly to lunar conditions. To address this issue, we introduce LunarStereo, the first open dataset of photorealistic stereo image pairs of the Moon, simulated using ray tracing based on high-resolution topography and reflectance models. It covers diverse altitudes, lighting conditions, and viewing angles around the lunar South Pole, offering physically grounded supervision for 3D reconstruction tasks. Based on this dataset, we adapt the MASt3R model to the lunar domain through fine-tuning on LunarStereo. We validate our approach through extensive qualitative and quantitative experiments on both synthetic and real lunar data, evaluating 3D surface reconstruction and relative pose estimation. Extensive experiments on synthetic and real lunar data validate the approach, demonstrating significant improvements over zero-shot baselines and paving the way for robust cross-scale generalization in extraterrestrial environments.
<div><strong>Authors:</strong> Clementine Grethen, Simone Gasparini, Geraldine Morin, Jeremy Lebreton, Lucas Marti, Manuel Sanchez-Gestido</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents LunarStereo, a novel photorealistic stereo dataset of the Moon created via ray tracing of high‑resolution topography and reflectance models, covering diverse altitudes, lighting, and viewing angles near the lunar South Pole. Using this dataset, the authors fine‑tune the MASt3R model for lunar 3D surface reconstruction and relative pose estimation, demonstrating significant performance gains over zero‑shot baselines on both synthetic and real lunar data.", "summary_cn": "本文推出 LunarStereo 数据集，通过光线追踪技术基于高分辨率月球地形和反射模型生成逼真的月球立体图像，涵盖不同高度、光照以及南极附近的视角。作者在此数据集上对 MASt3R 模型进行微调，实现月球三维表面重建和相对姿态估计，并在合成和真实月球数据上显著超越零样本基线。", "keywords": "stereo vision, lunar surface reconstruction, photorealistic dataset, MASt3R, pose estimation, deep learning, planetary imagery, cross-scale generalization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Clementine Grethen", "Simone Gasparini", "Geraldine Morin", "Jeremy Lebreton", "Lucas Marti", "Manuel Sanchez-Gestido"]}
]]></acme>

<pubDate>2025-10-20T23:50:52+00:00</pubDate>
</item>
<item>
<title>World-in-World: World Models in a Closed-Loop World</title>
<link>https://papers.cool/arxiv/2510.18135</link>
<guid>https://papers.cool/arxiv/2510.18135</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces World-in-World, a closed-loop benchmarking platform that evaluates generative world models based on their utility for embodied decision‑making rather than visual fidelity alone. It presents four environments, a unified online planning interface, and the first scaling law for world models in embodied settings, uncovering that controllability, post‑training data scaling, and inference‑time compute are more critical than visual quality for task success.<br /><strong>Summary (CN):</strong> 本文推出了 World-in-World 平台，在闭环环境中评估生成式世界模型对具身智能体决策的实用性，而不仅仅是视觉质量。通过四个环境、统一的在线规划接口以及首个针对具身设置的规模律研究，发现可控性、后训练数据扩展和推理计算资源比视觉逼真度更能提升任务成功率。<br /><strong>Keywords:</strong> world models, closed-loop benchmark, embodied agents, planning, scaling laws, generative models, task success, inference compute<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellappa, Tianmin Shu, Alan Yuille, Yilun Du, Jieneng Chen</div>
Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.
<div><strong>Authors:</strong> Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellappa, Tianmin Shu, Alan Yuille, Yilun Du, Jieneng Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces World-in-World, a closed-loop benchmarking platform that evaluates generative world models based on their utility for embodied decision‑making rather than visual fidelity alone. It presents four environments, a unified online planning interface, and the first scaling law for world models in embodied settings, uncovering that controllability, post‑training data scaling, and inference‑time compute are more critical than visual quality for task success.", "summary_cn": "本文推出了 World-in-World 平台，在闭环环境中评估生成式世界模型对具身智能体决策的实用性，而不仅仅是视觉质量。通过四个环境、统一的在线规划接口以及首个针对具身设置的规模律研究，发现可控性、后训练数据扩展和推理计算资源比视觉逼真度更能提升任务成功率。", "keywords": "world models, closed-loop benchmark, embodied agents, planning, scaling laws, generative models, task success, inference compute", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Jiahan Zhang", "Muqing Jiang", "Nanru Dai", "Taiming Lu", "Arda Uzunoglu", "Shunchi Zhang", "Yana Wei", "Jiahao Wang", "Vishal M. Patel", "Paul Pu Liang", "Daniel Khashabi", "Cheng Peng", "Rama Chellappa", "Tianmin Shu", "Alan Yuille", "Yilun Du", "Jieneng Chen"]}
]]></acme>

<pubDate>2025-10-20T22:09:15+00:00</pubDate>
</item>
<item>
<title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title>
<link>https://papers.cool/arxiv/2510.18123</link>
<guid>https://papers.cool/arxiv/2510.18123</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper systematically studies safety and security challenges of using natural language as the communication medium in V2X collaborative driving, presenting a taxonomy of attack strategies and proposing the SafeCoop defense pipeline with semantic firewalls and multi‑source consensus. Experiments in CARLA across 32 critical scenarios demonstrate significant improvements in driving performance and attack detection under malicious conditions.<br /><strong>Summary (CN):</strong> 本文系统性地研究了在 V2X 协同驾驶中使用自然语言作为通信介质所带来的安全与安全性挑战，提出了攻击策略分类并设计了 SafeCoop 防御流水线，包括语义防火墙和多源共识机制。 在 CARLA 仿真中的 32 种关键场景实验表明，在恶意攻击下该系统显著提升了驾驶性能和攻击检测能力。<br /><strong>Keywords:</strong> natural language communication, V2X, collaborative driving, safety, security, adversarial attacks, semantic firewall, CARLA simulation, agentic defense, multi-source consensus<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is https://xiangbogaobarry.github.io/SafeCoop.
<div><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper systematically studies safety and security challenges of using natural language as the communication medium in V2X collaborative driving, presenting a taxonomy of attack strategies and proposing the SafeCoop defense pipeline with semantic firewalls and multi‑source consensus. Experiments in CARLA across 32 critical scenarios demonstrate significant improvements in driving performance and attack detection under malicious conditions.", "summary_cn": "本文系统性地研究了在 V2X 协同驾驶中使用自然语言作为通信介质所带来的安全与安全性挑战，提出了攻击策略分类并设计了 SafeCoop 防御流水线，包括语义防火墙和多源共识机制。 在 CARLA 仿真中的 32 种关键场景实验表明，在恶意攻击下该系统显著提升了驾驶性能和攻击检测能力。", "keywords": "natural language communication, V2X, collaborative driving, safety, security, adversarial attacks, semantic firewall, CARLA simulation, agentic defense, multi-source consensus", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}
]]></acme>

<pubDate>2025-10-20T21:41:28+00:00</pubDate>
</item>
<item>
<title>Online In-Context Distillation for Low-Resource Vision Language Models</title>
<link>https://papers.cool/arxiv/2510.18117</link>
<guid>https://papers.cool/arxiv/2510.18117</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Online In-Context Distillation (ICD), a method where a small vision-language model learns from a stronger teacher at inference time using sparse cross-modal demonstrations, enabling strong performance with minimal teacher annotations and limited compute. It presents an analysis of when vision-language in-context learning is feasible, proposes a novel demonstration selection strategy, test-time teacher scaling, and student uncertainty conditioning to reduce queries. Experiments show up to 33% performance gains for small models with as few as 4% of teacher data, approaching the teacher's zero-shot ability.<br /><strong>Summary (CN):</strong> 本文提出了在线上下文蒸馏（Online In-Context Distillation，ICD）方法，让小型视觉语言模型在推理时通过稀疏的跨模态示例向更强的教师模型学习，从而在计算和注释资源极其有限的情况下实现显著性能提升。文章先分析了视觉语言模型何时能够进行上下文学习，并提出了跨模态示例选择、教师测试时尺度缩放以及学生不确定性条件化等新技术，以减少教师查询次数。实验表明，在仅使用约 4% 教师标注的情况下，小模型性能提升最高可达 33%，并可逼近教师的 zero-shot 表现。<br /><strong>Keywords:</strong> online in-context distillation, vision-language models, low-resource adaptation, teacher-student distillation, cross-modal demonstration selection, test-time scaling, uncertainty conditioning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari</div>
As the field continues its push for ever more resources, this work turns the spotlight on a critical question: how can vision-language models (VLMs) be adapted to thrive in low-resource, budget-constrained settings? While large VLMs offer strong performance, they are impractical to deploy in such settings. Small VLMs, on the other hand, are efficient but typically require costly fine-tuning to close the performance gap with larger models in the deployment domain. Inspired by the in-context learning framework, we propose an online In-Context Distillation (ICD) method, in which a small VLM collaborates with a stronger teacher model at inference time, distilling its knowledge via sparse demonstrations to efficiently bridge the gap between them. Our method is built on an in-depth analysis that identifies the scale and the choice of models for which vision-language ICL is currently feasible, and demonstrates the advantage of ICL over fine-tuning under constrained compute budgets. We enhance our method with a novel cross-modal demonstration selection strategy, teacher test-time scaling to reduce noise, and student uncertainty conditioning to dynamically populate a demonstration pool and minimize teacher queries. Our ICD method significantly boosts the performance of small models (up to 33%) using scarce teacher annotations (as low as 4%), and competes with the teacher's zero-shot performance.
<div><strong>Authors:</strong> Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Online In-Context Distillation (ICD), a method where a small vision-language model learns from a stronger teacher at inference time using sparse cross-modal demonstrations, enabling strong performance with minimal teacher annotations and limited compute. It presents an analysis of when vision-language in-context learning is feasible, proposes a novel demonstration selection strategy, test-time teacher scaling, and student uncertainty conditioning to reduce queries. Experiments show up to 33% performance gains for small models with as few as 4% of teacher data, approaching the teacher's zero-shot ability.", "summary_cn": "本文提出了在线上下文蒸馏（Online In-Context Distillation，ICD）方法，让小型视觉语言模型在推理时通过稀疏的跨模态示例向更强的教师模型学习，从而在计算和注释资源极其有限的情况下实现显著性能提升。文章先分析了视觉语言模型何时能够进行上下文学习，并提出了跨模态示例选择、教师测试时尺度缩放以及学生不确定性条件化等新技术，以减少教师查询次数。实验表明，在仅使用约 4% 教师标注的情况下，小模型性能提升最高可达 33%，并可逼近教师的 zero-shot 表现。", "keywords": "online in-context distillation, vision-language models, low-resource adaptation, teacher-student distillation, cross-modal demonstration selection, test-time scaling, uncertainty conditioning", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zhiqi Kang", "Rahaf Aljundi", "Vaggelis Dorovatas", "Karteek Alahari"]}
]]></acme>

<pubDate>2025-10-20T21:35:17+00:00</pubDate>
</item>
<item>
<title>From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</title>
<link>https://papers.cool/arxiv/2510.18101</link>
<guid>https://papers.cool/arxiv/2510.18101</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This tutorial provides a concise overview of the 3D Gaussian Splatting (3DGS) pipeline, describing its splatting formulation and addressing limitations such as high memory usage, baked lighting, and limited secondary-ray support. It also surveys applications of 3DGS in surface reconstruction, avatar modeling, animation, and content generation, highlighting its real-time rendering capabilities and suitability for feed‑forward pipelines.<br /><strong>Summary (CN):</strong> 本文综述了 3D 高斯点绘（3DGS）管线，包括其体素渲染公式、内存占用大、光照直接烘焙以及次级光线支持有限等局限，并探讨了针对这些问题的改进。随后展示了 3DGS 在表面重建、化身建模、动画和内容生成等应用中的高效实时渲染优势，强调其在前向渲染流水线中的适用性。<br /><strong>Keywords:</strong> 3D Gaussian Splatting, volume rendering, novel view synthesis, real-time rendering, surface reconstruction, avatar modeling, animation, content generation, memory efficiency, graphics pipeline<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello</div>
The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.
<div><strong>Authors:</strong> Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This tutorial provides a concise overview of the 3D Gaussian Splatting (3DGS) pipeline, describing its splatting formulation and addressing limitations such as high memory usage, baked lighting, and limited secondary-ray support. It also surveys applications of 3DGS in surface reconstruction, avatar modeling, animation, and content generation, highlighting its real-time rendering capabilities and suitability for feed‑forward pipelines.", "summary_cn": "本文综述了 3D 高斯点绘（3DGS）管线，包括其体素渲染公式、内存占用大、光照直接烘焙以及次级光线支持有限等局限，并探讨了针对这些问题的改进。随后展示了 3DGS 在表面重建、化身建模、动画和内容生成等应用中的高效实时渲染优势，强调其在前向渲染流水线中的适用性。", "keywords": "3D Gaussian Splatting, volume rendering, novel view synthesis, real-time rendering, surface reconstruction, avatar modeling, animation, content generation, memory efficiency, graphics pipeline", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vitor Pereira Matias", "Daniel Perazzo", "Vinicius Silva", "Alberto Raposo", "Luiz Velho", "Afonso Paiva", "Tiago Novello"]}
]]></acme>

<pubDate>2025-10-20T20:52:41+00:00</pubDate>
</item>
<item>
<title>Accelerating Vision Transformers with Adaptive Patch Sizes</title>
<link>https://papers.cool/arxiv/2510.18091</link>
<guid>https://papers.cool/arxiv/2510.18091</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Patch Transformers (APT), which employ multiple patch sizes within a single image to allocate larger patches to homogeneous regions and smaller patches to complex regions. This adaptive tokenization reduces the total number of tokens, yielding 40%‑50% speedups for large ViT models while preserving downstream performance, and can be applied to already fine‑tuned models with rapid convergence.<br /><strong>Summary (CN):</strong> 本文提出了自适应切片视觉变换器（Adaptive Patch Transformers, APT），在同一图像中使用不同尺度的切片：在内容均匀的区域使用更大的切片，在复杂区域使用更小的切片。该方法通过减少 token 数量，使得 ViT‑L 和 ViT‑H 的推理及训练吞吐量分别提升约 40%‑50%，且对下游任务性能影响不大，可直接应用于已微调的 ViT 并在约 1 个 epoch 内收敛。<br /><strong>Keywords:</strong> vision transformer, adaptive patch size, dynamic token allocation, efficient inference, model acceleration, computer vision, dense prediction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\% faster training and inference in visual QA, object detection, and semantic segmentation.
<div><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Patch Transformers (APT), which employ multiple patch sizes within a single image to allocate larger patches to homogeneous regions and smaller patches to complex regions. This adaptive tokenization reduces the total number of tokens, yielding 40%‑50% speedups for large ViT models while preserving downstream performance, and can be applied to already fine‑tuned models with rapid convergence.", "summary_cn": "本文提出了自适应切片视觉变换器（Adaptive Patch Transformers, APT），在同一图像中使用不同尺度的切片：在内容均匀的区域使用更大的切片，在复杂区域使用更小的切片。该方法通过减少 token 数量，使得 ViT‑L 和 ViT‑H 的推理及训练吞吐量分别提升约 40%‑50%，且对下游任务性能影响不大，可直接应用于已微调的 ViT 并在约 1 个 epoch 内收敛。", "keywords": "vision transformer, adaptive patch size, dynamic token allocation, efficient inference, model acceleration, computer vision, dense prediction", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rohan Choudhury", "JungEun Kim", "Jinhyung Park", "Eunho Yang", "László A. Jeni", "Kris M. Kitani"]}
]]></acme>

<pubDate>2025-10-20T20:37:11+00:00</pubDate>
</item>
<item>
<title>Big Data, Tiny Targets: An Exploratory Study in Machine Learning-enhanced Detection of Microplastic from Filters</title>
<link>https://papers.cool/arxiv/2510.18089</link>
<guid>https://papers.cool/arxiv/2510.18089</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper explores the use of machine learning, specifically YOLO object detection models, to identify and quantify microplastic particles and fibres in scanning electron microscopy (SEM) images of filtered samples. It evaluates model performance, emphasizes the importance of preprocessing, and highlights challenges such as limited labeled data for reliable training.<br /><strong>Summary (CN):</strong> 本文研究了利用机器学习（YOLO 目标检测模型）在扫描电子显微镜（SEM）图像中检测并量化过滤样本中的微塑料颗粒和纤维的方法。文中评估了模型效果，强调了预处理的重要性，并指出了如专家标注数据不足等训练挑战。<br /><strong>Keywords:</strong> microplastics, SEM imaging, object detection, YOLO, machine learning, environmental monitoring<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Paul-Tiberiu Miclea, Martin Sboron, Hardik Vaghasiya, Hoang Thinh Nguyen, Meet Gadara, Thomas Schmid</div>
Microplastics (MPs) are ubiquitous pollutants with demonstrated potential to impact ecosystems and human health. Their microscopic size complicates detection, classification, and removal, especially in biological and environmental samples. While techniques like optical microscopy, Scanning Electron Microscopy (SEM), and Atomic Force Microscopy (AFM) provide a sound basis for detection, applying these approaches requires usually manual analysis and prevents efficient use in large screening studies. To this end, machine learning (ML) has emerged as a powerful tool in advancing microplastic detection. In this exploratory study, we investigate potential, limitations and future directions of advancing the detection and quantification of MP particles and fibres using a combination of SEM imaging and machine learning-based object detection. For simplicity, we focus on a filtration scenario where image backgrounds exhibit a symmetric and repetitive pattern. Our findings indicate differences in the quality of YOLO models for the given task and the relevance of optimizing preprocessing. At the same time, we identify open challenges, such as limited amounts of expert-labeled data necessary for reliable training of ML models.
<div><strong>Authors:</strong> Paul-Tiberiu Miclea, Martin Sboron, Hardik Vaghasiya, Hoang Thinh Nguyen, Meet Gadara, Thomas Schmid</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper explores the use of machine learning, specifically YOLO object detection models, to identify and quantify microplastic particles and fibres in scanning electron microscopy (SEM) images of filtered samples. It evaluates model performance, emphasizes the importance of preprocessing, and highlights challenges such as limited labeled data for reliable training.", "summary_cn": "本文研究了利用机器学习（YOLO 目标检测模型）在扫描电子显微镜（SEM）图像中检测并量化过滤样本中的微塑料颗粒和纤维的方法。文中评估了模型效果，强调了预处理的重要性，并指出了如专家标注数据不足等训练挑战。", "keywords": "microplastics, SEM imaging, object detection, YOLO, machine learning, environmental monitoring", "scoring": {"interpretability": 2, "understanding": 3, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Paul-Tiberiu Miclea", "Martin Sboron", "Hardik Vaghasiya", "Hoang Thinh Nguyen", "Meet Gadara", "Thomas Schmid"]}
]]></acme>

<pubDate>2025-10-20T20:35:50+00:00</pubDate>
</item>
<item>
<title>Chimera: Compositional Image Generation using Part-based Concepting</title>
<link>https://papers.cool/arxiv/2510.18083</link>
<guid>https://papers.cool/arxiv/2510.18083</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Chimera, a personalized image generation model that can compose novel objects by combining specified parts from multiple source images based on textual instructions, without requiring explicit masks or annotations. It builds a dataset of semantic atoms (part, subject pairs), trains a diffusion prior with part-conditional guidance, and proposes the PartEval metric to evaluate part fidelity and compositional accuracy, showing significant improvements over baselines.<br /><strong>Summary (CN):</strong> 本文提出 Chimera，一种个性化图像生成模型，能够根据文本指令将多个源图像的指定部件组合成新对象，无需用户提供掩码或标注。作者构建了语义原子（部件、主题）数据集，训练了带部件条件引导的扩散先验模型，并推出 PartEval 指标评估部件保真度和组合准确性，实验表明相较基线在部件对齐和视觉质量上均有显著提升。<br /><strong>Keywords:</strong> compositional image generation, diffusion prior, part-conditional guidance, semantic atoms, PartEval, personalized generative models<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</div>
Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.
<div><strong>Authors:</strong> Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Chimera, a personalized image generation model that can compose novel objects by combining specified parts from multiple source images based on textual instructions, without requiring explicit masks or annotations. It builds a dataset of semantic atoms (part, subject pairs), trains a diffusion prior with part-conditional guidance, and proposes the PartEval metric to evaluate part fidelity and compositional accuracy, showing significant improvements over baselines.", "summary_cn": "本文提出 Chimera，一种个性化图像生成模型，能够根据文本指令将多个源图像的指定部件组合成新对象，无需用户提供掩码或标注。作者构建了语义原子（部件、主题）数据集，训练了带部件条件引导的扩散先验模型，并推出 PartEval 指标评估部件保真度和组合准确性，实验表明相较基线在部件对齐和视觉质量上均有显著提升。", "keywords": "compositional image generation, diffusion prior, part-conditional guidance, semantic atoms, PartEval, personalized generative models", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shivam Singh", "Yiming Chen", "Agneet Chatterjee", "Amit Raj", "James Hays", "Yezhou Yang", "Chitra Baral"]}
]]></acme>

<pubDate>2025-10-20T20:20:47+00:00</pubDate>
</item>
<item>
<title>HouseTour: A Virtual Real Estate A(I)gent</title>
<link>https://papers.cool/arxiv/2510.18054</link>
<guid>https://papers.cool/arxiv/2510.18054</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HouseTour presents a pipeline that generates spatially-aware 3D camera trajectories and natural language real‑estate descriptions from collections of images, using a diffusion process constrained by known camera poses and 3D Gaussian splatting for novel view synthesis. The authors also release the HouseTour dataset containing over 1,200 house‑tour videos with poses, 3D reconstructions, and professional descriptions, and introduce a joint metric to evaluate end‑to‑end performance.<br /><strong>Summary (CN):</strong> HouseTour 提出了一套从房屋图片集合生成空间感知的 3D 相机轨迹以及自然语言房产描述的系统，采用受相机位姿约束的扩散过程并使用 3D 高斯 splatting 合成新视角视频。论文同时发布了包含 1200 多段房屋巡游视频、相机位姿、3D 重建和专业描述的 HouseTour 数据集，并引入联合评估指标来衡量端到端表现。<br /><strong>Keywords:</strong> 3D trajectory generation, diffusion model, Gaussian splatting, vision-language model, real estate video, dataset, spatial reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.
<div><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HouseTour presents a pipeline that generates spatially-aware 3D camera trajectories and natural language real‑estate descriptions from collections of images, using a diffusion process constrained by known camera poses and 3D Gaussian splatting for novel view synthesis. The authors also release the HouseTour dataset containing over 1,200 house‑tour videos with poses, 3D reconstructions, and professional descriptions, and introduce a joint metric to evaluate end‑to‑end performance.", "summary_cn": "HouseTour 提出了一套从房屋图片集合生成空间感知的 3D 相机轨迹以及自然语言房产描述的系统，采用受相机位姿约束的扩散过程并使用 3D 高斯 splatting 合成新视角视频。论文同时发布了包含 1200 多段房屋巡游视频、相机位姿、3D 重建和专业描述的 HouseTour 数据集，并引入联合评估指标来衡量端到端表现。", "keywords": "3D trajectory generation, diffusion model, Gaussian splatting, vision-language model, real estate video, dataset, spatial reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"]}
]]></acme>

<pubDate>2025-10-20T19:47:35+00:00</pubDate>
</item>
<item>
<title>TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation</title>
<link>https://papers.cool/arxiv/2510.18038</link>
<guid>https://papers.cool/arxiv/2510.18038</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces TriggerNet, an explainable AI framework that combines Grad‑CAM, RISE, FullGrad and TCAV to produce visual explanations for deep‑learning plant classification and disease detection models. It evaluates a range of CNN, EfficientNet, MobileNet, ViT, ResNet‑50, InceptionV3 and classic classifiers on a multi‑species RGB dataset for red‑palm‑mite infestation, and uses Snorkel’s heuristic‑guided labeling to annotate disease categories efficiently.<br /><strong>Summary (CN):</strong> 本文提出了 TriggerNet 可解释人工智能框架，将 Grad‑CAM、RISE、FullGrad 和 TCAV 融合生成深度学习植物分类和病害检测模型的可视化解释。针对红棕螨侵害，作者在包含 11 种植物的 RGB 数据集上评估了 CNN、EfficientNet、MobileNet、ViT、ResNet‑50、InceptionV3 等模型以及传统分类器，并使用 Snorkel 的启发式规则进行病害类别的高效标注。<br /><strong>Keywords:</strong> explainable AI, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, Snorkel, multi-model comparison<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
The red palm mite infestation has become a serious concern, particularly in regions with extensive palm cultivation, leading to reduced productivity and economic losses. Accurate and early identification of mite-infested plants is critical for effective management. The current study focuses on evaluating and comparing the ML model for classifying the affected plants and detecting the infestation. TriggerNet is a novel interpretable AI framework that integrates Grad-CAM, RISE, FullGrad, and TCAV to generate novel visual explanations for deep learning models in plant classification and disease detection. This study applies TriggerNet to address red palm mite (Raoiella indica) infestation, a major threat to palm cultivation and agricultural productivity. A diverse set of RGB images across 11 plant species, Arecanut, Date Palm, Bird of Paradise, Coconut Palm, Ginger, Citrus Tree, Palm Oil, Orchid, Banana Palm, Avocado Tree, and Cast Iron Plant was utilized for training and evaluation. Advanced deep learning models like CNN, EfficientNet, MobileNet, ViT, ResNet50, and InceptionV3, alongside machine learning classifiers such as Random Forest, SVM, and KNN, were employed for plant classification. For disease classification, all plants were categorized into four classes: Healthy, Yellow Spots, Reddish Bronzing, and Silk Webbing. Snorkel was used to efficiently label these disease classes by leveraging heuristic rules and patterns, reducing manual annotation time and improving dataset reliability.
<div><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces TriggerNet, an explainable AI framework that combines Grad‑CAM, RISE, FullGrad and TCAV to produce visual explanations for deep‑learning plant classification and disease detection models. It evaluates a range of CNN, EfficientNet, MobileNet, ViT, ResNet‑50, InceptionV3 and classic classifiers on a multi‑species RGB dataset for red‑palm‑mite infestation, and uses Snorkel’s heuristic‑guided labeling to annotate disease categories efficiently.", "summary_cn": "本文提出了 TriggerNet 可解释人工智能框架，将 Grad‑CAM、RISE、FullGrad 和 TCAV 融合生成深度学习植物分类和病害检测模型的可视化解释。针对红棕螨侵害，作者在包含 11 种植物的 RGB 数据集上评估了 CNN、EfficientNet、MobileNet、ViT、ResNet‑50、InceptionV3 等模型以及传统分类器，并使用 Snorkel 的启发式规则进行病害类别的高效标注。", "keywords": "explainable AI, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, Snorkel, multi-model comparison", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Harshini Suresha", "Kavitha SH"]}
]]></acme>

<pubDate>2025-10-20T19:23:17+00:00</pubDate>
</item>
<item>
<title>SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection</title>
<link>https://papers.cool/arxiv/2510.18034</link>
<guid>https://papers.cool/arxiv/2510.18034</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents SAVANT, a structured reasoning framework that leverages vision-language models to detect semantic anomalies in autonomous driving scenes through a two-phase pipeline of scene description extraction and multi-modal evaluation across four semantic layers. SAVANT achieves up to 93.8% accuracy and 90.8% recall using a fine-tuned open-source 7B model, outperforming baseline approaches and enabling cost-effective local deployment. The framework also automatically labels thousands of real-world images, addressing data scarcity for anomaly detection.<br /><strong>Summary (CN):</strong> 本文提出了 SAVANT 框架，通过分层场景描述提取和多模态评估，将视觉语言模型的推理转化为结构化分析，以检测自动驾驶中的语义异常。该方法在四个语义层面（街道、基础设施、可移动物体、环境）上实现了最高 93.8% 的准确率和 90.8% 的召回率，显著优于无结构基线，并支持本地部署的开源 7B 模型。SAVANT 还能自动标注数千张真实图像，缓解异常检测的数据稀缺问题。<br /><strong>Keywords:</strong> anomaly detection, autonomous driving, vision-language model, structured reasoning, semantic layers, open-source model, VLM prompting, out-of-distribution, safety monitoring, multi-modal evaluation<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</div>
Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.
<div><strong>Authors:</strong> Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents SAVANT, a structured reasoning framework that leverages vision-language models to detect semantic anomalies in autonomous driving scenes through a two-phase pipeline of scene description extraction and multi-modal evaluation across four semantic layers. SAVANT achieves up to 93.8% accuracy and 90.8% recall using a fine-tuned open-source 7B model, outperforming baseline approaches and enabling cost-effective local deployment. The framework also automatically labels thousands of real-world images, addressing data scarcity for anomaly detection.", "summary_cn": "本文提出了 SAVANT 框架，通过分层场景描述提取和多模态评估，将视觉语言模型的推理转化为结构化分析，以检测自动驾驶中的语义异常。该方法在四个语义层面（街道、基础设施、可移动物体、环境）上实现了最高 93.8% 的准确率和 90.8% 的召回率，显著优于无结构基线，并支持本地部署的开源 7B 模型。SAVANT 还能自动标注数千张真实图像，缓解异常检测的数据稀缺问题。", "keywords": "anomaly detection, autonomous driving, vision-language model, structured reasoning, semantic layers, open-source model, VLM prompting, out-of-distribution, safety monitoring, multi-modal evaluation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"]}
]]></acme>

<pubDate>2025-10-20T19:14:29+00:00</pubDate>
</item>
<item>
<title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
<link>https://papers.cool/arxiv/2510.18016</link>
<guid>https://papers.cool/arxiv/2510.18016</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ViBED-Net is a video‑based engagement detection network that uses a dual‑stream architecture to process facial crops and full‑scene video frames with EfficientNetV2 for spatial features, then models temporal dynamics with LSTM or Transformer encoders, achieving 73.43% accuracy on the DAiSEE dataset and outperforming prior methods.<br /><strong>Summary (CN):</strong> 本文提出ViBED-Net，一个基于视频的学生参与度检测网络，采用双流架构同时捕获面部表情和全场景信息，通过EfficientNetV2提取空间特征，再利用LSTM或Transformer进行时序建模，在DAiSEE数据集上实现73.43%的准确率，显示面部和场景线索的结合显著提升了参与度检测效果。<br /><strong>Keywords:</strong> engagement detection, affective computing, video analysis, dual‑stream network, EfficientNetV2, LSTM, Transformer, spatiotemporal cues, DAiSEE dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas</div>
Engagement detection in online learning environments is vital for improving student outcomes and personalizing instruction. We present ViBED-Net (Video-Based Engagement Detection Network), a novel deep learning framework designed to assess student engagement from video data using a dual-stream architecture. ViBED-Net captures both facial expressions and full-scene context by processing facial crops and entire video frames through EfficientNetV2 for spatial feature extraction. These features are then analyzed over time using two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and Transformer encoders. Our model is evaluated on the DAiSEE dataset, a large-scale benchmark for affective state recognition in e-learning. To enhance performance on underrepresented engagement classes, we apply targeted data augmentation techniques. Among the tested variants, ViBED-Net with LSTM achieves 73.43\% accuracy, outperforming existing state-of-the-art approaches. ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal cues significantly improves engagement detection accuracy. Its modular design allows flexibility for application across education, user experience research, and content personalization. This work advances video-based affective computing by offering a scalable, high-performing solution for real-world engagement analysis. The source code for this project is available on https://github.com/prateek-gothwal/ViBED-Net .
<div><strong>Authors:</strong> Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ViBED-Net is a video‑based engagement detection network that uses a dual‑stream architecture to process facial crops and full‑scene video frames with EfficientNetV2 for spatial features, then models temporal dynamics with LSTM or Transformer encoders, achieving 73.43% accuracy on the DAiSEE dataset and outperforming prior methods.", "summary_cn": "本文提出ViBED-Net，一个基于视频的学生参与度检测网络，采用双流架构同时捕获面部表情和全场景信息，通过EfficientNetV2提取空间特征，再利用LSTM或Transformer进行时序建模，在DAiSEE数据集上实现73.43%的准确率，显示面部和场景线索的结合显著提升了参与度检测效果。", "keywords": "engagement detection, affective computing, video analysis, dual‑stream network, EfficientNetV2, LSTM, Transformer, spatiotemporal cues, DAiSEE dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Prateek Gothwal", "Deeptimaan Banerjee", "Ashis Kumer Biswas"]}
]]></acme>

<pubDate>2025-10-20T18:48:25+00:00</pubDate>
</item>
<item>
<title>ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy</title>
<link>https://papers.cool/arxiv/2510.18014</link>
<guid>https://papers.cool/arxiv/2510.18014</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces ManzaiSet, a large-scale multimodal dataset comprising facial video and audio recordings of 241 participants reacting to Japanese manzai comedy performances, and presents analyses revealing distinct viewer response types and a positive viewing order effect. The dataset aims to mitigate Western bias in affective computing and supports culturally aware emotion AI development. Automated humor classification experiments show no significant differences across viewer types after statistical correction.<br /><strong>Summary (CN):</strong> 本文推出了 ManzaySet，一个包含 241 名参与者观看日本相声（manzai）表演时面部视频和音频的多模态大型数据集，并通过分析发现了三类观众反应模式以及观看顺序的正向效应。该数据集旨在降低情感计算中的西方中心偏差，促进面向非西方文化的情感 AI 开发。自动幽默分类实验表明在进行多重检验校正后，不同观众类型之间没有显著差异。<br /><strong>Keywords:</strong> multimodal dataset, viewer responses, Japanese manzai, affective computing, emotion AI, facial expression, humor classification, cultural bias<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kazuki Kawamura, Kengo Nakai, Jun Rekimoto</div>
We present ManzaiSet, the first large scale multimodal dataset of viewer responses to Japanese manzai comedy, capturing facial videos and audio from 241 participants watching up to 10 professional performances in randomized order (94.6 percent watched >= 8; analyses focus on n=228). This addresses the Western centric bias in affective computing. Three key findings emerge: (1) k means clustering identified three distinct viewer types: High and Stable Appreciators (72.8 percent, n=166), Low and Variable Decliners (13.2 percent, n=30), and Variable Improvers (14.0 percent, n=32), with heterogeneity of variance (Brown Forsythe p < 0.001); (2) individual level analysis revealed a positive viewing order effect (mean slope = 0.488, t(227) = 5.42, p < 0.001, permutation p < 0.001), contradicting fatigue hypotheses; (3) automated humor classification (77 instances, 131 labels) plus viewer level response modeling found no type wise differences after FDR correction. The dataset enables culturally aware emotion AI development and personalized entertainment systems tailored to non Western contexts.
<div><strong>Authors:</strong> Kazuki Kawamura, Kengo Nakai, Jun Rekimoto</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces ManzaiSet, a large-scale multimodal dataset comprising facial video and audio recordings of 241 participants reacting to Japanese manzai comedy performances, and presents analyses revealing distinct viewer response types and a positive viewing order effect. The dataset aims to mitigate Western bias in affective computing and supports culturally aware emotion AI development. Automated humor classification experiments show no significant differences across viewer types after statistical correction.", "summary_cn": "本文推出了 ManzaySet，一个包含 241 名参与者观看日本相声（manzai）表演时面部视频和音频的多模态大型数据集，并通过分析发现了三类观众反应模式以及观看顺序的正向效应。该数据集旨在降低情感计算中的西方中心偏差，促进面向非西方文化的情感 AI 开发。自动幽默分类实验表明在进行多重检验校正后，不同观众类型之间没有显著差异。", "keywords": "multimodal dataset, viewer responses, Japanese manzai, affective computing, emotion AI, facial expression, humor classification, cultural bias", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kazuki Kawamura", "Kengo Nakai", "Jun Rekimoto"]}
]]></acme>

<pubDate>2025-10-20T18:47:09+00:00</pubDate>
</item>
<item>
<title>Investigating Demographic Bias in Brain MRI Segmentation: A Comparative Study of Deep-Learning and Non-Deep-Learning Methods</title>
<link>https://papers.cool/arxiv/2510.17999</link>
<guid>https://papers.cool/arxiv/2510.17999</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates demographic bias in brain MRI segmentation by comparing three deep‑learning models (UNesT, nnU‑Net, CoTr) and a traditional atlas‑based method (ANTs) on left and right nucleus accumbens across four race‑sex subgroups. It quantifies fairness in segmentation performance and examines how race and sex affect segmentation accuracy and derived volumes, finding that race‑matched training improves some models while nnU‑Net remains robust, and that sex effects persist while race effects largely disappear.<br /><strong>Summary (CN):</strong> 本文评估了脑部 MRI 分割中的人口统计偏差，比较了三种深度学习模型（UNesT、nnU‑Net、CoTr）和一种传统的基于图谱的方法（ANTs）在四个人口子组（黑人女性、黑人男性、白人女性、白人男性）对左右伏隔核的分割表现。通过公平性指标量化分割性能，并分析种族和性别对分割准确性及体积的影响，发现种族匹配训练可提升部分模型的表现，而 nnU‑Net 在不同人口组间保持稳健；性别效应在手工和偏差模型中均可观察到，而种族效应在大多数模型中消失。<br /><strong>Keywords:</strong> ographic bias, medical image segmentation, MRI, fairness, UNesT, nnU-Net, CoTr, ANTs, nucleus accumbens, linear mixed models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Ghazal Danaee, Marc Niethammer, Jarrett Rushmore, Sylvain Bouix</div>
Deep-learning-based segmentation algorithms have substantially advanced the field of medical image analysis, particularly in structural delineations in MRIs. However, an important consideration is the intrinsic bias in the data. Concerns about unfairness, such as performance disparities based on sensitive attributes like race and sex, are increasingly urgent. In this work, we evaluate the results of three different segmentation models (UNesT, nnU-Net, and CoTr) and a traditional atlas-based method (ANTs), applied to segment the left and right nucleus accumbens (NAc) in MRI images. We utilize a dataset including four demographic subgroups: black female, black male, white female, and white male. We employ manually labeled gold-standard segmentations to train and test segmentation models. This study consists of two parts: the first assesses the segmentation performance of models, while the second measures the volumes they produce to evaluate the effects of race, sex, and their interaction. Fairness is quantitatively measured using a metric designed to quantify fairness in segmentation performance. Additionally, linear mixed models analyze the impact of demographic variables on segmentation accuracy and derived volumes. Training on the same race as the test subjects leads to significantly better segmentation accuracy for some models. ANTs and UNesT show notable improvements in segmentation accuracy when trained and tested on race-matched data, unlike nnU-Net, which demonstrates robust performance independent of demographic matching. Finally, we examine sex and race effects on the volume of the NAc using segmentations from the manual rater and from our biased models. Results reveal that the sex effects observed with manual segmentation can also be observed with biased models, whereas the race effects disappear in all but one model.
<div><strong>Authors:</strong> Ghazal Danaee, Marc Niethammer, Jarrett Rushmore, Sylvain Bouix</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates demographic bias in brain MRI segmentation by comparing three deep‑learning models (UNesT, nnU‑Net, CoTr) and a traditional atlas‑based method (ANTs) on left and right nucleus accumbens across four race‑sex subgroups. It quantifies fairness in segmentation performance and examines how race and sex affect segmentation accuracy and derived volumes, finding that race‑matched training improves some models while nnU‑Net remains robust, and that sex effects persist while race effects largely disappear.", "summary_cn": "本文评估了脑部 MRI 分割中的人口统计偏差，比较了三种深度学习模型（UNesT、nnU‑Net、CoTr）和一种传统的基于图谱的方法（ANTs）在四个人口子组（黑人女性、黑人男性、白人女性、白人男性）对左右伏隔核的分割表现。通过公平性指标量化分割性能，并分析种族和性别对分割准确性及体积的影响，发现种族匹配训练可提升部分模型的表现，而 nnU‑Net 在不同人口组间保持稳健；性别效应在手工和偏差模型中均可观察到，而种族效应在大多数模型中消失。", "keywords": "ographic bias, medical image segmentation, MRI, fairness, UNesT, nnU-Net, CoTr, ANTs, nucleus accumbens, linear mixed models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Ghazal Danaee", "Marc Niethammer", "Jarrett Rushmore", "Sylvain Bouix"]}
]]></acme>

<pubDate>2025-10-20T18:25:38+00:00</pubDate>
</item>
<item>
<title>3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</title>
<link>https://papers.cool/arxiv/2510.17875</link>
<guid>https://papers.cool/arxiv/2510.17875</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a 3D weakly supervised semantic segmentation approach that refines pseudo‑labels using a class‑aware module followed by a geometry‑aware module and a self‑training label‑update strategy. The method improves pseudo‑label quality and coverage, achieving state‑of‑the‑art results on ScanNet and S3DIS benchmarks. Experiments also demonstrate strong generalization in unsupervised settings.<br /><strong>Summary (CN):</strong> 本文提出了一种 3D 弱监督语义分割方法，通过类感知标签细化模块和几何感知标签细化模块以及自训练标签更新策略来优化伪标签。该方法提升了伪标签的质量和覆盖范围，在 ScanNet 和 S3DIS 基准上实现了最新水平的性能，并在无监督设置下表现出良好的泛化能力。<br /><strong>Keywords:</strong> weakly supervised semantic segmentation, 3D geometry, pseudo-label refinement, class-aware guidance, self-training, ScanNet, S3DIS<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</div>
3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.
<div><strong>Authors:</strong> Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a 3D weakly supervised semantic segmentation approach that refines pseudo‑labels using a class‑aware module followed by a geometry‑aware module and a self‑training label‑update strategy. The method improves pseudo‑label quality and coverage, achieving state‑of‑the‑art results on ScanNet and S3DIS benchmarks. Experiments also demonstrate strong generalization in unsupervised settings.", "summary_cn": "本文提出了一种 3D 弱监督语义分割方法，通过类感知标签细化模块和几何感知标签细化模块以及自训练标签更新策略来优化伪标签。该方法提升了伪标签的质量和覆盖范围，在 ScanNet 和 S3DIS 基准上实现了最新水平的性能，并在无监督设置下表现出良好的泛化能力。", "keywords": "weakly supervised semantic segmentation, 3D geometry, pseudo-label refinement, class-aware guidance, self-training, ScanNet, S3DIS", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaoxu Xu", "Xuexun Liu", "Jinlong Li", "Yitian Yuan", "Qiudan Zhang", "Lin Ma", "Nicu Sebe", "Xu Wang"]}
]]></acme>

<pubDate>2025-10-17T03:53:43+00:00</pubDate>
</item>
<item>
<title>Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach</title>
<link>https://papers.cool/arxiv/2510.17873</link>
<guid>https://papers.cool/arxiv/2510.17873</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper audits five popular gender classification datasets, revealing widespread intersectional underrepresentation, and demonstrates that even classifiers trained on the most balanced datasets still exhibit significant bias across gender and race. It introduces BalancedFace, a new dataset that equalizes demographic subgroups across 189 intersections, and shows that training on this data substantially reduces fairness gaps while maintaining overall accuracy. The work highlights the effectiveness of data-centric interventions for fair gender classification.<br /><strong>Summary (CN):</strong> 本文审计了五个常用的性别分类数据集，发现它们普遍存在交叉维度的代表性不足，即使在最均衡的数据集上训练的分类器仍显示出显著的性别和种族偏差。文章推出了 BalancedFace 数据集，针对年龄、种族和性别的 189 种交叉子群实现了均衡，并证明在该数据上训练的模型在降低公平性差距方面表现出色，同时整体准确率几乎不受影响。此研究强调了以数据为中心的干预在实现公平性方面的巨大价值。<br /><strong>Keywords:</strong> gender classification, bias mitigation, data-centric, BalancedFace, FairFace, intersectional fairness, demographic imbalance, MobileNetV2<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - alignment<br /><strong>Authors:</strong> Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges, Dheeraj Kumar Singh</div>
Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying female faces at a higher rate than male faces and amplifying existing racial skew. To counter these data-induced biases, we construct BalancedFace, a new public dataset created by blending images from FairFace and UTKFace, supplemented with images from other collections to fill missing demographic gaps. It is engineered to equalize subgroup shares across 189 intersections of age, race, and gender using only real, unedited images. When a standard classifier is trained on BalancedFace, it reduces the maximum True Positive Rate gap across racial subgroups by over 50% and brings the average Disparate Impact score 63% closer to the ideal of 1.0 compared to the next-best dataset, all with a minimal loss of overall accuracy. These results underline the profound value of data-centric interventions and provide an openly available resource for fair gender classification research.
<div><strong>Authors:</strong> Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges, Dheeraj Kumar Singh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper audits five popular gender classification datasets, revealing widespread intersectional underrepresentation, and demonstrates that even classifiers trained on the most balanced datasets still exhibit significant bias across gender and race. It introduces BalancedFace, a new dataset that equalizes demographic subgroups across 189 intersections, and shows that training on this data substantially reduces fairness gaps while maintaining overall accuracy. The work highlights the effectiveness of data-centric interventions for fair gender classification.", "summary_cn": "本文审计了五个常用的性别分类数据集，发现它们普遍存在交叉维度的代表性不足，即使在最均衡的数据集上训练的分类器仍显示出显著的性别和种族偏差。文章推出了 BalancedFace 数据集，针对年龄、种族和性别的 189 种交叉子群实现了均衡，并证明在该数据上训练的模型在降低公平性差距方面表现出色，同时整体准确率几乎不受影响。此研究强调了以数据为中心的干预在实现公平性方面的巨大价值。", "keywords": "gender classification, bias mitigation, data-centric, BalancedFace, FairFace, intersectional fairness, demographic imbalance, MobileNetV2", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "alignment"}, "authors": ["Tadesse K Bahiru", "Natnael Tilahun Sinshaw", "Teshager Hailemariam Moges", "Dheeraj Kumar Singh"]}
]]></acme>

<pubDate>2025-10-17T02:09:17+00:00</pubDate>
</item>
<item>
<title>GAN-based Content-Conditioned Generation of Handwritten Musical Symbols</title>
<link>https://papers.cool/arxiv/2510.17869</link>
<guid>https://papers.cool/arxiv/2510.17869</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a symbol-level Generative Adversarial Network that creates realistic handwritten musical symbols, which are assembled into full scores using the Smashcima engraving software; visual fidelity evaluations show the generated symbols are highly realistic, addressing data scarcity in Optical Music Recognition.<br /><strong>Summary (CN):</strong> 该论文提出基于 GAN 的符号级手写音乐符号生成方法，利用 Smashcima 将生成的符号组装成完整乐谱，并通过视觉保真度评估证明合成符号高度逼真，可缓解光学乐谱识别中的标注数据稀缺问题。<br /><strong>Keywords:</strong> GAN, synthetic data, optical music recognition, handwritten musical symbols, data augmentation, generative adversarial network, OMR<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Gerard Asbert, Pau Torras, Lei Kang, Alicia Fornés, Josep Lladós</div>
The field of Optical Music Recognition (OMR) is currently hindered by the scarcity of real annotated data, particularly when dealing with handwritten historical musical scores. In similar fields, such as Handwritten Text Recognition, it was proven that synthetic examples produced with image generation techniques could help to train better-performing recognition architectures. This study explores the generation of realistic, handwritten-looking scores by implementing a music symbol-level Generative Adversarial Network (GAN) and assembling its output into a full score using the Smashcima engraving software. We have systematically evaluated the visual fidelity of these generated samples, concluding that the generated symbols exhibit a high degree of realism, marking significant progress in synthetic score generation.
<div><strong>Authors:</strong> Gerard Asbert, Pau Torras, Lei Kang, Alicia Fornés, Josep Lladós</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a symbol-level Generative Adversarial Network that creates realistic handwritten musical symbols, which are assembled into full scores using the Smashcima engraving software; visual fidelity evaluations show the generated symbols are highly realistic, addressing data scarcity in Optical Music Recognition.", "summary_cn": "该论文提出基于 GAN 的符号级手写音乐符号生成方法，利用 Smashcima 将生成的符号组装成完整乐谱，并通过视觉保真度评估证明合成符号高度逼真，可缓解光学乐谱识别中的标注数据稀缺问题。", "keywords": "GAN, synthetic data, optical music recognition, handwritten musical symbols, data augmentation, generative adversarial network, OMR", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Gerard Asbert", "Pau Torras", "Lei Kang", "Alicia Fornés", "Josep Lladós"]}
]]></acme>

<pubDate>2025-10-16T11:21:53+00:00</pubDate>
</item>
<item>
<title>MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation</title>
<link>https://papers.cool/arxiv/2510.17866</link>
<guid>https://papers.cool/arxiv/2510.17866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MUSE is a training‑free framework for zero‑shot 2D object detection and segmentation that uses 3D‑derived multi‑view templates and uncertainty‑aware similarity matching. It combines class and patch embeddings with GeM pooling, a joint absolute‑relative similarity metric, and an uncertainty‑aware prior to refine scores, achieving state‑of‑the‑art results on the BOP Challenge 2025 without any additional training.<br /><strong>Summary (CN):</strong> MUSE 是一种无需训练的零样本二维目标检测和分割框架，利用来自三维未见对象的多视角模板和不确定性感知相似度匹配。它在嵌入阶段结合类别嵌入和使用 GeM（广义均值池化）归一化的局部贴片嵌入，在匹配阶段采用绝对与相对相似度的联合度量，并通过不确定性感知的对象先验对相似度进行校正，从而在 BOP Challenge 2025 中实现了最先进的表现。<br /><strong>Keywords:</strong> zero-shot detection, uncertainty-aware similarity, multi-view templates, GeM pooling, object segmentation, training-free framework<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sungmin Cho, Sungbum Park, Insoo Oh</div>
In this work, we introduce MUSE (Model-based Uncertainty-aware Similarity Estimation), a training-free framework designed for model-based zero-shot 2D object detection and segmentation. MUSE leverages 2D multi-view templates rendered from 3D unseen objects and 2D object proposals extracted from input query images. In the embedding stage, it integrates class and patch embeddings, where the patch embeddings are normalized using generalized mean pooling (GeM) to capture both global and local representations efficiently. During the matching stage, MUSE employs a joint similarity metric that combines absolute and relative similarity scores, enhancing the robustness of matching under challenging scenarios. Finally, the similarity score is refined through an uncertainty-aware object prior that adjusts for proposal reliability. Without any additional training or fine-tuning, MUSE achieves state-of-the-art performance on the BOP Challenge 2025, ranking first across the Classic Core, H3, and Industrial tracks. These results demonstrate that MUSE offers a powerful and generalizable framework for zero-shot 2D object detection and segmentation.
<div><strong>Authors:</strong> Sungmin Cho, Sungbum Park, Insoo Oh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MUSE is a training‑free framework for zero‑shot 2D object detection and segmentation that uses 3D‑derived multi‑view templates and uncertainty‑aware similarity matching. It combines class and patch embeddings with GeM pooling, a joint absolute‑relative similarity metric, and an uncertainty‑aware prior to refine scores, achieving state‑of‑the‑art results on the BOP Challenge 2025 without any additional training.", "summary_cn": "MUSE 是一种无需训练的零样本二维目标检测和分割框架，利用来自三维未见对象的多视角模板和不确定性感知相似度匹配。它在嵌入阶段结合类别嵌入和使用 GeM（广义均值池化）归一化的局部贴片嵌入，在匹配阶段采用绝对与相对相似度的联合度量，并通过不确定性感知的对象先验对相似度进行校正，从而在 BOP Challenge 2025 中实现了最先进的表现。", "keywords": "zero-shot detection, uncertainty-aware similarity, multi-view templates, GeM pooling, object segmentation, training-free framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sungmin Cho", "Sungbum Park", "Insoo Oh"]}
]]></acme>

<pubDate>2025-10-15T22:16:09+00:00</pubDate>
</item>
<item>
<title>InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation</title>
<link>https://papers.cool/arxiv/2510.17864</link>
<guid>https://papers.cool/arxiv/2510.17864</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> InsideOut extends 3D Gaussian splatting to jointly model high-fidelity RGB surface appearance and subsurface X-ray structures, enabling comprehensive 3D object representation useful for medical diagnostics, cultural heritage restoration, and manufacturing. The paper introduces a new paired RGB‑X‑ray dataset, a hierarchical fitting pipeline that aligns RGB and X-ray radiative Gaussian splats, and an X-ray reference loss that preserves internal consistency. Experiments demonstrate that the method bridges modality gaps and improves visualization, simulation, and non‑destructive testing capabilities.<br /><strong>Summary (CN):</strong> InsideOut 将三维高斯点云扩展至同时建模高分辨率 RGB 表面细节和内部 X 光结构，从而实现对三维对象的完整表征，适用于医学诊断、文化遗产修复和制造业等领域。论文提供了新的 RGB 与 X 光配对数据集，提出了层次化拟合流程以对齐两种辐射高斯点，并设计了 X 光参考损失以保证内部结构的一致性。实验表明该方法成功弥合了两种模态的差异，提升了可视化、仿真和无损检测能力。<br /><strong>Keywords:</strong> 3D Gaussian splatting, RGB-X-ray fusion, radiative imaging, multimodal representation, subsurface visualization, medical imaging, cultural heritage, non-destructive testing<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jungmin Lee, Seonghyuk Hong, Juyong Lee, Jaeyoon Lee, Jongwon Choi</div>
We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.
<div><strong>Authors:</strong> Jungmin Lee, Seonghyuk Hong, Juyong Lee, Jaeyoon Lee, Jongwon Choi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "InsideOut extends 3D Gaussian splatting to jointly model high-fidelity RGB surface appearance and subsurface X-ray structures, enabling comprehensive 3D object representation useful for medical diagnostics, cultural heritage restoration, and manufacturing. The paper introduces a new paired RGB‑X‑ray dataset, a hierarchical fitting pipeline that aligns RGB and X-ray radiative Gaussian splats, and an X-ray reference loss that preserves internal consistency. Experiments demonstrate that the method bridges modality gaps and improves visualization, simulation, and non‑destructive testing capabilities.", "summary_cn": "InsideOut 将三维高斯点云扩展至同时建模高分辨率 RGB 表面细节和内部 X 光结构，从而实现对三维对象的完整表征，适用于医学诊断、文化遗产修复和制造业等领域。论文提供了新的 RGB 与 X 光配对数据集，提出了层次化拟合流程以对齐两种辐射高斯点，并设计了 X 光参考损失以保证内部结构的一致性。实验表明该方法成功弥合了两种模态的差异，提升了可视化、仿真和无损检测能力。", "keywords": "3D Gaussian splatting, RGB-X-ray fusion, radiative imaging, multimodal representation, subsurface visualization, medical imaging, cultural heritage, non-destructive testing", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jungmin Lee", "Seonghyuk Hong", "Juyong Lee", "Jaeyoon Lee", "Jongwon Choi"]}
]]></acme>

<pubDate>2025-10-15T20:51:25+00:00</pubDate>
</item>
<item>
<title>Robotic Classification of Divers' Swimming States using Visual Pose Keypoints as IMUs</title>
<link>https://papers.cool/arxiv/2510.17863</link>
<guid>https://papers.cool/arxiv/2510.17863</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a hybrid method that converts 3D human joint keypoints extracted from underwater video into a pseudo‑inertial measurement unit (pseudo‑IMU) to classify scuba divers' swimming states. By deploying this classifier on an autonomous underwater vehicle, the system can detect anomalous behaviors indicative of medical emergencies such as cardiac arrest, addressing the challenge of wireless signal loss for traditional wearable sensors. Experiments with simulated distress scenarios demonstrate the approach’s effectiveness for robotic diver‑monitoring and safety enhancement.<br /><strong>Summary (CN):</strong> 本文提出一种混合方法，将水下视频中提取的三维人体关节关键点转化为伪惯性测量单元（pseudo‑IMU），用于分类潜水员的泳姿状态。该系统可在自主水下机器人上运行，检测诸如心脏骤停等医学急救的异常行为，克服了传统穿戴式传感器在水中无线信号衰减的问题。通过模拟紧急情境的实验表明，该方法在机器人潜水员监测与安全提升方面具备实用性。<br /><strong>Keywords:</strong> underwater activity recognition, pose estimation, pseudo-IMU, diver safety, anomaly detection, AUV monitoring, visual inertial sensing, robotic surveillance<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Demetrious T. Kutzke, Ying-Kun Wu, Elizabeth Terveen, Junaed Sattar</div>
Traditional human activity recognition uses either direct image analysis or data from wearable inertial measurement units (IMUs), but can be ineffective in challenging underwater environments. We introduce a novel hybrid approach that bridges this gap to monitor scuba diver safety. Our method leverages computer vision to generate high-fidelity motion data, effectively creating a ``pseudo-IMU'' from a stream of 3D human joint keypoints. This technique circumvents the critical problem of wireless signal attenuation in water, which plagues conventional diver-worn sensors communicating with an Autonomous Underwater Vehicle (AUV). We apply this system to the vital task of identifying anomalous scuba diver behavior that signals the onset of a medical emergency such as cardiac arrest -- a leading cause of scuba diving fatalities. By integrating our classifier onboard an AUV and conducting experiments with simulated distress scenarios, we demonstrate the utility and effectiveness of our method for advancing robotic monitoring and diver safety.
<div><strong>Authors:</strong> Demetrious T. Kutzke, Ying-Kun Wu, Elizabeth Terveen, Junaed Sattar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a hybrid method that converts 3D human joint keypoints extracted from underwater video into a pseudo‑inertial measurement unit (pseudo‑IMU) to classify scuba divers' swimming states. By deploying this classifier on an autonomous underwater vehicle, the system can detect anomalous behaviors indicative of medical emergencies such as cardiac arrest, addressing the challenge of wireless signal loss for traditional wearable sensors. Experiments with simulated distress scenarios demonstrate the approach’s effectiveness for robotic diver‑monitoring and safety enhancement.", "summary_cn": "本文提出一种混合方法，将水下视频中提取的三维人体关节关键点转化为伪惯性测量单元（pseudo‑IMU），用于分类潜水员的泳姿状态。该系统可在自主水下机器人上运行，检测诸如心脏骤停等医学急救的异常行为，克服了传统穿戴式传感器在水中无线信号衰减的问题。通过模拟紧急情境的实验表明，该方法在机器人潜水员监测与安全提升方面具备实用性。", "keywords": "underwater activity recognition, pose estimation, pseudo-IMU, diver safety, anomaly detection, AUV monitoring, visual inertial sensing, robotic surveillance", "scoring": {"interpretability": 3, "understanding": 4, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Demetrious T. Kutzke", "Ying-Kun Wu", "Elizabeth Terveen", "Junaed Sattar"]}
]]></acme>

<pubDate>2025-10-15T19:57:03+00:00</pubDate>
</item>
<item>
<title>Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch</title>
<link>https://papers.cool/arxiv/2510.17858</link>
<guid>https://papers.cool/arxiv/2510.17858</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an ultra-efficient post-training technique that shortcuts large pre-trained flow-matching diffusion models into few-step samplers by self-distilling the velocity field, eliminating the need for step-size embeddings and requiring minimal compute. The method can also be integrated during pretraining to produce models that inherently generate efficient flows and enables a few-shot distillation for massive diffusion models with negligible cost. Experiments show a 3-step Flux model trained in under one A100-day without quality loss.<br /><strong>Summary (CN):</strong> 该论文提出一种超高效的后训练方法，通过对速度场进行自蒸馏，将大规模预训练的流匹配扩散模型快捷为少步采样器，省去步长嵌入并几乎不耗算力。该技术既可在后训练阶段加入，也可嵌入预训练过程，使模型天然学习高效少步流，并实现对数十亿参数扩散模型的少样本蒸馏，成本极低。实验表明，3 步 Flux 模型在不足一天的 A100 计算下即可训练完成，质量保持领先。<br /><strong>Keywords:</strong> flow matching, diffusion models, velocity field self-distillation, few-step sampler, shortcutting, post-training, few-shot distillation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</div>
We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation. While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch$\unicode{x2013}$a process nearly as costly as pretraining itself. Our key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding. Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux less than one A100 day. Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality. This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.
<div><strong>Authors:</strong> Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an ultra-efficient post-training technique that shortcuts large pre-trained flow-matching diffusion models into few-step samplers by self-distilling the velocity field, eliminating the need for step-size embeddings and requiring minimal compute. The method can also be integrated during pretraining to produce models that inherently generate efficient flows and enables a few-shot distillation for massive diffusion models with negligible cost. Experiments show a 3-step Flux model trained in under one A100-day without quality loss.", "summary_cn": "该论文提出一种超高效的后训练方法，通过对速度场进行自蒸馏，将大规模预训练的流匹配扩散模型快捷为少步采样器，省去步长嵌入并几乎不耗算力。该技术既可在后训练阶段加入，也可嵌入预训练过程，使模型天然学习高效少步流，并实现对数十亿参数扩散模型的少样本蒸馏，成本极低。实验表明，3 步 Flux 模型在不足一天的 A100 计算下即可训练完成，质量保持领先。", "keywords": "flow matching, diffusion models, velocity field self-distillation, few-step sampler, shortcutting, post-training, few-shot distillation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xu Cai", "Yang Wu", "Qianli Chen", "Haoran Wu", "Lichuan Xiang", "Hongkai Wen"]}
]]></acme>

<pubDate>2025-10-15T09:19:05+00:00</pubDate>
</item>
<item>
<title>CMIS-Net: A Cascaded Multi-Scale Individual Standardization Network for Backchannel Agreement Estimation</title>
<link>https://papers.cool/arxiv/2510.17855</link>
<guid>https://papers.cool/arxiv/2510.17855</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CMIS-Net, a cascaded multi‑scale individual standardization network that normalizes backchannel cues by removing person‑specific neutral baselines at both frame and sequence levels, enabling more accurate agreement detection. An implicit data augmentation module is also proposed to mitigate training data bias and improve generalization. Experiments show state‑of‑the‑art performance on backchannel agreement estimation tasks.<br /><strong>Summary (CN):</strong> 本文提出了 CMIS-Net，一种级联多尺度个体标准化网络，通过在帧层和序列层去除个人的中性基线，对回声信号进行标准化，从而更准确地检测同意行为。同时引入了隐式数据增强模块，以缓解训练数据分布偏差并提升模型的泛化能力。实验结果表明该方法在回声同意估计任务上达到了最新的性能水平。<br /><strong>Keywords:</strong> backchannel, agreement detection, multi-scale modeling, individual normalization, cascaded network, data augmentation, conversational AI, speech emotion recognition<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuxuan Huang, Kangzhong Wang, Eugene Yujun Fu, Grace Ngai, Peter H. F. Ng</div>
Backchannels are subtle listener responses, such as nods, smiles, or short verbal cues like "yes" or "uh-huh," which convey understanding and agreement in conversations. These signals provide feedback to speakers, improve the smoothness of interaction, and play a crucial role in developing human-like, responsive AI systems. However, the expression of backchannel behaviors is often significantly influenced by individual differences, operating across multiple scales: from instant dynamics such as response intensity (frame-level) to temporal patterns such as frequency and rhythm preferences (sequence-level). This presents a complex pattern recognition problem that contemporary emotion recognition methods have yet to fully address. Particularly, existing individualized methods in emotion recognition often operate at a single scale, overlooking the complementary nature of multi-scale behavioral cues. To address these challenges, we propose a novel Cascaded Multi-Scale Individual Standardization Network (CMIS-Net) that extracts individual-normalized backchannel features by removing person-specific neutral baselines from observed expressions. Operating at both frame and sequence levels, this normalization allows model to focus on relative changes from each person's baseline rather than absolute expression values. Furthermore, we introduce an implicit data augmentation module to address the observed training data distributional bias, improving model generalization. Comprehensive experiments and visualizations demonstrate that CMIS-Net effectively handles individual differences and data imbalance, achieving state-of-the-art performance in backchannel agreement detection.
<div><strong>Authors:</strong> Yuxuan Huang, Kangzhong Wang, Eugene Yujun Fu, Grace Ngai, Peter H. F. Ng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CMIS-Net, a cascaded multi‑scale individual standardization network that normalizes backchannel cues by removing person‑specific neutral baselines at both frame and sequence levels, enabling more accurate agreement detection. An implicit data augmentation module is also proposed to mitigate training data bias and improve generalization. Experiments show state‑of‑the‑art performance on backchannel agreement estimation tasks.", "summary_cn": "本文提出了 CMIS-Net，一种级联多尺度个体标准化网络，通过在帧层和序列层去除个人的中性基线，对回声信号进行标准化，从而更准确地检测同意行为。同时引入了隐式数据增强模块，以缓解训练数据分布偏差并提升模型的泛化能力。实验结果表明该方法在回声同意估计任务上达到了最新的性能水平。", "keywords": "backchannel, agreement detection, multi-scale modeling, individual normalization, cascaded network, data augmentation, conversational AI, speech emotion recognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuxuan Huang", "Kangzhong Wang", "Eugene Yujun Fu", "Grace Ngai", "Peter H. F. Ng"]}
]]></acme>

<pubDate>2025-10-15T03:21:51+00:00</pubDate>
</item>
<item>
<title>Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</title>
<link>https://papers.cool/arxiv/2510.17854</link>
<guid>https://papers.cool/arxiv/2510.17854</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a framework that uses image embeddings and vector similarity to differentiate AI‑generated images from human‑created ones, based on the hypothesis that AI‑generated images cluster together in embedding space. Experiments across five benchmark embedding models show the method is robust to moderate perturbations and can reliably trace image provenance, with a blockchain component for tamper‑evident recording. The approach aims to provide an efficient, generalizable tool for authenticating digital visual content.<br /><strong>Summary (CN):</strong> 本文提出一种利用图像嵌入和向量相似度区分 AI 生成图像与人工创作图像的框架，核心假设是 AI 生成的图像在嵌入空间中会聚成相似簇。通过在五种基准嵌入模型上进行实验，证明该方法对中等程度的扰动具有鲁棒性，并可保持对原图的相似匹配，同时结合区块链实现防篡改的溯源记录。该方案旨在提供一种高效且通用的数字图像真实性验证工具。<br /><strong>Keywords:</strong> AI-generated image detection, embedding similarity, provenance, blockchain, deepfake detection, vector similarity, image embeddings, authenticity verification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Jitendra Sharma, Arthur Carvalho, Suman Bhunia</div>
Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.
<div><strong>Authors:</strong> Jitendra Sharma, Arthur Carvalho, Suman Bhunia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a framework that uses image embeddings and vector similarity to differentiate AI‑generated images from human‑created ones, based on the hypothesis that AI‑generated images cluster together in embedding space. Experiments across five benchmark embedding models show the method is robust to moderate perturbations and can reliably trace image provenance, with a blockchain component for tamper‑evident recording. The approach aims to provide an efficient, generalizable tool for authenticating digital visual content.", "summary_cn": "本文提出一种利用图像嵌入和向量相似度区分 AI 生成图像与人工创作图像的框架，核心假设是 AI 生成的图像在嵌入空间中会聚成相似簇。通过在五种基准嵌入模型上进行实验，证明该方法对中等程度的扰动具有鲁棒性，并可保持对原图的相似匹配，同时结合区块链实现防篡改的溯源记录。该方案旨在提供一种高效且通用的数字图像真实性验证工具。", "keywords": "AI-generated image detection, embedding similarity, provenance, blockchain, deepfake detection, vector similarity, image embeddings, authenticity verification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Jitendra Sharma", "Arthur Carvalho", "Suman Bhunia"]}
]]></acme>

<pubDate>2025-10-15T00:49:56+00:00</pubDate>
</item>
<item>
<title>Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model</title>
<link>https://papers.cool/arxiv/2510.17851</link>
<guid>https://papers.cool/arxiv/2510.17851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a latent diffusion model that generates post‑treatment glioblastoma MRI scans from pre‑treatment scans using concatenated conditioning on tumor location and classifier‑free guidance informed by survival data. Trained on a dataset of 140 patients with paired pre‑ and post‑treatment T1‑Gd MRI and expert tumor delineations, the model aims to enable early visual prediction of treatment response. Results demonstrate that the approach can capture tumor evolution trends, offering a potential tool for personalized medicine.<br /><strong>Summary (CN):</strong> 本文提出一种潜在扩散模型，利用预处理 MRI 中的肿瘤定位信息以及基于生存数据的 classifier‑free 引导，将术后 MRI 生成于术前 MRI，实现早期视觉化的治疗反应预测。模型在包含 140 名患者的配对术前/术后 T1‑Gd MRI 数据及专家标注的肿瘤分割上进行训练和测试，展示了对肿瘤演变趋势的捕捉能力，为个性化医疗提供潜在工具。<br /><strong>Keywords:</strong> latent diffusion, MRI prediction, glioblastoma, treatment response prediction, disease progression modeling, classifier-free guidance, medical imaging, generative models, survival conditioning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</div>
Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.
<div><strong>Authors:</strong> Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a latent diffusion model that generates post‑treatment glioblastoma MRI scans from pre‑treatment scans using concatenated conditioning on tumor location and classifier‑free guidance informed by survival data. Trained on a dataset of 140 patients with paired pre‑ and post‑treatment T1‑Gd MRI and expert tumor delineations, the model aims to enable early visual prediction of treatment response. Results demonstrate that the approach can capture tumor evolution trends, offering a potential tool for personalized medicine.", "summary_cn": "本文提出一种潜在扩散模型，利用预处理 MRI 中的肿瘤定位信息以及基于生存数据的 classifier‑free 引导，将术后 MRI 生成于术前 MRI，实现早期视觉化的治疗反应预测。模型在包含 140 名患者的配对术前/术后 T1‑Gd MRI 数据及专家标注的肿瘤分割上进行训练和测试，展示了对肿瘤演变趋势的捕捉能力，为个性化医疗提供潜在工具。", "keywords": "latent diffusion, MRI prediction, glioblastoma, treatment response prediction, disease progression modeling, classifier-free guidance, medical imaging, generative models, survival conditioning", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandre G. Leclercq", "Sébastien Bougleux", "Noémie N. Moreau", "Alexis Desmonts", "Romain Hérault", "Aurélien Corroyer-Dulmont"]}
]]></acme>

<pubDate>2025-10-13T20:32:06+00:00</pubDate>
</item>
<item>
<title>CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization</title>
<link>https://papers.cool/arxiv/2510.17847</link>
<guid>https://papers.cool/arxiv/2510.17847</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes CoIDO, a dual-objective framework that jointly optimizes data importance and diversity for visual instruction tuning of multimodal large language models. By training a lightweight plug-in scorer on a small random sample and using a homoscedastic uncertainty formulation, CoIDO efficiently selects a 20% subset of data that achieves 98.2% of full-data fine-tuning performance on LLaVA-1.5-7B across ten downstream tasks.<br /><strong>Summary (CN):</strong> 本文提出 CoIDO 框架，通过联合优化数据重要性与多样性，实现视觉指令微调数据的高效挑选。利用在少量随机样本上训练的轻量插件评分器，并基于同方差不确定性公式平衡两者，CoIDO 在整个数据集上选取 20% 子集，在 LLaVA-1.5-7B 上的十个下游任务中达到约 98.2% 的全数据微调性能。<br /><strong>Keywords:</strong> data selection, visual instruction tuning, multimodal LLMs, importance-diversity optimization, efficient training, CoIDO, homoscedastic uncertainty<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yichen Yan, Ming Zhong, Qi Zhu, Xiaoling Gu, Jinpeng Chen, Huan Li</div>
Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity. We introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling efficient and scalable data selection. In our experiments, we trained the CoIDO scorer using only 20 percent of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20 percent subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2 percent of the performance of full-data fine-tuning, on average.
<div><strong>Authors:</strong> Yichen Yan, Ming Zhong, Qi Zhu, Xiaoling Gu, Jinpeng Chen, Huan Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes CoIDO, a dual-objective framework that jointly optimizes data importance and diversity for visual instruction tuning of multimodal large language models. By training a lightweight plug-in scorer on a small random sample and using a homoscedastic uncertainty formulation, CoIDO efficiently selects a 20% subset of data that achieves 98.2% of full-data fine-tuning performance on LLaVA-1.5-7B across ten downstream tasks.", "summary_cn": "本文提出 CoIDO 框架，通过联合优化数据重要性与多样性，实现视觉指令微调数据的高效挑选。利用在少量随机样本上训练的轻量插件评分器，并基于同方差不确定性公式平衡两者，CoIDO 在整个数据集上选取 20% 子集，在 LLaVA-1.5-7B 上的十个下游任务中达到约 98.2% 的全数据微调性能。", "keywords": "data selection, visual instruction tuning, multimodal LLMs, importance-diversity optimization, efficient training, CoIDO, homoscedastic uncertainty", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yichen Yan", "Ming Zhong", "Qi Zhu", "Xiaoling Gu", "Jinpeng Chen", "Huan Li"]}
]]></acme>

<pubDate>2025-10-11T09:41:21+00:00</pubDate>
</item>
<item>
<title>MAT-Agent: Adaptive Multi-Agent Training Optimization</title>
<link>https://papers.cool/arxiv/2510.17845</link>
<guid>https://papers.cool/arxiv/2510.17845</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MAT-Agent introduces a collaborative multi-agent framework that dynamically adjusts data augmentation, optimizers, learning rates, and loss functions during multi-label image classification training, using non‑stationary multi‑armed bandit algorithms to balance exploration and exploitation. The system is evaluated on Pascal VOC, COCO, and VG‑256, achieving higher mAP and F1 scores than several baselines while showing faster convergence and better cross‑domain generalization.<br /><strong>Summary (CN):</strong> MAT-Agent 提出一种协作式多智能体框架，在多标签图像分类训练过程中动态调节数据增强、优化器、学习率和损失函数，采用非平稳多臂赌博机算法实现探索与利用的平衡。实验证明该方法在 Pascal VOC、COCO 和 VG‑256 数据集上相较于多种基线实现了更高的 mAP 与 F1 分数，并加快了收敛速度，提升了跨域泛化能力。<br /><strong>Keywords:</strong> multi-agent training, multi-label classification, non-stationary multi-armed bandit, adaptive optimization, data augmentation, mAP<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.
<div><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MAT-Agent introduces a collaborative multi-agent framework that dynamically adjusts data augmentation, optimizers, learning rates, and loss functions during multi-label image classification training, using non‑stationary multi‑armed bandit algorithms to balance exploration and exploitation. The system is evaluated on Pascal VOC, COCO, and VG‑256, achieving higher mAP and F1 scores than several baselines while showing faster convergence and better cross‑domain generalization.", "summary_cn": "MAT-Agent 提出一种协作式多智能体框架，在多标签图像分类训练过程中动态调节数据增强、优化器、学习率和损失函数，采用非平稳多臂赌博机算法实现探索与利用的平衡。实验证明该方法在 Pascal VOC、COCO 和 VG‑256 数据集上相较于多种基线实现了更高的 mAP 与 F1 分数，并加快了收敛速度，提升了跨域泛化能力。", "keywords": "multi-agent training, multi-label classification, non-stationary multi-armed bandit, adaptive optimization, data augmentation, mAP", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Ningyuan Liu", "Keze Wang"]}
]]></acme>

<pubDate>2025-10-10T19:41:50+00:00</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18866</link>
<guid>https://papers.cool/arxiv/2510.18866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LightMem, a lightweight memory-augmented generation system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson‑Shiffrin model. LightMem filters irrelevant data, groups content by topic, and performs offline consolidation, achieving higher accuracy while drastically reducing token usage, API calls, and runtime compared to existing memory systems.<br /><strong>Summary (CN):</strong> 本文提出 LightMem，一种面向大语言模型的轻量级记忆增强生成系统，受 Atkinson‑Shiffrin 人类记忆模型启发，将记忆划分为感官记忆、短期记忆和长期记忆三个阶段。感官记忆通过轻量压缩快速过滤无关信息并按主题分组，短期记忆对主题组进行整理与摘要，长期记忆在离线“睡眠”阶段进行整合，从而在提升准确率的同时将 token 使用量降低至原来的 1/117，API 调用降低至 1/159，运行时间提升超过 12 倍。<br /><strong>Keywords:</strong> memory-augmented generation, lightweight memory, LLM, hierarchical memory, efficiency, Atkinson-Shiffrin model, token reduction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
<div><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LightMem, a lightweight memory-augmented generation system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson‑Shiffrin model. LightMem filters irrelevant data, groups content by topic, and performs offline consolidation, achieving higher accuracy while drastically reducing token usage, API calls, and runtime compared to existing memory systems.", "summary_cn": "本文提出 LightMem，一种面向大语言模型的轻量级记忆增强生成系统，受 Atkinson‑Shiffrin 人类记忆模型启发，将记忆划分为感官记忆、短期记忆和长期记忆三个阶段。感官记忆通过轻量压缩快速过滤无关信息并按主题分组，短期记忆对主题组进行整理与摘要，长期记忆在离线“睡眠”阶段进行整合，从而在提升准确率的同时将 token 使用量降低至原来的 1/117，API 调用降低至 1/159，运行时间提升超过 12 倍。", "keywords": "memory-augmented generation, lightweight memory, LLM, hierarchical memory, efficiency, Atkinson-Shiffrin model, token reduction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:58:17+00:00</pubDate>
</item>
<item>
<title>Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation</title>
<link>https://papers.cool/arxiv/2510.18751</link>
<guid>https://papers.cool/arxiv/2510.18751</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper presents ALGOS, a system that combines vision-language models with GeoSAM-assisted human evaluation to segment harmful algal blooms in remote-sensing images and estimate their severity. By fine-tuning on NASA's Cyanobacteria Aggregated Manual Labels, the method achieves strong performance on both mask generation and bloom severity prediction, offering a scalable approach for automated HAB monitoring.<br /><strong>Summary (CN):</strong> 本文提出了ALGOS系统，利用视觉语言模型和GeoSAM辅助的人类评估，对遥感图像中的有害藻华进行分割并估计其严重程度。通过在NASA的蓝藻聚合手工标签（CAML）上进行微调，实现了高质量的掩码生成和藻华严重程度预测，为自动化藻华监测提供了可扩展的解决方案。<br /><strong>Keywords:</strong> algal bloom, cyanobacteria, vision-language model, remote sensing, segmentation, severity estimation, GeoSAM, environmental monitoring<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Patterson Hsieh, Jerry Yeh, Mao-Chi He, Wen-Han Hsieh, Elvis Hsieh</div>
Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.
<div><strong>Authors:</strong> Patterson Hsieh, Jerry Yeh, Mao-Chi He, Wen-Han Hsieh, Elvis Hsieh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper presents ALGOS, a system that combines vision-language models with GeoSAM-assisted human evaluation to segment harmful algal blooms in remote-sensing images and estimate their severity. By fine-tuning on NASA's Cyanobacteria Aggregated Manual Labels, the method achieves strong performance on both mask generation and bloom severity prediction, offering a scalable approach for automated HAB monitoring.", "summary_cn": "本文提出了ALGOS系统，利用视觉语言模型和GeoSAM辅助的人类评估，对遥感图像中的有害藻华进行分割并估计其严重程度。通过在NASA的蓝藻聚合手工标签（CAML）上进行微调，实现了高质量的掩码生成和藻华严重程度预测，为自动化藻华监测提供了可扩展的解决方案。", "keywords": "algal bloom, cyanobacteria, vision-language model, remote sensing, segmentation, severity estimation, GeoSAM, environmental monitoring", "scoring": {"interpretability": 3, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"]}
]]></acme>

<pubDate>2025-10-21T15:59:00+00:00</pubDate>
</item>
<item>
<title>Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches</title>
<link>https://papers.cool/arxiv/2510.18668</link>
<guid>https://papers.cool/arxiv/2510.18668</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a tiny multi‑modal convolutional neural network that early‑fuses synchronized ECG and PCG recordings to perform binary cardiovascular classification on resource‑constrained edge devices. Trained on the PhysioNet 2016 challenge data, the model achieves competitive accuracy while reducing memory and compute requirements by three orders of magnitude, and its energy consumption is demonstrated on a microcontroller‑based sensor patch.<br /><strong>Summary (CN):</strong> 本文提出了一种用于心血管二分类的轻量级多模态卷积神经网络，通过对同步的 ECG 与 PCG 信号进行早期融合，在资源受限的医学边缘设备上运行。模型在 PhysioNet 2016 挑战数据上训练，保持竞争精度的同时将内存和计算成本降低了三个数量级，并在微控制器传感器贴片上验证了其能源消耗优势。<br /><strong>Keywords:</strong> multi-modal, tiny CNN, ECG, PCG, edge devices, medical monitoring, early fusion, binary classification, energy-efficient inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mustafa Fuad Rifet Ibrahim, Tunc Alkanat, Maurice Meijer, Felix Manthey, Alexander Schlaefer, Peer Stelldinger</div>
The vast majority of cardiovascular diseases may be preventable if early signs and risk factors are detected. Cardiovascular monitoring with body-worn sensor devices like sensor patches allows for the detection of such signs while preserving the freedom and comfort of patients. However, the analysis of the sensor data must be robust, reliable, efficient, and highly accurate. Deep learning methods can automate data interpretation, reducing the workload of clinicians. In this work, we analyze the feasibility of applying deep learning models to the classification of synchronized electrocardiogram (ECG) and phonocardiogram (PCG) recordings on resource-constrained medical edge devices. We propose a convolutional neural network with early fusion of data to solve a binary classification problem. We train and validate our model on the synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset. Our approach reduces memory footprint and compute cost by three orders of magnitude compared to the state-of-the-art while maintaining competitive accuracy. We demonstrate the applicability of our proposed model on medical edge devices by analyzing energy consumption on a microcontroller and an experimental sensor device setup, confirming that on-device inference can be more energy-efficient than continuous data streaming.
<div><strong>Authors:</strong> Mustafa Fuad Rifet Ibrahim, Tunc Alkanat, Maurice Meijer, Felix Manthey, Alexander Schlaefer, Peer Stelldinger</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a tiny multi‑modal convolutional neural network that early‑fuses synchronized ECG and PCG recordings to perform binary cardiovascular classification on resource‑constrained edge devices. Trained on the PhysioNet 2016 challenge data, the model achieves competitive accuracy while reducing memory and compute requirements by three orders of magnitude, and its energy consumption is demonstrated on a microcontroller‑based sensor patch.", "summary_cn": "本文提出了一种用于心血管二分类的轻量级多模态卷积神经网络，通过对同步的 ECG 与 PCG 信号进行早期融合，在资源受限的医学边缘设备上运行。模型在 PhysioNet 2016 挑战数据上训练，保持竞争精度的同时将内存和计算成本降低了三个数量级，并在微控制器传感器贴片上验证了其能源消耗优势。", "keywords": "multi-modal, tiny CNN, ECG, PCG, edge devices, medical monitoring, early fusion, binary classification, energy-efficient inference", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mustafa Fuad Rifet Ibrahim", "Tunc Alkanat", "Maurice Meijer", "Felix Manthey", "Alexander Schlaefer", "Peer Stelldinger"]}
]]></acme>

<pubDate>2025-10-21T14:23:20+00:00</pubDate>
</item>
<item>
<title>CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent</title>
<link>https://papers.cool/arxiv/2510.18596</link>
<guid>https://papers.cool/arxiv/2510.18596</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CUARewardBench introduces the first comprehensive benchmark for assessing reward models on computer‑using agents (CUAs), covering both outcome reward models (ORM) and process reward models (PRM) across trajectory‑level and step‑level evaluation. The benchmark provides a diverse, expertly annotated dataset spanning ten software categories and seven agent architectures, and presents extensive analyses that reveal current VLM‑based reward models’ limitations in visual reasoning and knowledge. It also proposes the Unanimous Prompt Ensemble (UPE), which improves reward model reliability through strict unanimous voting and strategic prompt‑template configurations, achieving substantially higher precision and NPV than single models or traditional ensembles.<br /><strong>Summary (CN):</strong> CUARewardBench 提出了首个用于评估计算机使用代理（CUA）奖励模型的全面基准，涵盖结果奖励模型（ORM）和过程奖励模型（PRM），并对轨迹级和步骤级进行评估。该基准提供了覆盖十类软件、七种代理架构的多样化、专家标注数据集，并通过大量实验揭示当前基于视觉语言模型的奖励模型在视觉推理和知识方面的局限。文中进一步提出一致性提示集成（UPE）方法，通过严格的一致投票和提示模板配置显著提升奖励模型的可靠性，精度和负预测值均大幅超越单模型和传统集成。<br /><strong>Keywords:</strong> computer-using agents, reward models, benchmark, trajectory evaluation, vision-language models, ensemble, outcome reward, process reward, alignment, evaluation metrics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Haojia Lin, Xiaoyu Tan, Yulei Qin, Zihan Xu, Yuchen Shi, Zongyi Li, Gang Li, Shaofei Cai, Siqi Cai, Chaoyou Fu, Ke Li, Xing Sun</div>
Computer-using agents (CUAs) enable task completion through natural interaction with operating systems and software interfaces. While script-based verifiers are widely adopted for evaluation, they suffer from limited scalability and inability to provide step-wise assessment. Reward models offer promising alternatives, but their effectiveness on CUA evaluation remains largely underexplored. To address this gap, we present CUARewardBench, comprising four key contributions: (1) First-ever Comprehensive CUA Reward Benchmark: We introduce the first benchmark for evaluating both outcome reward models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic assessment across trajectory-level and step-level evaluation. (2) Diverse, Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10 software categories and 7 agent architectures with varying performance levels (25.9%-50.8% success rates). All trajectories are expertly annotated through carefully designed protocols, with rigorous quality control to ensure reliability and practical applicability. (3) Comprehensive Analysis and Insights: Through extensive experiments across 7 vision-language models and 3 prompt templates, we reveal critical limitations of current CUA RMs, including insufficient visual reasoning capabilities, knowledge deficiencies, and the superiority of general VLMs over specialized CUA models for reward evaluation. (4) Unanimous Prompt Ensemble (UPE): Based on the insights from our comprehensive analysis, we propose UPE, a novel ensemble method that significantly enhances reward model reliability through strict unanimous voting and strategic prompt-template configurations. UPE achieves 89.8% precision and 93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially outperforming single VLMs and traditional ensemble approaches.
<div><strong>Authors:</strong> Haojia Lin, Xiaoyu Tan, Yulei Qin, Zihan Xu, Yuchen Shi, Zongyi Li, Gang Li, Shaofei Cai, Siqi Cai, Chaoyou Fu, Ke Li, Xing Sun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CUARewardBench introduces the first comprehensive benchmark for assessing reward models on computer‑using agents (CUAs), covering both outcome reward models (ORM) and process reward models (PRM) across trajectory‑level and step‑level evaluation. The benchmark provides a diverse, expertly annotated dataset spanning ten software categories and seven agent architectures, and presents extensive analyses that reveal current VLM‑based reward models’ limitations in visual reasoning and knowledge. It also proposes the Unanimous Prompt Ensemble (UPE), which improves reward model reliability through strict unanimous voting and strategic prompt‑template configurations, achieving substantially higher precision and NPV than single models or traditional ensembles.", "summary_cn": "CUARewardBench 提出了首个用于评估计算机使用代理（CUA）奖励模型的全面基准，涵盖结果奖励模型（ORM）和过程奖励模型（PRM），并对轨迹级和步骤级进行评估。该基准提供了覆盖十类软件、七种代理架构的多样化、专家标注数据集，并通过大量实验揭示当前基于视觉语言模型的奖励模型在视觉推理和知识方面的局限。文中进一步提出一致性提示集成（UPE）方法，通过严格的一致投票和提示模板配置显著提升奖励模型的可靠性，精度和负预测值均大幅超越单模型和传统集成。", "keywords": "computer-using agents, reward models, benchmark, trajectory evaluation, vision-language models, ensemble, outcome reward, process reward, alignment, evaluation metrics", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Haojia Lin", "Xiaoyu Tan", "Yulei Qin", "Zihan Xu", "Yuchen Shi", "Zongyi Li", "Gang Li", "Shaofei Cai", "Siqi Cai", "Chaoyou Fu", "Ke Li", "Xing Sun"]}
]]></acme>

<pubDate>2025-10-21T12:53:40+00:00</pubDate>
</item>
<item>
<title>Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers</title>
<link>https://papers.cool/arxiv/2510.18358</link>
<guid>https://papers.cool/arxiv/2510.18358</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Hydra Ensembles, an efficient transformer ensemble that creates diverse members by pruning attention heads and combines them using a novel grouped multi-head attention mechanism. This approach achieves uncertainty quantification performance comparable to or better than Deep Ensembles while retaining inference speed close to a single model, and it includes an analysis showing that careful pruning preserves calibration. Experiments on image and text classification, including zero-shot ImageNet‑1k, demonstrate consistent improvements over existing methods without additional training.<br /><strong>Summary (CN):</strong> 本文提出了 Hydra Ensembles，一种通过裁剪注意力头部并使用新型分组多头注意力层将其合并的高效 Transformer 集成方法。该方法在保持接近单模型推理速度的同时，提供与 Deep Ensembles 相当或更优的不确定性量化性能，并通过实验表明谨慎的裁剪能够保持模型校准。实验在图像和文本分类任务（包括零样本 ImageNet‑1k）上展示了相对于现有方法的一致提升，无需额外训练。<br /><strong>Keywords:</strong> uncertainty quantification, deep ensembles, transformer pruning, Hydra Ensembles, calibrated inference, efficient ensembles, zero-shot classification<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Firas Gabetni, Giuseppe Curci, Andrea Pilzer, Subhankar Roy, Elisa Ricci, Gianni Franchi</div>
Uncertainty quantification (UQ) is essential for deploying deep neural networks in safety-critical settings. Although methods like Deep Ensembles achieve strong UQ performance, their high computational and memory costs hinder scalability to large models. We introduce Hydra Ensembles, an efficient transformer-based ensemble that prunes attention heads to create diverse members and merges them via a new multi-head attention with grouped fully-connected layers. This yields a compact model with inference speed close to a single network, matching or surpassing Deep Ensembles in UQ performance without retraining from scratch. We also provide an in-depth analysis of pruning, showing that naive approaches can harm calibration, whereas Hydra Ensembles preserves robust uncertainty. Experiments on image and text classification tasks, with various architectures, show consistent gains over Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our approach surpasses state of the art methods, even without requiring additional training.
<div><strong>Authors:</strong> Firas Gabetni, Giuseppe Curci, Andrea Pilzer, Subhankar Roy, Elisa Ricci, Gianni Franchi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Hydra Ensembles, an efficient transformer ensemble that creates diverse members by pruning attention heads and combines them using a novel grouped multi-head attention mechanism. This approach achieves uncertainty quantification performance comparable to or better than Deep Ensembles while retaining inference speed close to a single model, and it includes an analysis showing that careful pruning preserves calibration. Experiments on image and text classification, including zero-shot ImageNet‑1k, demonstrate consistent improvements over existing methods without additional training.", "summary_cn": "本文提出了 Hydra Ensembles，一种通过裁剪注意力头部并使用新型分组多头注意力层将其合并的高效 Transformer 集成方法。该方法在保持接近单模型推理速度的同时，提供与 Deep Ensembles 相当或更优的不确定性量化性能，并通过实验表明谨慎的裁剪能够保持模型校准。实验在图像和文本分类任务（包括零样本 ImageNet‑1k）上展示了相对于现有方法的一致提升，无需额外训练。", "keywords": "uncertainty quantification, deep ensembles, transformer pruning, Hydra Ensembles, calibrated inference, efficient ensembles, zero-shot classification", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Firas Gabetni", "Giuseppe Curci", "Andrea Pilzer", "Subhankar Roy", "Elisa Ricci", "Gianni Franchi"]}
]]></acme>

<pubDate>2025-10-21T07:26:38+00:00</pubDate>
</item>
<item>
<title>From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation</title>
<link>https://papers.cool/arxiv/2510.18263</link>
<guid>https://papers.cool/arxiv/2510.18263</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper addresses the trade-off between identity preservation and prompt adherence in subject-driven image generation by improving online reinforcement learning methods. It introduces Customized-GRPO, featuring Synergy-Aware Reward Shaping and Time-Aware Dynamic Weighting to align reward signals with diffusion temporal dynamics, mitigating competitive degradation seen in naive GRPO. Experiments show the method yields images with better fidelity and editability compared to baselines.<br /><strong>Summary (CN):</strong> 本文针对主题驱动图像生成中身份保持与提示匹配之间的权衡，提出改进的在线强化学习方法。通过引入Customized‑GRPO，包含协同感知奖励塑形（Synergy‑Aware Reward Shaping）和时间感知动态权重（Time‑Aware Dynamic Weighting），使奖励信号与扩散过程的时间动态保持一致，从而克服了简单GRPO导致的竞争退化。实验表明该方法在保持关键身份特征的同时，更准确地遵循复杂文本提示，优于基线。<br /><strong>Keywords:</strong> reinforcement learning, diffusion models, subject-driven image generation, reward shaping, temporal dynamics, identity preservation, prompt adherence, Customized-GRPO, synergy-aware reward shaping, dynamic weighting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</div>
Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.
<div><strong>Authors:</strong> Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper addresses the trade-off between identity preservation and prompt adherence in subject-driven image generation by improving online reinforcement learning methods. It introduces Customized-GRPO, featuring Synergy-Aware Reward Shaping and Time-Aware Dynamic Weighting to align reward signals with diffusion temporal dynamics, mitigating competitive degradation seen in naive GRPO. Experiments show the method yields images with better fidelity and editability compared to baselines.", "summary_cn": "本文针对主题驱动图像生成中身份保持与提示匹配之间的权衡，提出改进的在线强化学习方法。通过引入Customized‑GRPO，包含协同感知奖励塑形（Synergy‑Aware Reward Shaping）和时间感知动态权重（Time‑Aware Dynamic Weighting），使奖励信号与扩散过程的时间动态保持一致，从而克服了简单GRPO导致的竞争退化。实验表明该方法在保持关键身份特征的同时，更准确地遵循复杂文本提示，优于基线。", "keywords": "reinforcement learning, diffusion models, subject-driven image generation, reward shaping, temporal dynamics, identity preservation, prompt adherence, Customized-GRPO, synergy-aware reward shaping, dynamic weighting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziwei Huang", "Ying Shu", "Hao Fang", "Quanyu Long", "Wenya Wang", "Qiushi Guo", "Tiezheng Ge", "Leilei Gan"]}
]]></acme>

<pubDate>2025-10-21T03:32:26+00:00</pubDate>
</item>
<item>
<title>DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing</title>
<link>https://papers.cool/arxiv/2510.18218</link>
<guid>https://papers.cool/arxiv/2510.18218</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes DualHash, a stochastic primal‑dual algorithm with provable complexity bounds for deep hashing, addressing the discrete quantization challenge by transforming W‑type regularization via Fenchel duality to obtain a closed‑form proximal operator. Two algorithmic variants are derived, achieving \(O(\varepsilon^{-4})\) and \(O(\varepsilon^{-3})\) complexities, and experiments on three image‑retrieval datasets show improved retrieval performance.<br /><strong>Summary (CN):</strong> 本文提出 DualHash，一种具有理论复杂度保证的随机原始‑对偶深度哈希算法，通过 Fenchel 对偶将 W‑型正则化转化至对偶空间，从而获得闭式近端算子并解决二值化难题。文中给出加速动量版 (\(O(\varepsilon^{-4})\)) 与方差约减版 (\(O(\varepsilon^{-3})\)) 两种实现，并在三个图像检索数据库上验证了其性能提升。<br /><strong>Keywords:</strong> deep hashing, binary codes, stochastic primal-dual, Fenchel duality, variance reduction, image retrieval<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Luxuan Li, Xiao Wang, Chunfeng Cui</div>
Deep hashing converts high-dimensional feature vectors into compact binary codes, enabling efficient large-scale retrieval. A fundamental challenge in deep hashing stems from the discrete nature of quantization in generating the codes. W-type regularizations, such as $||z|-1|$, have been proven effective as they encourage variables toward binary values. However, existing methods often directly optimize these regularizations without convergence guarantees. While proximal gradient methods offer a promising solution, the coupling between W-type regularizers and neural network outputs results in composite forms that generally lack closed-form proximal solutions. In this paper, we present a stochastic primal-dual hashing algorithm, referred to as DualHash, that provides rigorous complexity bounds. Using Fenchel duality, we partially transform the nonconvex W-type regularization optimization into the dual space, which results in a proximal operator that admits closed-form solutions. We derive two algorithm instances: a momentum-accelerated version with $\mathcal{O}(\varepsilon^{-4})$ complexity and an improved $\mathcal{O}(\varepsilon^{-3})$ version using variance reduction. Experiments on three image retrieval databases demonstrate the superior performance of DualHash.
<div><strong>Authors:</strong> Luxuan Li, Xiao Wang, Chunfeng Cui</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes DualHash, a stochastic primal‑dual algorithm with provable complexity bounds for deep hashing, addressing the discrete quantization challenge by transforming W‑type regularization via Fenchel duality to obtain a closed‑form proximal operator. Two algorithmic variants are derived, achieving \\(O(\\varepsilon^{-4})\\) and \\(O(\\varepsilon^{-3})\\) complexities, and experiments on three image‑retrieval datasets show improved retrieval performance.", "summary_cn": "本文提出 DualHash，一种具有理论复杂度保证的随机原始‑对偶深度哈希算法，通过 Fenchel 对偶将 W‑型正则化转化至对偶空间，从而获得闭式近端算子并解决二值化难题。文中给出加速动量版 (\\(O(\\varepsilon^{-4})\\)) 与方差约减版 (\\(O(\\varepsilon^{-3})\\)) 两种实现，并在三个图像检索数据库上验证了其性能提升。", "keywords": "deep hashing, binary codes, stochastic primal-dual, Fenchel duality, variance reduction, image retrieval", "scoring": {"interpretability": 1, "understanding": 4, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Luxuan Li", "Xiao Wang", "Chunfeng Cui"]}
]]></acme>

<pubDate>2025-10-21T01:52:46+00:00</pubDate>
</item>
<item>
<title>FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo</title>
<link>https://papers.cool/arxiv/2510.18193</link>
<guid>https://papers.cool/arxiv/2510.18193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FST.ai 2.0 is an explainable AI ecosystem for Olympic and Paralympic Taekwondo that combines graph convolutional network‑based pose action recognition, credal‑set epistemic uncertainty modeling, and visual explainability overlays to assist referees, coaches, and athletes in real time. The system provides interactive dashboards for human‑AI collaboration, fairness monitoring, and policy analytics, achieving an 85% reduction in decision review time and high referee trust. The work demonstrates how transparent, uncertainty‑aware AI can be integrated into sport officiating and performance assessment.<br /><strong>Summary (CN):</strong> FST.ai 2.0 是一个面向奥运会和残奥会跆拳道的可解释 AI 生态系统，融合了基于图卷积网络的姿态动作识别、credal 集合的认知不确定性建模以及可视化解释叠加，以实时辅助裁判、教练和运动员。系统提供交互式仪表板，实现人机协作、公平性监测和政策分析，实验显示决策复审时间降低 85%，裁判对 AI 决策的信任度高。该工作展示了透明且具不确定性意识的 AI 如何在体育裁判和运动员评估中得到可靠集成。<br /><strong>Keywords:</strong> explainable AI, graph convolutional networks, pose action recognition, credal sets, epistemic uncertainty, sports fairness, human-AI collaboration, decision support, Taekwondo, interactive dashboards<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 5, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athlete performance analysis, and Para-Taekwondo classification. Beyond automated scoring, FST.ai~2.0 incorporates modules for referee training, fairness monitoring, and policy-level analytics within the World Taekwondo ecosystem. Experimental validation on competition data demonstrates an {85\% reduction in decision review time} and {93\% referee trust} in AI-assisted decisions. The framework thus establishes a transparent and extensible pipeline for trustworthy, data-driven officiating and athlete assessment. By bridging real-time perception, explainable inference, and governance-aware design, FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned AI in sports.
<div><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FST.ai 2.0 is an explainable AI ecosystem for Olympic and Paralympic Taekwondo that combines graph convolutional network‑based pose action recognition, credal‑set epistemic uncertainty modeling, and visual explainability overlays to assist referees, coaches, and athletes in real time. The system provides interactive dashboards for human‑AI collaboration, fairness monitoring, and policy analytics, achieving an 85% reduction in decision review time and high referee trust. The work demonstrates how transparent, uncertainty‑aware AI can be integrated into sport officiating and performance assessment.", "summary_cn": "FST.ai 2.0 是一个面向奥运会和残奥会跆拳道的可解释 AI 生态系统，融合了基于图卷积网络的姿态动作识别、credal 集合的认知不确定性建模以及可视化解释叠加，以实时辅助裁判、教练和运动员。系统提供交互式仪表板，实现人机协作、公平性监测和政策分析，实验显示决策复审时间降低 85%，裁判对 AI 决策的信任度高。该工作展示了透明且具不确定性意识的 AI 如何在体育裁判和运动员评估中得到可靠集成。", "keywords": "explainable AI, graph convolutional networks, pose action recognition, credal sets, epistemic uncertainty, sports fairness, human-AI collaboration, decision support, Taekwondo, interactive dashboards", "scoring": {"interpretability": 7, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Keivan Shariatmadar", "Ahmad Osman", "Ramin Ray", "Usman Dildar", "Kisam Kim"]}
]]></acme>

<pubDate>2025-10-21T00:35:56+00:00</pubDate>
</item>
<item>
<title>A Generalizable Light Transport 3D Embedding for Global Illumination</title>
<link>https://papers.cool/arxiv/2510.18189</link>
<guid>https://papers.cool/arxiv/2510.18189</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a generalizable 3D light transport embedding that predicts global illumination directly from point‑cloud scene representations using a scalable transformer. By encoding geometric and material features into neural primitives and retrieving them with cross‑attention at render time, the method can estimate diffuse irradiance across diverse indoor scenes and be fine‑tuned for other tasks such as glossy radiance fields and path guiding. Results show view‑consistent predictions without relying on rasterized or path‑traced cues.<br /><strong>Summary (CN):</strong> 本文提出了一种通用的 3D 光传输嵌入（3D light transport embedding），利用可扩展的 Transformer 从点云场景表示直接预测全局光照。它将几何和材质特征编码为神经基元，在渲染时通过交叉注意力检索并聚合这些基元，以估计多种室内场景的漫反射辐照度，并可在有限微调后用于光泽材质的空间‑方向辐射场或路径引导。实验表明该方法在无需光栅化或路径追踪提示的情况下实现了一致的视图预测。<br /><strong>Keywords:</strong> global illumination, neural rendering, 3D light transport embedding, transformer, point cloud, irradiance estimation, path guiding<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Bing Xu, Mukund Varma T, Cheng Wang, Tzumao Li, Lifan Wu, Bartlomiej Wronski, Ravi Ramamoorthi, Marco Salvi</div>
Global illumination (GI) is essential for realistic rendering but remains computationally expensive due to the complexity of simulating indirect light transport. Recent neural methods have mainly relied on per-scene optimization, sometimes extended to handle changes in camera or geometry. Efforts toward cross-scene generalization have largely stayed in 2D screen space, such as neural denoising or G-buffer based GI prediction, which often suffer from view inconsistency and limited spatial understanding. We propose a generalizable 3D light transport embedding that approximates global illumination directly from 3D scene configurations, without using rasterized or path-traced cues. Each scene is represented as a point cloud with geometric and material features. A scalable transformer models global point-to-point interactions to encode these features into neural primitives. At render time, each query point retrieves nearby primitives via nearest-neighbor search and aggregates their latent features through cross-attention to predict the desired rendering quantity. We demonstrate results on diffuse global illumination prediction across diverse indoor scenes with varying layouts, geometry, and materials. The embedding trained for irradiance estimation can be quickly adapted to new rendering tasks with limited fine-tuning. We also present preliminary results for spatial-directional radiance field estimation for glossy materials and show how the normalized field can accelerate unbiased path guiding. This approach highlights a path toward integrating learned priors into rendering pipelines without explicit ray-traced illumination cues.
<div><strong>Authors:</strong> Bing Xu, Mukund Varma T, Cheng Wang, Tzumao Li, Lifan Wu, Bartlomiej Wronski, Ravi Ramamoorthi, Marco Salvi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a generalizable 3D light transport embedding that predicts global illumination directly from point‑cloud scene representations using a scalable transformer. By encoding geometric and material features into neural primitives and retrieving them with cross‑attention at render time, the method can estimate diffuse irradiance across diverse indoor scenes and be fine‑tuned for other tasks such as glossy radiance fields and path guiding. Results show view‑consistent predictions without relying on rasterized or path‑traced cues.", "summary_cn": "本文提出了一种通用的 3D 光传输嵌入（3D light transport embedding），利用可扩展的 Transformer 从点云场景表示直接预测全局光照。它将几何和材质特征编码为神经基元，在渲染时通过交叉注意力检索并聚合这些基元，以估计多种室内场景的漫反射辐照度，并可在有限微调后用于光泽材质的空间‑方向辐射场或路径引导。实验表明该方法在无需光栅化或路径追踪提示的情况下实现了一致的视图预测。", "keywords": "global illumination, neural rendering, 3D light transport embedding, transformer, point cloud, irradiance estimation, path guiding", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bing Xu", "Mukund Varma T", "Cheng Wang", "Tzumao Li", "Lifan Wu", "Bartlomiej Wronski", "Ravi Ramamoorthi", "Marco Salvi"]}
]]></acme>

<pubDate>2025-10-21T00:29:09+00:00</pubDate>
</item>
<item>
<title>Demystifying Transition Matching: When and Why It Can Beat Flow Matching</title>
<link>https://papers.cool/arxiv/2510.17991</link>
<guid>https://papers.cool/arxiv/2510.17991</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates when Transition Matching (TM) outperforms Flow Matching (FM) in generative sampling. It proves that for unimodal Gaussian targets TM achieves lower KL divergence and faster convergence than FM, and extends the analysis to Gaussian mixtures, showing TM’s advantage in regimes with well‑separated modes and non‑vanishing variance, while the benefit disappears as variance approaches zero. Experiments on synthetic and real image/video data corroborate the theoretical findings.<br /><strong>Summary (CN):</strong> 本文研究了何时过渡匹配（Transition Matching, TM）优于流匹配（Flow Matching, FM）在生成模型中的采样性能。研究证明，在单峰高斯目标下，TM 在有限步数内获得更低的 KL 散度并且收敛更快；在高斯混合分布中，当各模式之间距离较大且方差不接近零时，TM 仍能超越 FM，而方差趋于零时两者表现趋同。实验在合成高斯分布以及图像、视频生成任务上验证了理论结果。<br /><strong>Keywords:</strong> transition matching, flow matching, generative models, KL divergence, Gaussian mixtures, sampling dynamics, convergence analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</div>
Flow Matching (FM) underpins many state-of-the-art generative models, yet recent results indicate that Transition Matching (TM) can achieve higher quality with fewer sampling steps. This work answers the question of when and why TM outperforms FM. First, when the target is a unimodal Gaussian distribution, we prove that TM attains strictly lower KL divergence than FM for finite number of steps. The improvement arises from stochastic difference latent updates in TM, which preserve target covariance that deterministic FM underestimates. We then characterize convergence rates, showing that TM achieves faster convergence than FM under a fixed compute budget, establishing its advantage in the unimodal Gaussian setting. Second, we extend the analysis to Gaussian mixtures and identify local-unimodality regimes in which the sampling dynamics approximate the unimodal case, where TM can outperform FM. The approximation error decreases as the minimal distance between component means increases, highlighting that TM is favored when the modes are well separated. However, when the target variance approaches zero, each TM update converges to the FM update, and the performance advantage of TM diminishes. In summary, we show that TM outperforms FM when the target distribution has well-separated modes and non-negligible variances. We validate our theoretical results with controlled experiments on Gaussian distributions, and extend the comparison to real-world applications in image and video generation.
<div><strong>Authors:</strong> Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates when Transition Matching (TM) outperforms Flow Matching (FM) in generative sampling. It proves that for unimodal Gaussian targets TM achieves lower KL divergence and faster convergence than FM, and extends the analysis to Gaussian mixtures, showing TM’s advantage in regimes with well‑separated modes and non‑vanishing variance, while the benefit disappears as variance approaches zero. Experiments on synthetic and real image/video data corroborate the theoretical findings.", "summary_cn": "本文研究了何时过渡匹配（Transition Matching, TM）优于流匹配（Flow Matching, FM）在生成模型中的采样性能。研究证明，在单峰高斯目标下，TM 在有限步数内获得更低的 KL 散度并且收敛更快；在高斯混合分布中，当各模式之间距离较大且方差不接近零时，TM 仍能超越 FM，而方差趋于零时两者表现趋同。实验在合成高斯分布以及图像、视频生成任务上验证了理论结果。", "keywords": "transition matching, flow matching, generative models, KL divergence, Gaussian mixtures, sampling dynamics, convergence analysis", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jaihoon Kim", "Rajarshi Saha", "Minhyuk Sung", "Youngsuk Park"]}
]]></acme>

<pubDate>2025-10-20T18:11:29+00:00</pubDate>
</item>
<item>
<title>NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</title>
<link>https://papers.cool/arxiv/2510.17914</link>
<guid>https://papers.cool/arxiv/2510.17914</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> NeuCo-Bench is a benchmark framework that evaluates lossy neural compression and representation learning for Earth Observation by using fixed-size embeddings as task-agnostic representations. It includes an evaluation pipeline, a hidden-task challenge leaderboard to reduce pretraining bias, and a scoring system that balances accuracy and stability, along with the released SSL4EO-S12-downstream dataset. Initial results from a public CVPR EARTHVISION challenge and ablations with foundation models demonstrate its utility for standardized assessment of neural embeddings.<br /><strong>Summary (CN):</strong> NeuCo-Bench 是一个用于评估地球观测（Earth Observation）中（损失）神经压缩和表征学习的基准框架，采用固定大小的嵌入向量作为任务无关的紧凑表示。它包括可复用的评估流水线、通过隐藏任务排行榜降低预训练偏差的挑战模式以及在准确性与稳定性之间平衡的评分系统，并发布了 SSL4EO-S12-downstream 多光谱多时相数据集。初步在 2025 CVPR EARTHVISION 工作坊的公开挑战以及对最先进基础模型的消融实验展示了该框架在标准化神经嵌入评估方面的价值。<br /><strong>Keywords:</strong> neural embeddings, Earth observation, benchmark, representation learning, SSL4EO, multispectral, multitemporal, downstream tasks, evaluation pipeline, challenge leaderboard<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy) neural compression and representation learning in the context of Earth Observation (EO). Our approach builds on fixed-size embeddings that act as compact, task-agnostic representations applicable to a broad range of downstream tasks. NeuCo-Bench comprises three core components: (i) an evaluation pipeline built around reusable embeddings, (ii) a new challenge mode with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii) a scoring system that balances accuracy and stability. To support reproducibility, we release SSL4EO-S12-downstream, a curated multispectral, multitemporal EO dataset. We present initial results from a public challenge at the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art foundation models. NeuCo-Bench provides a first step towards community-driven, standardized evaluation of neural embeddings for EO and beyond.
<div><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "NeuCo-Bench is a benchmark framework that evaluates lossy neural compression and representation learning for Earth Observation by using fixed-size embeddings as task-agnostic representations. It includes an evaluation pipeline, a hidden-task challenge leaderboard to reduce pretraining bias, and a scoring system that balances accuracy and stability, along with the released SSL4EO-S12-downstream dataset. Initial results from a public CVPR EARTHVISION challenge and ablations with foundation models demonstrate its utility for standardized assessment of neural embeddings.", "summary_cn": "NeuCo-Bench 是一个用于评估地球观测（Earth Observation）中（损失）神经压缩和表征学习的基准框架，采用固定大小的嵌入向量作为任务无关的紧凑表示。它包括可复用的评估流水线、通过隐藏任务排行榜降低预训练偏差的挑战模式以及在准确性与稳定性之间平衡的评分系统，并发布了 SSL4EO-S12-downstream 多光谱多时相数据集。初步在 2025 CVPR EARTHVISION 工作坊的公开挑战以及对最先进基础模型的消融实验展示了该框架在标准化神经嵌入评估方面的价值。", "keywords": "neural embeddings, Earth observation, benchmark, representation learning, SSL4EO, multispectral, multitemporal, downstream tasks, evaluation pipeline, challenge leaderboard", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rikard Vinge", "Isabelle Wittmann", "Jannik Schneider", "Michael Marszalek", "Luis Gilch", "Thomas Brunschwiler", "Conrad M Albrecht"]}
]]></acme>

<pubDate>2025-10-19T23:47:33+00:00</pubDate>
</item>
<item>
<title>Conformal Lesion Segmentation for 3D Medical Images</title>
<link>https://papers.cool/arxiv/2510.17897</link>
<guid>https://papers.cool/arxiv/2510.17897</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Conformal Lesion Segmentation (CLS), a risk-constrained framework that calibrates data-driven thresholds via conformal prediction to guarantee that the false negative rate (FNR) of 3D lesion segmentation remains below a user-specified tolerance with a desired confidence level. By holding out a calibration set, CLS computes critical thresholds for each sample that meet the FNR target, then uses the (1-α) quantile of these thresholds as the test-time decision boundary, providing statistical guarantees while improving segmentation accuracy. Experiments on six 3D‑LS datasets and five backbone models demonstrate the method's statistical soundness and practical performance gains for clinical deployment.<br /><strong>Summary (CN):</strong> 本文提出了“Conformal Lesion Segmentation (CLS)”风险约束框架，通过共形预测校准数据驱动阈值，以确保 3D 病灶分割的假阴性率 (FNR) 在给定容忍度 ε 下，并在期望置信水平 α 下提供统计保证。CLS 在校准集上为每个样本计算满足 FNR 目标的关键阈值，然后取这些阈值的 (1-α) 分位数作为测试时的决策阈值，实现了统计可靠性并提升了分割精度。六个 3D‑LS 数据集和五种主干模型的实验验证了方法的统计可靠性和在临床部署中的实际性能提升。<br /><strong>Keywords:</strong> conformal prediction, lesion segmentation, false negative rate, risk-constrained segmentation, 3D medical imaging, calibration, uncertainty quantification<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Binyu Tan, Zhiyuan Wang, Jinhao Duan, Kaidi Xu, Heng Tao Shen, Xiaoshuang Shi, Fumin Shen</div>
Medical image segmentation serves as a critical component of precision medicine, enabling accurate localization and delineation of pathological regions, such as lesions. However, existing models empirically apply fixed thresholds (e.g., 0.5) to differentiate lesions from the background, offering no statistical guarantees on key metrics such as the false negative rate (FNR). This lack of principled risk control undermines their reliable deployment in high-stakes clinical applications, especially in challenging scenarios like 3D lesion segmentation (3D-LS). To address this issue, we propose a risk-constrained framework, termed Conformal Lesion Segmentation (CLS), that calibrates data-driven thresholds via conformalization to ensure the test-time FNR remains below a target tolerance $\varepsilon$ under desired risk levels. CLS begins by holding out a calibration set to analyze the threshold setting for each sample under the FNR tolerance, drawing on the idea of conformal prediction. We define an FNR-specific loss function and identify the critical threshold at which each calibration data point just satisfies the target tolerance. Given a user-specified risk level $\alpha$, we then determine the approximate $1-\alpha$ quantile of all the critical thresholds in the calibration set as the test-time confidence threshold. By conformalizing such critical thresholds, CLS generalizes the statistical regularities observed in the calibration set to new test data, providing rigorous FNR constraint while yielding more precise and reliable segmentations. We validate the statistical soundness and predictive performance of CLS on six 3D-LS datasets across five backbone models, and conclude with actionable insights for deploying risk-aware segmentation in clinical practice.
<div><strong>Authors:</strong> Binyu Tan, Zhiyuan Wang, Jinhao Duan, Kaidi Xu, Heng Tao Shen, Xiaoshuang Shi, Fumin Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Conformal Lesion Segmentation (CLS), a risk-constrained framework that calibrates data-driven thresholds via conformal prediction to guarantee that the false negative rate (FNR) of 3D lesion segmentation remains below a user-specified tolerance with a desired confidence level. By holding out a calibration set, CLS computes critical thresholds for each sample that meet the FNR target, then uses the (1-α) quantile of these thresholds as the test-time decision boundary, providing statistical guarantees while improving segmentation accuracy. Experiments on six 3D‑LS datasets and five backbone models demonstrate the method's statistical soundness and practical performance gains for clinical deployment.", "summary_cn": "本文提出了“Conformal Lesion Segmentation (CLS)”风险约束框架，通过共形预测校准数据驱动阈值，以确保 3D 病灶分割的假阴性率 (FNR) 在给定容忍度 ε 下，并在期望置信水平 α 下提供统计保证。CLS 在校准集上为每个样本计算满足 FNR 目标的关键阈值，然后取这些阈值的 (1-α) 分位数作为测试时的决策阈值，实现了统计可靠性并提升了分割精度。六个 3D‑LS 数据集和五种主干模型的实验验证了方法的统计可靠性和在临床部署中的实际性能提升。", "keywords": "conformal prediction, lesion segmentation, false negative rate, risk-constrained segmentation, 3D medical imaging, calibration, uncertainty quantification", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Binyu Tan", "Zhiyuan Wang", "Jinhao Duan", "Kaidi Xu", "Heng Tao Shen", "Xiaoshuang Shi", "Fumin Shen"]}
]]></acme>

<pubDate>2025-10-19T08:21:00+00:00</pubDate>
</item>
<item>
<title>Metrics and evaluations for computational and sustainable AI efficiency</title>
<link>https://papers.cool/arxiv/2510.17885</link>
<guid>https://papers.cool/arxiv/2510.17885</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a unified, reproducible methodology for evaluating AI model inference that combines computational performance with environmental impact metrics such as energy use and location‑adjusted carbon emissions. It applies this framework across a range of hardware platforms and software stacks, producing Pareto frontiers that clarify trade‑offs between accuracy, latency, energy, and carbon under realistic serving conditions.<br /><strong>Summary (CN):</strong> 本文提出了一套统一且可复现的 AI 推理评估方法，将计算性能与能源消耗、地点校正的碳排放等环境影响指标相结合。通过在多种硬件平台和软件栈上实验，生成展示准确率、延迟、能耗和碳排放权衡的 Pareto 前沿，帮助实现可持续的 AI 部署。<br /><strong>Keywords:</strong> AI efficiency, carbon emissions, benchmarking, inference latency, energy consumption, sustainable AI, multi-precision models, hardware evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
The rapid advancement of Artificial Intelligence (AI) has created unprecedented demands for computational power, yet methods for evaluating the performance, efficiency, and environmental impact of deployed models remain fragmented. Current approaches often fail to provide a holistic view, making it difficult to compare and optimise systems across heterogeneous hardware, software stacks, and numeric precisions. To address this gap, we propose a unified and reproducible methodology for AI model inference that integrates computational and environmental metrics under realistic serving conditions. Our framework provides a pragmatic, carbon-aware evaluation by systematically measuring latency and throughput distributions, energy consumption, and location-adjusted carbon emissions, all while maintaining matched accuracy constraints for valid comparisons. We apply this methodology to multi-precision models across diverse hardware platforms, from data-centre accelerators like the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream software stacks including PyTorch, TensorRT, and ONNX Runtime. By systematically categorising these factors, our work establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying the trade-offs between accuracy, latency, energy, and carbon. The accompanying open-source code enables independent verification and facilitates adoption, empowering researchers and practitioners to make evidence-based decisions for sustainable AI deployment.
<div><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a unified, reproducible methodology for evaluating AI model inference that combines computational performance with environmental impact metrics such as energy use and location‑adjusted carbon emissions. It applies this framework across a range of hardware platforms and software stacks, producing Pareto frontiers that clarify trade‑offs between accuracy, latency, energy, and carbon under realistic serving conditions.", "summary_cn": "本文提出了一套统一且可复现的 AI 推理评估方法，将计算性能与能源消耗、地点校正的碳排放等环境影响指标相结合。通过在多种硬件平台和软件栈上实验，生成展示准确率、延迟、能耗和碳排放权衡的 Pareto 前沿，帮助实现可持续的 AI 部署。", "keywords": "AI efficiency, carbon emissions, benchmarking, inference latency, energy consumption, sustainable AI, multi-precision models, hardware evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}
]]></acme>

<pubDate>2025-10-18T03:30:15+00:00</pubDate>
</item>
<item>
<title>DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association</title>
<link>https://papers.cool/arxiv/2510.17860</link>
<guid>https://papers.cool/arxiv/2510.17860</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DMTrack presents a deformable state-space tracking framework for UAV-based multi-object tracking, featuring DeformMamba for adaptive trajectory modeling, MotionGate for Kalman-Mamba prediction fusion, and an uncertainty-aware association method to improve identity consistency without relying on appearance cues.<br /><strong>Summary (CN):</strong> DMTrack 提出了一种用于 UAV 多目标跟踪的可变形状态空间框架，包括 DeformMamba 用于自适应轨迹建模、MotionGate 用于卡尔曼与 Mamba 预测的融合以及基于不确定性的关联策略，以在不使用外观模型的情况下提升身份保持和追踪准确性。<br /><strong>Keywords:</strong> multi-object tracking, UAV, deformable state-space, Kalman fusion, uncertainty-aware association, motion modeling, DeformMamba, MotionGate, VisDrone-MOT, UAVDT<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zenghuang Fu, Xiaofeng Han, Mingda Jia, Jin ming Yang, Qi Zeng, Muyang Zahng, Changwei Wang, Weiliang Meng, Xiaopeng Zhang</div>
Multi-object tracking (MOT) from unmanned aerial vehicles (UAVs) presents unique challenges due to unpredictable object motion, frequent occlusions, and limited appearance cues inherent to aerial viewpoints. These issues are further exacerbated by abrupt UAV movements, leading to unreliable trajectory estimation and identity switches. Conventional motion models, such as Kalman filters or static sequence encoders, often fall short in capturing both linear and non-linear dynamics under such conditions. To tackle these limitations, we propose DMTrack, a deformable motion tracking framework tailored for UAV-based MOT. Our DMTrack introduces three key components: DeformMamba, a deformable state-space predictor that dynamically aggregates historical motion states for adaptive trajectory modeling; MotionGate, a lightweight gating module that fuses Kalman and Mamba predictions based on motion context and uncertainty; and an uncertainty-aware association strategy that enhances identity preservation by aligning motion trends with prediction confidence. Extensive experiments on the VisDrone-MOT and UAVDT benchmarks demonstrate that our DMTrack achieves state-of-the-art performance in identity consistency and tracking accuracy, particularly under high-speed and non-linear motion. Importantly, our method operates without appearance models and maintains competitive efficiency, highlighting its practicality for robust UAV-based tracking.
<div><strong>Authors:</strong> Zenghuang Fu, Xiaofeng Han, Mingda Jia, Jin ming Yang, Qi Zeng, Muyang Zahng, Changwei Wang, Weiliang Meng, Xiaopeng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DMTrack presents a deformable state-space tracking framework for UAV-based multi-object tracking, featuring DeformMamba for adaptive trajectory modeling, MotionGate for Kalman-Mamba prediction fusion, and an uncertainty-aware association method to improve identity consistency without relying on appearance cues.", "summary_cn": "DMTrack 提出了一种用于 UAV 多目标跟踪的可变形状态空间框架，包括 DeformMamba 用于自适应轨迹建模、MotionGate 用于卡尔曼与 Mamba 预测的融合以及基于不确定性的关联策略，以在不使用外观模型的情况下提升身份保持和追踪准确性。", "keywords": "multi-object tracking, UAV, deformable state-space, Kalman fusion, uncertainty-aware association, motion modeling, DeformMamba, MotionGate, VisDrone-MOT, UAVDT", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zenghuang Fu", "Xiaofeng Han", "Mingda Jia", "Jin ming Yang", "Qi Zeng", "Muyang Zahng", "Changwei Wang", "Weiliang Meng", "Xiaopeng Zhang"]}
]]></acme>

<pubDate>2025-10-15T13:54:25+00:00</pubDate>
</item>
<item>
<title>Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing</title>
<link>https://papers.cool/arxiv/2510.17816</link>
<guid>https://papers.cool/arxiv/2510.17816</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces WiAnchor, a training framework that leverages near-field Wi-Fi signals from personal devices to enable multi-person human activity recognition across domains, even when some activity categories are missing. It enlarges inter-class margins during pre-training, uses an anchor-matching mechanism for fine-tuning to filter subject-specific interference, and refines predictions via feature similarity to anchors, achieving over 90% cross-domain accuracy on a newly collected dataset.<br /><strong>Summary (CN):</strong> 本文提出 WiAnchor 框架，利用个人设备的近场 Wi‑Fi 信号实现多人与跨域的人体活动识别，即使部分活动类别缺失。该框架在预训练阶段扩大类间特征间距，在微调阶段采用锚点匹配机制过滤受试者特定干扰，并通过与锚点的特征相似度进一步提升识别准确率，在新构建的数据集上实现超过 90% 的跨域准确率。<br /><strong>Keywords:</strong> Wi-Fi sensing, human activity recognition, cross-domain adaptation, near-field effect, anchor matching, fine-tuning, multi-person HAR<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xin Li, Jingzhi Hu, Yinghui He, Hongbo Wang, Jin Gan, Jun Luo</div>
Wi-Fi-based human activity recognition (HAR) provides substantial convenience and has emerged as a thriving research field, yet the coarse spatial resolution inherent to Wi-Fi significantly hinders its ability to distinguish multiple subjects. By exploiting the near-field domination effect, establishing a dedicated sensing link for each subject through their personal Wi-Fi device offers a promising solution for multi-person HAR under native traffic. However, due to the subject-specific characteristics and irregular patterns of near-field signals, HAR neural network models require fine-tuning (FT) for cross-domain adaptation, which becomes particularly challenging with certain categories unavailable. In this paper, we propose WiAnchor, a novel training framework for efficient cross-domain adaptation in the presence of incomplete activity categories. This framework processes Wi-Fi signals embedded with irregular time information in three steps: during pre-training, we enlarge inter-class feature margins to enhance the separability of activities; in the FT stage, we innovate an anchor matching mechanism for cross-domain adaptation, filtering subject-specific interference informed by incomplete activity categories, rather than attempting to extract complete features from them; finally, the recognition of input samples is further improved based on their feature-level similarity with anchors. We construct a comprehensive dataset to thoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with absent activity categories.
<div><strong>Authors:</strong> Xin Li, Jingzhi Hu, Yinghui He, Hongbo Wang, Jin Gan, Jun Luo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces WiAnchor, a training framework that leverages near-field Wi-Fi signals from personal devices to enable multi-person human activity recognition across domains, even when some activity categories are missing. It enlarges inter-class margins during pre-training, uses an anchor-matching mechanism for fine-tuning to filter subject-specific interference, and refines predictions via feature similarity to anchors, achieving over 90% cross-domain accuracy on a newly collected dataset.", "summary_cn": "本文提出 WiAnchor 框架，利用个人设备的近场 Wi‑Fi 信号实现多人与跨域的人体活动识别，即使部分活动类别缺失。该框架在预训练阶段扩大类间特征间距，在微调阶段采用锚点匹配机制过滤受试者特定干扰，并通过与锚点的特征相似度进一步提升识别准确率，在新构建的数据集上实现超过 90% 的跨域准确率。", "keywords": "Wi-Fi sensing, human activity recognition, cross-domain adaptation, near-field effect, anchor matching, fine-tuning, multi-person HAR", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xin Li", "Jingzhi Hu", "Yinghui He", "Hongbo Wang", "Jin Gan", "Jun Luo"]}
]]></acme>

<pubDate>2025-09-27T03:22:15+00:00</pubDate>
</item>
<item>
<title>Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain</title>
<link>https://papers.cool/arxiv/2510.17801</link>
<guid>https://papers.cool/arxiv/2510.17801</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This work introduces RoboBench, a comprehensive benchmark for evaluating multimodal large language models as embodied brains in robotic manipulation. It defines five evaluation dimensions—instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis—across 14 capabilities, 25 tasks, and thousands of QA pairs, and includes an MLLM-as‑world‑simulator framework to test plan feasibility. Experiments on 14 MLLMs highlight fundamental limitations in implicit instruction understanding, spatiotemporal reasoning, cross‑scenario planning, fine‑grained affordance prediction, and execution failure diagnosis.<br /><strong>Summary (CN):</strong> 本研究提出 RoboBench，一个用于评估多模态大语言模型（MLLM）作为具身大脑在机器人操作任务中表现的全面基准。它从指令理解、感知推理、通用规划、可供性预测和失败分析五个维度，覆盖 14 项能力、25 项任务和数千个问答对，并引入 MLLM‑as‑world‑simulator 框架评估计划的可执行性。对 14 种 MLLM 的实验揭示了在隐式指令理解、时空推理、跨场景规划、细粒度可供性以及执行失败诊断方面的基本局限。<br /><strong>Keywords:</strong> multimodal large language model, embodied AI, benchmark, instruction comprehension, perception reasoning, planning, affordance prediction, failure analysis, robotic manipulation, evaluation framework<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang</div>
Building robots that can perceive, reason, and act in dynamic, unstructured environments remains a core challenge. Recent embodied systems often adopt a dual-system paradigm, where System 2 handles high-level reasoning while System 1 executes low-level control. In this work, we refer to System 2 as the embodied brain, emphasizing its role as the cognitive core for reasoning and decision-making in manipulation tasks. Given this role, systematic evaluation of the embodied brain is essential. Yet existing benchmarks emphasize execution success, or when targeting high-level reasoning, suffer from incomplete dimensions and limited task realism, offering only a partial picture of cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark that systematically evaluates multimodal large language models (MLLMs) as embodied brains. Motivated by the critical roles across the full manipulation pipeline, RoboBench defines five dimensions-instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure realism, we curate datasets across diverse embodiments, attribute-rich objects, and multi-view scenes, drawing from large-scale real robotic data. For planning, RoboBench introduces an evaluation framework, MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether predicted plans can achieve critical object-state changes. Experiments on 14 MLLMs reveal fundamental limitations: difficulties with implicit instruction comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis. RoboBench provides a comprehensive scaffold to quantify high-level cognition, and guide the development of next-generation embodied MLLMs. The project page is in https://robo-bench.github.io.
<div><strong>Authors:</strong> Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This work introduces RoboBench, a comprehensive benchmark for evaluating multimodal large language models as embodied brains in robotic manipulation. It defines five evaluation dimensions—instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis—across 14 capabilities, 25 tasks, and thousands of QA pairs, and includes an MLLM-as‑world‑simulator framework to test plan feasibility. Experiments on 14 MLLMs highlight fundamental limitations in implicit instruction understanding, spatiotemporal reasoning, cross‑scenario planning, fine‑grained affordance prediction, and execution failure diagnosis.", "summary_cn": "本研究提出 RoboBench，一个用于评估多模态大语言模型（MLLM）作为具身大脑在机器人操作任务中表现的全面基准。它从指令理解、感知推理、通用规划、可供性预测和失败分析五个维度，覆盖 14 项能力、25 项任务和数千个问答对，并引入 MLLM‑as‑world‑simulator 框架评估计划的可执行性。对 14 种 MLLM 的实验揭示了在隐式指令理解、时空推理、跨场景规划、细粒度可供性以及执行失败诊断方面的基本局限。", "keywords": "multimodal large language model, embodied AI, benchmark, instruction comprehension, perception reasoning, planning, affordance prediction, failure analysis, robotic manipulation, evaluation framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"]}
]]></acme>

<pubDate>2025-10-20T17:59:03+00:00</pubDate>
</item>
</channel>
</rss>