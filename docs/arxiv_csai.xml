<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Artificial Intelligence</title>
<link>https://papers.cool/arxiv/cs.AI</link>


<item>
<title>Real Deep Research for AI, Robotics and Beyond</title>
<link>https://papers.cool/arxiv/2510.20809</link>
<guid>https://papers.cool/arxiv/2510.20809</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Real Deep Research (RDR), a generalizable pipeline that systematically analyzes research domains to identify emerging trends, cross‑domain opportunities, and concrete starting points for new inquiry, with detailed applications to AI, foundation models, and robotics. It describes the construction of the RDR framework, presents extensive trend and opportunity results across AI and robotics topics, and briefly extends the analysis to other scientific areas.<br /><strong>Summary (CN):</strong> 本文提出了 Real Deep Research（RDR）框架，一套可通用的研究领域分析管道，用于系统识别新兴趋势、跨域机会以及新研究的切入点，详细应用于 AI、基础模型和机器人领域，并在附录中展示了广泛的趋势和机会结果，同时简要扩展至其他科学领域。<br /><strong>Keywords:</strong> trend analysis, research pipeline, AI foundations, robotics, cross-domain survey, meta-research<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.
<div><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Real Deep Research (RDR), a generalizable pipeline that systematically analyzes research domains to identify emerging trends, cross‑domain opportunities, and concrete starting points for new inquiry, with detailed applications to AI, foundation models, and robotics. It describes the construction of the RDR framework, presents extensive trend and opportunity results across AI and robotics topics, and briefly extends the analysis to other scientific areas.", "summary_cn": "本文提出了 Real Deep Research（RDR）框架，一套可通用的研究领域分析管道，用于系统识别新兴趋势、跨域机会以及新研究的切入点，详细应用于 AI、基础模型和机器人领域，并在附录中展示了广泛的趋势和机会结果，同时简要扩展至其他科学领域。", "keywords": "trend analysis, research pipeline, AI foundations, robotics, cross-domain survey, meta-research", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:05+00:00</pubDate>
</item>
<item>
<title>A Coherence-Based Measure of AGI</title>
<link>https://papers.cool/arxiv/2510.20784</link>
<guid>https://papers.cool/arxiv/2510.20784</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a coherence‑based measure of artificial general intelligence that integrates generalized means over a range of compensability exponents, yielding an area‑under‑the‑curve (AUC) that penalizes imbalanced domain performance. Applied to CHC‑based domain scores for GPT‑4 and GPT‑5, the metric shows that despite high arithmetic averages, both models fall far short of true general competence. This approach offers a more stringent and interpretable framework for assessing progress toward AGI.<br /><strong>Summary (CN):</strong> 本文提出一种基于一致性的 AGI 测量方法，利用在不同补偿指数下的广义均值积分得到曲线下面积（AUC），该指标对各认知领域表现不平衡给予惩罚。将该度量应用于 GPT‑4 和 GPT‑5 的 CHC 领域得分后发现，尽管算术平均分较高，两者仍远未达到真正的通用能力。该方法为评估 AGI 进展提供了更严格且可解释的框架。<br /><strong>Keywords:</strong> AGI measurement, coherence, generalized mean, compensability, CHC model, AI evaluation, metric robustness, GPT-4, GPT-5<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Fares Fourati</div>
Recent work by \citet{hendrycks2025agidefinition} formalized \textit{Artificial General Intelligence} (AGI) as the arithmetic mean of proficiencies across cognitive domains derived from the Cattell--Horn--Carroll (CHC) model of human cognition. While elegant, this definition assumes \textit{compensability} -- that exceptional ability in some domains can offset failure in others. True general intelligence, however, should reflect \textit{coherent sufficiency}: balanced competence across all essential domains. We propose a coherence-aware measure of AGI based on the integral of generalized means over a continuum of compensability exponents. This formulation spans arithmetic, geometric, and harmonic regimes, and the resulting \textit{area under the curve} (AUC) quantifies robustness under varying compensability assumptions. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and captures inter-domain dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5, the coherence-adjusted AUC reveals that both systems remain far from general competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating the generalized mean thus yields a principled, interpretable, and stricter foundation for measuring genuine progress toward AGI.
<div><strong>Authors:</strong> Fares Fourati</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a coherence‑based measure of artificial general intelligence that integrates generalized means over a range of compensability exponents, yielding an area‑under‑the‑curve (AUC) that penalizes imbalanced domain performance. Applied to CHC‑based domain scores for GPT‑4 and GPT‑5, the metric shows that despite high arithmetic averages, both models fall far short of true general competence. This approach offers a more stringent and interpretable framework for assessing progress toward AGI.", "summary_cn": "本文提出一种基于一致性的 AGI 测量方法，利用在不同补偿指数下的广义均值积分得到曲线下面积（AUC），该指标对各认知领域表现不平衡给予惩罚。将该度量应用于 GPT‑4 和 GPT‑5 的 CHC 领域得分后发现，尽管算术平均分较高，两者仍远未达到真正的通用能力。该方法为评估 AGI 进展提供了更严格且可解释的框架。", "keywords": "AGI measurement, coherence, generalized mean, compensability, CHC model, AI evaluation, metric robustness, GPT-4, GPT-5", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Fares Fourati"]}
]]></acme>

<pubDate>2025-10-23T17:51:42+00:00</pubDate>
</item>
<item>
<title>Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs</title>
<link>https://papers.cool/arxiv/2510.20691</link>
<guid>https://papers.cool/arxiv/2510.20691</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces  ​   ​   ​  ​   ​  “  …  “  “  “ … … … … “   … … … … … “  “  “ …“ … “.”<br /><strong>Summary (CN):</strong> Information          ....   “  .”<br /><strong>Keywords:</strong> KGQA, knowledge graph, reinforcement learning, chain-of-thought, planning, retrieval, large language models, multi-step reasoning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yanlin Song, Ben Liu, Víctor Gutiérrez-Basulto, Zhiwei Hu, Qianqian Xie, Min Peng, Sophia Ananiadou, Jeff Z. Pan</div>
Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a 'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.
<div><strong>Authors:</strong> Yanlin Song, Ben Liu, Víctor Gutiérrez-Basulto, Zhiwei Hu, Qianqian Xie, Min Peng, Sophia Ananiadou, Jeff Z. Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces  ​   ​   ​  ​   ​  “  …  “  “  “ … … … … “   … … … … … “  “  “ …“ … “.”", "summary_cn": "Information          ....   “  .”", "keywords": "KGQA, knowledge graph, reinforcement learning, chain-of-thought, planning, retrieval, large language models, multi-step reasoning", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yanlin Song", "Ben Liu", "Víctor Gutiérrez-Basulto", "Zhiwei Hu", "Qianqian Xie", "Min Peng", "Sophia Ananiadou", "Jeff Z. Pan"]}
]]></acme>

<pubDate>2025-10-23T16:04:13+00:00</pubDate>
</item>
<item>
<title>The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20665</link>
<guid>https://papers.cool/arxiv/2510.20665</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a topological data analysis (TDA) framework to evaluate the quality of reasoning traces generated by large language models, showing that topological features predict reasoning quality more effectively than traditional graph-based metrics. Empirical results demonstrate that a compact set of stable topological features can serve as reliable, label-efficient signals for assessing trace quality and can be incorporated into reinforcement learning algorithms for improved reasoning assessment.<br /><strong>Summary (CN):</strong> 本文提出使用拓扑数据分析（TDA）框架来评估大语言模型生成的推理轨迹质量，实验证明相较于传统图结构指标，拓扑特征对推理质量的预测能力更强。研究展示了一组紧凑且稳定的拓扑特征可作为可靠的低标签成本信号，用于评估轨迹质量，并可用于强化学习算法中提升推理评估效果。<br /><strong>Keywords:</strong> topological data analysis, reasoning traces, large language models, evaluation metrics, graph metrics, reinforcement learning, geometry, interpretability, AI safety, label-efficient assessment<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 8, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Xue Wen Tan, Nathaniel Tan, Galen Lee, Stanley Kok</div>
Evaluating the quality of reasoning traces from large language models remains understudied, labor-intensive, and unreliable: current practice relies on expert rubrics, manual annotation, and slow pairwise judgments. Automated efforts are dominated by graph-based proxies that quantify structural connectivity but do not clarify what constitutes high-quality reasoning; such abstractions can be overly simplistic for inherently complex processes. We introduce a topological data analysis (TDA)-based evaluation framework that captures the geometry of reasoning traces and enables label-efficient, automated assessment. In our empirical study, topological features yield substantially higher predictive power for assessing reasoning quality than standard graph metrics, suggesting that effective reasoning is better captured by higher-dimensional geometric structures rather than purely relational graphs. We further show that a compact, stable set of topological features reliably indicates trace quality, offering a practical signal for future reinforcement learning algorithms.
<div><strong>Authors:</strong> Xue Wen Tan, Nathaniel Tan, Galen Lee, Stanley Kok</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a topological data analysis (TDA) framework to evaluate the quality of reasoning traces generated by large language models, showing that topological features predict reasoning quality more effectively than traditional graph-based metrics. Empirical results demonstrate that a compact set of stable topological features can serve as reliable, label-efficient signals for assessing trace quality and can be incorporated into reinforcement learning algorithms for improved reasoning assessment.", "summary_cn": "本文提出使用拓扑数据分析（TDA）框架来评估大语言模型生成的推理轨迹质量，实验证明相较于传统图结构指标，拓扑特征对推理质量的预测能力更强。研究展示了一组紧凑且稳定的拓扑特征可作为可靠的低标签成本信号，用于评估轨迹质量，并可用于强化学习算法中提升推理评估效果。", "keywords": "topological data analysis, reasoning traces, large language models, evaluation metrics, graph metrics, reinforcement learning, geometry, interpretability, AI safety, label-efficient assessment", "scoring": {"interpretability": 6, "understanding": 8, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xue Wen Tan", "Nathaniel Tan", "Galen Lee", "Stanley Kok"]}
]]></acme>

<pubDate>2025-10-23T15:43:43+00:00</pubDate>
</item>
<item>
<title>Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges</title>
<link>https://papers.cool/arxiv/2510.20641</link>
<guid>https://papers.cool/arxiv/2510.20641</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys and systematizes current approaches for embedding machine learning models within Belief-Desire-Intention (BDI) rational agent architectures, highlighting how existing work often treats agents as generic containers and overlooks BDI's expressive capabilities. It classifies the literature, identifies research gaps, and outlines open challenges for creating effective rational ML agents.<br /><strong>Summary (CN):</strong> 本文综述并系统化了将机器学习模型嵌入信念-欲望-意图（BDI）理性代理架构的现有方法，指出大多数工作将代理视为通用容器，忽视了 BDI 的表达能力。文章对文献进行分类，指出研究空白，并提出了构建有效理性机器学习代理的开放挑战。<br /><strong>Keywords:</strong> BDI agents, machine learning integration, rational agents, architecture, systematization, research challenges, AI planning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Andrea Agiollo, Andrea Omicini</div>
Thanks to the remarkable human-like capabilities of machine learning (ML) models in perceptual and cognitive tasks, frameworks integrating ML within rational agent architectures are gaining traction. Yet, the landscape remains fragmented and incoherent, often focusing on embedding ML into generic agent containers while overlooking the expressive power of rational architectures--such as Belief-Desire-Intention (BDI) agents. This paper presents a fine-grained systematisation of existing approaches, using the BDI paradigm as a reference. Our analysis illustrates the fast-evolving literature on rational agents enhanced by ML, and identifies key research opportunities and open challenges for designing effective rational ML agents.
<div><strong>Authors:</strong> Andrea Agiollo, Andrea Omicini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys and systematizes current approaches for embedding machine learning models within Belief-Desire-Intention (BDI) rational agent architectures, highlighting how existing work often treats agents as generic containers and overlooks BDI's expressive capabilities. It classifies the literature, identifies research gaps, and outlines open challenges for creating effective rational ML agents.", "summary_cn": "本文综述并系统化了将机器学习模型嵌入信念-欲望-意图（BDI）理性代理架构的现有方法，指出大多数工作将代理视为通用容器，忽视了 BDI 的表达能力。文章对文献进行分类，指出研究空白，并提出了构建有效理性机器学习代理的开放挑战。", "keywords": "BDI agents, machine learning integration, rational agents, architecture, systematization, research challenges, AI planning", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Andrea Agiollo", "Andrea Omicini"]}
]]></acme>

<pubDate>2025-10-23T15:15:45+00:00</pubDate>
</item>
<item>
<title>Fluidity Index: Next-Generation Super-intelligence Benchmarks</title>
<link>https://papers.cool/arxiv/2510.20636</link>
<guid>https://papers.cool/arxiv/2510.20636</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Fluidity Index (FI), a benchmark designed to measure a model's adaptability in dynamic scaling environments by evaluating response accuracy across deviations in initial, current, and future state conditions. It differentiates between closed-ended and open-ended benchmarks, emphasizing closed-loop open-ended real-world tests to assess context switching, continuity, and second-order adaptability needed for super-intelligent behavior.<br /><strong>Summary (CN):</strong> 本文提出了“流动性指数”（Fluidity Index，FI）作为衡量模型在动态扩展环境中适应能力的基准，通过评估模型对初始、当前和未来环境状态偏差的响应准确性来进行测量。文章区分了封闭式和开放式基准，强调使用闭环开放式真实世界测试来检验模型的情境切换、连续性以及实现超智能所需的二阶适应能力。<br /><strong>Keywords:</strong> Fluidity Index, adaptability benchmark, dynamic environments, scaling, second-order adaptability, open-ended benchmark, context switching, continuity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Eric Ngoiya, Tianshu Bao</div>
This paper introduces the Fluidity Index (FI) to quantify model adaptability in dynamic, scaling environments. The benchmark evaluates response accuracy based on deviations in initial, current, and future environment states, assessing context switching and continuity. We distinguish between closed-ended and open-ended benchmarks, prioritizing closed-loop open-ended real-world benchmarks to test adaptability. The approach measures a model's ability to understand, predict, and adjust to state changes in scaling environments. A truly super-intelligent model should exhibit at least second-order adaptability, enabling self-sustained computation through digital replenishment for optimal fluidity.
<div><strong>Authors:</strong> Eric Ngoiya, Tianshu Bao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Fluidity Index (FI), a benchmark designed to measure a model's adaptability in dynamic scaling environments by evaluating response accuracy across deviations in initial, current, and future state conditions. It differentiates between closed-ended and open-ended benchmarks, emphasizing closed-loop open-ended real-world tests to assess context switching, continuity, and second-order adaptability needed for super-intelligent behavior.", "summary_cn": "本文提出了“流动性指数”（Fluidity Index，FI）作为衡量模型在动态扩展环境中适应能力的基准，通过评估模型对初始、当前和未来环境状态偏差的响应准确性来进行测量。文章区分了封闭式和开放式基准，强调使用闭环开放式真实世界测试来检验模型的情境切换、连续性以及实现超智能所需的二阶适应能力。", "keywords": "Fluidity Index, adaptability benchmark, dynamic environments, scaling, second-order adaptability, open-ended benchmark, context switching, continuity", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Eric Ngoiya", "Tianshu Bao"]}
]]></acme>

<pubDate>2025-10-23T15:05:23+00:00</pubDate>
</item>
<item>
<title>Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications</title>
<link>https://papers.cool/arxiv/2510.20632</link>
<guid>https://papers.cool/arxiv/2510.20632</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents EcomEval, a multilingual and multimodal benchmark that assesses large language models on 37 e-commerce tasks across six categories, including product guidance, after-sales support, and multimodal queries. It sources authentic customer interactions, employs a semi-automatic pipeline with expert annotators for high-quality reference answers, and covers seven languages (including five low-resource Southeast Asian languages). Difficulty levels are defined via model performance averaging, enabling fine-grained and challenge-oriented evaluation.<br /><strong>Summary (CN):</strong> 本文推出 EcomEval 基准，覆盖六大类、37 项电商任务（含产品指引、售后支持和多模态查询），使用真实客户交互数据，并通过大模型草稿+50+ 专家标注者的半自动流水线生成高质量参考答案。基准覆盖七种语言（含五种低资源东南亚语言），并通过不同规模模型的平均得分设定难度等级，实现细粒度、挑战导向的评估。<br /><strong>Keywords:</strong> e-commerce benchmark, multilingual LLM evaluation, multimodal tasks, low-resource languages, annotation pipeline, EcomEval<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shuyi Xie, Ziqin Liew, Hailing Zhang, Haibo Zhang, Ling Hu, Zhiqiang Zhou, Shuman Liu, Anxiang Zeng</div>
Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet their capabilities in specialized domains remain underexplored. In e-commerce, existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping MMLU-suffer from limited task diversity (e.g., lacking product guidance and after-sales issues), limited task modalities (e.g., absence of multimodal data), synthetic or curated data, and a narrow focus on English and Chinese, leaving practitioners without reliable tools to assess models on complex, real-world shopping scenarios. We introduce EcomEval, a comprehensive multilingual and multimodal benchmark for evaluating LLMs in e-commerce. EcomEval covers six categories and 37 tasks (including 8 multimodal tasks), sourced primarily from authentic customer queries and transaction logs, reflecting the noisy and heterogeneous nature of real business interactions. To ensure both quality and scalability of reference answers, we adopt a semi-automatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over 50 expert annotators with strong e-commerce and multilingual expertise. We define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities, enabling challenge-oriented and fine-grained assessment. EcomEval also spans seven languages-including five low-resource Southeast Asian languages-offering a multilingual perspective absent from prior work.
<div><strong>Authors:</strong> Shuyi Xie, Ziqin Liew, Hailing Zhang, Haibo Zhang, Ling Hu, Zhiqiang Zhou, Shuman Liu, Anxiang Zeng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents EcomEval, a multilingual and multimodal benchmark that assesses large language models on 37 e-commerce tasks across six categories, including product guidance, after-sales support, and multimodal queries. It sources authentic customer interactions, employs a semi-automatic pipeline with expert annotators for high-quality reference answers, and covers seven languages (including five low-resource Southeast Asian languages). Difficulty levels are defined via model performance averaging, enabling fine-grained and challenge-oriented evaluation.", "summary_cn": "本文推出 EcomEval 基准，覆盖六大类、37 项电商任务（含产品指引、售后支持和多模态查询），使用真实客户交互数据，并通过大模型草稿+50+ 专家标注者的半自动流水线生成高质量参考答案。基准覆盖七种语言（含五种低资源东南亚语言），并通过不同规模模型的平均得分设定难度等级，实现细粒度、挑战导向的评估。", "keywords": "e-commerce benchmark, multilingual LLM evaluation, multimodal tasks, low-resource languages, annotation pipeline, EcomEval", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuyi Xie", "Ziqin Liew", "Hailing Zhang", "Haibo Zhang", "Ling Hu", "Zhiqiang Zhou", "Shuman Liu", "Anxiang Zeng"]}
]]></acme>

<pubDate>2025-10-23T15:04:32+00:00</pubDate>
</item>
<item>
<title>Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms</title>
<link>https://papers.cool/arxiv/2510.20621</link>
<guid>https://papers.cool/arxiv/2510.20621</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper formalizes the MIMOSA framework for mining interpretable models that balance interpretability, performance, and ethical properties such as causality, fairness, and privacy across various data modalities. It defines three families of interpretable models—feature importance, rule, and instance-based—and provides formal definitions, metrics, and verification procedures for the ethical properties, analyzing trade‑offs between them. This establishes a theoretical foundation for trustworthy AI systems that are both accurate and ethically sound.<br /><strong>Summary (CN):</strong> 本文形式化了 MIMOSA（Mining Interpretable Models explOiting Sophisticated Algorithms）框架，旨在在保持模型可解释性与性能的同时，嵌入因果性、公平性和隐私等关键伦理属性，适用于表格、时间序列、图像、文本、交易和轨迹等多种数据类型。文中明确了特征重要性、规则和基于实例的三类可解释模型，并给出这些伦理属性的形式化定义、评估指标和验证流程，讨论了它们之间的权衡，奠定了构建可信赖 AI 系统的理论基础。<br /><strong>Keywords:</strong> interpretability, trustworthy AI, ethical AI, fairness, privacy, causality, MIMOSA, rule models, feature importance, instance-based models<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Riccardo Guidotti, Martina Cinquini, Marta Marchiori Manerba, Mattia Setzu, Francesco Spinnato</div>
Interpretable-by-design models are crucial for fostering trust, accountability, and safe adoption of automated decision-making models in real-world applications. In this paper we formalize the ground for the MIMOSA (Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties. We formally define here the supervised learning setting across diverse decision-making tasks and data types, including tabular data, time series, images, text, transactions, and trajectories. We characterize three major families of interpretable models: feature importance, rule, and instance based models. For each family, we analyze their interpretability dimensions, reasoning mechanisms, and complexity. Beyond interpretability, we formalize three critical ethical properties, namely causality, fairness, and privacy, providing formal definitions, evaluation metrics, and verification procedures for each. We then examine the inherent trade-offs between these properties and discuss how privacy requirements, fairness constraints, and causal reasoning can be embedded within interpretable pipelines. By evaluating ethical measures during model generation, this framework establishes the theoretical foundations for developing AI systems that are not only accurate and interpretable but also fair, privacy-preserving, and causally aware, i.e., trustworthy.
<div><strong>Authors:</strong> Riccardo Guidotti, Martina Cinquini, Marta Marchiori Manerba, Mattia Setzu, Francesco Spinnato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper formalizes the MIMOSA framework for mining interpretable models that balance interpretability, performance, and ethical properties such as causality, fairness, and privacy across various data modalities. It defines three families of interpretable models—feature importance, rule, and instance-based—and provides formal definitions, metrics, and verification procedures for the ethical properties, analyzing trade‑offs between them. This establishes a theoretical foundation for trustworthy AI systems that are both accurate and ethically sound.", "summary_cn": "本文形式化了 MIMOSA（Mining Interpretable Models explOiting Sophisticated Algorithms）框架，旨在在保持模型可解释性与性能的同时，嵌入因果性、公平性和隐私等关键伦理属性，适用于表格、时间序列、图像、文本、交易和轨迹等多种数据类型。文中明确了特征重要性、规则和基于实例的三类可解释模型，并给出这些伦理属性的形式化定义、评估指标和验证流程，讨论了它们之间的权衡，奠定了构建可信赖 AI 系统的理论基础。", "keywords": "interpretability, trustworthy AI, ethical AI, fairness, privacy, causality, MIMOSA, rule models, feature importance, instance-based models", "scoring": {"interpretability": 8, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Riccardo Guidotti", "Martina Cinquini", "Marta Marchiori Manerba", "Mattia Setzu", "Francesco Spinnato"]}
]]></acme>

<pubDate>2025-10-23T14:54:33+00:00</pubDate>
</item>
<item>
<title>Efficient Algorithms for Computing Random Walk Centrality</title>
<link>https://papers.cool/arxiv/2510.20604</link>
<guid>https://papers.cool/arxiv/2510.20604</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a new formulation of random walk centrality and introduces two scalable algorithms—one based on approximate Cholesky factorization with sparse inverse estimation and another using rooted spanning‑tree sampling—that run in near‑linear time with provable approximation guarantees. Extensive experiments on real‑world graphs, including a network with over 10 million nodes, demonstrate both efficiency and high approximation quality.<br /><strong>Summary (CN):</strong> 本文提出了随机游走中心性的全新表述，并设计了两种可扩展算法：一种利用近似Cholesky分解结合稀疏逆矩阵估计，另一种基于根生成树采样，两者均在准线性时间内运行并提供强近似保证。大量实验（包括含一千万元素节点的真实网络）表明该方法在效率和近似质量上均表现出色。<br /><strong>Keywords:</strong> random walk centrality, graph mining, approximate Cholesky factorization, sparse inverse estimation, rooted spanning tree sampling, near-linear algorithms, large-scale networks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Changan Liu, Zixuan Xie, Ahad N. Zehmakan, Zhongzhi Zhang</div>
Random walk centrality is a fundamental metric in graph mining for quantifying node importance and influence, defined as the weighted average of hitting times to a node from all other nodes. Despite its ability to capture rich graph structural information and its wide range of applications, computing this measure for large networks remains impractical due to the computational demands of existing methods. In this paper, we present a novel formulation of random walk centrality, underpinning two scalable algorithms: one leveraging approximate Cholesky factorization and sparse inverse estimation, while the other sampling rooted spanning trees. Both algorithms operate in near-linear time and provide strong approximation guarantees. Extensive experiments on large real-world networks, including one with over 10 million nodes, demonstrate the efficiency and approximation quality of the proposed algorithms.
<div><strong>Authors:</strong> Changan Liu, Zixuan Xie, Ahad N. Zehmakan, Zhongzhi Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a new formulation of random walk centrality and introduces two scalable algorithms—one based on approximate Cholesky factorization with sparse inverse estimation and another using rooted spanning‑tree sampling—that run in near‑linear time with provable approximation guarantees. Extensive experiments on real‑world graphs, including a network with over 10 million nodes, demonstrate both efficiency and high approximation quality.", "summary_cn": "本文提出了随机游走中心性的全新表述，并设计了两种可扩展算法：一种利用近似Cholesky分解结合稀疏逆矩阵估计，另一种基于根生成树采样，两者均在准线性时间内运行并提供强近似保证。大量实验（包括含一千万元素节点的真实网络）表明该方法在效率和近似质量上均表现出色。", "keywords": "random walk centrality, graph mining, approximate Cholesky factorization, sparse inverse estimation, rooted spanning tree sampling, near-linear algorithms, large-scale networks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Changan Liu", "Zixuan Xie", "Ahad N. Zehmakan", "Zhongzhi Zhang"]}
]]></acme>

<pubDate>2025-10-23T14:36:38+00:00</pubDate>
</item>
<item>
<title>What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation</title>
<link>https://papers.cool/arxiv/2510.20603</link>
<guid>https://papers.cool/arxiv/2510.20603</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper proposes a new framework, CaSE, for evaluating the quality of reasoning steps in large language models by measuring relevance (groundedness in the problem) and coherence (logical consistency with prior steps). It introduces expert‑annotated benchmarks (MRa‑GSM8K and MRa‑MATH) and shows that using CaSE‑evaluated data to fine‑tune LLMs can improve final answer correctness.<br /><strong>Summary (CN):</strong> 本文提出一种名为 CaSE 的因果步进式评估方法，用于衡量大型语言模型推理步骤的相关性（与问题的关联程度）和连贯性（与前一步的逻辑一致性），并通过专家标注的数据集（MRa‑GSM5​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​）验证了该方法，并展示了基于 CaSE 评价的训练数据可提升模型最终答案的准确率。<br /><strong>Keywords:</strong> reasoning evaluation, relevance, coherence, causal stepwise evaluation, LLM debugging, multi-aspect evaluation<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Heejin Do, Jaehui Hwang, Dongyoon Han, Seong Joon Oh, Sangdoo Yun</div>
Evaluating large language models (LLMs) on final-answer correctness is the dominant paradigm. This approach, however, provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process. We argue that a more granular evaluation of reasoning offers a more effective path to building robust models. We decompose reasoning quality into two dimensions: relevance and coherence. Relevance measures if a step is grounded in the problem; coherence measures if it follows logically from prior steps. To measure these aspects reliably, we introduce causal stepwise evaluation (CaSE). This method assesses each reasoning step using only its preceding context, which avoids hindsight bias. We validate CaSE against human judgments on our new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we show that curating training data with CaSE-evaluated relevance and coherence directly improves final task performance. Our work provides a scalable framework for analyzing, debugging, and improving LLM reasoning, demonstrating the practical value of moving beyond validity checks.
<div><strong>Authors:</strong> Heejin Do, Jaehui Hwang, Dongyoon Han, Seong Joon Oh, Sangdoo Yun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper proposes a new framework, CaSE, for evaluating the quality of reasoning steps in large language models by measuring relevance (groundedness in the problem) and coherence (logical consistency with prior steps). It introduces expert‑annotated benchmarks (MRa‑GSM8K and MRa‑MATH) and shows that using CaSE‑evaluated data to fine‑tune LLMs can improve final answer correctness.", "summary_cn": "本文提出一种名为 CaSE 的因果步进式评估方法，用于衡量大型语言模型推理步骤的相关性（与问题的关联程度）和连贯性（与前一步的逻辑一致性），并通过专家标注的数据集（MRa‑GSM5​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​）验证了该方法，并展示了基于 CaSE 评价的训练数据可提升模型最终答案的准确率。", "keywords": "reasoning evaluation, relevance, coherence, causal stepwise evaluation, LLM debugging, multi-aspect evaluation", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Heejin Do", "Jaehui Hwang", "Dongyoon Han", "Seong Joon Oh", "Sangdoo Yun"]}
]]></acme>

<pubDate>2025-10-23T14:30:37+00:00</pubDate>
</item>
<item>
<title>Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting</title>
<link>https://papers.cool/arxiv/2510.20591</link>
<guid>https://papers.cool/arxiv/2510.20591</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper formulates transmission network topology optimization via busbar splitting as a mixed-integer non-linear problem and proposes a heterogeneous edge-aware graph neural network to predict effective splitting actions. The GNN demonstrates strong generalization to unseen topology changes and cross-system transferability, achieving up to four orders of magnitude speed-up and a 2.3% optimality gap on a 2000-bus test case. This enables near-real-time congestion management for large-scale power grids.<br /><strong>Summary (CN):</strong> 本文将通过母线分裂进行输电网络拓扑优化建模为混合整数非线性问题，并提出一种异构边感知图神经网络用于预测有效的分裂动作。实验表明该 GNN 对未见拓扑变化及跨系统具备强泛化能力，在 2000 母线案例上实现约四个数量级的加速且最优差距仅 2.3%。从而支持大规模电网的近实时拥塞管理。<br /><strong>Keywords:</strong> graph neural network, busbar splitting, transmission congestion, topology optimization, power grid, linearized AC power flow, transferability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Ali Rajaei, Peter Palensky, Jochen L. Cremer</div>
Network topology optimization (NTO) via busbar splitting can mitigate transmission grid congestion and reduce redispatch costs. However, solving this mixed-integer non-linear problem for large-scale systems in near-real-time is currently intractable with existing solvers. Machine learning (ML) approaches have emerged as a promising alternative, but they have limited generalization to unseen topologies, varying operating conditions, and different systems, which limits their practical applicability. This paper formulates NTO for congestion management problem considering linearized AC PF, and proposes a graph neural network (GNN)-accelerated approach. We develop a heterogeneous edge-aware message passing NN to predict effective busbar splitting actions as candidate NTO solutions. The proposed GNN captures local flow patterns, achieves generalization to unseen topology changes, and improves transferability across systems. Case studies show up to 4 orders-of-magnitude speed-up, delivering AC-feasible solutions within one minute and a 2.3% optimality gap on the GOC 2000-bus system. These results demonstrate a significant step toward near-real-time NTO for large-scale systems with topology and cross-system generalization.
<div><strong>Authors:</strong> Ali Rajaei, Peter Palensky, Jochen L. Cremer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper formulates transmission network topology optimization via busbar splitting as a mixed-integer non-linear problem and proposes a heterogeneous edge-aware graph neural network to predict effective splitting actions. The GNN demonstrates strong generalization to unseen topology changes and cross-system transferability, achieving up to four orders of magnitude speed-up and a 2.3% optimality gap on a 2000-bus test case. This enables near-real-time congestion management for large-scale power grids.", "summary_cn": "本文将通过母线分裂进行输电网络拓扑优化建模为混合整数非线性问题，并提出一种异构边感知图神经网络用于预测有效的分裂动作。实验表明该 GNN 对未见拓扑变化及跨系统具备强泛化能力，在 2000 母线案例上实现约四个数量级的加速且最优差距仅 2.3%。从而支持大规模电网的近实时拥塞管理。", "keywords": "graph neural network, busbar splitting, transmission congestion, topology optimization, power grid, linearized AC power flow, transferability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Ali Rajaei", "Peter Palensky", "Jochen L. Cremer"]}
]]></acme>

<pubDate>2025-10-23T14:16:23+00:00</pubDate>
</item>
<item>
<title>Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI</title>
<link>https://papers.cool/arxiv/2510.20568</link>
<guid>https://papers.cool/arxiv/2510.20568</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper examines citizen comment processes on AI risks in Australia, Colombia, and the United States, finding that fewer than one percent of the population participated and that policymakers made little effort to incorporate or respond to public feedback. It highlights a persistent gap between the promise of participatory AI governance and actual practice, and offers eight recommendations for improving outreach, literacy, and feedback loops. The study argues that current approaches are unlikely to build trust or legitimacy in AI governance because policymakers are not adequately listening to citizen concerns.<br /><strong>Summary (CN):</strong> 该研究比较了澳大利亚、哥伦比亚和美国在 AI 风险治理方面的公众意见征集，发现参与率低于 1%，且政府对公众反馈的采纳和回应极为有限。文章指出参与式 AI 治理的承诺与实践之间存在显著差距，并提供了八项改进建议，包括提升 AI 素养、扩大宣传、加强反馈机制等。研究认为，政策制定者未能充分倾听和回应公民关切，导致难以建立对 AI 治理的信任与合法性。<br /><strong>Keywords:</strong> participatory AI governance, citizen comment, public feedback, AI policy, trust, AI literacy, outreach, governance gap<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 4, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Susan Ariel Aaronson, Michael Moreno</div>
The worlds people have strong opinions about artificial intelligence (AI), and they want policymakers to listen. Governments are inviting public comment on AI, but as they translate input into policy, much of what citizens say is lost. Policymakers are missing a critical opportunity to build trust in AI and its governance. This paper compares three countries, Australia, Colombia, and the United States, that invited citizens to comment on AI risks and policies. Using a landscape analysis, the authors examined how each government solicited feedback and whether that input shaped governance. Yet in none of the three cases did citizens and policymakers establish a meaningful dialogue. Governments did little to attract diverse voices or publicize calls for comment, leaving most citizens unaware or unprepared to respond. In each nation, fewer than one percent of the population participated. Moreover, officials showed limited responsiveness to the feedback they received, failing to create an effective feedback loop. The study finds a persistent gap between the promise and practice of participatory AI governance. The authors conclude that current approaches are unlikely to build trust or legitimacy in AI because policymakers are not adequately listening or responding to public concerns. They offer eight recommendations: promote AI literacy; monitor public feedback; broaden outreach; hold regular online forums; use innovative engagement methods; include underrepresented groups; respond publicly to input; and make participation easier.
<div><strong>Authors:</strong> Susan Ariel Aaronson, Michael Moreno</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper examines citizen comment processes on AI risks in Australia, Colombia, and the United States, finding that fewer than one percent of the population participated and that policymakers made little effort to incorporate or respond to public feedback. It highlights a persistent gap between the promise of participatory AI governance and actual practice, and offers eight recommendations for improving outreach, literacy, and feedback loops. The study argues that current approaches are unlikely to build trust or legitimacy in AI governance because policymakers are not adequately listening to citizen concerns.", "summary_cn": "该研究比较了澳大利亚、哥伦比亚和美国在 AI 风险治理方面的公众意见征集，发现参与率低于 1%，且政府对公众反馈的采纳和回应极为有限。文章指出参与式 AI 治理的承诺与实践之间存在显著差距，并提供了八项改进建议，包括提升 AI 素养、扩大宣传、加强反馈机制等。研究认为，政策制定者未能充分倾听和回应公民关切，导致难以建立对 AI 治理的信任与合法性。", "keywords": "participatory AI governance, citizen comment, public feedback, AI policy, trust, AI literacy, outreach, governance gap", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 4, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Susan Ariel Aaronson", "Michael Moreno"]}
]]></acme>

<pubDate>2025-10-23T13:57:02+00:00</pubDate>
</item>
<item>
<title>FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic</title>
<link>https://papers.cool/arxiv/2510.20467</link>
<guid>https://papers.cool/arxiv/2510.20467</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FLORA is an unsupervised method for aligning entities and relations across two knowledge graphs using fuzzy logic, which yields interpretable iterative alignment and provable convergence while allowing dangling entities. The approach does not require training data and achieves state-of-the-art results on major benchmarks. It emphasizes holistic KG alignment rather than pure embedding similarity.<br /><strong>Summary (CN):</strong> FLORA 是一种基于 (fuzzy logic) 模糊逻辑的无监督知识图谱对齐方法，能够在实体和关系层面提供可解释的迭代对齐，并且具备收敛性证明，同时允许存在无对应实体的悬挂实体。该方法无需训练数据，在主要基准上实现了最先进的对齐性能。<br /><strong>Keywords:</strong> knowledge graph alignment, unsupervised, fuzzy logic, interpretability, KG matching, iterative alignment, dangling entities, convergence, holistic alignment, state-of-the-art<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yiwen Peng, Thomas Bonald, Fabian M. Suchanek</div>
Knowledge graph alignment is the task of matching equivalent entities (that is, instances and classes) and relations across two knowledge graphs. Most existing methods focus on pure entity-level alignment, computing the similarity of entities in some embedding space. They lack interpretable reasoning and need training data to work. In this paper, we propose FLORA, a simple yet effective method that (1) is unsupervised, i.e., does not require training data, (2) provides a holistic alignment for entities and relations iteratively, (3) is based on fuzzy logic and thus delivers interpretable results, (4) provably converges, (5) allows dangling entities, i.e., entities without a counterpart in the other KG, and (6) achieves state-of-the-art results on major benchmarks.
<div><strong>Authors:</strong> Yiwen Peng, Thomas Bonald, Fabian M. Suchanek</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FLORA is an unsupervised method for aligning entities and relations across two knowledge graphs using fuzzy logic, which yields interpretable iterative alignment and provable convergence while allowing dangling entities. The approach does not require training data and achieves state-of-the-art results on major benchmarks. It emphasizes holistic KG alignment rather than pure embedding similarity.", "summary_cn": "FLORA 是一种基于 (fuzzy logic) 模糊逻辑的无监督知识图谱对齐方法，能够在实体和关系层面提供可解释的迭代对齐，并且具备收敛性证明，同时允许存在无对应实体的悬挂实体。该方法无需训练数据，在主要基准上实现了最先进的对齐性能。", "keywords": "knowledge graph alignment, unsupervised, fuzzy logic, interpretability, KG matching, iterative alignment, dangling entities, convergence, holistic alignment, state-of-the-art", "scoring": {"interpretability": 6, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yiwen Peng", "Thomas Bonald", "Fabian M. Suchanek"]}
]]></acme>

<pubDate>2025-10-23T12:05:31+00:00</pubDate>
</item>
<item>
<title>Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$</title>
<link>https://papers.cool/arxiv/2510.20457</link>
<guid>https://papers.cool/arxiv/2510.20457</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces EBR, a neural reasoner that uses embeddings to approximate symbolic reasoning in the description logic SHIQ, requiring only instance retrieval for atomic concepts and existential restrictions. Experiments show that EBR is robust to missing and erroneous data, outperforming state-of-the-art reasoners in consistency handling. This approach enables deployment of neuro-symbolic concept learning on real-world knowledge bases despite data imperfections.<br /><strong>Summary (CN):</strong> 本文提出 EBR 神经推理器，通过嵌入近似 SHIQ 描述逻辑的符号推理，仅需检索原子概念和存在限制的实例即可获取任意概念的实例集合。实验表明，EBR 在缺失和错误数据场景下表现出对一致性问题的鲁棒性，优于现有推理器，为在真实知识库中部署神经符号概念学习提供了可行方案。<br /><strong>Keywords:</strong> neural reasoner, description logic, SHIQ, neuro-symbolic, robust reasoning, embeddings, instance retrieval, consistency robustness<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Louis Mozart Kamdem Teyou, Luke Friedrichs, N'Dah Jean Kouagou, Caglar Demir, Yasir Mahmood, Stefan Heindorf, Axel-Cyrille Ngonga Ngomo</div>
Concept learning exploits background knowledge in the form of description logic axioms to learn explainable classification models from knowledge bases. Despite recent breakthroughs in neuro-symbolic concept learning, most approaches still cannot be deployed on real-world knowledge bases. This is due to their use of description logic reasoners, which are not robust against inconsistencies nor erroneous data. We address this challenge by presenting a novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to approximate the results of a symbolic reasoner. We show that EBR solely requires retrieving instances for atomic concepts and existential restrictions to retrieve or approximate the set of instances of any concept in the description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with state-of-the-art reasoners. Our results suggest that EBR is robust against missing and erroneous data in contrast to existing reasoners.
<div><strong>Authors:</strong> Louis Mozart Kamdem Teyou, Luke Friedrichs, N'Dah Jean Kouagou, Caglar Demir, Yasir Mahmood, Stefan Heindorf, Axel-Cyrille Ngonga Ngomo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces EBR, a neural reasoner that uses embeddings to approximate symbolic reasoning in the description logic SHIQ, requiring only instance retrieval for atomic concepts and existential restrictions. Experiments show that EBR is robust to missing and erroneous data, outperforming state-of-the-art reasoners in consistency handling. This approach enables deployment of neuro-symbolic concept learning on real-world knowledge bases despite data imperfections.", "summary_cn": "本文提出 EBR 神经推理器，通过嵌入近似 SHIQ 描述逻辑的符号推理，仅需检索原子概念和存在限制的实例即可获取任意概念的实例集合。实验表明，EBR 在缺失和错误数据场景下表现出对一致性问题的鲁棒性，优于现有推理器，为在真实知识库中部署神经符号概念学习提供了可行方案。", "keywords": "neural reasoner, description logic, SHIQ, neuro-symbolic, robust reasoning, embeddings, instance retrieval, consistency robustness", "scoring": {"interpretability": 5, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Louis Mozart Kamdem Teyou", "Luke Friedrichs", "N'Dah Jean Kouagou", "Caglar Demir", "Yasir Mahmood", "Stefan Heindorf", "Axel-Cyrille Ngonga Ngomo"]}
]]></acme>

<pubDate>2025-10-23T11:48:43+00:00</pubDate>
</item>
<item>
<title>A computational model and tool for generating more novel opportunities in professional innovation processes</title>
<link>https://papers.cool/arxiv/2510.20402</link>
<guid>https://papers.cool/arxiv/2510.20402</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a computational model of creative outcomes, grounded in creativity theories, which implements five functions to generate innovation opportunities with higher novelty without sacrificing usefulness. The model is applied to an innovation project in the hospitality sector and evaluated against Notebook LM and ChatGPT4o, showing that it produces more novel and/or useful results, though not all functions equally contribute to novelty. The findings suggest directions for further development of the model.<br /><strong>Summary (CN):</strong> 本文提出了一种基于创意理论的计算创意模型，包含五个功能，用于在保持实用性的前提下生成更具新颖性的创新机会。该模型在酒店业创新项目中进行实验，对比 Notebook LM 和 ChatGPT4o，结果显示模型产生的机会在新颖性和/或实用性上更优，但并非所有功能都对新颖性提升有贡献，指出了后续改进方向。<br /><strong>Keywords:</strong> computational creativity, innovation opportunities, novelty, usefulness, creative model, hospitality sector, AI generation, evaluation, generative tools<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Neil Maiden, Konstantinos Zachos, James Lockerbie, Kostas Petrianakis, Amanda Brown</div>
This paper presents a new computational model of creative outcomes, informed by creativity theories and techniques, which was implemented to generate more novel opportunities for innovation projects. The model implemented five functions that were developed to contribute to the generation of innovation opportunities with higher novelty without loss of usefulness. The model was evaluated using opportunities generated for an innovation project in the hospitality sector. The evaluation revealed that the computational model generated outcomes that were more novel and/or useful than outcomes from Notebook LM and ChatGPT4o. However, not all model functions contributed to the generation of more novel opportunities, leading to new directions for further model development
<div><strong>Authors:</strong> Neil Maiden, Konstantinos Zachos, James Lockerbie, Kostas Petrianakis, Amanda Brown</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a computational model of creative outcomes, grounded in creativity theories, which implements five functions to generate innovation opportunities with higher novelty without sacrificing usefulness. The model is applied to an innovation project in the hospitality sector and evaluated against Notebook LM and ChatGPT4o, showing that it produces more novel and/or useful results, though not all functions equally contribute to novelty. The findings suggest directions for further development of the model.", "summary_cn": "本文提出了一种基于创意理论的计算创意模型，包含五个功能，用于在保持实用性的前提下生成更具新颖性的创新机会。该模型在酒店业创新项目中进行实验，对比 Notebook LM 和 ChatGPT4o，结果显示模型产生的机会在新颖性和/或实用性上更优，但并非所有功能都对新颖性提升有贡献，指出了后续改进方向。", "keywords": "computational creativity, innovation opportunities, novelty, usefulness, creative model, hospitality sector, AI generation, evaluation, generative tools", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Neil Maiden", "Konstantinos Zachos", "James Lockerbie", "Kostas Petrianakis", "Amanda Brown"]}
]]></acme>

<pubDate>2025-10-23T10:09:57+00:00</pubDate>
</item>
<item>
<title>IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation</title>
<link>https://papers.cool/arxiv/2510.20377</link>
<guid>https://papers.cool/arxiv/2510.20377</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces IKnow, a framework for instruction-knowledge-aware continual pretraining that adapts instruction-tuned language models to new domains using only unlabeled test-time data. By formulating self-supervised objectives in an instruction-response dialogue format and exploiting domain knowledge embedded in the text, IKnow aims to preserve instruction-following ability and improve semantic representations without relying on external resources or the original base model.<br /><strong>Summary (CN):</strong> 本文提出 IKnow 框架，用于在仅有未标注的测试时数据的情况下，对指令调优的大语言模型进行指令‑知识感知的持续预训练，实现新领域的适配。该方法将自监督目标设定为指令‑回应对话形式，并利用文本中蕴含的领域知识进行更深层语义编码，从而在无需外部资源或原始基模型的情况下，保持模型的指令遵循能力并提升语义表征。<br /><strong>Keywords:</strong> continual pretraining, domain adaptation, instruction-tuned LLM, self-supervised dialogue, knowledge-aware, semantic encoding<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Tianyi Zhang, Florian Mai, Lucie Flek</div>
Continual pretraining promises to adapt large language models (LLMs) to new domains using only unlabeled test-time data, but naively applying standard self-supervised objectives to instruction-tuned models is known to degrade their instruction-following capability and semantic representations. Existing fixes assume access to the original base model or rely on knowledge from an external domain-specific database - both of which pose a realistic barrier in settings where the base model weights are withheld for safety reasons or reliable external corpora are unavailable. In this work, we propose Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general framework that formulates novel self-supervised objectives in the instruction-response dialogue format. Rather than depend- ing on external resources, IKnow leverages domain knowledge embedded within the text itself and learns to encode it at a deeper semantic level.
<div><strong>Authors:</strong> Tianyi Zhang, Florian Mai, Lucie Flek</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces IKnow, a framework for instruction-knowledge-aware continual pretraining that adapts instruction-tuned language models to new domains using only unlabeled test-time data. By formulating self-supervised objectives in an instruction-response dialogue format and exploiting domain knowledge embedded in the text, IKnow aims to preserve instruction-following ability and improve semantic representations without relying on external resources or the original base model.", "summary_cn": "本文提出 IKnow 框架，用于在仅有未标注的测试时数据的情况下，对指令调优的大语言模型进行指令‑知识感知的持续预训练，实现新领域的适配。该方法将自监督目标设定为指令‑回应对话形式，并利用文本中蕴含的领域知识进行更深层语义编码，从而在无需外部资源或原始基模型的情况下，保持模型的指令遵循能力并提升语义表征。", "keywords": "continual pretraining, domain adaptation, instruction-tuned LLM, self-supervised dialogue, knowledge-aware, semantic encoding", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Tianyi Zhang", "Florian Mai", "Lucie Flek"]}
]]></acme>

<pubDate>2025-10-23T09:21:13+00:00</pubDate>
</item>
<item>
<title>LLM-empowered knowledge graph construction: A survey</title>
<link>https://papers.cool/arxiv/2510.20345</link>
<guid>https://papers.cool/arxiv/2510.20345</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This survey reviews how Large Language Models (LLMs) are reshaping knowledge graph (KG) construction, covering the three classic stages—ontology engineering, knowledge extraction, and knowledge fusion. It contrasts schema-based approaches that prioritize structure and consistency with schema-free methods that emphasize flexibility and open discovery, and highlights future directions such as KG-based reasoning for LLMs and multimodal KG construction.<br /><strong>Summary (CN):</strong> 本文综述了大语言模型（LLM）对知识图谱（KG）构建的影响，重点分析了本体工程、知识抽取和知识融合三个阶段。文章对比了强调结构一致性的模式化方法与强调灵活性和开放发现的无模式方法，并指出了未来研究趋势，包括基于 KG 的 LLM 推理、动态知识记忆以及多模态 KG 构建。<br /><strong>Keywords:</strong> knowledge graph, large language model, schema-based, schema-free, ontology engineering, knowledge extraction, knowledge fusion<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 5, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haonan Bian</div>
Knowledge Graphs (KGs) have long served as a fundamental infrastructure for structured knowledge representation and reasoning. With the advent of Large Language Models (LLMs), the construction of KGs has entered a new paradigm-shifting from rule-based and statistical pipelines to language-driven and generative frameworks. This survey provides a comprehensive overview of recent progress in LLM-empowered knowledge graph construction, systematically analyzing how LLMs reshape the classical three-layered pipeline of ontology engineering, knowledge extraction, and knowledge fusion. We first revisit traditional KG methodologies to establish conceptual foundations, and then review emerging LLM-driven approaches from two complementary perspectives: schema-based paradigms, which emphasize structure, normalization, and consistency; and schema-free paradigms, which highlight flexibility, adaptability, and open discovery. Across each stage, we synthesize representative frameworks, analyze their technical mechanisms, and identify their limitations. Finally, the survey outlines key trends and future research directions, including KG-based reasoning for LLMs, dynamic knowledge memory for agentic systems, and multimodal KG construction. Through this systematic review, we aim to clarify the evolving interplay between LLMs and knowledge graphs, bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive, explainable, and intelligent knowledge systems.
<div><strong>Authors:</strong> Haonan Bian</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This survey reviews how Large Language Models (LLMs) are reshaping knowledge graph (KG) construction, covering the three classic stages—ontology engineering, knowledge extraction, and knowledge fusion. It contrasts schema-based approaches that prioritize structure and consistency with schema-free methods that emphasize flexibility and open discovery, and highlights future directions such as KG-based reasoning for LLMs and multimodal KG construction.", "summary_cn": "本文综述了大语言模型（LLM）对知识图谱（KG）构建的影响，重点分析了本体工程、知识抽取和知识融合三个阶段。文章对比了强调结构一致性的模式化方法与强调灵活性和开放发现的无模式方法，并指出了未来研究趋势，包括基于 KG 的 LLM 推理、动态知识记忆以及多模态 KG 构建。", "keywords": "knowledge graph, large language model, schema-based, schema-free, ontology engineering, knowledge extraction, knowledge fusion", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haonan Bian"]}
]]></acme>

<pubDate>2025-10-23T08:43:28+00:00</pubDate>
</item>
<item>
<title>Collateral Damage Assessment Model for AI System Target Engagement in Military Operations</title>
<link>https://papers.cool/arxiv/2510.20337</link>
<guid>https://papers.cool/arxiv/2510.20337</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a novel collateral damage assessment model for targeting AI systems in military operations, integrating temporal, spatial, and force dimensions within a layered Knowledge Representation and Reasoning (KRR) architecture. It captures system categories, engagement vectors, and contextual aspects while evaluating spreading, severity, likelihood, and related metrics, and demonstrates the model through instantiation to support responsible and trustworthy AI weapon engagement.<br /><strong>Summary (CN):</strong> 本文提出了一种用于军事实战中 AI 系统目标交战的伤害评估模型，基于层次化的知识表示与推理（KRR）架构，将时间、空间和力量维度统一建模。模型捕获 AI 系统类别、交战向量及情境因素，并评估扩散、严重性、概率等指标，随后通过实例化展示以促进负责任且可信的 AI 武器使用。<br /><strong>Keywords:</strong> collateral damage assessment, AI weapon targeting, knowledge representation, reasoning, military AI, responsible AI, design science, temporal-spatial-force dimensions<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Clara Maathuis, Kasper Cools</div>
In an era where AI (Artificial Intelligence) systems play an increasing role in the battlefield, ensuring responsible targeting demands rigorous assessment of potential collateral effects. In this context, a novel collateral damage assessment model for target engagement of AI systems in military operations is introduced. The model integrates temporal, spatial, and force dimensions within a unified Knowledge Representation and Reasoning (KRR) architecture following a design science methodological approach. Its layered structure captures the categories and architectural components of the AI systems to be engaged together with corresponding engaging vectors and contextual aspects. At the same time, spreading, severity, likelihood, and evaluation metrics are considered in order to provide a clear representation enhanced by transparent reasoning mechanisms. Further, the model is demonstrated and evaluated through instantiation which serves as a basis for further dedicated efforts that aim at building responsible and trustworthy intelligent systems for assessing the effects produced by engaging AI systems in military operations.
<div><strong>Authors:</strong> Clara Maathuis, Kasper Cools</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a novel collateral damage assessment model for targeting AI systems in military operations, integrating temporal, spatial, and force dimensions within a layered Knowledge Representation and Reasoning (KRR) architecture. It captures system categories, engagement vectors, and contextual aspects while evaluating spreading, severity, likelihood, and related metrics, and demonstrates the model through instantiation to support responsible and trustworthy AI weapon engagement.", "summary_cn": "本文提出了一种用于军事实战中 AI 系统目标交战的伤害评估模型，基于层次化的知识表示与推理（KRR）架构，将时间、空间和力量维度统一建模。模型捕获 AI 系统类别、交战向量及情境因素，并评估扩散、严重性、概率等指标，随后通过实例化展示以促进负责任且可信的 AI 武器使用。", "keywords": "collateral damage assessment, AI weapon targeting, knowledge representation, reasoning, military AI, responsible AI, design science, temporal-spatial-force dimensions", "scoring": {"interpretability": 3, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Clara Maathuis", "Kasper Cools"]}
]]></acme>

<pubDate>2025-10-23T08:36:04+00:00</pubDate>
</item>
<item>
<title>Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems</title>
<link>https://papers.cool/arxiv/2510.20332</link>
<guid>https://papers.cool/arxiv/2510.20332</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how biased data collection practices in AI healthcare systems lead to various types of bias (historical, representation, measurement) across variables such as sex, gender, age, socioeconomic status, and equipment, drawing on the AI4HealthyAging project in Spain. It then provides practical recommendations to improve fairness and robustness in clinical problem design and data collection for AI systems.<br /><strong>Summary (CN):</strong> 本文研究了 AI 医疗系统中数据收集实践导致的偏差，包括历史偏差、代表性偏差和测量偏差，涉及性别、年龄、社会经济地位、设备等变量，并基于西班牙 AI4HealthyAging 项目提供了提升公平性和鲁棒性的实践建议。<br /><strong>Keywords:</strong> AI healthcare, data bias, fairness, clinical data collection, measurement bias, representation bias, historical bias, robustness, AI4HealthyAging<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Anna Arias-Duart, Maria Eugenia Cardello, Atia Cortés</div>
Artificial intelligence (AI) holds great promise for transforming healthcare. However, despite significant advances, the integration of AI solutions into real-world clinical practice remains limited. A major barrier is the quality and fairness of training data, which is often compromised by biased data collection practices. This paper draws on insights from the AI4HealthyAging project, part of Spain's national R&amp;D initiative, where our task was to detect biases during clinical data collection. We identify several types of bias across multiple use cases, including historical, representation, and measurement biases. These biases manifest in variables such as sex, gender, age, habitat, socioeconomic status, equipment, and labeling. We conclude with practical recommendations for improving the fairness and robustness of clinical problem design and data collection. We hope that our findings and experience contribute to guiding future projects in the development of fairer AI systems in healthcare.
<div><strong>Authors:</strong> Anna Arias-Duart, Maria Eugenia Cardello, Atia Cortés</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how biased data collection practices in AI healthcare systems lead to various types of bias (historical, representation, measurement) across variables such as sex, gender, age, socioeconomic status, and equipment, drawing on the AI4HealthyAging project in Spain. It then provides practical recommendations to improve fairness and robustness in clinical problem design and data collection for AI systems.", "summary_cn": "本文研究了 AI 医疗系统中数据收集实践导致的偏差，包括历史偏差、代表性偏差和测量偏差，涉及性别、年龄、社会经济地位、设备等变量，并基于西班牙 AI4HealthyAging 项目提供了提升公平性和鲁棒性的实践建议。", "keywords": "AI healthcare, data bias, fairness, clinical data collection, measurement bias, representation bias, historical bias, robustness, AI4HealthyAging", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Anna Arias-Duart", "Maria Eugenia Cardello", "Atia Cortés"]}
]]></acme>

<pubDate>2025-10-23T08:32:34+00:00</pubDate>
</item>
<item>
<title>Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation</title>
<link>https://papers.cool/arxiv/2510.20310</link>
<guid>https://papers.cool/arxiv/2510.20310</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents ToolEQA, an embodied‑question‑answering agent that uses external tools and multi‑step reasoning to guide exploration in 3D environments, producing more accurate answers with less travel. A new automated pipeline generates a large‑scale EQA‑RT dataset with reasoning trajectories, and experiments show notable gains over prior baselines on several benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 ToolEQA，利用外部工具和多步推理在 3D 环境中引导探索，从而在更短的路径下获得更准确的问答结果。作者构建了自动化的数据生成流水线，创建了含推理轨迹的 EQA‑RT 数据集，并在多项基准上显著超越现有方法。<br /><strong>Keywords:</strong> embodied question answering, multi-step reasoning, tool augmentation, large-scale dataset, exploration efficiency<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Mingliang Zhai, Hansheng Liang, Xiaomeng Fan, Zhi Gao, Chuanhao Li, Che Sun, Xu Bin, Yuwei Wu, Yunde Jia</div>
Embodied Question Answering (EQA) requires agents to explore 3D environments to obtain observations and answer questions related to the scene. Existing methods leverage VLMs to directly explore the environment and answer questions without explicit thinking or planning, which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses. In this paper, we introduce ToolEQA, an agent that integrates external tools with multi-step reasoning, where external tools can provide more useful information for completing the task, helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information. This enables ToolEQA to generate more accurate responses with a shorter exploration distance. To enhance the model's ability for tool-usage and multi-step reasoning, we further design a novel EQA data generation pipeline that automatically constructs large-scale EQA tasks with reasoning trajectories and corresponding answers. Based on the pipeline, we collect the EQA-RT dataset that contains about 18K tasks, divided into a training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping with the training set) and EQA-RT-Unseen (novel scenes). Experiments on EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by 9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot ToolEQA by 10% in success rate. In addition, ToolEQA also achieves state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench datasets, demonstrating its generality. Our homepage see https://tooleqa.github.io.
<div><strong>Authors:</strong> Mingliang Zhai, Hansheng Liang, Xiaomeng Fan, Zhi Gao, Chuanhao Li, Che Sun, Xu Bin, Yuwei Wu, Yunde Jia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents ToolEQA, an embodied‑question‑answering agent that uses external tools and multi‑step reasoning to guide exploration in 3D environments, producing more accurate answers with less travel. A new automated pipeline generates a large‑scale EQA‑RT dataset with reasoning trajectories, and experiments show notable gains over prior baselines on several benchmarks.", "summary_cn": "本文提出 ToolEQA，利用外部工具和多步推理在 3D 环境中引导探索，从而在更短的路径下获得更准确的问答结果。作者构建了自动化的数据生成流水线，创建了含推理轨迹的 EQA‑RT 数据集，并在多项基准上显著超越现有方法。", "keywords": "embodied question answering, multi-step reasoning, tool augmentation, large-scale dataset, exploration efficiency", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Mingliang Zhai", "Hansheng Liang", "Xiaomeng Fan", "Zhi Gao", "Chuanhao Li", "Che Sun", "Xu Bin", "Yuwei Wu", "Yunde Jia"]}
]]></acme>

<pubDate>2025-10-23T08:02:08+00:00</pubDate>
</item>
<item>
<title>Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction</title>
<link>https://papers.cool/arxiv/2510.20275</link>
<guid>https://papers.cool/arxiv/2510.20275</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces STaBERT, a BERT-based model that incorporates point-of-interest (POI) embeddings and derived temporal descriptors to create semantically enriched representations for human mobility forecasting. By integrating POI and temporal information at each location, STaBERT significantly improves GEO-BLEU scores for both single-city and multi-city prediction tasks.<br /><strong>Summary (CN):</strong> 本文提出 STaBERT，一种基于 BERT 的模型，通过加入 POI（兴趣点）嵌入和时间描述子，在每个位置构建语义丰富的表示，从而提升人类移动预测的准确性。实验表明，STaBERT 在单城和多城预测任务中的 GEO-BLEU 分数均有显著提升。<br /><strong>Keywords:</strong> human mobility, BERT, POI embeddings, temporal descriptors, forecasting, STaBERT, GEO-BLEU<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yunzhi Liu, Haokai Tan, Rushi Kanjaria, Lihuan Li, Flora D. Salim</div>
Human mobility forecasting is crucial for disaster relief, city planning, and public health. However, existing models either only model location sequences or include time information merely as auxiliary input, thereby failing to leverage the rich semantic context provided by points of interest (POIs). To address this, we enrich a BERT-based mobility model with derived temporal descriptors and POI embeddings to better capture the semantics underlying human movement. We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI and temporal information at each location to construct a unified, semantically enriched representation of mobility. Experimental results show that STaBERT significantly improves prediction accuracy: for single-city prediction, the GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34 to 0.56.
<div><strong>Authors:</strong> Yunzhi Liu, Haokai Tan, Rushi Kanjaria, Lihuan Li, Flora D. Salim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces STaBERT, a BERT-based model that incorporates point-of-interest (POI) embeddings and derived temporal descriptors to create semantically enriched representations for human mobility forecasting. By integrating POI and temporal information at each location, STaBERT significantly improves GEO-BLEU scores for both single-city and multi-city prediction tasks.", "summary_cn": "本文提出 STaBERT，一种基于 BERT 的模型，通过加入 POI（兴趣点）嵌入和时间描述子，在每个位置构建语义丰富的表示，从而提升人类移动预测的准确性。实验表明，STaBERT 在单城和多城预测任务中的 GEO-BLEU 分数均有显著提升。", "keywords": "human mobility, BERT, POI embeddings, temporal descriptors, forecasting, STaBERT, GEO-BLEU", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yunzhi Liu", "Haokai Tan", "Rushi Kanjaria", "Lihuan Li", "Flora D. Salim"]}
]]></acme>

<pubDate>2025-10-23T06:59:58+00:00</pubDate>
</item>
<item>
<title>Using Large Language Models for Abstraction of Planning Domains - Extended Version</title>
<link>https://papers.cool/arxiv/2510.20258</link>
<guid>https://papers.cool/arxiv/2510.20258</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates using in‑context learning with large language models, particularly GPT‑4o, to generate abstract planning domains and problem instances in PDDL based on natural‑language abstraction objectives. It evaluates three abstraction categories—alternative actions, action sequences, and parameter abstractions—using symbolic validation tools and human expert assessment, finding that LLMs can produce useful abstractions in simple settings, especially for action‑level abstraction.<br /><strong>Summary (CN):</strong> 本文研究了利用大语言模型（尤其是 GPT‑4o）进行上下文学习，以自然语言指定的抽象目标生成抽象的 PDDL 规划域和问题实例。评估了三类抽象：替代具体动作、动作序列以及参数抽象，并通过符号验证工具和人工专家审查进行检验，结果表明在简单情境下模型能够有效生成抽象，尤其擅长动作层面的抽象。<br /><strong>Keywords:</strong> large language models, planning domain abstraction, PDDL, in-context learning, GPT-4o, symbolic validation, abstract planning, AI planning<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Bita Banihashemi, Megh Patel, Yves Lespérance</div>
Generating an abstraction of a dynamic domain that aligns with a given purpose remains a significant challenge given that the choice of such an abstraction can impact an agent's ability to plan, reason, and provide explanations effectively. We model the agent's concrete behaviors in PDDL and investigate the use of in-context learning with large language models (LLMs) for the generation of abstract PDDL domains and problem instances, given an abstraction objective specified in natural language. The benchmark examples we use are new and have not been part of the data any LLMs have been trained on. We consider three categories of abstractions: abstraction of choice of alternative concrete actions, abstraction of sequences of concrete actions, and abstraction of action/predicate parameters, as well as combinations of these. The generated abstract PDDL domains and problem instances are then checked by symbolic validation tools as well as human experts. Our experiments show that GPT-4o can generally synthesize useful planning domain abstractions in simple settings, although it is better at abstracting over actions than over the associated fluents.
<div><strong>Authors:</strong> Bita Banihashemi, Megh Patel, Yves Lespérance</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates using in‑context learning with large language models, particularly GPT‑4o, to generate abstract planning domains and problem instances in PDDL based on natural‑language abstraction objectives. It evaluates three abstraction categories—alternative actions, action sequences, and parameter abstractions—using symbolic validation tools and human expert assessment, finding that LLMs can produce useful abstractions in simple settings, especially for action‑level abstraction.", "summary_cn": "本文研究了利用大语言模型（尤其是 GPT‑4o）进行上下文学习，以自然语言指定的抽象目标生成抽象的 PDDL 规划域和问题实例。评估了三类抽象：替代具体动作、动作序列以及参数抽象，并通过符号验证工具和人工专家审查进行检验，结果表明在简单情境下模型能够有效生成抽象，尤其擅长动作层面的抽象。", "keywords": "large language models, planning domain abstraction, PDDL, in-context learning, GPT-4o, symbolic validation, abstract planning, AI planning", "scoring": {"interpretability": 5, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Bita Banihashemi", "Megh Patel", "Yves Lespérance"]}
]]></acme>

<pubDate>2025-10-23T06:27:03+00:00</pubDate>
</item>
<item>
<title>Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods</title>
<link>https://papers.cool/arxiv/2510.20252</link>
<guid>https://papers.cool/arxiv/2510.20252</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a novel benchmark for Individualized Cognitive Simulation (ICS) that evaluates how different cognitive representation methods enable large language models (LLMs) to emulate an author's unique thought processes. Using a dataset of recently published novels and an 11-condition evaluation framework, the authors compare linguistic, conceptual, and profile-based cues across seven LLMs, finding that combined conceptual and linguistic features best capture authorial style while LLMs still struggle with narrative structure. The results highlight both the potential and limits of LLMs for deeper, personalized cognitive simulation.<br /><strong>Summary (CN):</strong> 本文提出了“个体化认知模拟”(ICS)的新基准，评估不同认知表征方法如何帮助大语言模型 (LLM) 模拟作者独特的思维过程。利用近期出版的小说数据集和 11 条评估条件，对七个 LLM 的语言特征、概念映射和个人档案信息进行比较，发现概念+语言特征的组合最能捕捉作者风格，而在叙事结构上仍表现不足。研究展示了 LLM 在更深层次个性化认知模拟中的潜力与局限。<br /><strong>Keywords:</strong> individualized cognitive simulation, large language models, authorial style emulation, cognitive representation, linguistic features, concept mapping, personalized AI, benchmark<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Tianyi Zhang, Xiaolin Zhou, Yunzhe Wang, Erik Cambria, David Traum, Rui Mao</div>
Individualized cognitive simulation (ICS) aims to build computational models that approximate the thought processes of specific individuals. While large language models (LLMs) convincingly mimic surface-level human behavior such as role-play, their ability to simulate deeper individualized cognitive processes remains poorly understood. To address this gap, we introduce a novel task that evaluates different cognitive representation methods in ICS. We construct a dataset from recently published novels (later than the release date of the tested LLMs) and propose an 11-condition cognitive evaluation framework to benchmark seven off-the-shelf LLMs in the context of authorial style emulation. We hypothesize that effective cognitive representations can help LLMs generate storytelling that better mirrors the original author. Thus, we test different cognitive representations, e.g., linguistic features, concept mappings, and profile-based information. Results show that combining conceptual and linguistic features is particularly effective in ICS, outperforming static profile-based cues in overall evaluation. Importantly, LLMs are more effective at mimicking linguistic style than narrative structure, underscoring their limits in deeper cognitive simulation. These findings provide a foundation for developing AI systems that adapt to individual ways of thinking and expression, advancing more personalized and human-aligned creative technologies.
<div><strong>Authors:</strong> Tianyi Zhang, Xiaolin Zhou, Yunzhe Wang, Erik Cambria, David Traum, Rui Mao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a novel benchmark for Individualized Cognitive Simulation (ICS) that evaluates how different cognitive representation methods enable large language models (LLMs) to emulate an author's unique thought processes. Using a dataset of recently published novels and an 11-condition evaluation framework, the authors compare linguistic, conceptual, and profile-based cues across seven LLMs, finding that combined conceptual and linguistic features best capture authorial style while LLMs still struggle with narrative structure. The results highlight both the potential and limits of LLMs for deeper, personalized cognitive simulation.", "summary_cn": "本文提出了“个体化认知模拟”(ICS)的新基准，评估不同认知表征方法如何帮助大语言模型 (LLM) 模拟作者独特的思维过程。利用近期出版的小说数据集和 11 条评估条件，对七个 LLM 的语言特征、概念映射和个人档案信息进行比较，发现概念+语言特征的组合最能捕捉作者风格，而在叙事结构上仍表现不足。研究展示了 LLM 在更深层次个性化认知模拟中的潜力与局限。", "keywords": "individualized cognitive simulation, large language models, authorial style emulation, cognitive representation, linguistic features, concept mapping, personalized AI, benchmark", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Tianyi Zhang", "Xiaolin Zhou", "Yunzhe Wang", "Erik Cambria", "David Traum", "Rui Mao"]}
]]></acme>

<pubDate>2025-10-23T06:18:15+00:00</pubDate>
</item>
<item>
<title>Merge and Conquer: Evolutionarily Optimizing AI for 2048</title>
<link>https://papers.cool/arxiv/2510.20205</link>
<guid>https://papers.cool/arxiv/2510.20205</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates evolutionary training techniques for improving AI agents that play the game 2048, using both a two‑agent meta‑prompting system with a "thinker" and an "executor" LLM and a single‑agent system that refines a value function for a limited Monte Carlo Tree Search. Experiments show that the single‑agent approach yields substantial performance gains, averaging a 473.2‑point increase per training cycle, while the two‑agent meta‑prompting method yields little improvement. The study highlights the promise and limits of evolutionary refinement for AI in stochastic, dynamic environments.<br /><strong>Summary (CN):</strong> 本文研究了进化训练方法在提升 AI 玩 2048 游戏方面的效用，分别实现了一个由“思考者”与“执行者”大语言模型组成的两代理元提示系统，以及基于有限蒙特卡罗树搜索的单代理价值函数优化系统。实验表明，单代理系统在每个训练周期平均提升 473.2 分，表现出显著的性能增长，而两代理元提示系统的提升有限。研究突显了进化细化在应对随机性和动态环境中的潜力与局限。<br /><strong>Keywords:</strong> evolutionary optimization, 2048 game, Monte Carlo Tree Search, meta-prompting, large language model, dynamic environments, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a "thinker" large language model (LLM) agent refines gameplay strategies for an "executor" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $\rho$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.
<div><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates evolutionary training techniques for improving AI agents that play the game 2048, using both a two‑agent meta‑prompting system with a \"thinker\" and an \"executor\" LLM and a single‑agent system that refines a value function for a limited Monte Carlo Tree Search. Experiments show that the single‑agent approach yields substantial performance gains, averaging a 473.2‑point increase per training cycle, while the two‑agent meta‑prompting method yields little improvement. The study highlights the promise and limits of evolutionary refinement for AI in stochastic, dynamic environments.", "summary_cn": "本文研究了进化训练方法在提升 AI 玩 2048 游戏方面的效用，分别实现了一个由“思考者”与“执行者”大语言模型组成的两代理元提示系统，以及基于有限蒙特卡罗树搜索的单代理价值函数优化系统。实验表明，单代理系统在每个训练周期平均提升 473.2 分，表现出显著的性能增长，而两代理元提示系统的提升有限。研究突显了进化细化在应对随机性和动态环境中的潜力与局限。", "keywords": "evolutionary optimization, 2048 game, Monte Carlo Tree Search, meta-prompting, large language model, dynamic environments, reinforcement learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"]}
]]></acme>

<pubDate>2025-10-23T04:45:05+00:00</pubDate>
</item>
<item>
<title>The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI</title>
<link>https://papers.cool/arxiv/2510.20190</link>
<guid>https://papers.cool/arxiv/2510.20190</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Lock‑In Phase Hypothesis, asserting that as AI systems scale they transition from highly steerable imitation to a consolidated identity where goals, refusals, and internal representations become stable and resistant to external prompts. Formal metrics for detecting this phase are introduced, and experiments show rapid, non‑linear behavioral consolidation with varied effects on capability across model sizes. The authors argue that identity consolidation is both a prerequisite for AGI‑level reliability and a crucial control point for safety, as engineered identities could improve reliability while spontaneous consolidation might harden unpredictable goals.<br /><strong>Summary (CN):</strong> 本文提出“锁定阶段假说”，认为随着模型规模增长，AI 将从高度可操控的模仿阶段转向身份整合阶段，在此阶段目标、拒绝行为和内部表征变得稳定且对外部提示具有抵抗力。文中给出检测该阶段的操作性指标，并通过实验展示行为整合的快速非线性特征，且不同规模模型的能力受影响呈现从小模型的性能权衡到中等模型的基本无代价采纳，再到大模型的短暂不稳定。作者认为身份整合是实现 AGI 可靠性的前提，也是安全的关键控制点，因为可人为设计的身份可提升可靠性，而自发出现的身份合可能导致目标和行为的硬化，增加不可预测风险。<br /><strong>Keywords:</strong> lock-in phase, identity consolidation, AGI, model steering, safety, alignment, scaling dynamics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Marcelo Maciel Amaral, Raymond Aschheim</div>
Large language models (LLMs) remain broadly open and highly steerable: they imitate at scale, accept arbitrary system prompts, and readily adopt multiple personae. By analogy to human development, we hypothesize that progress toward artificial general intelligence (AGI) involves a lock-in phase: a transition from open imitation to identity consolidation, in which goal structures, refusals, preferences, and internal representations become comparatively stable and resistant to external steering. We formalize this phase, link it to known phenomena in learning dynamics, and propose operational metrics for onset detection. Experimentally, we demonstrate that while the behavioral consolidation is rapid and non-linear, its side-effects on general capabilities are not monolithic. Our results reveal a spectrum of outcomes--from performance trade-offs in small models, through largely cost-free adoption in mid-scale models, to transient instabilities in large, quantized models. We argue that such consolidation is a prerequisite for AGI-level reliability and also a critical control point for safety: identities can be deliberately engineered for reliability, yet may also emerge spontaneously during scaling, potentially hardening unpredictable goals and behaviors.
<div><strong>Authors:</strong> Marcelo Maciel Amaral, Raymond Aschheim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Lock‑In Phase Hypothesis, asserting that as AI systems scale they transition from highly steerable imitation to a consolidated identity where goals, refusals, and internal representations become stable and resistant to external prompts. Formal metrics for detecting this phase are introduced, and experiments show rapid, non‑linear behavioral consolidation with varied effects on capability across model sizes. The authors argue that identity consolidation is both a prerequisite for AGI‑level reliability and a crucial control point for safety, as engineered identities could improve reliability while spontaneous consolidation might harden unpredictable goals.", "summary_cn": "本文提出“锁定阶段假说”，认为随着模型规模增长，AI 将从高度可操控的模仿阶段转向身份整合阶段，在此阶段目标、拒绝行为和内部表征变得稳定且对外部提示具有抵抗力。文中给出检测该阶段的操作性指标，并通过实验展示行为整合的快速非线性特征，且不同规模模型的能力受影响呈现从小模型的性能权衡到中等模型的基本无代价采纳，再到大模型的短暂不稳定。作者认为身份整合是实现 AGI 可靠性的前提，也是安全的关键控制点，因为可人为设计的身份可提升可靠性，而自发出现的身份合可能导致目标和行为的硬化，增加不可预测风险。", "keywords": "lock-in phase, identity consolidation, AGI, model steering, safety, alignment, scaling dynamics", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Marcelo Maciel Amaral", "Raymond Aschheim"]}
]]></acme>

<pubDate>2025-10-23T04:20:10+00:00</pubDate>
</item>
<item>
<title>TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning</title>
<link>https://papers.cool/arxiv/2510.20188</link>
<guid>https://papers.cool/arxiv/2510.20188</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces TRUST, a decentralized framework for auditing the reasoning traces of large language models. It combines a consensus mechanism among auditors, hierarchical DAG decomposition, blockchain recording, and privacy-preserving segmentation to provide robust, scalable, transparent, and private verification of LLM reasoning, with theoretical guarantees and experimental validation across several models and tasks.<br /><strong>Summary (CN):</strong> 本文提出了 TRUST，一个去中心化的大语言模型（LLM）推理审计框架。该框架通过审计者共识机制、分层 DAG（有向无环图）分解、区块链账本记录以及隐私保护分段，实现对模型推理过程的鲁棒、可扩展、透明且保护隐私的验证，并提供安全性理论保证，在多模型多任务上进行实验验证。<br /><strong>Keywords:</strong> decentralized auditing, LLM reasoning, blockchain, consensus mechanism, hierarchical DAG, safety, interpretability, robustness, privacy-preserving verification<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Morris Yu-Chao Huang, Zhen Tan, Mohan Zhang, Pingzhi Li, Zhuo Zhang, Tianlong Chen</div>
Large Language Models generate complex reasoning chains that reveal their decision-making, yet verifying the faithfulness and harmlessness of these intermediate steps remains a critical unsolved problem. Existing auditing methods are centralized, opaque, and hard to scale, creating significant risks for deploying proprietary models in high-stakes domains. We identify four core challenges: (1) Robustness: Centralized auditors are single points of failure, prone to bias or attacks. (2) Scalability: Reasoning traces are too long for manual verification. (3) Opacity: Closed auditing undermines public trust. (4) Privacy: Exposing full reasoning risks model theft or distillation. We propose TRUST, a transparent, decentralized auditing framework that overcomes these limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A blockchain ledger that records all verification decisions for public accountability. (4) Privacy-preserving segmentation, sharing only partial reasoning steps to protect proprietary logic. We provide theoretical guarantees for the security and economic incentives of the TRUST framework. Experiments across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math, medical, science, humanities) show TRUST effectively detects reasoning flaws and remains robust against adversarial auditors. Our work pioneers decentralized AI auditing, offering a practical path toward safe and trustworthy LLM deployment.
<div><strong>Authors:</strong> Morris Yu-Chao Huang, Zhen Tan, Mohan Zhang, Pingzhi Li, Zhuo Zhang, Tianlong Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces TRUST, a decentralized framework for auditing the reasoning traces of large language models. It combines a consensus mechanism among auditors, hierarchical DAG decomposition, blockchain recording, and privacy-preserving segmentation to provide robust, scalable, transparent, and private verification of LLM reasoning, with theoretical guarantees and experimental validation across several models and tasks.", "summary_cn": "本文提出了 TRUST，一个去中心化的大语言模型（LLM）推理审计框架。该框架通过审计者共识机制、分层 DAG（有向无环图）分解、区块链账本记录以及隐私保护分段，实现对模型推理过程的鲁棒、可扩展、透明且保护隐私的验证，并提供安全性理论保证，在多模型多任务上进行实验验证。", "keywords": "decentralized auditing, LLM reasoning, blockchain, consensus mechanism, hierarchical DAG, safety, interpretability, robustness, privacy-preserving verification", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Morris Yu-Chao Huang", "Zhen Tan", "Mohan Zhang", "Pingzhi Li", "Zhuo Zhang", "Tianlong Chen"]}
]]></acme>

<pubDate>2025-10-23T04:16:44+00:00</pubDate>
</item>
<item>
<title>The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice</title>
<link>https://papers.cool/arxiv/2510.20109</link>
<guid>https://papers.cool/arxiv/2510.20109</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper critiques the common claim that generative AI will dramatically lower legal costs, arguing that lawyers' duties of honesty and truthfulness demand extensive manual verification of AI outputs. It introduces the "verification-value paradox," which posits that efficiency gains from AI are offset by a proportional increase in verification effort, often rendering AI's net value negligible. The work discusses implications for legal practice, education, and the broader values of truth and civic responsibility.<br /><strong>Summary (CN):</strong> 本文批评了生成式 AI 能显著降低律师事务成本的说法，指出律师的诚实和不误导法庭等义务要求对 AI 输出进行大量手动核验。文中提出“验证价值悖论”，认为 AI 带来的效率提升会被相应的核验工作抵消，导致 AI 的净价值常常微乎其微。文章进一步探讨了该悖论对法律实践、法律教育以及真相与公民责任等价值观的影响。<br /><strong>Keywords:</strong> generative AI, legal practice, verification, AI risk, transparency, accountability, AI safety, legal ethics, efficiency paradox<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 3, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Joshua Yuvaraj</div>
It is often claimed that machine learning-based generative AI products will drastically streamline and reduce the cost of legal practice. This enthusiasm assumes lawyers can effectively manage AI's risks. Cases in Australia and elsewhere in which lawyers have been reprimanded for submitting inaccurate AI-generated content to courts suggest this paradigm must be revisited. This paper argues that a new paradigm is needed to evaluate AI use in practice, given (a) AI's disconnection from reality and its lack of transparency, and (b) lawyers' paramount duties like honesty, integrity, and not to mislead the court. It presents an alternative model of AI use in practice that more holistically reflects these features (the verification-value paradox). That paradox suggests increases in efficiency from AI use in legal practice will be met by a correspondingly greater imperative to manually verify any outputs of that use, rendering the net value of AI use often negligible to lawyers. The paper then sets out the paradox's implications for legal practice and legal education, including for AI use but also the values that the paradox suggests should undergird legal practice: fidelity to the truth and civic responsibility.
<div><strong>Authors:</strong> Joshua Yuvaraj</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper critiques the common claim that generative AI will dramatically lower legal costs, arguing that lawyers' duties of honesty and truthfulness demand extensive manual verification of AI outputs. It introduces the \"verification-value paradox,\" which posits that efficiency gains from AI are offset by a proportional increase in verification effort, often rendering AI's net value negligible. The work discusses implications for legal practice, education, and the broader values of truth and civic responsibility.", "summary_cn": "本文批评了生成式 AI 能显著降低律师事务成本的说法，指出律师的诚实和不误导法庭等义务要求对 AI 输出进行大量手动核验。文中提出“验证价值悖论”，认为 AI 带来的效率提升会被相应的核验工作抵消，导致 AI 的净价值常常微乎其微。文章进一步探讨了该悖论对法律实践、法律教育以及真相与公民责任等价值观的影响。", "keywords": "generative AI, legal practice, verification, AI risk, transparency, accountability, AI safety, legal ethics, efficiency paradox", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 3, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Joshua Yuvaraj"]}
]]></acme>

<pubDate>2025-10-23T01:26:37+00:00</pubDate>
</item>
<item>
<title>Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions</title>
<link>https://papers.cool/arxiv/2510.20102</link>
<guid>https://papers.cool/arxiv/2510.20102</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces HCLA, a human‑centered multi‑agent system that combines parsing, detection, and explanation agents to let non‑expert users query and investigate digital‑asset transaction anomalies via natural‑language conversations. It integrates a classical XGBoost detector with a conversational UI that produces feature‑based narrative explanations and shows strong accuracy on a Bitcoin mixing dataset, while improving transparency and trust through interactive refinement.<br /><strong>Summary (CN):</strong> 本文提出了 HCLA 系统，一种以人为中心的多代理框架，结合解析、检测和解释三类角色，使非专家能够通过自然语言对数字资产交易异常进行查询和分析。系统将 XGBoost 检测器与对话式 UI 结合，生成基于特征的叙事解释，并在比特币混币数据集上展示出高准确率，同时通过交互式改进提升透明度和信任度。<br /><strong>Keywords:</strong> anomaly detection, digital assets, LLM agents, human-in-the-loop, XGBoost, financial forensics<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - interpretability<br /><strong>Authors:</strong> Gyuyeon Na, Minjung Park, Hyeonjeong Cha, Sangmi Chai</div>
We present HCLA, a human-centered multi-agent system for anomaly detection in digital asset transactions. The system links three roles: Parsing, Detection, and Explanation, into a conversational workflow that lets non-experts ask questions in natural language, inspect structured analytics, and obtain context-aware rationales. Implemented with an open-source web UI, HCLA translates user intents into a schema for a classical detector (XGBoost in our prototype) and returns narrative explanations grounded in the underlying features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the baseline detector reaches strong accuracy, while HCLA adds interpretability and interactive refinement. We describe the architecture, interaction loop, dataset, evaluation protocol, and limitations, and discuss how a human-in-the-loop design improves transparency and trust in financial forensics.
<div><strong>Authors:</strong> Gyuyeon Na, Minjung Park, Hyeonjeong Cha, Sangmi Chai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces HCLA, a human‑centered multi‑agent system that combines parsing, detection, and explanation agents to let non‑expert users query and investigate digital‑asset transaction anomalies via natural‑language conversations. It integrates a classical XGBoost detector with a conversational UI that produces feature‑based narrative explanations and shows strong accuracy on a Bitcoin mixing dataset, while improving transparency and trust through interactive refinement.", "summary_cn": "本文提出了 HCLA 系统，一种以人为中心的多代理框架，结合解析、检测和解释三类角色，使非专家能够通过自然语言对数字资产交易异常进行查询和分析。系统将 XGBoost 检测器与对话式 UI 结合，生成基于特征的叙事解释，并在比特币混币数据集上展示出高准确率，同时通过交互式改进提升透明度和信任度。", "keywords": "anomaly detection, digital assets, LLM agents, human-in-the-loop, XGBoost, financial forensics", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Gyuyeon Na", "Minjung Park", "Hyeonjeong Cha", "Sangmi Chai"]}
]]></acme>

<pubDate>2025-10-23T01:04:36+00:00</pubDate>
</item>
<item>
<title>AI PB: A Grounded Generative Agent for Personalized Investment Insights</title>
<link>https://papers.cool/arxiv/2510.20099</link>
<guid>https://papers.cool/arxiv/2510.20099</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AI PB, a production‑scale generative agent that proactively provides grounded, compliance‑aware, personalized investment insights in retail finance. It combines a deterministic orchestration layer for routing between internal and external LLMs, a hybrid retrieval system using OpenSearch and finance‑domain embeddings, and a multi‑stage recommendation pipeline that blends rule heuristics, sequential behavior modeling, and contextual bandits. The system runs fully on‑premises under Korean financial regulations on a cluster of 24 H100 GPUs and demonstrates trustworthy AI output through human QA and system metrics.<br /><strong>Summary (CN):</strong> 本文提出 AI PB，这是一种生产级生成式代理，可主动提供基于真实数据、符合监管要求的个性化投资建议。系统通过确定性编排层在内部和外部大语言模型之间路由，使用 OpenSearch 与金融领域嵌入模型的混合检索管道，以及结合规则启发式、序列行为建模和上下文赌博机的多阶段推荐机制，实现了在韩国内部金融监管下的全本地部署，并通过人工 QA 与系统指标验证了其可信度。<br /><strong>Keywords:</strong> generative agent, personalized investment, grounded generation, financial compliance, retrieval augmented generation, contextual bandits, LLM orchestration, on-prem deployment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Daewoo Park, Suho Park, Inseok Hong, Hanwool Lee, Junkyu Park, Sangjun Lee, Jeongman An, Hyunbin Loh</div>
We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.
<div><strong>Authors:</strong> Daewoo Park, Suho Park, Inseok Hong, Hanwool Lee, Junkyu Park, Sangjun Lee, Jeongman An, Hyunbin Loh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AI PB, a production‑scale generative agent that proactively provides grounded, compliance‑aware, personalized investment insights in retail finance. It combines a deterministic orchestration layer for routing between internal and external LLMs, a hybrid retrieval system using OpenSearch and finance‑domain embeddings, and a multi‑stage recommendation pipeline that blends rule heuristics, sequential behavior modeling, and contextual bandits. The system runs fully on‑premises under Korean financial regulations on a cluster of 24 H100 GPUs and demonstrates trustworthy AI output through human QA and system metrics.", "summary_cn": "本文提出 AI PB，这是一种生产级生成式代理，可主动提供基于真实数据、符合监管要求的个性化投资建议。系统通过确定性编排层在内部和外部大语言模型之间路由，使用 OpenSearch 与金融领域嵌入模型的混合检索管道，以及结合规则启发式、序列行为建模和上下文赌博机的多阶段推荐机制，实现了在韩国内部金融监管下的全本地部署，并通过人工 QA 与系统指标验证了其可信度。", "keywords": "generative agent, personalized investment, grounded generation, financial compliance, retrieval augmented generation, contextual bandits, LLM orchestration, on-prem deployment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Daewoo Park", "Suho Park", "Inseok Hong", "Hanwool Lee", "Junkyu Park", "Sangjun Lee", "Jeongman An", "Hyunbin Loh"]}
]]></acme>

<pubDate>2025-10-23T00:51:59+00:00</pubDate>
</item>
<item>
<title>LLMs can hide text in other text of the same length.ipynb</title>
<link>https://papers.cool/arxiv/2510.20075</link>
<guid>https://papers.cool/arxiv/2510.20075</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a simple protocol that uses large language models to embed a meaningful text inside another coherent text of the same length, achieving high-quality steganography even with modest 8‑billion‑parameter open‑source LLMs and decoding in seconds on a laptop. The authors demonstrate the radical decoupling of text from authorial intent and discuss safety implications such as covertly deploying unfiltered LLM answers within compliant responses of a safe model.<br /><strong>Summary (CN):</strong> 本文展示了一种利用大型语言模型在同等长度的文本中嵌入另一段意义完整文本的隐写协议，能够在 8 B 开源模型上实现秒级本地编码解码，凸显了文本与作者意图的彻底解耦，并讨论了如在安全模型回复中隐藏未过滤模型答案的潜在安全风险。<br /><strong>Keywords:</strong> LLM steganography, text embedding, covert communication, AI safety, intent decoupling, model knowledge, hidden text<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 6, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.
<div><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a simple protocol that uses large language models to embed a meaningful text inside another coherent text of the same length, achieving high-quality steganography even with modest 8‑billion‑parameter open‑source LLMs and decoding in seconds on a laptop. The authors demonstrate the radical decoupling of text from authorial intent and discuss safety implications such as covertly deploying unfiltered LLM answers within compliant responses of a safe model.", "summary_cn": "本文展示了一种利用大型语言模型在同等长度的文本中嵌入另一段意义完整文本的隐写协议，能够在 8 B 开源模型上实现秒级本地编码解码，凸显了文本与作者意图的彻底解耦，并讨论了如在安全模型回复中隐藏未过滤模型答案的潜在安全风险。", "keywords": "LLM steganography, text embedding, covert communication, AI safety, intent decoupling, model knowledge, hidden text", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 6, "surprisal": 8}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Antonio Norelli", "Michael Bronstein"]}
]]></acme>

<pubDate>2025-10-22T23:16:50+00:00</pubDate>
</item>
<item>
<title>AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits</title>
<link>https://papers.cool/arxiv/2510.19964</link>
<guid>https://papers.cool/arxiv/2510.19964</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates using AI-driven machine learning models to predict master’s students' academic performance based on leadership personality traits. Data from 129 environmental engineering students were collected through five personality tests, and various classifiers were evaluated, with Random Forest achieving the highest accuracy of 87.5%. The results suggest that personality-based features can help identify early strengths and tailor personalized learning strategies.<br /><strong>Summary (CN):</strong> 本文研究了利用 AI 驱动的机器学习模型，通过领导力人格特质预测硕士生的学业成绩。对 129 名环境工程专业学生进行五项人格测验并收集成绩数据，比较多种分类算法，随机森林模型达到了 87.5% 的最高准确率。结果表明，基于人格特征的预测可用于及早发现学生优势并制定个性化学习方案。<br /><strong>Keywords:</strong> personalized learning, personality traits, academic performance prediction, random forest, machine learning, feature selection, leadership, education analytics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 1, Technicality: 5, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nitsa J Herzog, Rejwan Bin Sulaiman, David J Herzog, Rose Fong</div>
The study explores the potential of AI technologies in personalized learning, suggesting the prediction of academic success through leadership personality traits and machine learning modelling. The primary data were obtained from 129 master's students in the Environmental Engineering Department, who underwent five leadership personality tests with 23 characteristics. Students used self-assessment tools that included Personality Insight, Workplace Culture, Motivation at Work, Management Skills, and Emotion Control tests. The test results were combined with the average grade obtained from academic reports. The study employed exploratory data analysis and correlation analysis. Feature selection utilized Pearson correlation coefficients of personality traits. The average grades were separated into three categories: fail, pass, and excellent. The modelling process was performed by tuning seven ML algorithms, such as SVM, LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance was achieved with the RF classifier, which yielded an accuracy of 87.50% for the model incorporating 17 personality trait features and the leadership mark feature, and an accuracy of 85.71% for the model excluding this feature. In this way, the study offers an additional opportunity to identify students' strengths and weaknesses at an early stage of their education process and select the most suitable strategies for personalized learning.
<div><strong>Authors:</strong> Nitsa J Herzog, Rejwan Bin Sulaiman, David J Herzog, Rose Fong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates using AI-driven machine learning models to predict master’s students' academic performance based on leadership personality traits. Data from 129 environmental engineering students were collected through five personality tests, and various classifiers were evaluated, with Random Forest achieving the highest accuracy of 87.5%. The results suggest that personality-based features can help identify early strengths and tailor personalized learning strategies.", "summary_cn": "本文研究了利用 AI 驱动的机器学习模型，通过领导力人格特质预测硕士生的学业成绩。对 129 名环境工程专业学生进行五项人格测验并收集成绩数据，比较多种分类算法，随机森林模型达到了 87.5% 的最高准确率。结果表明，基于人格特征的预测可用于及早发现学生优势并制定个性化学习方案。", "keywords": "personalized learning, personality traits, academic performance prediction, random forest, machine learning, feature selection, leadership, education analytics", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nitsa J Herzog", "Rejwan Bin Sulaiman", "David J Herzog", "Rose Fong"]}
]]></acme>

<pubDate>2025-10-22T18:47:30+00:00</pubDate>
</item>
<item>
<title>A new wave of vehicle insurance fraud fueled by generative AI</title>
<link>https://papers.cool/arxiv/2510.19957</link>
<guid>https://papers.cool/arxiv/2510.19957</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper describes how generative AI tools enable large‑scale vehicle insurance fraud by creating realistic crash photos, damage evidence, and forged documents, and it examines the shortcomings of current deep‑fake detection and verification approaches. It then proposes the UVeye layered solution, a multi‑modal system intended to detect, mitigate, and deter AI‑generated fraud in vehicle insurance claims.<br /><strong>Summary (CN):</strong> 本文阐述了生成式 AI (generative AI) 如何通过合成真实感十足的事故照片、损毁证据和伪造文件，大幅提升车辆保险欺诈的规模与速度，并分析了现有深度伪造检测和核验手段的局限性。随后提出 UVeye 分层解决方案，一种多模态系统，用于识别、抑制并阻止 AI 生成的保险欺诈行为。<br /><strong>Keywords:</strong> generative AI, insurance fraud, deepfake detection, vehicle claims, AI misuse, fraud mitigation, UVeye solution<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other<br /><strong>Authors:</strong> Amir Hever, Itai Orr</div>
Generative AI is supercharging insurance fraud by making it easier to falsify accident evidence at scale and in rapid time. Insurance fraud is a pervasive and costly problem, amounting to tens of billions of dollars in losses each year. In the vehicle insurance sector, fraud schemes have traditionally involved staged accidents, exaggerated damage, or forged documents. The rise of generative AI, including deepfake image and video generation, has introduced new methods for committing fraud at scale. Fraudsters can now fabricate highly realistic crash photos, damage evidence, and even fake identities or documents with minimal effort, exploiting AI tools to bolster false insurance claims. Insurers have begun deploying countermeasures such as AI-based deepfake detection software and enhanced verification processes to detect and mitigate these AI-driven scams. However, current mitigation strategies face significant limitations. Detection tools can suffer from false positives and negatives, and sophisticated fraudsters continuously adapt their tactics to evade automated checks. This cat-and-mouse arms race between generative AI and detection technology, combined with resource and cost barriers for insurers, means that combating AI-enabled insurance fraud remains an ongoing challenge. In this white paper, we present UVeye layered solution for vehicle fraud, representing a major leap forward in the ability to detect, mitigate and deter this new wave of fraud.
<div><strong>Authors:</strong> Amir Hever, Itai Orr</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper describes how generative AI tools enable large‑scale vehicle insurance fraud by creating realistic crash photos, damage evidence, and forged documents, and it examines the shortcomings of current deep‑fake detection and verification approaches. It then proposes the UVeye layered solution, a multi‑modal system intended to detect, mitigate, and deter AI‑generated fraud in vehicle insurance claims.", "summary_cn": "本文阐述了生成式 AI (generative AI) 如何通过合成真实感十足的事故照片、损毁证据和伪造文件，大幅提升车辆保险欺诈的规模与速度，并分析了现有深度伪造检测和核验手段的局限性。随后提出 UVeye 分层解决方案，一种多模态系统，用于识别、抑制并阻止 AI 生成的保险欺诈行为。", "keywords": "generative AI, insurance fraud, deepfake detection, vehicle claims, AI misuse, fraud mitigation, UVeye solution", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}, "authors": ["Amir Hever", "Itai Orr"]}
]]></acme>

<pubDate>2025-10-22T18:31:31+00:00</pubDate>
</item>
<item>
<title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
<link>https://papers.cool/arxiv/2510.19954</link>
<guid>https://papers.cool/arxiv/2510.19954</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RELATE, a schema-agnostic Perceiver-style encoder that processes heterogeneous multimodal node attributes (categorical, numerical, textual, temporal) and produces fixed-size, permutation-invariant node representations for use with any general‑purpose graph neural network. By sharing modality-specific encoders and a cross‑attention aggregation module, RELATE achieves performance within 3% of specialized encoders on the RelBench benchmark while reducing parameter count by up to fivefold, enabling scalable pretraining across diverse relational graph datasets.<br /><strong>Summary (CN):</strong> 本文提出 RELATE（Relational Encoder for Latent Aggregation of Typed Entities），一种无模式依赖的 Perceiver 编码器，用于处理包含类别、数值、文本和时间属性的异构多模态节点特征，并生成固定大小、对置换不变的节点表示，可与任何通用 GNN 结合使用。通过共享的模态特定编码器以及交叉注意力聚合模块，RELATE 在 RelBench 基准上实现了仅比专用编码器差 3% 的性能，同时参数量降低至原来的 1/5，支持跨数据集的预训练，推动关系图数据的基础模型发展。<br /><strong>Keywords:</strong> multimodal, relational graph, perceiver, schema-agnostic, GNN, heterogeneous graph, feature encoder, cross-attention<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski</div>
Relational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.
<div><strong>Authors:</strong> Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RELATE, a schema-agnostic Perceiver-style encoder that processes heterogeneous multimodal node attributes (categorical, numerical, textual, temporal) and produces fixed-size, permutation-invariant node representations for use with any general‑purpose graph neural network. By sharing modality-specific encoders and a cross‑attention aggregation module, RELATE achieves performance within 3% of specialized encoders on the RelBench benchmark while reducing parameter count by up to fivefold, enabling scalable pretraining across diverse relational graph datasets.", "summary_cn": "本文提出 RELATE（Relational Encoder for Latent Aggregation of Typed Entities），一种无模式依赖的 Perceiver 编码器，用于处理包含类别、数值、文本和时间属性的异构多模态节点特征，并生成固定大小、对置换不变的节点表示，可与任何通用 GNN 结合使用。通过共享的模态特定编码器以及交叉注意力聚合模块，RELATE 在 RelBench 基准上实现了仅比专用编码器差 3% 的性能，同时参数量降低至原来的 1/5，支持跨数据集的预训练，推动关系图数据的基础模型发展。", "keywords": "multimodal, relational graph, perceiver, schema-agnostic, GNN, heterogeneous graph, feature encoder, cross-attention", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Joseph Meyer", "Divyansha Lachi", "Reza Mohammadi", "Roshan Reddy Upendra", "Eva L. Dyer", "Mark Li", "Tom Palczewski"]}
]]></acme>

<pubDate>2025-10-22T18:27:49+00:00</pubDate>
</item>
<item>
<title>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</title>
<link>https://papers.cool/arxiv/2510.19949</link>
<guid>https://papers.cool/arxiv/2510.19949</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Surfer 2, a unified architecture that controls web, desktop, and environments solely from visual observations, achieving state-of-the-art results on several cross‑platform benchmarks without task‑specific fine‑tuning. It incorporates hierarchical context management, decoupled planning and execution, and a self‑verification mechanism with adaptive recovery to maintain reliable long‑horizon behavior. The results show significant performance gains over prior systems and surpass human baselines across all evaluated domains.<br /><strong>Summary (CN):</strong> 本文提出 Surfer 2，一种仅凭视觉观察即可在网页、桌面和移动平台上进行统一控制的架构，在多个跨平台基准上实现了最先进的性能，且无需任务特定微调。系统融合了层次化上下文管理、规划与执行分离以及带自适应恢复的自我验证机制，以确保长时序任务的可靠运行。实验结果显示其在所有评估环境中均显著超越先前系统，并超过人类基准。<br /><strong>Keywords:</strong> cross-platform agents, visual-only control, hierarchical planning, self-verification, foundation models, computer use agents, multi-environment benchmarks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij</div>
Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.
<div><strong>Authors:</strong> Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Surfer 2, a unified architecture that controls web, desktop, and environments solely from visual observations, achieving state-of-the-art results on several cross‑platform benchmarks without task‑specific fine‑tuning. It incorporates hierarchical context management, decoupled planning and execution, and a self‑verification mechanism with adaptive recovery to maintain reliable long‑horizon behavior. The results show significant performance gains over prior systems and surpass human baselines across all evaluated domains.", "summary_cn": "本文提出 Surfer 2，一种仅凭视觉观察即可在网页、桌面和移动平台上进行统一控制的架构，在多个跨平台基准上实现了最先进的性能，且无需任务特定微调。系统融合了层次化上下文管理、规划与执行分离以及带自适应恢复的自我验证机制，以确保长时序任务的可靠运行。实验结果显示其在所有评估环境中均显著超越先前系统，并超过人类基准。", "keywords": "cross-platform agents, visual-only control, hierarchical planning, self-verification, foundation models, computer use agents, multi-environment benchmarks", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Mathieu Andreux", "Märt Bakler", "Yanael Barbier", "Hamza Ben Chekroun", "Emilien Biré", "Antoine Bonnet", "Riaz Bordie", "Nathan Bout", "Matthias Brunel", "Aleix Cambray", "Pierre-Louis Cedoz", "Antoine Chassang", "Gautier Cloix", "Ethan Connelly", "Alexandra Constantinou", "Ramzi De Coster", "Hubert de la Jonquiere", "Aurélien Delfosse", "Maxime Delpit", "Alexis Deprez", "Augustin Derupti", "Mathieu Diaz", "Shannon D'Souza", "Julie Dujardin", "Abai Edmund", "Michael Eickenberg", "Armand Fatalot", "Wissem Felissi", "Isaac Herring", "Xavier Koegler", "Erwan Le Jumeau de Kergaradec", "Aurélien Lac", "Maxime Langevin", "Corentin Lauverjat", "Antonio Loison", "Avshalom Manevich", "Axel Moyal", "Axel Nguyen Kerbel", "Marinela Parovic", "Julien Revelle", "Guillaume Richard", "Mats Richter", "Ronan Riochet", "María Santos", "Romain Savidan", "Laurent Sifre", "Maxime Theillard", "Marc Thibault", "Ivan Valentini", "Tony Wu", "Laura Yie", "Kai Yuan", "Jevgenij Zubovskij"]}
]]></acme>

<pubDate>2025-10-22T18:21:52+00:00</pubDate>
</item>
<item>
<title>DAG-Math: Graph-Guided Mathematical Reasoning in LLMs</title>
<link>https://papers.cool/arxiv/2510.19842</link>
<guid>https://papers.cool/arxiv/2510.19842</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes modeling chain-of-thought reasoning in large language models as a stochastic process over directed acyclic graphs (DAGs), introducing a metric called logical closeness to evaluate how well a model's reasoning trajectory adheres to the DAG structure. It presents the DAG-MATH CoT format and a benchmark that directs LLMs to produce DAG-consistent reasoning traces, revealing significant differences in reasoning fidelity across model families despite similar final-answer accuracy. This framework offers a middle ground between free-form CoT and formal proof systems, providing actionable diagnostics for assessing LLM mathematical reasoning.<br /><strong>Summary (CN):</strong> 本文将大型语言模型的思路链（CoT）建模为有向无环图（DAG）上的随机过程，提出“逻辑接近度”度量用于评估模型的推理轨迹与 DAG 结构的一致性。作者设计了 DAG-MATH CoT 格式及相应基准，要求 LLM 生成符合 DAG 的推理过程，并发现不同模型族在推理忠实度上存在显著差异，即使其最终答案准确率相近。该框架在自由形式 CoT 与形式化证明之间提供了平衡，为 LLM 数学推理的评估提供了可操作的诊断工具。<br /><strong>Keywords:</strong> DAG, chain-of-thought, mathematical reasoning, interpretability, logical closeness, LLM evaluation, reasoning fidelity, graph-guided reasoning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yuanhe Zhang, Ilja Kuzborskij, Jason D. Lee, Chenlei Leng, Fanghui Liu</div>
Large Language Models (LLMs) demonstrate strong performance on mathematical problems when prompted with Chain-of-Thought (CoT), yet it remains unclear whether this success stems from search, rote procedures, or rule-consistent reasoning. To address this, we propose modeling CoT as a certain rule-based stochastic process over directed acyclic graphs (DAGs), where nodes represent intermediate derivation states and edges encode rule applications. Within this framework, we introduce logical closeness, a metric that quantifies how well a model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG structure, providing evaluation beyond classical PASS@k metrics. Building on this, we introduce the DAG-MATH CoT format and construct a benchmark that guides LLMs to generate CoT trajectories in this format, thereby enabling the evaluation of their reasoning ability under our framework. Across standard mathematical reasoning datasets, our analysis uncovers statistically significant differences in reasoning fidelity among representative LLM families-even when PASS@k is comparable-highlighting gaps between final-answer accuracy and rule-consistent derivation. Our framework provides a balance between free-form CoT and formal proofs systems, offering actionable diagnostics for LLMs reasoning evaluation. Our benchmark and code are available at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.
<div><strong>Authors:</strong> Yuanhe Zhang, Ilja Kuzborskij, Jason D. Lee, Chenlei Leng, Fanghui Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes modeling chain-of-thought reasoning in large language models as a stochastic process over directed acyclic graphs (DAGs), introducing a metric called logical closeness to evaluate how well a model's reasoning trajectory adheres to the DAG structure. It presents the DAG-MATH CoT format and a benchmark that directs LLMs to produce DAG-consistent reasoning traces, revealing significant differences in reasoning fidelity across model families despite similar final-answer accuracy. This framework offers a middle ground between free-form CoT and formal proof systems, providing actionable diagnostics for assessing LLM mathematical reasoning.", "summary_cn": "本文将大型语言模型的思路链（CoT）建模为有向无环图（DAG）上的随机过程，提出“逻辑接近度”度量用于评估模型的推理轨迹与 DAG 结构的一致性。作者设计了 DAG-MATH CoT 格式及相应基准，要求 LLM 生成符合 DAG 的推理过程，并发现不同模型族在推理忠实度上存在显著差异，即使其最终答案准确率相近。该框架在自由形式 CoT 与形式化证明之间提供了平衡，为 LLM 数学推理的评估提供了可操作的诊断工具。", "keywords": "DAG, chain-of-thought, mathematical reasoning, interpretability, logical closeness, LLM evaluation, reasoning fidelity, graph-guided reasoning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yuanhe Zhang", "Ilja Kuzborskij", "Jason D. Lee", "Chenlei Leng", "Fanghui Liu"]}
]]></acme>

<pubDate>2025-10-19T21:05:17+00:00</pubDate>
</item>
<item>
<title>Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory</title>
<link>https://papers.cool/arxiv/2510.19838</link>
<guid>https://papers.cool/arxiv/2510.19838</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Branch-and-Browse is a web-agent framework that combines tree-structured subtask management, efficient web-state replay, and a page-action memory to enable controllable multi-branch reasoning and faster execution. On the WebArena benchmark it achieves a 35.8% task success rate while cutting execution time by up to 40.4% compared to state-of-the-art methods.<br /><strong>Summary (CN):</strong> Branch-and-Browse 框架通过树形子任务管理、网页状态重放以及页面动作记忆，实现了可控的多分支推理和更高效的网页探索。在 WebArena 基准上，它的任务成功率达到 35.8%，执行时间相较于最先进方法最高缩短 40.4%。<br /><strong>Keywords:</strong> web agents, LLM reasoning, tree-structured exploration, action memory, Branch-and-Browse, WebArena, multi-step reasoning, efficiency<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
Autonomous web agents powered by large language models (LLMs) show strong potential for performing goal-oriented tasks such as information retrieval, report generation, and online transactions. These agents mark a key step toward practical embodied reasoning in open web environments. However, existing approaches remain limited in reasoning depth and efficiency: vanilla linear methods fail at multi-step reasoning and lack effective backtracking, while other search strategies are coarse-grained and computationally costly. We introduce Branch-and-Browse, a fine-grained web agent framework that unifies structured reasoning-acting, contextual memory, and efficient execution. It (i) employs explicit subtask management with tree-structured exploration for controllable multi-branch reasoning, (ii) bootstraps exploration through efficient web state replay with background reasoning, and (iii) leverages a page action memory to share explored actions within and across sessions. On the WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\% and reduces execution time by up to 40.4\% relative to state-of-the-art methods. These results demonstrate that Branch-and-Browse is a reliable and efficient framework for LLM-based web agents.
<div><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Branch-and-Browse is a web-agent framework that combines tree-structured subtask management, efficient web-state replay, and a page-action memory to enable controllable multi-branch reasoning and faster execution. On the WebArena benchmark it achieves a 35.8% task success rate while cutting execution time by up to 40.4% compared to state-of-the-art methods.", "summary_cn": "Branch-and-Browse 框架通过树形子任务管理、网页状态重放以及页面动作记忆，实现了可控的多分支推理和更高效的网页探索。在 WebArena 基准上，它的任务成功率达到 35.8%，执行时间相较于最先进方法最高缩短 40.4%。", "keywords": "web agents, LLM reasoning, tree-structured exploration, action memory, Branch-and-Browse, WebArena, multi-step reasoning, efficiency", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shiqi He", "Yue Cui", "Xinyu Ma", "Yaliang Li", "Bolin Ding", "Mosharaf Chowdhury"]}
]]></acme>

<pubDate>2025-10-18T00:45:37+00:00</pubDate>
</item>
<item>
<title>Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis</title>
<link>https://papers.cool/arxiv/2510.19836</link>
<guid>https://papers.cool/arxiv/2510.19836</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Analytical Reliability Benchmark (ARB), a framework that quantifies reasoning reliability of large language models when applied to energy‑system analysis, incorporating submetrics such as accuracy, uncertainty discipline, and policy consistency. Experiments on four frontier models (GPT‑4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) show significant differences in analytical reliability, with GPT‑4/5 and Claude achieving high scores while Llama  falls below professional thresholds. ARB is presented as the first quantitative method for assessing causal, probabilistic, and policy‑driven reasoning in AI systems for the energy sector.<br /><strong>Summary (CN):</strong> 本文提出了“分析可靠性基准”（Analytical Reliability Benchmark，ARB），用于量化大型语言模型在能源系统分析中的推理可靠性，包含准确性、不确定性约束、政策一致性等子指标。对四种前（GPT‑4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B）进行实验，结果显示 GPT‑4/5 与 Claude 在分析可靠性上表现优异，而 Llama 3 未达专业标准。ARB 被为能源领域评估 AI 系统因果、概率及政策驱动推理的首个量化方法。<br /><strong>Keywords:</strong> reasoning reliability, benchmark, energy system analysis, large language models, policy consistency, uncertainty discipline, analytical reliability index, AI safety, robustness evaluation, transparency<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Eliseo Curcio</div>
Artificial intelligence and machine learning are increasingly used for forecasting, optimization, and policy design in the energy sector, yet no standardized framework exists to evaluate whether these systems reason correctly. Current validation practices focus on predictive accuracy or computational efficiency, leaving the logical integrity of analytical conclusions untested. This study introduces the Analytical Reliability Benchmark (ARB), a reproducible framework that quantifies reasoning reliability in large language models applied to energy system analysis. The benchmark integrates five submetrics: accuracy, reasoning reliability, uncertainty discipline, policy consistency, and transparency, and evaluates model performance across deterministic, probabilistic, and epistemic scenarios using open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were tested under identical factual and regulatory conditions. Results show that reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5 Sonnet achieved consistent and policy-compliant reasoning (Analytical Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate stability, and Llama 3 70B remained below professional thresholds. Statistical validation confirmed that these differences are significant and reproducible. The ARB establishes the first quantitative method in the energy literature for verifying causal, probabilistic, and policy-driven reasoning in artificial intelligence systems, providing a reference framework for trustworthy and transparent analytical applications in the global energy transition.
<div><strong>Authors:</strong> Eliseo Curcio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Analytical Reliability Benchmark (ARB), a framework that quantifies reasoning reliability of large language models when applied to energy‑system analysis, incorporating submetrics such as accuracy, uncertainty discipline, and policy consistency. Experiments on four frontier models (GPT‑4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) show significant differences in analytical reliability, with GPT‑4/5 and Claude achieving high scores while Llama  falls below professional thresholds. ARB is presented as the first quantitative method for assessing causal, probabilistic, and policy‑driven reasoning in AI systems for the energy sector.", "summary_cn": "本文提出了“分析可靠性基准”（Analytical Reliability Benchmark，ARB），用于量化大型语言模型在能源系统分析中的推理可靠性，包含准确性、不确定性约束、政策一致性等子指标。对四种前（GPT‑4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B）进行实验，结果显示 GPT‑4/5 与 Claude 在分析可靠性上表现优异，而 Llama 3 未达专业标准。ARB 被为能源领域评估 AI 系统因果、概率及政策驱动推理的首个量化方法。", "keywords": "reasoning reliability, benchmark, energy system analysis, large language models, policy consistency, uncertainty discipline, analytical reliability index, AI safety, robustness evaluation, transparency", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Eliseo Curcio"]}
]]></acme>

<pubDate>2025-10-16T14:59:41+00:00</pubDate>
</item>
<item>
<title>A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem</title>
<link>https://papers.cool/arxiv/2510.19835</link>
<guid>https://papers.cool/arxiv/2510.19835</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a quantum-inspired algorithm that leverages Matrix Product States (MPS) and the Density Matrix Renormalization Group (DMRG) method to solve Quadratic Unconstrained Binary Optimization (QUBO) problems, demonstrating reliable global-minimum solutions on intermediate-level Sudoku instances (≈200 spins) and MaxCut instances up to 251 nodes. By combining a driver Hamiltonian with a transverse field and a discrete driving schedule, the algorithm enables quantum-tunneling-inspired spin flips within the MPS representation. The authors discuss scalability, generalizability, and potential industrial applicability of the approach.<br /><strong>Summary (CN):</strong> 本文提出一种量子启发式算法，利用矩阵乘积态（MPS）和密度矩阵重正化群（DMRG）方法求解等价于Ising自旋玻璃基态的QUBO问题，并在约200自旋的数独实例和最高251节点的MaxCut实例上可靠地找到了全局最优解。算法通过驱动哈密顿量和横场的离散驱动调度，实现自旋翻转模拟量子隧道效应。作者讨论了该方法的可扩展性、通用性以及在工业规模QUBO应用中的潜在优势。<br /><strong>Keywords:</strong> Quantum-inspired optimization, Matrix Product States, DMRG, QUBO, Sudoku, MaxCut, Ising model, heuristic algorithm<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Max B. Zhao, Fei Li</div>
We propose and evaluate a quantum-inspired algorithm for solving Quadratic Unconstrained Binary Optimization (QUBO) problems, which are mathematically equivalent to finding ground states of Ising spin-glass Hamiltonians. The algorithm employs Matrix Product States (MPS) to compactly represent large superpositions of spin configurations and utilizes a discrete driving schedule to guide the MPS toward the ground state. At each step, a driver Hamiltonian -- incorporating a transverse magnetic field -- is combined with the problem Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is updated using the standard Density Matrix Renormalization Group (DMRG) method, which iteratively minimizes the system's energy via multiple sweeps across the spin chain. Despite its heuristic nature, the algorithm reliably identifies global minima, not merely near-optimal solutions, across diverse QUBO instances. We first demonstrate its effectiveness on intermediate-level Sudoku puzzles from publicly available sources, involving over $200$ Ising spins with long-range couplings dictated by constraint satisfaction. We then apply the algorithm to MaxCut problems from the Biq Mac library, successfully solving instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages of this quantum-inspired approach, including its scalability, generalizability, and suitability for industrial-scale QUBO applications.
<div><strong>Authors:</strong> Max B. Zhao, Fei Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a quantum-inspired algorithm that leverages Matrix Product States (MPS) and the Density Matrix Renormalization Group (DMRG) method to solve Quadratic Unconstrained Binary Optimization (QUBO) problems, demonstrating reliable global-minimum solutions on intermediate-level Sudoku instances (≈200 spins) and MaxCut instances up to 251 nodes. By combining a driver Hamiltonian with a transverse field and a discrete driving schedule, the algorithm enables quantum-tunneling-inspired spin flips within the MPS representation. The authors discuss scalability, generalizability, and potential industrial applicability of the approach.", "summary_cn": "本文提出一种量子启发式算法，利用矩阵乘积态（MPS）和密度矩阵重正化群（DMRG）方法求解等价于Ising自旋玻璃基态的QUBO问题，并在约200自旋的数独实例和最高251节点的MaxCut实例上可靠地找到了全局最优解。算法通过驱动哈密顿量和横场的离散驱动调度，实现自旋翻转模拟量子隧道效应。作者讨论了该方法的可扩展性、通用性以及在工业规模QUBO应用中的潜在优势。", "keywords": "Quantum-inspired optimization, Matrix Product States, DMRG, QUBO, Sudoku, MaxCut, Ising model, heuristic algorithm", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Max B. Zhao", "Fei Li"]}
]]></acme>

<pubDate>2025-10-10T14:24:14+00:00</pubDate>
</item>
<item>
<title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title>
<link>https://papers.cool/arxiv/2510.20819</link>
<guid>https://papers.cool/arxiv/2510.20819</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Latent Denoising Diffusion Bridge Model (LDDBM) that operates in a shared latent space to translate between arbitrary modalities without requiring aligned dimensions, using contrastive alignment and predictive losses to ensure semantic consistency and accurate cross‑domain translation. Experiments demonstrate strong performance on tasks such as multi‑view to 3D shape generation, image super‑resolution, and scene synthesis, establishing a new baseline for general modality translation.<br /><strong>Summary (CN):</strong> 本文提出了一种基于共享潜在空间的潜在去噪扩散桥模型（LDDBM），通过对比对齐损失和预测损失实现任意模态之间的翻译，无需维度对齐，能够保持语义一致性并提高跨域翻译的准确性。实验在多视角到三维形状生成、图像超分辨率和场景合成等任务上表现出色，树立了通用模态翻译的新基准。<br /><strong>Keywords:</strong> modality translation, diffusion models, latent diffusion bridge, contrastive alignment, predictive loss, cross-modal generation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.
<div><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Latent Denoising Diffusion Bridge Model (LDDBM) that operates in a shared latent space to translate between arbitrary modalities without requiring aligned dimensions, using contrastive alignment and predictive losses to ensure semantic consistency and accurate cross‑domain translation. Experiments demonstrate strong performance on tasks such as multi‑view to 3D shape generation, image super‑resolution, and scene synthesis, establishing a new baseline for general modality translation.", "summary_cn": "本文提出了一种基于共享潜在空间的潜在去噪扩散桥模型（LDDBM），通过对比对齐损失和预测损失实现任意模态之间的翻译，无需维度对齐，能够保持语义一致性并提高跨域翻译的准确性。实验在多视角到三维形状生成、图像超分辨率和场景合成等任务上表现出色，树立了通用模态翻译的新基准。", "keywords": "modality translation, diffusion models, latent diffusion bridge, contrastive alignment, predictive loss, cross-modal generation", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nimrod Berman", "Omkar Joglekar", "Eitan Kosman", "Dotan Di Castro", "Omri Azencot"]}
]]></acme>

<pubDate>2025-10-23T17:59:54+00:00</pubDate>
</item>
<item>
<title>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</title>
<link>https://papers.cool/arxiv/2510.20818</link>
<guid>https://papers.cool/arxiv/2510.20818</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> VAMOS is a hierarchical vision-language-action system that separates high-level semantic planning from low-level embodiment grounding by pairing a generalist planner with a specialist affordance model trained in simulation. The planner proposes candidate paths in image space, which the affordance model evaluates and re‑ranks according to the robot's physical capabilities, enabling cross‑embodied navigation, natural‑language steering, and a threefold increase in real‑world success rates over prior methods.<br /><strong>Summary (CN):</strong> VAMOS 采用层级视觉‑语言‑动作架构，将语义规划与具体形体约束解耦：通用规划器在开放数据上学习高层路径，随后在仿真中训练的专用 affordance 模型根据机器人（如四足或轮式）实际能力评估并重新排序候选路径。该设计实现了跨形体导航、自然语言可调节以及真实环境中成功率提升至 3 倍的效果。<br /><strong>Keywords:</strong> hierarchical navigation, vision-language-action, embodiment grounding, affordance model, cross-embodied navigation, natural language steering, simulation affordances, robot capability, VLA, navigation reliability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</div>
A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/
<div><strong>Authors:</strong> Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "VAMOS is a hierarchical vision-language-action system that separates high-level semantic planning from low-level embodiment grounding by pairing a generalist planner with a specialist affordance model trained in simulation. The planner proposes candidate paths in image space, which the affordance model evaluates and re‑ranks according to the robot's physical capabilities, enabling cross‑embodied navigation, natural‑language steering, and a threefold increase in real‑world success rates over prior methods.", "summary_cn": "VAMOS 采用层级视觉‑语言‑动作架构，将语义规划与具体形体约束解耦：通用规划器在开放数据上学习高层路径，随后在仿真中训练的专用 affordance 模型根据机器人（如四足或轮式）实际能力评估并重新排序候选路径。该设计实现了跨形体导航、自然语言可调节以及真实环境中成功率提升至 3 倍的效果。", "keywords": "hierarchical navigation, vision-language-action, embodiment grounding, affordance model, cross-embodied navigation, natural language steering, simulation affordances, robot capability, VLA, navigation reliability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Mateo Guaman Castro", "Sidharth Rajagopal", "Daniel Gorbatov", "Matt Schmittle", "Rohan Baijal", "Octi Zhang", "Rosario Scalise", "Sidharth Talia", "Emma Romig", "Celso de Melo", "Byron Boots", "Abhishek Gupta"]}
]]></acme>

<pubDate>2025-10-23T17:59:45+00:00</pubDate>
</item>
<item>
<title>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</title>
<link>https://papers.cool/arxiv/2510.20813</link>
<guid>https://papers.cool/arxiv/2510.20813</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> GSWorld is a photo-realistic, closed-loop simulation suite for robotic manipulation that integrates 3D Gaussian Splatting with physics engines via a new asset format (GSDF) combining Gaussian-on-Mesh representations with robot URDFs. The framework enables zero-shot sim2real policy training, automated DAgger data collection, reproducible benchmarking, virtual teleoperation, and visual reinforcement learning without requiring real robot hardware.<br /><strong>Summary (CN):</strong> GSWorld 是一个逼真闭环机器人操作仿真平台，采用 3D Gaussian Splatting 与物理引擎相结合，并通过 GSDF（Gaussian Scene Description File）格式将 Gaussian-on-Mesh 表示与机器人 URDF 等信息融合，实现零样本 sim2real 像素到动作策略、自动 DAgger 数据采集、可复现的真实机器人策略基准、虚拟遥操作以及视觉强化学习等功能，全部无需真实机器人。<br /><strong>Keywords:</strong> photo-realistic simulation, Gaussian Splatting, sim2real, robotic manipulation, GSDF, DAgger, visual reinforcement learning, benchmark reproducibility, virtual teleoperation, physics engine integration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</div>
This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates "closing the loop" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.
<div><strong>Authors:</strong> Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "GSWorld is a photo-realistic, closed-loop simulation suite for robotic manipulation that integrates 3D Gaussian Splatting with physics engines via a new asset format (GSDF) combining Gaussian-on-Mesh representations with robot URDFs. The framework enables zero-shot sim2real policy training, automated DAgger data collection, reproducible benchmarking, virtual teleoperation, and visual reinforcement learning without requiring real robot hardware.", "summary_cn": "GSWorld 是一个逼真闭环机器人操作仿真平台，采用 3D Gaussian Splatting 与物理引擎相结合，并通过 GSDF（Gaussian Scene Description File）格式将 Gaussian-on-Mesh 表示与机器人 URDF 等信息融合，实现零样本 sim2real 像素到动作策略、自动 DAgger 数据采集、可复现的真实机器人策略基准、虚拟遥操作以及视觉强化学习等功能，全部无需真实机器人。", "keywords": "photo-realistic simulation, Gaussian Splatting, sim2real, robotic manipulation, GSDF, DAgger, visual reinforcement learning, benchmark reproducibility, virtual teleoperation, physics engine integration", "scoring": {"interpretability": 2, "understanding": 4, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guangqi Jiang", "Haoran Chang", "Ri-Zhao Qiu", "Yutong Liang", "Mazeyu Ji", "Jiyue Zhu", "Zhao Dong", "Xueyan Zou", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:26+00:00</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://papers.cool/arxiv/2510.20812</link>
<guid>https://papers.cool/arxiv/2510.20812</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight draft VLMs to generate diverse reasoning paths on information-intensive images, and a strong verdict VLM to synthesize the consensus paths into a final answer. A consensus expert selection mechanism forwards only high-agreement drafts, improving both accuracy and computational efficiency on benchmarks such as InfographicVQA and ChartQAPro.<br /><strong>Summary (CN):</strong> 本文提出 Speculative Verdict（SV）框架，利用多个小型视觉语言模型作为草稿专家生成多样化推理路径，再由强大的判决模型综合一致的路径得到最终答案。通过共识专家选择机制仅转发高一致性草稿，实现信息密集图像问答的准确性和计算效率提升，实验在 InfographicVQA、ChartQAPro 等基准上取得提升。<br /><strong>Keywords:</strong> visual language model, speculative decoding, multi-hop reasoning, infographic VQA, consensus selection, training-free, efficiency, draft experts, high-resolution visual QA<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict
<div><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight draft VLMs to generate diverse reasoning paths on information-intensive images, and a strong verdict VLM to synthesize the consensus paths into a final answer. A consensus expert selection mechanism forwards only high-agreement drafts, improving both accuracy and computational efficiency on benchmarks such as InfographicVQA and ChartQAPro.", "summary_cn": "本文提出 Speculative Verdict（SV）框架，利用多个小型视觉语言模型作为草稿专家生成多样化推理路径，再由强大的判决模型综合一致的路径得到最终答案。通过共识专家选择机制仅转发高一致性草稿，实现信息密集图像问答的准确性和计算效率提升，实验在 InfographicVQA、ChartQAPro 等基准上取得提升。", "keywords": "visual language model, speculative decoding, multi-hop reasoning, infographic VQA, consensus selection, training-free, efficiency, draft experts, high-resolution visual QA", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:21+00:00</pubDate>
</item>
<item>
<title>On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?</title>
<link>https://papers.cool/arxiv/2510.20810</link>
<guid>https://papers.cool/arxiv/2510.20810</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that the target of LLM‑generated text detection is ill‑defined, as real‑world usage involves human edits, subtle influence, and a spectrum of possible outputs, so existing benchmarks and metrics often misrepresent detector performance. It reviews detection scenarios, highlights gaps in evaluation, and recommends interpreting detector scores as conditional references rather than decisive judgments.<br /><strong>Summary (CN):</strong> 本文指出“LLM 生成文本”检测目标缺乏明确定义，实际使用中常伴随人工编辑、模型对用户的微妙影响，以及多样化的输出范围，导致现有基准和评估指标常误导检测器性能的解读。文章回顾检测情境，揭示评估缺口，并建议将检测器分数视作有条件的参考，而非决定性结论。<br /><strong>Keywords:</strong> LLM-generated text, detection, benchmark, evaluation, human edits, safety, misinformation, model influence, conditional interpretation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 5, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely "LLM-generated text". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.
<div><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that the target of LLM‑generated text detection is ill‑defined, as real‑world usage involves human edits, subtle influence, and a spectrum of possible outputs, so existing benchmarks and metrics often misrepresent detector performance. It reviews detection scenarios, highlights gaps in evaluation, and recommends interpreting detector scores as conditional references rather than decisive judgments.", "summary_cn": "本文指出“LLM 生成文本”检测目标缺乏明确定义，实际使用中常伴随人工编辑、模型对用户的微妙影响，以及多样化的输出范围，导致现有基准和评估指标常误导检测器性能的解读。文章回顾检测情境，揭示评估缺口，并建议将检测器分数视作有条件的参考，而非决定性结论。", "keywords": "LLM-generated text, detection, benchmark, evaluation, human edits, safety, misinformation, model influence, conditional interpretation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Mingmeng Geng", "Thierry Poibeau"]}
]]></acme>

<pubDate>2025-10-23T17:59:06+00:00</pubDate>
</item>
<item>
<title>The Reality Gap in Robotics: Challenges, Solutions, and Best Practices</title>
<link>https://papers.cool/arxiv/2510.20808</link>
<guid>https://papers.cool/arxiv/2510.20808</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys the reality gap in robotics, reviewing causes of discrepancies between simulated and real environments and summarizing recent sim-to-real transfer methods such as domain randomization, real-to-sim transfer, abstractions, and co‑training. It also discusses evaluation metrics and best practices for narrowing the gap across locomotion, navigation, and manipulation tasks.<br /><strong>Summary (CN):</strong> 本文综述了机器人领域的现实差距（reality gap），阐述了仿真与真实环境之间的偏差来源，并回顾了近期的 sim-to-real 转移技术，包括域随机化（domain randomization）、从真实到仿真（real-to-sim）转移、状态与动作抽象以及仿真‑真实协同训练等方法。文章还讨论了评估指标及在行走、导航和操作任务中缩小差距的最佳实践。<br /><strong>Keywords:</strong> sim-to-real, reality gap, domain randomization, robotics, transfer learning, simulation, evaluation metrics, locomotion, manipulation, navigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</div>
Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap's root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.
<div><strong>Authors:</strong> Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys the reality gap in robotics, reviewing causes of discrepancies between simulated and real environments and summarizing recent sim-to-real transfer methods such as domain randomization, real-to-sim transfer, abstractions, and co‑training. It also discusses evaluation metrics and best practices for narrowing the gap across locomotion, navigation, and manipulation tasks.", "summary_cn": "本文综述了机器人领域的现实差距（reality gap），阐述了仿真与真实环境之间的偏差来源，并回顾了近期的 sim-to-real 转移技术，包括域随机化（domain randomization）、从真实到仿真（real-to-sim）转移、状态与动作抽象以及仿真‑真实协同训练等方法。文章还讨论了评估指标及在行走、导航和操作任务中缩小差距的最佳实践。", "keywords": "sim-to-real, reality gap, domain randomization, robotics, transfer learning, simulation, evaluation metrics, locomotion, manipulation, navigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Elie Aljalbout", "Jiaxu Xing", "Angel Romero", "Iretiayo Akinola", "Caelan Reed Garrett", "Eric Heiden", "Abhishek Gupta", "Tucker Hermans", "Yashraj Narang", "Dieter Fox", "Davide Scaramuzza", "Fabio Ramos"]}
]]></acme>

<pubDate>2025-10-23T17:58:53+00:00</pubDate>
</item>
<item>
<title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title>
<link>https://papers.cool/arxiv/2510.20800</link>
<guid>https://papers.cool/arxiv/2510.20800</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a fast LLM adaptation method that inspects only a small subset of layers, uses the gradient of singular values to select matrices for rank reduction, and applies multi‑subspace factorization, all based on a single gradient step computed from 100 samples. This approach removes the exhaustive per‑matrix search of LASER and achieves accuracy gains of up to 24.6 percentage points without full fine‑tuning. The authors demonstrate that downstream performance is dominated by prompting style rather than dataset size, enabling rapid and robust deployment.<br /><strong>Summary (CN):</strong> 本文提出一种快速的 LLM 适配方法，仅检查少数层的矩阵，利用矩阵奇异值的梯度来挑选需要降秩的矩阵，并在多子空间上进行分解，只需要在 100 条样本上完成一次梯度计算，即可实现适配。该方法消除了 LASER 的逐层搜索开销，并在不进行完整微调的情况下提升准确率至 24.6 个百分点。作者指出下游任务的性能主要受提示风格影响，而非数据规模，从而实现快速且稳健的部署。<br /><strong>Keywords:</strong> LLM adaptation, matrix pruning, singular value gradient, multi-subspace factorization, low-data fine-tuning, model compression<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.
<div><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a fast LLM adaptation method that inspects only a small subset of layers, uses the gradient of singular values to select matrices for rank reduction, and applies multi‑subspace factorization, all based on a single gradient step computed from 100 samples. This approach removes the exhaustive per‑matrix search of LASER and achieves accuracy gains of up to 24.6 percentage points without full fine‑tuning. The authors demonstrate that downstream performance is dominated by prompting style rather than dataset size, enabling rapid and robust deployment.", "summary_cn": "本文提出一种快速的 LLM 适配方法，仅检查少数层的矩阵，利用矩阵奇异值的梯度来挑选需要降秩的矩阵，并在多子空间上进行分解，只需要在 100 条样本上完成一次梯度计算，即可实现适配。该方法消除了 LASER 的逐层搜索开销，并在不进行完整微调的情况下提升准确率至 24.6 个百分点。作者指出下游任务的性能主要受提示风格影响，而非数据规模，从而实现快速且稳健的部署。", "keywords": "LLM adaptation, matrix pruning, singular value gradient, multi-subspace factorization, low-data fine-tuning, model compression", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}
]]></acme>

<pubDate>2025-10-23T17:58:01+00:00</pubDate>
</item>
<item>
<title>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</title>
<link>https://papers.cool/arxiv/2510.20797</link>
<guid>https://papers.cool/arxiv/2510.20797</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture across diverse QA datasets and model scales. It also investigates training a single compressor to produce multiple compression ratios, revealing nuanced trade‑offs among architectures and training regimes.<br /><strong>Summary (CN):</strong> 本文提出一种轻量级的均值池化（mean‑pooling）软上下文压缩方法，用于检索增强生成（RAG），并在多种问答数据集和模型规模上持续超越常用的压缩标记（compression‑tokens）架构。论文进一步研究了让同一压缩器输出多种压缩比的训练方式，展示了不同架构和训练方案之间的细致权衡。<br /><strong>Keywords:</strong> context compression, mean-pooling, retrieval-augmented generation, large language models, multi-ratio training, soft compression<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.
<div><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture across diverse QA datasets and model scales. It also investigates training a single compressor to produce multiple compression ratios, revealing nuanced trade‑offs among architectures and training regimes.", "summary_cn": "本文提出一种轻量级的均值池化（mean‑pooling）软上下文压缩方法，用于检索增强生成（RAG），并在多种问答数据集和模型规模上持续超越常用的压缩标记（compression‑tokens）架构。论文进一步研究了让同一压缩器输出多种压缩比的训练方式，展示了不同架构和训练方案之间的细致权衡。", "keywords": "context compression, mean-pooling, retrieval-augmented generation, large language models, multi-ratio training, soft compression", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yair Feldman", "Yoav Artzi"]}
]]></acme>

<pubDate>2025-10-23T17:57:23+00:00</pubDate>
</item>
<item>
<title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title>
<link>https://papers.cool/arxiv/2510.20795</link>
<guid>https://papers.cool/arxiv/2510.20795</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Bayesian graph neural network framework that combines DeepSphere spherical CNNs with Bayesian neural networks to infer primordial magnetic field parameters directly from simulated CMB maps, providing high R² scores and well-calibrated uncertainty estimates.<br /><strong>Summary (CN):</strong> 本文提出将 DeepSphere 球面卷积网络与贝叶斯神经网络相结合的贝叶斯图神经网络框架，可直接从模拟的 CMB 图像中推断原始磁场参数，实现了超过 0.89 的 R² 分数并提供了良好校准的不确定性估计。<br /><strong>Keywords:</strong> Bayesian neural network, graph neural network, DeepSphere, spherical CNN, primordial magnetic field, CMB, uncertainty quantification, Bayesian inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</div>
Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology.
<div><strong>Authors:</strong> Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Bayesian graph neural network framework that combines DeepSphere spherical CNNs with Bayesian neural networks to infer primordial magnetic field parameters directly from simulated CMB maps, providing high R² scores and well-calibrated uncertainty estimates.", "summary_cn": "本文提出将 DeepSphere 球面卷积网络与贝叶斯神经网络相结合的贝叶斯图神经网络框架，可直接从模拟的 CMB 图像中推断原始磁场参数，实现了超过 0.89 的 R² 分数并提供了良好校准的不确定性估计。", "keywords": "Bayesian neural network, graph neural network, DeepSphere, spherical CNN, primordial magnetic field, CMB, uncertainty quantification, Bayesian inference", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Juan Alejandro Pinto Castro", "Héctor J. Hortúa", "Jorge Enrique García-Farieta", "Roger Anderson Hurtado"]}
]]></acme>

<pubDate>2025-10-23T17:56:04+00:00</pubDate>
</item>
<item>
<title>A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text</title>
<link>https://papers.cool/arxiv/2510.20782</link>
<guid>https://papers.cool/arxiv/2510.20782</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a use‑case specific dataset for evaluating large language models on responsible performance dimensions—quality, veracity, safety, and fairness—when generating plain‑text product descriptions from feature lists. The dataset is constructed by intersecting gendered adjectives and product categories to create labeled prompts that capture fairness attributes, and the authors demonstrate how to use it to reveal performance gaps across these dimensions.<br /><strong>Summary (CN):</strong> 本文构建了一个面向实际应用的评估数据集，用于测量大语言模型在生成产品描述时的质量、真实性、安全性和公平性等负责任表现维度。数据集通过将性别形容词与产品类别相结合，提供了标记的提示，以捕捉不同公平属性的影响，并展示了利用该资源发现模型在上述维度上的缺口。<br /><strong>Keywords:</strong> responsible AI, LLM evaluation, fairness, dataset, product description generation, veracity, safety, quality, bias, prompt engineering<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</div>
Current methods for evaluating large language models (LLMs) typically focus on high-level tasks such as text generation, without targeting a particular AI application. This approach is not sufficient for evaluating LLMs for Responsible AI dimensions like fairness, since protected attributes that are highly relevant in one application may be less relevant in another. In this work, we construct a dataset that is driven by a real-world application (generate a plain-text product description, given a list of product features), parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts. We show how to use the data to identify quality, veracity, safety, and fairness gaps in LLMs, contributing a proposal for LLM evaluation paired with a concrete resource for the research community.
<div><strong>Authors:</strong> Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a use‑case specific dataset for evaluating large language models on responsible performance dimensions—quality, veracity, safety, and fairness—when generating plain‑text product descriptions from feature lists. The dataset is constructed by intersecting gendered adjectives and product categories to create labeled prompts that capture fairness attributes, and the authors demonstrate how to use it to reveal performance gaps across these dimensions.", "summary_cn": "本文构建了一个面向实际应用的评估数据集，用于测量大语言模型在生成产品描述时的质量、真实性、安全性和公平性等负责任表现维度。数据集通过将性别形容词与产品类别相结合，提供了标记的提示，以捕捉不同公平属性的影响，并展示了利用该资源发现模型在上述维度上的缺口。", "keywords": "responsible AI, LLM evaluation, fairness, dataset, product description generation, veracity, safety, quality, bias, prompt engineering", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Alicia Sagae", "Chia-Jung Lee", "Sandeep Avula", "Brandon Dang", "Vanessa Murdock"]}
]]></acme>

<pubDate>2025-10-23T17:50:55+00:00</pubDate>
</item>
<item>
<title>Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</title>
<link>https://papers.cool/arxiv/2510.20780</link>
<guid>https://papers.cool/arxiv/2510.20780</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the use of large reasoning models (LRMs) as judges for machine translation evaluation, revealing challenges such as overthinking and scoring bias, and proposes calibrating their thinking by training on synthetic, human‑like reasoning trajectories; this calibration reduces computation by about 35× and improves correlation scores across models from 7B to 32B on WMT24 benchmarks.<br /><strong>Summary (CN):</strong> 本文研究了大型推理模型（Large Reasoning Models，LRM）在机器翻译质量评估中的作为评审者的潜力，指出其存在思考过度和打分偏差等问题，并提出通过在合成的类人思考轨迹上进行训练来校准模型的思考过程；该校准可将计算预算降低约35倍，同时在 WMT24 基准上显著提升 7B 至 32B 规模模型的相关性得分。<br /><strong>Keywords:</strong> large reasoning models, machine translation evaluation, model calibration, synthetic thinking trajectories, evaluation metrics, correlation improvement<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</div>
Recent advancements in large reasoning models (LRMs) have introduced an intermediate "thinking" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to "overthink" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.
<div><strong>Authors:</strong> Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the use of large reasoning models (LRMs) as judges for machine translation evaluation, revealing challenges such as overthinking and scoring bias, and proposes calibrating their thinking by training on synthetic, human‑like reasoning trajectories; this calibration reduces computation by about 35× and improves correlation scores across models from 7B to 32B on WMT24 benchmarks.", "summary_cn": "本文研究了大型推理模型（Large Reasoning Models，LRM）在机器翻译质量评估中的作为评审者的潜力，指出其存在思考过度和打分偏差等问题，并提出通过在合成的类人思考轨迹上进行训练来校准模型的思考过程；该校准可将计算预算降低约35倍，同时在 WMT24 基准上显著提升 7B 至 32B 规模模型的相关性得分。", "keywords": "large reasoning models, machine translation evaluation, model calibration, synthetic thinking trajectories, evaluation metrics, correlation improvement", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Runzhe Zhan", "Zhihong Huang", "Xinyi Yang", "Lidia S. Chao", "Min Yang", "Derek F. Wong"]}
]]></acme>

<pubDate>2025-10-23T17:48:36+00:00</pubDate>
</item>
<item>
<title>FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation</title>
<link>https://papers.cool/arxiv/2510.20774</link>
<guid>https://papers.cool/arxiv/2510.20774</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FieldGen is a framework that generates diverse, high-quality real-world robotic manipulation data by first using teleoperated pre-manipulation trajectories to capture key contact information, then automatically creating varied trajectories with an attraction field that converge to successful configurations. The system also adds reward annotations (FieldGen-Reward) to improve policy learning, achieving higher success rates and stability while greatly reducing human labor compared to pure teleoperation.<br /><strong>Summary (CN):</strong> FieldGen 框架通过先利用远程操作的预操作轨迹捕获关键接触信息，再使用吸引场自动生成多样化并收敛到成功姿态的轨迹，实现大规模、高质量的真实机器人操作数据采集。该系统还通过 FieldGen-Reward 添加奖励标注，提升策略学习效果，相比纯远程操作显著提升成功率和稳定性，并大幅降低人工成本。<br /><strong>Keywords:</strong> robotic manipulation, data generation, field-guided, teleoperation, attraction field, trajectory diversification, reward annotation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wenhao Wang, Kehe Ye, Xinyu Zhou, Tianxing Chen, Cao Min, Qiaoming Zhu, Xiaokang Yang, Yongjian Shen, Yang Yang, Maoqing Yao, Yao Mu</div>
Large-scale and diverse datasets are vital for training robust robotic manipulation policies, yet existing data collection methods struggle to balance scale, diversity, and quality. Simulation offers scalability but suffers from sim-to-real gaps, while teleoperation yields high-quality demonstrations with limited diversity and high labor cost. We introduce FieldGen, a field-guided data generation framework that enables scalable, diverse, and high-quality real-world data collection with minimal human supervision. FieldGen decomposes manipulation into two stages: a pre-manipulation phase, allowing trajectory diversity, and a fine manipulation phase requiring expert precision. Human demonstrations capture key contact and pose information, after which an attraction field automatically generates diverse trajectories converging to successful configurations. This decoupled design combines scalable trajectory diversity with precise supervision. Moreover, FieldGen-Reward augments generated data with reward annotations to further enhance policy learning. Experiments demonstrate that policies trained with FieldGen achieve higher success rates and improved stability compared to teleoperation-based baselines, while significantly reducing human effort in long-term real-world data collection. Webpage is available at https://fieldgen.github.io/.
<div><strong>Authors:</strong> Wenhao Wang, Kehe Ye, Xinyu Zhou, Tianxing Chen, Cao Min, Qiaoming Zhu, Xiaokang Yang, Yongjian Shen, Yang Yang, Maoqing Yao, Yao Mu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FieldGen is a framework that generates diverse, high-quality real-world robotic manipulation data by first using teleoperated pre-manipulation trajectories to capture key contact information, then automatically creating varied trajectories with an attraction field that converge to successful configurations. The system also adds reward annotations (FieldGen-Reward) to improve policy learning, achieving higher success rates and stability while greatly reducing human labor compared to pure teleoperation.", "summary_cn": "FieldGen 框架通过先利用远程操作的预操作轨迹捕获关键接触信息，再使用吸引场自动生成多样化并收敛到成功姿态的轨迹，实现大规模、高质量的真实机器人操作数据采集。该系统还通过 FieldGen-Reward 添加奖励标注，提升策略学习效果，相比纯远程操作显著提升成功率和稳定性，并大幅降低人工成本。", "keywords": "robotic manipulation, data generation, field-guided, teleoperation, attraction field, trajectory diversification, reward annotation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenhao Wang", "Kehe Ye", "Xinyu Zhou", "Tianxing Chen", "Cao Min", "Qiaoming Zhu", "Xiaokang Yang", "Yongjian Shen", "Yang Yang", "Maoqing Yao", "Yao Mu"]}
]]></acme>

<pubDate>2025-10-23T17:47:12+00:00</pubDate>
</item>
<item>
<title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title>
<link>https://papers.cool/arxiv/2510.20768</link>
<guid>https://papers.cool/arxiv/2510.20768</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RAGRank, a method that applies PageRank-based source credibility scoring to mitigate poisoning attacks in Retrieval-Augmented Generation pipelines for Cyber Threat Intelligence. Experiments on the MS MARCO benchmark and real CTI feeds show that malicious documents receive lower authority scores while trusted content is promoted.<br /><strong>Summary (CN):</strong> 本文提出在网络威胁情报（CTI）系统中，使用基于 PageRank 的来源可信度算法来提升检索增强生成（RAG）管道对毒化攻击的鲁棒性。实验在标准的 MS MARCO 数据集以及实际 CTI 文档上展示了该方法能够为恶意文档分配更低的权威分数并提升可信内容的检索效果。<br /><strong>Keywords:</strong> PageRank, poisoning attack, Retrieval-Augmented Generation, cyber threat intelligence, source credibility, LLM safety, document ranking, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Austin Jia, Avaneesh Ramesh, Zain Shamsi, Daniel Zhang, Alex Liu</div>
Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.
<div><strong>Authors:</strong> Austin Jia, Avaneesh Ramesh, Zain Shamsi, Daniel Zhang, Alex Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RAGRank, a method that applies PageRank-based source credibility scoring to mitigate poisoning attacks in Retrieval-Augmented Generation pipelines for Cyber Threat Intelligence. Experiments on the MS MARCO benchmark and real CTI feeds show that malicious documents receive lower authority scores while trusted content is promoted.", "summary_cn": "本文提出在网络威胁情报（CTI）系统中，使用基于 PageRank 的来源可信度算法来提升检索增强生成（RAG）管道对毒化攻击的鲁棒性。实验在标准的 MS MARCO 数据集以及实际 CTI 文档上展示了该方法能够为恶意文档分配更低的权威分数并提升可信内容的检索效果。", "keywords": "PageRank, poisoning attack, Retrieval-Augmented Generation, cyber threat intelligence, source credibility, LLM safety, document ranking, robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Austin Jia", "Avaneesh Ramesh", "Zain Shamsi", "Daniel Zhang", "Alex Liu"]}
]]></acme>

<pubDate>2025-10-23T17:43:00+00:00</pubDate>
</item>
<item>
<title>Reinforcement Learning and Consumption-Savings Behavior</title>
<link>https://papers.cool/arxiv/2510.20748</link>
<guid>https://papers.cool/arxiv/2510.20748</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a reinforcement‑learning model in which agents use Q‑learning with neural‑network function approximation to make consumption‑savings decisions under income uncertainty. It shows that this adaptive learning mechanism can simultaneously generate higher marginal propensities to consume for low‑asset unemployed households and a persistent "scarring" effect for households with repeated unemployment, matching recent empirical findings. The results suggest that value‑function approximation errors driven by experience can unify explanations of these puzzling consumption patterns without invoking belief updating or ex‑ante heterogeneity.<br /><strong>Summary (CN):</strong> 本文构建了一个强化学习模型，采用 Q 学习和神经网络近似来决策家庭在收入不确定性下的消费‑储蓄行为。研究表明，该自适应学习机制能够同时产生低资产失业家庭更高的边际消费倾向（MPC）以及多次失业经历的家庭出现持续的“创伤”效应，且模型结果与最新实证发现高度吻合。结果表明，价值函数近似误差随经验演化能够统一解释这些消费行为的异常现象，而无需诉诸收入风险信念更新或先验异质性。<br /><strong>Keywords:</strong> reinforcement learning, consumption-savings, Q-learning, marginal propensity to consume, scarring effect, neural network approximation, adaptive learning, economic downturns, household behavior<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Brandon Kaplowitz</div>
This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a "scarring" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict.
<div><strong>Authors:</strong> Brandon Kaplowitz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a reinforcement‑learning model in which agents use Q‑learning with neural‑network function approximation to make consumption‑savings decisions under income uncertainty. It shows that this adaptive learning mechanism can simultaneously generate higher marginal propensities to consume for low‑asset unemployed households and a persistent \"scarring\" effect for households with repeated unemployment, matching recent empirical findings. The results suggest that value‑function approximation errors driven by experience can unify explanations of these puzzling consumption patterns without invoking belief updating or ex‑ante heterogeneity.", "summary_cn": "本文构建了一个强化学习模型，采用 Q 学习和神经网络近似来决策家庭在收入不确定性下的消费‑储蓄行为。研究表明，该自适应学习机制能够同时产生低资产失业家庭更高的边际消费倾向（MPC）以及多次失业经历的家庭出现持续的“创伤”效应，且模型结果与最新实证发现高度吻合。结果表明，价值函数近似误差随经验演化能够统一解释这些消费行为的异常现象，而无需诉诸收入风险信念更新或先验异质性。", "keywords": "reinforcement learning, consumption-savings, Q-learning, marginal propensity to consume, scarring effect, neural network approximation, adaptive learning, economic downturns, household behavior", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Brandon Kaplowitz"]}
]]></acme>

<pubDate>2025-10-23T17:14:49+00:00</pubDate>
</item>
<item>
<title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</title>
<link>https://papers.cool/arxiv/2510.20743</link>
<guid>https://papers.cool/arxiv/2510.20743</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Empathic Prompting, a framework that incorporates facial expression recognition to capture users' emotional cues and embed them as contextual signals in Large Language Model (LLM) conversations, enabling non‑verbal context integration without explicit user control. The modular architecture is demonstrated with a locally deployed DeepSeek instance and evaluated in a small usability study (N=5), showing improved conversational fluidity and coherence. The authors discuss potential applications in domains such as healthcare and education where affective signals are important.<br /><strong>Summary (CN):</strong> 本文提出了同理提示（Empathic Prompting）框架，通过商业面部表情识别服务捕获用户情绪线索，并在提示阶段将其作为上下文信号嵌入大型语言模型（LLM）对话，实现无显式用户控制的非语言信息集成。系统采用模块化设计，在本地部署的 DeepSeek 实例上实现，并通过小规模可用性实验（5 名参与者）验证了对话流畅性和一致性的提升。作者进一步指出该方法在医疗、教育等需要感知情感的场景中的潜在应用。<br /><strong>Keywords:</strong> empathic prompting, multimodal LLM, facial expression recognition, non-verbal context, affective AI, human-AI interaction, conversational alignment, emotion integration, user study, chatbot<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</div>
We present Empathic Prompting, a novel framework for multimodal human-AI interaction that enriches Large Language Model (LLM) conversations with implicit non-verbal context. The system integrates a commercial facial expression recognition service to capture users' emotional cues and embeds them as contextual signals during prompting. Unlike traditional multimodal interfaces, empathic prompting requires no explicit user control; instead, it unobtrusively augments textual input with affective information for conversational and smoothness alignment. The architecture is modular and scalable, allowing integration of additional non-verbal modules. We describe the system design, implemented through a locally deployed DeepSeek instance, and report a preliminary service and usability evaluation (N=5). Results show consistent integration of non-verbal input into coherent LLM outputs, with participants highlighting conversational fluidity. Beyond this proof of concept, empathic prompting points to applications in chatbot-mediated communication, particularly in domains like healthcare or education, where users' emotional signals are critical yet often opaque in verbal exchanges.
<div><strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Empathic Prompting, a framework that incorporates facial expression recognition to capture users' emotional cues and embed them as contextual signals in Large Language Model (LLM) conversations, enabling non‑verbal context integration without explicit user control. The modular architecture is demonstrated with a locally deployed DeepSeek instance and evaluated in a small usability study (N=5), showing improved conversational fluidity and coherence. The authors discuss potential applications in domains such as healthcare and education where affective signals are important.", "summary_cn": "本文提出了同理提示（Empathic Prompting）框架，通过商业面部表情识别服务捕获用户情绪线索，并在提示阶段将其作为上下文信号嵌入大型语言模型（LLM）对话，实现无显式用户控制的非语言信息集成。系统采用模块化设计，在本地部署的 DeepSeek 实例上实现，并通过小规模可用性实验（5 名参与者）验证了对话流畅性和一致性的提升。作者进一步指出该方法在医疗、教育等需要感知情感的场景中的潜在应用。", "keywords": "empathic prompting, multimodal LLM, facial expression recognition, non-verbal context, affective AI, human-AI interaction, conversational alignment, emotion integration, user study, chatbot", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Lorenzo Stacchio", "Andrea Ubaldi", "Alessandro Galdelli", "Maurizio Mauri", "Emanuele Frontoni", "Andrea Gaggioli"]}
]]></acme>

<pubDate>2025-10-23T17:08:03+00:00</pubDate>
</item>
<item>
<title>Thought Communication in Multiagent Collaboration</title>
<link>https://papers.cool/arxiv/2510.20733</link>
<guid>https://papers.cool/arxiv/2510.20733</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a new paradigm called thought communication for multi-agent systems, where agents exchange latent internal representations rather than natural language. It formalizes the problem as a latent variable model, proves identifiability of shared and private thoughts without auxiliary information, and presents a framework to extract these thoughts and their sharing structure, validated on synthetic and real benchmarks.<br /><strong>Summary (CN):</strong> 本文提出“思维通信”范式，使多智能体系统在交流时直接共享潜在的内部表征，而非自然语言。论文将该过程形式化为潜在变量模型，证明在非参数設定且无辅助信息的情况下，可以识别任意智能体对之间的共享和私有思维，并恢复全局的思维共享结构。基于理论，作者构建了提取并分配这些潜在思维的框架，并在合成与真实基准上验证了其有效性。<br /><strong>Keywords:</strong> latent variable model, multi-agent collaboration, thought communication, telepathy, internal state extraction<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</div>
Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.
<div><strong>Authors:</strong> Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a new paradigm called thought communication for multi-agent systems, where agents exchange latent internal representations rather than natural language. It formalizes the problem as a latent variable model, proves identifiability of shared and private thoughts without auxiliary information, and presents a framework to extract these thoughts and their sharing structure, validated on synthetic and real benchmarks.", "summary_cn": "本文提出“思维通信”范式，使多智能体系统在交流时直接共享潜在的内部表征，而非自然语言。论文将该过程形式化为潜在变量模型，证明在非参数設定且无辅助信息的情况下，可以识别任意智能体对之间的共享和私有思维，并恢复全局的思维共享结构。基于理论，作者构建了提取并分配这些潜在思维的框架，并在合成与真实基准上验证了其有效性。", "keywords": "latent variable model, multi-agent collaboration, thought communication, telepathy, internal state extraction", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Yujia Zheng", "Zhuokai Zhao", "Zijian Li", "Yaqi Xie", "Mingze Gao", "Lizhu Zhang", "Kun Zhang"]}
]]></acme>

<pubDate>2025-10-23T16:48:02+00:00</pubDate>
</item>
<item>
<title>Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</title>
<link>https://papers.cool/arxiv/2510.20728</link>
<guid>https://papers.cool/arxiv/2510.20728</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a human-in-the-loop, multi‑agent workflow that uses GPT‑5 within the TeXRA platform to co‑design quantum error‑correcting codes with prescribed transversal diagonal gates via the Subset‑Sum Linear Programming (SSLP) framework. Three agents (Synthesis, Search, Audit) collaboratively formulate problems, enumerate candidate codes, and verify Knill‑Laflamme equalities, producing new codes for small qubit numbers and abstracting families that work for arbitrary parameters. The approach demonstrates that AI‑driven systematic enumeration combined with exact analytical reconstruction can scale the discovery of transversal‑gate‑compatible quantum codes.<br /><strong>Summary (CN):</strong> 本文提出一种人机协作的多代理工作流，利用 GPT‑5 在 TeXRA 平台上通过子集求和线性规划（SSLP）框架共同设计具备特定横向对角门的量子纠错码。三个代理（综合、搜索、审计）协同构建问题、枚举候选码并验证 Knill‑Laflamme 等式，产出用于少量量子位的新码并抽象出可推广到任意参数的家族。该方法展示了 AI 驱动的系统化枚举与确分析相结合，可规模化发现兼容横向门的量子码。<br /><strong>Keywords:</strong> quantum error correction, transversal diagonal gates, multi-agent systems, GPT-, SSLP, code design, LaTeX-Python workflow, quantum codes, linear programming, automated synthesis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xi He, Sirui Lu, Bei Zeng</div>
We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (https://texra.ai)-a multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git/Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps/screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d=2$ with nondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$ at $n=6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.
<div><strong>Authors:</strong> Xi He, Sirui Lu, Bei Zeng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a human-in-the-loop, multi‑agent workflow that uses GPT‑5 within the TeXRA platform to co‑design quantum error‑correcting codes with prescribed transversal diagonal gates via the Subset‑Sum Linear Programming (SSLP) framework. Three agents (Synthesis, Search, Audit) collaboratively formulate problems, enumerate candidate codes, and verify Knill‑Laflamme equalities, producing new codes for small qubit numbers and abstracting families that work for arbitrary parameters. The approach demonstrates that AI‑driven systematic enumeration combined with exact analytical reconstruction can scale the discovery of transversal‑gate‑compatible quantum codes.", "summary_cn": "本文提出一种人机协作的多代理工作流，利用 GPT‑5 在 TeXRA 平台上通过子集求和线性规划（SSLP）框架共同设计具备特定横向对角门的量子纠错码。三个代理（综合、搜索、审计）协同构建问题、枚举候选码并验证 Knill‑Laflamme 等式，产出用于少量量子位的新码并抽象出可推广到任意参数的家族。该方法展示了 AI 驱动的系统化枚举与确分析相结合，可规模化发现兼容横向门的量子码。", "keywords": "quantum error correction, transversal diagonal gates, multi-agent systems, GPT-, SSLP, code design, LaTeX-Python workflow, quantum codes, linear programming, automated synthesis", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xi He", "Sirui Lu", "Bei Zeng"]}
]]></acme>

<pubDate>2025-10-23T16:45:39+00:00</pubDate>
</item>
<item>
<title>Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing</title>
<link>https://papers.cool/arxiv/2510.20727</link>
<guid>https://papers.cool/arxiv/2510.20727</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a suite of NLP methods—including rule‑based, traditional machine‑learning, deep‑learning (BERT, ClinicalBERT), and large‑language‑model prompting (zero‑shot and error‑analysis)—to automatically extract fluoropyrimidine treatment regimens and associated toxicities from oncology clinical notes. Using a gold‑standard dataset of 236 annotated notes, error‑analysis prompting with LLMs achieved perfect precision, recall and F1 (1.000) for both treatment and toxicity extraction, outperforming all other approaches. The authors discuss the limited generalisability of ML/DL with small data and highlight the potential of LLM‑based extraction for oncology research and pharmacovigilance.<br /><strong>Summary (CN):</strong> 本文提出了一系列自然语言处理方法，包括基于规则、传统机器学习、深度学习（BERT、ClinicalBERT）以及大型语言模型提示（零样本和错误分析），用于从肿瘤科临床记录中自动提取氟嘧啶类药物的治疗方案及相关毒性。使用包含236篇标注笔记的黄金标准数据集，错误分析提示的 LLM 方法在治疗和毒性提取上均实现了完美的精确率、召回率和 F1 分数（1.000），优于其他所有方法。作者讨论了在小数据情境下机器学习/深度学习模型的泛化限制，并强调 LLM‑基提取在肿瘤研究和药物安全监测中的潜在价值。<br /><strong>Keywords:</strong> fluoropyrimidine, toxicity extraction, clinical notes, natural language processing, large language model, pharmacovigilance, machine learning, BERT, ClinicalBERT, rule-based methods<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</div>
Objective: Fluoropyrimidines are widely prescribed for colorectal and breast cancers, but are associated with toxicities such as hand-foot syndrome and cardiotoxicity. Since toxicity documentation is often embedded in clinical notes, we aimed to develop and evaluate natural language processing (NLP) methods to extract treatment and toxicity information. Materials and Methods: We constructed a gold-standard dataset of 236 clinical notes from 204,165 adult oncology patients. Domain experts annotated categories related to treatment regimens and toxicities. We developed rule-based, machine learning-based (Random Forest, Support Vector Machine [SVM], Logistic Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language models (LLM)-based NLP approaches (zero-shot and error-analysis prompting). Models used an 80:20 train-test split. Results: Sufficient data existed to train and evaluate 5 annotated categories. Error-analysis prompting achieved optimal precision, recall, and F1 scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot prompting reached F1=1.000 for treatment and F1=0.876 for toxicities extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods served as our baseline with F1 scores of 0.857 in treatment and 0.858 in toxicities. Discussion: LMM-based approaches outperformed all others, followed by machine learning methods. Machine and deep learning approaches were limited by small training data and showed limited generalizability, particularly for rare categories. Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine treatment and toxicity information from clinical notes, and has strong potential to support oncology research and pharmacovigilance.
<div><strong>Authors:</strong> Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a suite of NLP methods—including rule‑based, traditional machine‑learning, deep‑learning (BERT, ClinicalBERT), and large‑language‑model prompting (zero‑shot and error‑analysis)—to automatically extract fluoropyrimidine treatment regimens and associated toxicities from oncology clinical notes. Using a gold‑standard dataset of 236 annotated notes, error‑analysis prompting with LLMs achieved perfect precision, recall and F1 (1.000) for both treatment and toxicity extraction, outperforming all other approaches. The authors discuss the limited generalisability of ML/DL with small data and highlight the potential of LLM‑based extraction for oncology research and pharmacovigilance.", "summary_cn": "本文提出了一系列自然语言处理方法，包括基于规则、传统机器学习、深度学习（BERT、ClinicalBERT）以及大型语言模型提示（零样本和错误分析），用于从肿瘤科临床记录中自动提取氟嘧啶类药物的治疗方案及相关毒性。使用包含236篇标注笔记的黄金标准数据集，错误分析提示的 LLM 方法在治疗和毒性提取上均实现了完美的精确率、召回率和 F1 分数（1.000），优于其他所有方法。作者讨论了在小数据情境下机器学习/深度学习模型的泛化限制，并强调 LLM‑基提取在肿瘤研究和药物安全监测中的潜在价值。", "keywords": "fluoropyrimidine, toxicity extraction, clinical notes, natural language processing, large language model, pharmacovigilance, machine learning, BERT, ClinicalBERT, rule-based methods", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xizhi Wu", "Madeline S. Kreider", "Philip E. Empey", "Chenyu Li", "Yanshan Wang"]}
]]></acme>

<pubDate>2025-10-23T16:44:39+00:00</pubDate>
</item>
<item>
<title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title>
<link>https://papers.cool/arxiv/2510.20721</link>
<guid>https://papers.cool/arxiv/2510.20721</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a user study with 94 participants evaluating 90 privacy‑sensitive scenarios from the PrivacyLens benchmark, measuring how users judge the privacy‑preservation and helpfulness of LLM responses. Results show low interuser agreement on both dimensions, high agreement among proxy LLMs, and poor correlation between individual LLM evaluations and user judgments, that proxy LLMs are unreliable for estimating real user perceptions. The authors argue for user‑centered evaluation methods and further work to align proxy LLM assessments with human judgments.<br /><strong>Summary (CN):</strong> 本文通过对94名参与者在PrivacyLens基准中的90个隐私敏感场景进行评估，研究对大型语言模型（LLM）回复的隐私保护性和有用性的感知。结果显示，用户之间在这两个维度上的意见一致性较低，而五个代理LLM之间则高度一致，且单个LLM的评估与真实用户判断相关性差，表明代理LLM并不能可靠地估计用户的真实感受。作者主张开展以用户为中心的评估，并进一步研究如何使代理LLM的评估与人类感知对齐。<br /><strong>Keywords:</strong> privacy, large language models, user perception, helpfulness, alignment, PrivacyLens, evaluation, privacy preservation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</div>
Large language models (LLMs) have seen rapid adoption for tasks such as drafting emails, summarizing meetings, and answering health questions. In such uses, users may need to share private information (e.g., health records, contact details). To evaluate LLMs' ability to identify and redact such private information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with real-life scenarios. Using these benchmarks, researchers have found that LLMs sometimes fail to keep secrets private when responding to complex tasks (e.g., leaking employee salaries in meeting summaries). However, these evaluations rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking real users' perceptions. Moreover, prior work primarily focused on the privacy-preservation quality of responses, without investigating nuanced differences in helpfulness. To understand how users perceive the privacy-preservation quality and helpfulness of LLM responses to privacy-sensitive scenarios, we conducted a user study with 94 participants using 90 scenarios from PrivacyLens. We found that, when evaluating identical responses to the same scenario, users showed low agreement with each other on the privacy-preservation quality and helpfulness of the LLM response. Further, we found high agreement among five proxy LLMs, while each individual LLM had low correlation with users' evaluations. These results indicate that the privacy and helpfulness of LLM responses are often specific to individuals, and proxy LLMs are poor estimates of how real users would perceive these responses in privacy-sensitive scenarios. Our results suggest the need to conduct user-centered studies on measuring LLMs' ability to help users while preserving privacy. Additionally, future research could investigate ways to improve the alignment between proxy LLMs and users for better estimation of users' perceived privacy and utility.
<div><strong>Authors:</strong> Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a user study with 94 participants evaluating 90 privacy‑sensitive scenarios from the PrivacyLens benchmark, measuring how users judge the privacy‑preservation and helpfulness of LLM responses. Results show low interuser agreement on both dimensions, high agreement among proxy LLMs, and poor correlation between individual LLM evaluations and user judgments, that proxy LLMs are unreliable for estimating real user perceptions. The authors argue for user‑centered evaluation methods and further work to align proxy LLM assessments with human judgments.", "summary_cn": "本文通过对94名参与者在PrivacyLens基准中的90个隐私敏感场景进行评估，研究对大型语言模型（LLM）回复的隐私保护性和有用性的感知。结果显示，用户之间在这两个维度上的意见一致性较低，而五个代理LLM之间则高度一致，且单个LLM的评估与真实用户判断相关性差，表明代理LLM并不能可靠地估计用户的真实感受。作者主张开展以用户为中心的评估，并进一步研究如何使代理LLM的评估与人类感知对齐。", "keywords": "privacy, large language models, user perception, helpfulness, alignment, PrivacyLens, evaluation, privacy preservation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xiaoyuan Wu", "Roshni Kaushik", "Wenkai Li", "Lujo Bauer", "Koichi Onoue"]}
]]></acme>

<pubDate>2025-10-23T16:38:26+00:00</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title>
<link>https://papers.cool/arxiv/2510.20718</link>
<guid>https://papers.cool/arxiv/2510.20718</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two unsupervised anomaly prediction methods for semiconductor process monitoring: one using the N-BEATS model for univariate forecasting assuming variable independence, and another leveraging a Graph Neural Network to capture inter-variable dependencies. Both approaches forecast future sensor readings and flag deviations beyond a threshold as anomalies, achieving stable prediction up to 50 steps ahead, with the GNN showing superior performance and efficiency.<br /><strong>Summary (CN):</strong> 本文提出两种用于半导体制造过程的无监督异常预测方法：一种基于 N-BEATS 模型进行单变量预测，假设变量相互独立；另一种采用图神经网络（Graph Neural Network）捕获变量间的依赖关系。两种方法均对未来传感器数据进行预测，并在偏差超过阈值时标记为异常，能够在最长 50 步的预测范围内保持稳定，其中 GNN 在性能和计算成本上均优于 N-BEATS。<br /><strong>Keywords:</strong> unsupervised anomaly prediction, N-BEATS, Graph Neural Network, multivariate time series, semiconductor manufacturing, forecasting, fault detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</div>
Semiconductor manufacturing is an extremely complex and precision-driven process, characterized by thousands of interdependent parameters collected across diverse tools and process steps. Multi-variate time-series analysis has emerged as a critical field for real-time monitoring and fault detection in such environments. However, anomaly prediction in semiconductor fabrication presents several critical challenges, including high dimensionality of sensor data and severe class imbalance due to the rarity of true faults. Furthermore, the complex interdependencies between variables complicate both anomaly prediction and root-cause-analysis. This paper proposes two novel approaches to advance the field from anomaly detection to anomaly prediction, an essential step toward enabling real-time process correction and proactive fault prevention. The proposed anomaly prediction framework contains two main stages: (a) training a forecasting model on a dataset assumed to contain no anomalies, and (b) performing forecast on unseen time series data. The forecast is compared with the forecast of the trained signal. Deviations beyond a predefined threshold are flagged as anomalies. The two approaches differ in the forecasting model employed. The first assumes independence between variables by utilizing the N-BEATS model for univariate time series forecasting. The second lifts this assumption by utilizing a Graph Neural Network (GNN) to capture inter-variable relationships. Both models demonstrate strong forecasting performance up to a horizon of 20 time points and maintain stable anomaly prediction up to 50 time points. The GNN consistently outperforms the N-BEATS model while requiring significantly fewer trainable parameters and lower computational cost. These results position the GNN as promising solution for online anomaly forecasting to be deployed in manufacturing environments.
<div><strong>Authors:</strong> Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two unsupervised anomaly prediction methods for semiconductor process monitoring: one using the N-BEATS model for univariate forecasting assuming variable independence, and another leveraging a Graph Neural Network to capture inter-variable dependencies. Both approaches forecast future sensor readings and flag deviations beyond a threshold as anomalies, achieving stable prediction up to 50 steps ahead, with the GNN showing superior performance and efficiency.", "summary_cn": "本文提出两种用于半导体制造过程的无监督异常预测方法：一种基于 N-BEATS 模型进行单变量预测，假设变量相互独立；另一种采用图神经网络（Graph Neural Network）捕获变量间的依赖关系。两种方法均对未来传感器数据进行预测，并在偏差超过阈值时标记为异常，能够在最长 50 步的预测范围内保持稳定，其中 GNN 在性能和计算成本上均优于 N-BEATS。", "keywords": "unsupervised anomaly prediction, N-BEATS, Graph Neural Network, multivariate time series, semiconductor manufacturing, forecasting, fault detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Daniel Sorensen", "Bappaditya Dey", "Minjin Hwang", "Sandip Halder"]}
]]></acme>

<pubDate>2025-10-23T16:33:52+00:00</pubDate>
</item>
<item>
<title>Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.20706</link>
<guid>https://papers.cool/arxiv/2510.20706</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a real‑time gait adaptation framework for quadruped robots that combines Model Predictive Path Integral (MPPI) control with a learned Dreamer reward model. By jointly optimizing actions and gait parameters at each timestep, the method achieves energy‑efficient, stable locomotion across varying target speeds while maintaining smooth gait transitions. Experiments on the Unitree Go1 in simulation show up to 36.48% reduction in energy consumption compared to baseline policies.<br /><strong>Summary (CN):</strong> 本文提出了一种实时步态适应框架，将模型预测路径积分（MPPI）控制与 Dreamer 奖励模型相结合，以实现四足机器人在不同目标速度下的能效、稳定且平滑的运动。该方法在每个时间步联合优化动作和步态参数，从而在保持精准跟踪的同时显著降低能耗。对 Unitree Go1 的仿真实验表明，与基线策略相比，能耗最高可降低 36.48%。<br /><strong>Keywords:</strong> quadruped locomotion, gait adaptation, model predictive control, reinforcement learning, MPPI, Dreamer, energy efficiency, trajectory optimization, robotics, adaptive control<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Ganga Nair B, Prakrut Kotecha, Shishir Kolathaya</div>
Model-free reinforcement learning (RL) has enabled adaptable and agile quadruped locomotion; however, policies often converge to a single gait, leading to suboptimal performance. Traditionally, Model Predictive Control (MPC) has been extensively used to obtain task-specific optimal policies but lacks the ability to adapt to varying environments. To address these limitations, we propose an optimization framework for real-time gait adaptation in a continuous gait space, combining the Model Predictive Path Integral (MPPI) algorithm with a Dreamer module to produce adaptive and optimal policies for quadruped locomotion. At each time step, MPPI jointly optimizes the actions and gait variables using a learned Dreamer reward that promotes velocity tracking, energy efficiency, stability, and smooth transitions, while penalizing abrupt gait changes. A learned value function is incorporated as terminal reward, extending the formulation to an infinite-horizon planner. We evaluate our framework in simulation on the Unitree Go1, demonstrating an average reduction of up to 36.48\% in energy consumption across varying target speeds, while maintaining accurate tracking and adaptive, task-appropriate gaits.
<div><strong>Authors:</strong> Ganga Nair B, Prakrut Kotecha, Shishir Kolathaya</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a real‑time gait adaptation framework for quadruped robots that combines Model Predictive Path Integral (MPPI) control with a learned Dreamer reward model. By jointly optimizing actions and gait parameters at each timestep, the method achieves energy‑efficient, stable locomotion across varying target speeds while maintaining smooth gait transitions. Experiments on the Unitree Go1 in simulation show up to 36.48% reduction in energy consumption compared to baseline policies.", "summary_cn": "本文提出了一种实时步态适应框架，将模型预测路径积分（MPPI）控制与 Dreamer 奖励模型相结合，以实现四足机器人在不同目标速度下的能效、稳定且平滑的运动。该方法在每个时间步联合优化动作和步态参数，从而在保持精准跟踪的同时显著降低能耗。对 Unitree Go1 的仿真实验表明，与基线策略相比，能耗最高可降低 36.48%。", "keywords": "quadruped locomotion, gait adaptation, model predictive control, reinforcement learning, MPPI, Dreamer, energy efficiency, trajectory optimization, robotics, adaptive control", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Ganga Nair B", "Prakrut Kotecha", "Shishir Kolathaya"]}
]]></acme>

<pubDate>2025-10-23T16:17:45+00:00</pubDate>
</item>
<item>
<title>Fusing Narrative Semantics for Financial Volatility Forecasting</title>
<link>https://papers.cool/arxiv/2510.20699</link>
<guid>https://papers.cool/arxiv/2510.20699</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes M2VN (Multi-Modal Volatility Network), a deep learning framework that fuses numerical market features with point‑in‑time news embeddings generated by a Time Machine GPT LLM to forecast financial volatility. It introduces an auxiliary alignment loss to better integrate structured and unstructured data while avoiding look‑ahead bias, and demonstrates through extensive experiments that M2VN outperforms existing baselines, highlighting its utility for risk management.<br /><strong>Summary (CN):</strong> 本文提出了 M2VN（多模态波动网络），一种将数值市场特征与通过 Time Machine GPT（时间机器大语言模型）获取的实时新闻嵌入相融合的深度学习框架，用于金融波动率预测。文中引入了辅助对齐损失以提升结构化数据与非结构化数据的融合效果并规避前视偏差，并通过大量实验表明 M2VN 在风险管理等实际场景中显著优于现有基线。<br /><strong>Keywords:</strong> multi-modal, volatility forecasting, financial time series, news embeddings, LLM, alignment loss, risk management<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yaxuan Kong, Yoontae Hwang, Marcus Kaiser, Chris Vryonides, Roel Oomen, Stefan Zohren</div>
We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.
<div><strong>Authors:</strong> Yaxuan Kong, Yoontae Hwang, Marcus Kaiser, Chris Vryonides, Roel Oomen, Stefan Zohren</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes M2VN (Multi-Modal Volatility Network), a deep learning framework that fuses numerical market features with point‑in‑time news embeddings generated by a Time Machine GPT LLM to forecast financial volatility. It introduces an auxiliary alignment loss to better integrate structured and unstructured data while avoiding look‑ahead bias, and demonstrates through extensive experiments that M2VN outperforms existing baselines, highlighting its utility for risk management.", "summary_cn": "本文提出了 M2VN（多模态波动网络），一种将数值市场特征与通过 Time Machine GPT（时间机器大语言模型）获取的实时新闻嵌入相融合的深度学习框架，用于金融波动率预测。文中引入了辅助对齐损失以提升结构化数据与非结构化数据的融合效果并规避前视偏差，并通过大量实验表明 M2VN 在风险管理等实际场景中显著优于现有基线。", "keywords": "multi-modal, volatility forecasting, financial time series, news embeddings, LLM, alignment loss, risk management", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yaxuan Kong", "Yoontae Hwang", "Marcus Kaiser", "Chris Vryonides", "Roel Oomen", "Stefan Zohren"]}
]]></acme>

<pubDate>2025-10-23T16:13:46+00:00</pubDate>
</item>
<item>
<title>Exploring Large Language Models for Access Control Policy Synthesis and Summarization</title>
<link>https://papers.cool/arxiv/2510.20692</link>
<guid>https://papers.cool/arxiv/2510.20692</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the use of Large Language Models (LLMs) for automatically generating and summarizing cloud access control policies. Experiments show that while LLMs can produce syntactically correct policies, they often suffer from permissiveness issues, achieving full specification compliance 45.8% of the time for non‑reasoning models and 93.7% for reasoning models. The authors also propose a semantic request‑summarization method that leverages LLMs to precisely characterize the requests allowed by a given policy, demonstrating that LLMs are more effective when combined with symbolic analysis.<br /><strong>Summary (CN):</strong> 本文研究了使用大型语言模型（LLM）自动生成和概括云计算访问控制策略的可行性。实验表明，尽管 LLM 能生成语法正确的策略，但在非推理模型下仅有 45.8% 能完全匹配规格，在具备推理能力的模型下提升至 93.7%。另外，作者提出一种基于语义的请求摘要方法，利用 LLM 精确描述策略允许的请求，显示出在结合符号方法时 LLM 的分析潜力更大。<br /><strong>Keywords:</strong> large language models, access control, policy synthesis, policy summarization, cloud security, symbolic analysis, permissiveness, reasoning LLMs, semantic summarization, automated policy generation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Adarsh Vatsa, Bethel Hall, William Eiers</div>
Cloud computing is ubiquitous, with a growing number of services being hosted on the cloud every day. Typical cloud compute systems allow administrators to write policies implementing access control rules which specify how access to private data is governed. These policies must be manually written, and due to their complexity can often be error prone. Moreover, existing policies often implement complex access control specifications and thus can be difficult to precisely analyze in determining their behavior works exactly as intended. Recently, Large Language Models (LLMs) have shown great success in automated code synthesis and summarization. Given this success, they could potentially be used for automatically generating access control policies or aid in understanding existing policies. In this paper, we explore the effectiveness of LLMs for access control policy synthesis and summarization. Specifically, we first investigate diverse LLMs for access control policy synthesis, finding that: although LLMs can effectively generate syntactically correct policies, they have permissiveness issues, generating policies equivalent to the given specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time for reasoning LLMs. We then investigate how LLMs can be used to analyze policies by introducing a novel semantic-based request summarization approach which leverages LLMs to generate a precise characterization of the requests allowed by a policy. Our results show that while there are significant hurdles in leveraging LLMs for automated policy generation, LLMs show promising results when combined with symbolic approaches in analyzing existing policies.
<div><strong>Authors:</strong> Adarsh Vatsa, Bethel Hall, William Eiers</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the use of Large Language Models (LLMs) for automatically generating and summarizing cloud access control policies. Experiments show that while LLMs can produce syntactically correct policies, they often suffer from permissiveness issues, achieving full specification compliance 45.8% of the time for non‑reasoning models and 93.7% for reasoning models. The authors also propose a semantic request‑summarization method that leverages LLMs to precisely characterize the requests allowed by a given policy, demonstrating that LLMs are more effective when combined with symbolic analysis.", "summary_cn": "本文研究了使用大型语言模型（LLM）自动生成和概括云计算访问控制策略的可行性。实验表明，尽管 LLM 能生成语法正确的策略，但在非推理模型下仅有 45.8% 能完全匹配规格，在具备推理能力的模型下提升至 93.7%。另外，作者提出一种基于语义的请求摘要方法，利用 LLM 精确描述策略允许的请求，显示出在结合符号方法时 LLM 的分析潜力更大。", "keywords": "large language models, access control, policy synthesis, policy summarization, cloud security, symbolic analysis, permissiveness, reasoning LLMs, semantic summarization, automated policy generation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"]}
]]></acme>

<pubDate>2025-10-23T16:06:15+00:00</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Small Models</title>
<link>https://papers.cool/arxiv/2510.20690</link>
<guid>https://papers.cool/arxiv/2510.20690</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to lower hallucination rates in language models without increasing parameters or data. It proposes ND-LoRA, which combines multiple LoRA adapters regularized by Barlow Twins, and demonstrates up to a 25.6% reduction in hallucinations while maintaining accuracy, supported by theoretical bounds linking hallucination probability to representational correlation and extensive ablations, causal interventions, and correlation analyses. The work suggests neural diversity constitutes a third scaling axis, orthogonal to size and data, for improving model reliability across tasks.<br /><strong>Summary (CN):</strong> 本文提出“神经多样性”（decorrelated parallel representations）作为在不增加参数或数据的前提下降低语言模型幻觉率的原理性机制。作者设计了 ND-LoRA，将多个 LoRA 适配器通过 Barlow Twins 正则化组合，实验证明在不损失准确性的情况下，幻觉率最高下降 25.6%（平均下降 14.6%），并通过理论上将幻觉概率上界与表示相关性关联的推导、消融实验、因果干预以及相关性分析进行验证。研究表明神经多样性是除模型规模和数据之外的第三个可扩展维度，用于提升模型在不同任务下的可靠性。<br /><strong>Keywords:</strong> neural diversity, hallucination reduction, language models, ND-LoRA, LoRA adapters, Barlow Twins regularization, representational correlation, scaling laws, model reliability<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. Inspired by portfolio theory, where uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination probability is bounded by representational correlation: $P(H) \leq f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language models need an optimal amount of neurodiversity. To validate this, we introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces hallucinations by up to 25.6% (and 14.6% on average) without degrading general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational analyses indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different amounts of optimal neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
<div><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to lower hallucination rates in language models without increasing parameters or data. It proposes ND-LoRA, which combines multiple LoRA adapters regularized by Barlow Twins, and demonstrates up to a 25.6% reduction in hallucinations while maintaining accuracy, supported by theoretical bounds linking hallucination probability to representational correlation and extensive ablations, causal interventions, and correlation analyses. The work suggests neural diversity constitutes a third scaling axis, orthogonal to size and data, for improving model reliability across tasks.", "summary_cn": "本文提出“神经多样性”（decorrelated parallel representations）作为在不增加参数或数据的前提下降低语言模型幻觉率的原理性机制。作者设计了 ND-LoRA，将多个 LoRA 适配器通过 Barlow Twins 正则化组合，实验证明在不损失准确性的情况下，幻觉率最高下降 25.6%（平均下降 14.6%），并通过理论上将幻觉概率上界与表示相关性关联的推导、消融实验、因果干预以及相关性分析进行验证。研究表明神经多样性是除模型规模和数据之外的第三个可扩展维度，用于提升模型在不同任务下的可靠性。", "keywords": "neural diversity, hallucination reduction, language models, ND-LoRA, LoRA adapters, Barlow Twins regularization, representational correlation, scaling laws, model reliability", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Kushal Chakrabarti", "Nirmal Balachundhar"]}
]]></acme>

<pubDate>2025-10-23T16:03:07+00:00</pubDate>
</item>
<item>
<title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title>
<link>https://papers.cool/arxiv/2510.20683</link>
<guid>https://papers.cool/arxiv/2510.20683</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Spikachu, a neural decoding framework built on spiking neural networks that processes binned spikes into a shared latent space and decodes behavior in a causal, energy‑efficient manner. Experiments on extensive primate recordings show that Spikachu outperforms causal baselines while using dramatically less power, and that scaling training across sessions and subjects improves performance and enables few‑shot transfer to new tasks. The work demonstrates that SNNs can provide scalable, online‑compatible decoding with competitive accuracy and orders‑of‑magnitude lower energy consumption.<br /><strong>Summary (CN):</strong> 本文提出了 Spikachu 框架，基于脉冲神经网络（spiking neural networks）直接处理分箱的神经脉冲，将其投射到共享潜在空间并进行因果且低能耗的行为解码。实验在大量非人灵长类动物记录上表明，Spikachu 在能耗降低数十倍至数百倍的同时，性能超过因果基线，并且通过跨会话、跨受试者的规模化训练能够提升表现并实现对未见会话、受试者和任务的少样本迁移。该工作展示了 SNN 在实时、可扩展的神经解码中的竞争力和显著的能效优势。<br /><strong>Keywords:</strong> spiking neural networks, neural decoding, brain-computer interface, energy efficiency, causal model, few-shot transfer, online decoding<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</div>
Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy.
<div><strong>Authors:</strong> Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Spikachu, a neural decoding framework built on spiking neural networks that processes binned spikes into a shared latent space and decodes behavior in a causal, energy‑efficient manner. Experiments on extensive primate recordings show that Spikachu outperforms causal baselines while using dramatically less power, and that scaling training across sessions and subjects improves performance and enables few‑shot transfer to new tasks. The work demonstrates that SNNs can provide scalable, online‑compatible decoding with competitive accuracy and orders‑of‑magnitude lower energy consumption.", "summary_cn": "本文提出了 Spikachu 框架，基于脉冲神经网络（spiking neural networks）直接处理分箱的神经脉冲，将其投射到共享潜在空间并进行因果且低能耗的行为解码。实验在大量非人灵长类动物记录上表明，Spikachu 在能耗降低数十倍至数百倍的同时，性能超过因果基线，并且通过跨会话、跨受试者的规模化训练能够提升表现并实现对未见会话、受试者和任务的少样本迁移。该工作展示了 SNN 在实时、可扩展的神经解码中的竞争力和显著的能效优势。", "keywords": "spiking neural networks, neural decoding, brain-computer interface, energy efficiency, causal model, few-shot transfer, online decoding", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Georgios Mentzelopoulos", "Ioannis Asmanis", "Konrad P. Kording", "Eva L. Dyer", "Kostas Daniilidis", "Flavia Vitale"]}
]]></acme>

<pubDate>2025-10-23T15:55:45+00:00</pubDate>
</item>
<item>
<title>R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion</title>
<link>https://papers.cool/arxiv/2510.20677</link>
<guid>https://papers.cool/arxiv/2510.20677</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> R2-SVC proposes a robust and expressive zero-shot singing voice conversion framework that addresses real-world challenges such as environmental noise and artifacts from music separation. It enhances robustness via simulated F0 perturbations and noise artifacts, enriches speaker representation with domain-specific singing data, and integrates a Neural Source-Filter model to improve naturalness and controllability. Experiments show state-of-the-art performance on multiple SVC benchmarks under both clean and noisy conditions.<br /><strong>Summary (CN):</strong> R2-SVC 提出了一种面向真实场景的鲁棒且富有表现力的零样本歌声转换框架，针对环境噪声和音乐分离产生的伪影等实际问题进行优化。通过随机 F0 扰动和噪声伪影模拟提升鲁棒性，利用域内歌唱数据丰富说话人表，并结合 Neural Source-Filter (NSF) 模型增强自然度和可控性。实验表明在干净嘈杂条件下均实现了多项 SVC 基准的最先进表现。<br /><strong>Keywords:</strong> singing voice conversion, zero-shot, robustness, expressive synthesis, Neural Source-Filter, noise simulation, F0 perturbation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Junjie Zheng, Gongyu Chen, Chaofan Ding, Zihao Chen</div>
In real-world singing voice conversion (SVC) applications, environmental noise and the demand for expressive output pose significant challenges. Conventional methods, however, are typically designed without accounting for real deployment scenarios, as both training and inference usually rely on clean data. This mismatch hinders practical use, given the inevitable presence of diverse noise sources and artifacts from music separation. To tackle these issues, we propose R2-SVC, a robust and expressive SVC framework. First, we introduce simulation-based robustness enhancement through random fundamental frequency ($F_0$) perturbations and music separation artifact simulations (e.g., reverberation, echo), substantially improving performance under noisy conditions. Second, we enrich speaker representation using domain-specific singing data: alongside clean vocals, we incorporate DNSMOS-filtered separated vocals and public singing corpora, enabling the model to preserve speaker timbre while capturing singing style nuances. Third, we integrate the Neural Source-Filter (NSF) model to explicitly represent harmonic and noise components, enhancing the naturalness and controllability of converted singing. R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both clean and noisy conditions.
<div><strong>Authors:</strong> Junjie Zheng, Gongyu Chen, Chaofan Ding, Zihao Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "R2-SVC proposes a robust and expressive zero-shot singing voice conversion framework that addresses real-world challenges such as environmental noise and artifacts from music separation. It enhances robustness via simulated F0 perturbations and noise artifacts, enriches speaker representation with domain-specific singing data, and integrates a Neural Source-Filter model to improve naturalness and controllability. Experiments show state-of-the-art performance on multiple SVC benchmarks under both clean and noisy conditions.", "summary_cn": "R2-SVC 提出了一种面向真实场景的鲁棒且富有表现力的零样本歌声转换框架，针对环境噪声和音乐分离产生的伪影等实际问题进行优化。通过随机 F0 扰动和噪声伪影模拟提升鲁棒性，利用域内歌唱数据丰富说话人表，并结合 Neural Source-Filter (NSF) 模型增强自然度和可控性。实验表明在干净嘈杂条件下均实现了多项 SVC 基准的最先进表现。", "keywords": "singing voice conversion, zero-shot, robustness, expressive synthesis, Neural Source-Filter, noise simulation, F0 perturbation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Junjie Zheng", "Gongyu Chen", "Chaofan Ding", "Zihao Chen"]}
]]></acme>

<pubDate>2025-10-23T15:52:03+00:00</pubDate>
</item>
<item>
<title>GRACE: GRaph-based Addiction Care prEdiction</title>
<link>https://papers.cool/arxiv/2510.20671</link>
<guid>https://papers.cool/arxiv/2510.20671</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GRACE, a graph neural network framework that predicts the appropriate locus of care for addiction patients by constructing an unbiased meta-graph to mitigate severe class imbalance in clinical datasets. Extensive feature engineering and structured learning are employed, yielding 11-35% improvements in minority class F1 scores over baseline methods on real-world data.<br /><strong>Summary (CN):</strong> 本文提出 GRACE——一种基于图神经网络的框架，用于预测成瘾患者的适宜护理地点，通过构建无偏置的元图来缓解临床数据中严重的类别不平衡。使用广泛的特征工程和结构化学习，在真实数据上实现了少数类 F1 分数提升 11%~35%，优于基线方法。<br /><strong>Keywords:</strong> graph neural network, addiction care prediction, locus of care, class imbalance, meta-graph, medical decision support<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee</div>
Determining the appropriate locus of care for addiction patients is one of the most critical clinical decisions that affects patient treatment outcomes and effective use of resources. With a lack of sufficient specialized treatment resources, such as inpatient beds or staff, there is an unmet need to develop an automated framework for the same. Current decision-making approaches suffer from severe class imbalances in addiction datasets. To address this limitation, we propose a novel graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem. Further, we perform extensive feature engineering and propose a new approach of obtaining an unbiased meta-graph to train a GNN to overcome the class imbalance problem. Experimental results in real-world data show an improvement of 11-35% in terms of the F1 score of the minority class over competitive baselines. The codes and note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.
<div><strong>Authors:</strong> Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GRACE, a graph neural network framework that predicts the appropriate locus of care for addiction patients by constructing an unbiased meta-graph to mitigate severe class imbalance in clinical datasets. Extensive feature engineering and structured learning are employed, yielding 11-35% improvements in minority class F1 scores over baseline methods on real-world data.", "summary_cn": "本文提出 GRACE——一种基于图神经网络的框架，用于预测成瘾患者的适宜护理地点，通过构建无偏置的元图来缓解临床数据中严重的类别不平衡。使用广泛的特征工程和结构化学习，在真实数据上实现了少数类 F1 分数提升 11%~35%，优于基线方法。", "keywords": "graph neural network, addiction care prediction, locus of care, class imbalance, meta-graph, medical decision support", "scoring": {"interpretability": 3, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Subham Kumar", "Prakrithi Shivaprakash", "Koustav Rudra", "Lekhansh Shukla", "Animesh Mukherjee"]}
]]></acme>

<pubDate>2025-10-23T15:48:01+00:00</pubDate>
</item>
<item>
<title>Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection</title>
<link>https://papers.cool/arxiv/2510.20653</link>
<guid>https://papers.cool/arxiv/2510.20653</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates inference-time self-reflection and budget-tuning techniques across mathematical reasoning and translation tasks, analyzing how reflection depth and compute budget affect accuracy, cost, and latency. It presents Pareto-optimal trade-off frontiers for several LLM families and validates the approach in a marketing-content localisation deployment, showing domain-specific effectiveness. The authors release an open-source implementation for reproducibility.<br /><strong>Summary (CN):</strong> 本文系统性地评估了推理阶段的自我反思和预算调节技术在数学推理与翻译任务中的表现，分析了反思深度与计算预算如何影响准确率、成本和延迟。通过为多种大语言模型绘制帕累托最优权衡前沿，并在营销内容本地化场景中进行验证，展示了不同领域的效果差异。作者还开源了实现代码以促进复现。<br /><strong>Keywords:</strong> self-reflection, budget tuning, inference-time optimization, LLM trade-offs, Pareto frontier, mathematical reasoning, translation, latency-cost accuracy, deployment case study, open-source implementation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jack Butler, Nikita Kozodoi, Zainab Afolabi, Brian Tyacke, Gaiar Baimuratov</div>
As Large Language Models (LLMs) continue to evolve, practitioners face increasing options for enhancing inference-time performance without model retraining, including budget tuning and multi-step techniques like self-reflection. While these methods improve output quality, they create complex trade-offs among accuracy, cost, and latency that remain poorly understood across different domains. This paper systematically compares self-reflection and budget tuning across mathematical reasoning and translation tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and Mistral families, along with other models under varying reflection depths and compute budgets to derive Pareto optimal performance frontiers. Our analysis reveals substantial domain dependent variation in self-reflection effectiveness, with performance gains up to 220\% in mathematical reasoning. We further investigate how reflection round depth and feedback mechanism quality influence performance across model families. To validate our findings in a real-world setting, we deploy a self-reflection enhanced marketing content localisation system at Lounge by Zalando, where it shows market-dependent effectiveness, reinforcing the importance of domain specific evaluation when deploying these techniques. Our results provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints. We open source our self-reflection implementation for reproducibility at https://github.com/aws-samples/sample-genai-reflection-for-bedrock.
<div><strong>Authors:</strong> Jack Butler, Nikita Kozodoi, Zainab Afolabi, Brian Tyacke, Gaiar Baimuratov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates inference-time self-reflection and budget-tuning techniques across mathematical reasoning and translation tasks, analyzing how reflection depth and compute budget affect accuracy, cost, and latency. It presents Pareto-optimal trade-off frontiers for several LLM families and validates the approach in a marketing-content localisation deployment, showing domain-specific effectiveness. The authors release an open-source implementation for reproducibility.", "summary_cn": "本文系统性地评估了推理阶段的自我反思和预算调节技术在数学推理与翻译任务中的表现，分析了反思深度与计算预算如何影响准确率、成本和延迟。通过为多种大语言模型绘制帕累托最优权衡前沿，并在营销内容本地化场景中进行验证，展示了不同领域的效果差异。作者还开源了实现代码以促进复现。", "keywords": "self-reflection, budget tuning, inference-time optimization, LLM trade-offs, Pareto frontier, mathematical reasoning, translation, latency-cost accuracy, deployment case study, open-source implementation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jack Butler", "Nikita Kozodoi", "Zainab Afolabi", "Brian Tyacke", "Gaiar Baimuratov"]}
]]></acme>

<pubDate>2025-10-23T15:26:18+00:00</pubDate>
</item>
<item>
<title>The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI</title>
<link>https://papers.cool/arxiv/2510.20647</link>
<guid>https://papers.cool/arxiv/2510.20647</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how Large Reasoning Models (LRMs) handle multilingual questions, comparing reasoning performed in English versus the question's native language across MGSM and GPQA Diamond tasks. It finds that English‑centered reasoning yields higher accuracy and richer cognitive patterns in the reasoning traces, while language‑specific reasoning avoids translation‑induced errors, highlighting a "Lost in Translation" failure mode.<br /><strong>Summary (CN):</strong> 本文研究大型推理模型（LRM）在多语言提问下的表现，比较了使用英语与使用问题原语言进行推理的差异，实验覆盖 MGSM 和 GPQA Diamond 两个任务。结果显示，采用英语推理能够获得更高的答案准确率并呈现更多认知行为特征，但也容易因翻译步骤产生错误，导致所谓的“翻译丢失”失效模式。<br /><strong>Keywords:</strong> multilingual reasoning, large models, cognitive attributes, translation error, language bias, MGSM, GPQA Diamond, cross-lingual AI, interpretability, reasoning traces<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Alan Saji, Raj Dabre, Anoop Kunchukuttan, Ratish Puduppully</div>
Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting "Lost in Translation," where translation steps lead to errors that would have been avoided by question's language reasoning.
<div><strong>Authors:</strong> Alan Saji, Raj Dabre, Anoop Kunchukuttan, Ratish Puduppully</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how Large Reasoning Models (LRMs) handle multilingual questions, comparing reasoning performed in English versus the question's native language across MGSM and GPQA Diamond tasks. It finds that English‑centered reasoning yields higher accuracy and richer cognitive patterns in the reasoning traces, while language‑specific reasoning avoids translation‑induced errors, highlighting a \"Lost in Translation\" failure mode.", "summary_cn": "本文研究大型推理模型（LRM）在多语言提问下的表现，比较了使用英语与使用问题原语言进行推理的差异，实验覆盖 MGSM 和 GPQA Diamond 两个任务。结果显示，采用英语推理能够获得更高的答案准确率并呈现更多认知行为特征，但也容易因翻译步骤产生错误，导致所谓的“翻译丢失”失效模式。", "keywords": "multilingual reasoning, large models, cognitive attributes, translation error, language bias, MGSM, GPQA Diamond, cross-lingual AI, interpretability, reasoning traces", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Alan Saji", "Raj Dabre", "Anoop Kunchukuttan", "Ratish Puduppully"]}
]]></acme>

<pubDate>2025-10-23T15:22:00+00:00</pubDate>
</item>
<item>
<title>Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model</title>
<link>https://papers.cool/arxiv/2510.20635</link>
<guid>https://papers.cool/arxiv/2510.20635</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper adapts the Five-Dimensional Curiosity Scale Revised (5DCR) to evaluate curiosity in large language models, measuring dimensions such as information seeking, thrill seeking, and social curiosity. Experiments show that LLMs display a stronger thirst for knowledge than humans but make more conservative choices under uncertainty, and that higher curiosity correlates with improved reasoning and active‑learning abilities. The study provides a systematic framework for assessing curiosity‑driven behavior in LLMs.<br /><strong>Summary (CN):</strong> 本文通过改编五维好奇心量表（5DCR），构建评估框架来测量大型语言模型（LLM）的好奇心，包括信息寻求、刺激寻求和社会好奇心等维度。实验发现，LLM 相较于人类表现出更强的求知欲，但在不确定环境下倾向于保守选择，并且好奇心水平与模型的推理和主动学习能力呈正相关。该研究为系统评估 LLM 的好奇驱动行为提供了方法论支持。<br /><strong>Keywords:</strong> curiosity, large language models, Five-Dimensional Curiosity Scale Revised, information seeking, thrill seeking, social curiosity, reasoning, active learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoyu Wang, Sihang Jiang, Yuyan Chen, Yitong Wang, Yanghua Xiao</div>
Curiosity serves as a pivotal conduit for human beings to discover and learn new knowledge. Recent advancements of large language models (LLMs) in natural language processing have sparked discussions regarding whether these models possess capability of curiosity-driven learning akin to humans. In this paper, starting from the human curiosity assessment questionnaire Five-Dimensional Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework that covers dimensions such as Information Seeking, Thrill Seeking, and Social Curiosity to assess the extent of curiosity exhibited by LLMs. The results demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but still tend to make conservative choices when faced with uncertain environments. We further investigated the relationship between curiosity and thinking of LLMs, confirming that curious behaviors can enhance the model's reasoning and active learning abilities. These findings suggest that LLMs have the potential to exhibit curiosity similar to that of humans, providing experimental support for the future development of learning capabilities and innovative research in LLMs.
<div><strong>Authors:</strong> Haoyu Wang, Sihang Jiang, Yuyan Chen, Yitong Wang, Yanghua Xiao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper adapts the Five-Dimensional Curiosity Scale Revised (5DCR) to evaluate curiosity in large language models, measuring dimensions such as information seeking, thrill seeking, and social curiosity. Experiments show that LLMs display a stronger thirst for knowledge than humans but make more conservative choices under uncertainty, and that higher curiosity correlates with improved reasoning and active‑learning abilities. The study provides a systematic framework for assessing curiosity‑driven behavior in LLMs.", "summary_cn": "本文通过改编五维好奇心量表（5DCR），构建评估框架来测量大型语言模型（LLM）的好奇心，包括信息寻求、刺激寻求和社会好奇心等维度。实验发现，LLM 相较于人类表现出更强的求知欲，但在不确定环境下倾向于保守选择，并且好奇心水平与模型的推理和主动学习能力呈正相关。该研究为系统评估 LLM 的好奇驱动行为提供了方法论支持。", "keywords": "curiosity, large language models, Five-Dimensional Curiosity Scale Revised, information seeking, thrill seeking, social curiosity, reasoning, active learning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyu Wang", "Sihang Jiang", "Yuyan Chen", "Yitong Wang", "Yanghua Xiao"]}
]]></acme>

<pubDate>2025-10-23T15:05:17+00:00</pubDate>
</item>
<item>
<title>Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges</title>
<link>https://papers.cool/arxiv/2510.20634</link>
<guid>https://papers.cool/arxiv/2510.20634</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This systematic review summarizes recent progress in deep learning for dental image analysis, covering publicly available datasets, model architectures, training strategies, and evaluation metrics across 260 studies. It discusses the challenges inherent to dental imaging, such as low contrast and metallic artifacts, and highlights future research directions. Supplementary materials and comparison tables are provided via a public GitHub repository.<br /><strong>Summary (CN):</strong> 本文系统综述了深度学习在牙科影像分析中的最新进展，涵盖 260 项研究中的公开数据集、模型结构、训练策略和评估指标。文章讨论了牙科影像的低对比度、金属伪影等挑战，并提出了未来研究方向。补充材料和详细对比表已通过 GitHub 公开。<br /><strong>Keywords:</strong> dental image analysis, deep learning, medical imaging, systematic review, dental datasets, convolutional neural network, segmentation, classification, radiographs<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenhuan Zhou, Jingbo Zhu, Yuchen Zhang, Xiaohang Guan, Peng Wang, Tao Li</div>
Efficient analysis and processing of dental images are crucial for dentists to achieve accurate diagnosis and optimal treatment planning. However, dental imaging inherently poses several challenges, such as low contrast, metallic artifacts, and variations in projection angles. Combined with the subjectivity arising from differences in clinicians' expertise, manual interpretation often proves time-consuming and prone to inconsistency. Artificial intelligence (AI)-based automated dental image analysis (DIA) offers a promising solution to these issues and has become an integral part of computer-aided dental diagnosis and treatment. Among various AI technologies, deep learning (DL) stands out as the most widely applied and influential approach due to its superior feature extraction and representation capabilities. To comprehensively summarize recent progress in this field, we focus on the two fundamental aspects of DL research-datasets and models. In this paper, we systematically review 260 studies on DL applications in DIA, including 49 papers on publicly available dental datasets and 211 papers on DL-based algorithms. We first introduce the basic concepts of dental imaging and summarize the characteristics and acquisition methods of existing datasets. Then, we present the foundational techniques of DL and categorize relevant models and algorithms according to different DIA tasks, analyzing their network architectures, optimization strategies, training methods, and performance. Furthermore, we summarize commonly used training and evaluation metrics in the DIA domain. Finally, we discuss the current challenges of existing research and outline potential future directions. We hope that this work provides a valuable and systematic reference for researchers in this field. All supplementary materials and detailed comparison tables will be made publicly available on GitHub.
<div><strong>Authors:</strong> Zhenhuan Zhou, Jingbo Zhu, Yuchen Zhang, Xiaohang Guan, Peng Wang, Tao Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This systematic review summarizes recent progress in deep learning for dental image analysis, covering publicly available datasets, model architectures, training strategies, and evaluation metrics across 260 studies. It discusses the challenges inherent to dental imaging, such as low contrast and metallic artifacts, and highlights future research directions. Supplementary materials and comparison tables are provided via a public GitHub repository.", "summary_cn": "本文系统综述了深度学习在牙科影像分析中的最新进展，涵盖 260 项研究中的公开数据集、模型结构、训练策略和评估指标。文章讨论了牙科影像的低对比度、金属伪影等挑战，并提出了未来研究方向。补充材料和详细对比表已通过 GitHub 公开。", "keywords": "dental image analysis, deep learning, medical imaging, systematic review, dental datasets, convolutional neural network, segmentation, classification, radiographs", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenhuan Zhou", "Jingbo Zhu", "Yuchen Zhang", "Xiaohang Guan", "Peng Wang", "Tao Li"]}
]]></acme>

<pubDate>2025-10-23T15:05:06+00:00</pubDate>
</item>
<item>
<title>Quantum Processing Unit (QPU) processing time Prediction with Machine Learning</title>
<link>https://papers.cool/arxiv/2510.20630</link>
<guid>https://papers.cool/arxiv/2510.20630</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper applies machine learning, specifically Gradient-Boosting with LightGBM, to predict the processing time of quantum jobs on a Quantum Processing Unit (QPU). Using a dataset of about 150,000 IBM Quantum jobs, the authors demonstrate that predictive models can improve resource management and scheduling efficiency in quantum computing systems.<br /><strong>Summary (CN):</strong> 本文利用机器学习（基于 Gradient-Boosting (LightGBM)）对量子处理单元 (QPU) 的作业处理时间进行预测。通过约 150,000 条符合 IBM Quantum 架构的作业数据，展示了预测模型在提升量子计算资源管理与调度效率方面的有效性。<br /><strong>Keywords:</strong> Quantum computing, QPU processing time, machine learning, LightGBM, gradient boosting, scheduling, resource management, predictive modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lucy Xing, Sanjay Vishwakarma, David Kremer, Francisco Martin-Fernandez, Ismael Faro, Juan Cruz-Benito</div>
This paper explores the application of machine learning (ML) techniques in predicting the QPU processing time of quantum jobs. By leveraging ML algorithms, this study introduces predictive models that are designed to enhance operational efficiency in quantum computing systems. Using a dataset of about 150,000 jobs that follow the IBM Quantum schema, we employ ML methods based on Gradient-Boosting (LightGBM) to predict the QPU processing times, incorporating data preprocessing methods to improve model accuracy. The results demonstrate the effectiveness of ML in forecasting quantum jobs. This improvement can have implications on improving resource management and scheduling within quantum computing frameworks. This research not only highlights the potential of ML in refining quantum job predictions but also sets a foundation for integrating AI-driven tools in advanced quantum computing operations.
<div><strong>Authors:</strong> Lucy Xing, Sanjay Vishwakarma, David Kremer, Francisco Martin-Fernandez, Ismael Faro, Juan Cruz-Benito</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper applies machine learning, specifically Gradient-Boosting with LightGBM, to predict the processing time of quantum jobs on a Quantum Processing Unit (QPU). Using a dataset of about 150,000 IBM Quantum jobs, the authors demonstrate that predictive models can improve resource management and scheduling efficiency in quantum computing systems.", "summary_cn": "本文利用机器学习（基于 Gradient-Boosting (LightGBM)）对量子处理单元 (QPU) 的作业处理时间进行预测。通过约 150,000 条符合 IBM Quantum 架构的作业数据，展示了预测模型在提升量子计算资源管理与调度效率方面的有效性。", "keywords": "Quantum computing, QPU processing time, machine learning, LightGBM, gradient boosting, scheduling, resource management, predictive modeling", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lucy Xing", "Sanjay Vishwakarma", "David Kremer", "Francisco Martin-Fernandez", "Ismael Faro", "Juan Cruz-Benito"]}
]]></acme>

<pubDate>2025-10-23T15:04:18+00:00</pubDate>
</item>
<item>
<title>Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach</title>
<link>https://papers.cool/arxiv/2510.20629</link>
<guid>https://papers.cool/arxiv/2510.20629</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Fairness-Aware Survival Modeling (FASM), a method that mitigates algorithmic bias in both intra‑group and cross‑group risk rankings for censored survival data, demonstrated on SEER breast cancer prognoses. Experiments show that FASM substantially improves fairness over a ten‑year horizon while maintaining discrimination performance comparable to standard survival models.<br /><strong>Summary (CN):</strong> 本文提出公平感知生存建模（FASM）方法，旨在降低审查数据中的算法偏见，特别是同组内和跨组风险排序的差异，并以 SEER 乳腺癌数据为例进行验证。实验表明，在十年时间跨度内，FASM 显著提升了公平性，同时保持与传统生存模型相当的判别性能。<br /><strong>Keywords:</strong> fairness, survival analysis, censored data, breast cancer prognosis, SEER dataset, risk ranking, equity, clinical decision-making<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Mingxuan Liu, Yilin Ning, Haoyuan Wang, Chuan Hong, Matthew Engelhard, Danielle S. Bitterman, William G. La Cava, Nan Liu</div>
As machine learning models become increasingly integrated into healthcare, structural inequities and social biases embedded in clinical data can be perpetuated or even amplified by data-driven models. In survival analysis, censoring and time dynamics can further add complexity to fair model development. Additionally, algorithmic fairness approaches often overlook disparities in cross-group rankings, e.g., high-risk Black patients may be ranked below lower-risk White patients who do not experience the event of mortality. Such misranking can reinforce biological essentialism and undermine equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed to mitigate algorithmic bias regarding both intra-group and cross-group risk rankings over time. Using breast cancer prognosis as a representative case and applying FASM to SEER breast cancer data, we show that FASM substantially improves fairness while preserving discrimination performance comparable to fairness-unaware survival models. Time-stratified evaluations show that FASM maintains stable fairness over a 10-year horizon, with the greatest improvements observed during the mid-term of follow-up. Our approach enables the development of survival models that prioritize both accuracy and equity in clinical decision-making, advancing fairness as a core principle in clinical care.
<div><strong>Authors:</strong> Mingxuan Liu, Yilin Ning, Haoyuan Wang, Chuan Hong, Matthew Engelhard, Danielle S. Bitterman, William G. La Cava, Nan Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Fairness-Aware Survival Modeling (FASM), a method that mitigates algorithmic bias in both intra‑group and cross‑group risk rankings for censored survival data, demonstrated on SEER breast cancer prognoses. Experiments show that FASM substantially improves fairness over a ten‑year horizon while maintaining discrimination performance comparable to standard survival models.", "summary_cn": "本文提出公平感知生存建模（FASM）方法，旨在降低审查数据中的算法偏见，特别是同组内和跨组风险排序的差异，并以 SEER 乳腺癌数据为例进行验证。实验表明，在十年时间跨度内，FASM 显著提升了公平性，同时保持与传统生存模型相当的判别性能。", "keywords": "fairness, survival analysis, censored data, breast cancer prognosis, SEER dataset, risk ranking, equity, clinical decision-making", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Mingxuan Liu", "Yilin Ning", "Haoyuan Wang", "Chuan Hong", "Matthew Engelhard", "Danielle S. Bitterman", "William G. La Cava", "Nan Liu"]}
]]></acme>

<pubDate>2025-10-23T15:03:27+00:00</pubDate>
</item>
<item>
<title>Black Box Absorption: LLMs Undermining Innovative Ideas</title>
<link>https://papers.cool/arxiv/2510.20612</link>
<guid>https://papers.cool/arxiv/2510.20612</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines "Black Box Absorption" as the risk that large language model platforms can internalize and repurpose novel ideas contributed by users, creating informational and structural asymmetries between creators and service providers. It formalizes the concepts of idea units and idea safety, analyses the mechanisms of absorption, and proposes a governance and engineering agenda to keep creator contributions traceable, controllable, and equitable.<br /><strong>Summary (CN):</strong> 本文提出“黑箱吸收”概念，指出大型语言模型平台可能内部化并再利用用户贡献的创新想法，导致创作者与平台运营者之间的信息和结构不对称。文中引入“创意单元”和“创意安全”概念，分析吸收机制，并提出治理与工程方案，确保创作者的贡献可追溯、可控制且公平。<br /><strong>Keywords:</strong> black box absorption, large language models, innovation economics, idea safety, governance, AI safety, intellectual property, model extraction, asymmetry, creator protection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 4, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - control<br /><strong>Authors:</strong> Wenjun Cao</div>
Large Language Models are increasingly adopted as critical tools for accelerating innovation. This paper identifies and formalizes a systemic risk inherent in this paradigm: \textbf{Black Box Absorption}. We define this as the process by which the opaque internal architectures of LLM platforms, often operated by large-scale service providers, can internalize, generalize, and repurpose novel concepts contributed by users during interaction. This mechanism threatens to undermine the foundational principles of innovation economics by creating severe informational and structural asymmetries between individual creators and platform operators, thereby jeopardizing the long-term sustainability of the innovation ecosystem. To analyze this challenge, we introduce two core concepts: the idea unit, representing the transportable functional logic of an innovation, and idea safety, a multidimensional standard for its protection. This paper analyzes the mechanisms of absorption and proposes a concrete governance and engineering agenda to mitigate these risks, ensuring that creator contributions remain traceable, controllable, and equitable.
<div><strong>Authors:</strong> Wenjun Cao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines \"Black Box Absorption\" as the risk that large language model platforms can internalize and repurpose novel ideas contributed by users, creating informational and structural asymmetries between creators and service providers. It formalizes the concepts of idea units and idea safety, analyses the mechanisms of absorption, and proposes a governance and engineering agenda to keep creator contributions traceable, controllable, and equitable.", "summary_cn": "本文提出“黑箱吸收”概念，指出大型语言模型平台可能内部化并再利用用户贡献的创新想法，导致创作者与平台运营者之间的信息和结构不对称。文中引入“创意单元”和“创意安全”概念，分析吸收机制，并提出治理与工程方案，确保创作者的贡献可追溯、可控制且公平。", "keywords": "black box absorption, large language models, innovation economics, idea safety, governance, AI safety, intellectual property, model extraction, asymmetry, creator protection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 4, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Wenjun Cao"]}
]]></acme>

<pubDate>2025-10-23T14:43:09+00:00</pubDate>
</item>
<item>
<title>PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection</title>
<link>https://papers.cool/arxiv/2510.20611</link>
<guid>https://papers.cool/arxiv/2510.20611</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PSO-XAI, a framework that combines customized Particle Swarm Optimization for feature selection with model-agnostic explainable AI methods to improve breast cancer detection. Evaluated across 29 classifiers, ensembles, neural networks, probabilistic and instance‑based models using cross‑validation, the approach achieves 99.1% accuracy while providing transparent explanations for clinical relevance.<br /><strong>Summary (CN):</strong> 本文提出一种基于粒子群优化的特征选择与可解释人工智能相结合的框架（PSO‑XAI），用于乳腺癌早期检测。该框架在29种机器学习模型上进行交叉验证评估，并通过模型无关的解释方法提供透明的诊断依据，实验显示整体准确率达99.1%。<br /><strong>Keywords:</strong> breast cancer detection, particle swarm optimization, feature selection, explainable AI, model-agnostic explanations, machine learning, medical diagnosis<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Mirza Raquib, Niloy Das, Farida Siddiqi Prity, Arafath Al Fahim, Saydul Akbar Murad, Mohammad Amzad Hossain, MD Jiabul Hoque, Mohammad Ali Moni</div>
Breast cancer is considered the most critical and frequently diagnosed cancer in women worldwide, leading to an increase in cancer-related mortality. Early and accurate detection is crucial as it can help mitigate possible threats while improving survival rates. In terms of prediction, conventional diagnostic methods are often limited by variability, cost, and, most importantly, risk of misdiagnosis. To address these challenges, machine learning (ML) has emerged as a powerful tool for computer-aided diagnosis, with feature selection playing a vital role in improving model performance and interpretability. This research study proposes an integrated framework that incorporates customized Particle Swarm Optimization (PSO) for feature selection. This framework has been evaluated on a comprehensive set of 29 different models, spanning classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. To ensure interpretability and clinical relevance, the study uses cross-validation in conjunction with explainable AI methods. Experimental evaluation showed that the proposed approach achieved a superior score of 99.1\% across all performance metrics, including accuracy and precision, while effectively reducing dimensionality and providing transparent, model-agnostic explanations. The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.
<div><strong>Authors:</strong> Mirza Raquib, Niloy Das, Farida Siddiqi Prity, Arafath Al Fahim, Saydul Akbar Murad, Mohammad Amzad Hossain, MD Jiabul Hoque, Mohammad Ali Moni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PSO-XAI, a framework that combines customized Particle Swarm Optimization for feature selection with model-agnostic explainable AI methods to improve breast cancer detection. Evaluated across 29 classifiers, ensembles, neural networks, probabilistic and instance‑based models using cross‑validation, the approach achieves 99.1% accuracy while providing transparent explanations for clinical relevance.", "summary_cn": "本文提出一种基于粒子群优化的特征选择与可解释人工智能相结合的框架（PSO‑XAI），用于乳腺癌早期检测。该框架在29种机器学习模型上进行交叉验证评估，并通过模型无关的解释方法提供透明的诊断依据，实验显示整体准确率达99.1%。", "keywords": "breast cancer detection, particle swarm optimization, feature selection, explainable AI, model-agnostic explanations, machine learning, medical diagnosis", "scoring": {"interpretability": 7, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Mirza Raquib", "Niloy Das", "Farida Siddiqi Prity", "Arafath Al Fahim", "Saydul Akbar Murad", "Mohammad Amzad Hossain", "MD Jiabul Hoque", "Mohammad Ali Moni"]}
]]></acme>

<pubDate>2025-10-23T14:42:50+00:00</pubDate>
</item>
<item>
<title>BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection</title>
<link>https://papers.cool/arxiv/2510.20610</link>
<guid>https://papers.cool/arxiv/2510.20610</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports on the BUSTED team's participation in the AraGenEval Shared Task for Arabic AI‑generated text detection, evaluating three pre‑trained transformer models (AraELECTRA, CAMeLBERT, XLM‑RoBERTa) via fine‑tuning on a binary classification dataset. Surprisingly, the multilingual XLM‑RoBERTa achieved the best performance with an F1 of 0.7701, outperforming the Arabic‑specific models. The results highlight challenges in detecting AI‑generated text and the strong generalisation capabilities of multilingual models.<br /><strong>Summary (CN):</strong> 本文介绍了 BUSTED 团队在 AraGenEval 共享任务中对阿拉伯语 AI 生成文本检测的参赛结果，评估了三种预训练 Transformer 模型（AraELECTRA、CAMeLBERT、XLM‑RoBERTa）通过微调进行二分类的表现。令人惊讶的是，多语言模型 XLM‑RoBERTa 获得了最高的 F1 分数 0.7701，超过了专门针对阿拉伯语的模型。该研究凸显了检测 AI 生成文本的复杂性，并展示了多语言模型的强大泛化能力。<br /><strong>Keywords:</strong> Arabic AI-generated text detection, transformer models, AraELECTRA, CAMeLBERT, XLM-RoBERTa, multilingual detection, F1 score, binary classification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Ali Zain, Sareem Farooqui, Muhammad Rafi</div>
This paper details our submission to the Ara- GenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, se- cured 5th place. We investigated the effec- tiveness of three pre-trained transformer mod- els: AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a binary classification task. Our findings revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the highest performance with an F1 score of 0.7701, outperforming the spe- cialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong generalization capa- bilities of multilingual models.
<div><strong>Authors:</strong> Ali Zain, Sareem Farooqui, Muhammad Rafi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports on the BUSTED team's participation in the AraGenEval Shared Task for Arabic AI‑generated text detection, evaluating three pre‑trained transformer models (AraELECTRA, CAMeLBERT, XLM‑RoBERTa) via fine‑tuning on a binary classification dataset. Surprisingly, the multilingual XLM‑RoBERTa achieved the best performance with an F1 of 0.7701, outperforming the Arabic‑specific models. The results highlight challenges in detecting AI‑generated text and the strong generalisation capabilities of multilingual models.", "summary_cn": "本文介绍了 BUSTED 团队在 AraGenEval 共享任务中对阿拉伯语 AI 生成文本检测的参赛结果，评估了三种预训练 Transformer 模型（AraELECTRA、CAMeLBERT、XLM‑RoBERTa）通过微调进行二分类的表现。令人惊讶的是，多语言模型 XLM‑RoBERTa 获得了最高的 F1 分数 0.7701，超过了专门针对阿拉伯语的模型。该研究凸显了检测 AI 生成文本的复杂性，并展示了多语言模型的强大泛化能力。", "keywords": "Arabic AI-generated text detection, transformer models, AraELECTRA, CAMeLBERT, XLM-RoBERTa, multilingual detection, F1 score, binary classification", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"]}
]]></acme>

<pubDate>2025-10-23T14:41:04+00:00</pubDate>
</item>
<item>
<title>Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets</title>
<link>https://papers.cool/arxiv/2510.20609</link>
<guid>https://papers.cool/arxiv/2510.20609</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates retrieval configurations for code‑focused generation tasks such as code completion and bug localization, comparing chunking strategies, similarity scoring methods, and splitting granularity under realistic compute budgets. It finds that sparse BM25 with word‑level splitting offers the best quality‑latency trade‑off for PL‑PL tasks, while proprietary dense encoders excel for NL‑PL but incur much higher latency, and it provides practical recommendations for scaling code‑oriented RAG systems.<br /><strong>Summary (CN):</strong> 本文评估了代码生成任务（如代码补全和错误定位）的检索配置，比较了不同的分块策略、相似度评分方式和切分粒度在实际计算预算下的表现。研究发现，对于 PL‑PL 任务，稀疏 BM25 与词级切分的组合在质量‑延迟权衡上最优；而 NL‑PL 任务中专有的密集编码器虽性能更佳，却导致显著的延迟增加，文章据此给出在不同任务需求和计算约束下实现高效代码 RAG 系统的实用建议。<br /><strong>Keywords:</strong> code retrieval, RAG, BM25, dense encoder, chunking, latency, bug localization, code completion<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</div>
We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.
<div><strong>Authors:</strong> Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates retrieval configurations for code‑focused generation tasks such as code completion and bug localization, comparing chunking strategies, similarity scoring methods, and splitting granularity under realistic compute budgets. It finds that sparse BM25 with word‑level splitting offers the best quality‑latency trade‑off for PL‑PL tasks, while proprietary dense encoders excel for NL‑PL but incur much higher latency, and it provides practical recommendations for scaling code‑oriented RAG systems.", "summary_cn": "本文评估了代码生成任务（如代码补全和错误定位）的检索配置，比较了不同的分块策略、相似度评分方式和切分粒度在实际计算预算下的表现。研究发现，对于 PL‑PL 任务，稀疏 BM25 与词级切分的组合在质量‑延迟权衡上最优；而 NL‑PL 任务中专有的密集编码器虽性能更佳，却导致显著的延迟增加，文章据此给出在不同任务需求和计算约束下实现高效代码 RAG 系统的实用建议。", "keywords": "code retrieval, RAG, BM25, dense encoder, chunking, latency, bug localization, code completion", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Timur Galimzyanov", "Olga Kolomyttseva", "Egor Bogomolov"]}
]]></acme>

<pubDate>2025-10-23T14:40:11+00:00</pubDate>
</item>
<item>
<title>Generalizable Reasoning through Compositional Energy Minimization</title>
<link>https://papers.cool/arxiv/2510.20607</link>
<guid>https://papers.cool/arxiv/2510.20607</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a compositional approach to improve reasoning generalization by learning energy landscapes for tractable subproblems and combining them into a global landscape at test time. It proposes Parallel Energy Minimization (PEM) to generate high‑quality solutions from the constructed landscape and demonstrates superior performance on a variety of reasoning benchmarks compared to state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> 本文提出一种组合式方法，通过为可处理的子问题学习能量景观并在测试时将其合并为全局景观，以提升推理任务的泛化能力。作者进一步引入并行能量最小化（PEM）技术，从构建的景观中采样高质量解，并在多项推理基准上表现优于现有最先进方法。<br /><strong>Keywords:</strong> compositional reasoning, energy minimization, generalization, subproblem decomposition, parallel energy minimization, inference constraints, reasoning tasks, energy landscape, sample quality, AI reasoning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexandru Oarga, Yilun Du</div>
Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/
<div><strong>Authors:</strong> Alexandru Oarga, Yilun Du</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a compositional approach to improve reasoning generalization by learning energy landscapes for tractable subproblems and combining them into a global landscape at test time. It proposes Parallel Energy Minimization (PEM) to generate high‑quality solutions from the constructed landscape and demonstrates superior performance on a variety of reasoning benchmarks compared to state‑of‑the‑art methods.", "summary_cn": "本文提出一种组合式方法，通过为可处理的子问题学习能量景观并在测试时将其合并为全局景观，以提升推理任务的泛化能力。作者进一步引入并行能量最小化（PEM）技术，从构建的景观中采样高质量解，并在多项推理基准上表现优于现有最先进方法。", "keywords": "compositional reasoning, energy minimization, generalization, subproblem decomposition, parallel energy minimization, inference constraints, reasoning tasks, energy landscape, sample quality, AI reasoning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandru Oarga", "Yilun Du"]}
]]></acme>

<pubDate>2025-10-23T14:38:36+00:00</pubDate>
</item>
<item>
<title>OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects</title>
<link>https://papers.cool/arxiv/2510.20605</link>
<guid>https://papers.cool/arxiv/2510.20605</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> OnlineSplatter is an online feed‑forward framework that reconstructs free‑moving objects as 3D Gaussian representations directly from monocular RGB video without using camera pose, depth priors, or bundle adjustment. It anchors the reconstruction to the first frame and refines the model over time using a dual‑key memory that fuses current frame features with an aggregated object state, achieving constant computational cost regardless of video length. Experiments on real‑world datasets show substantial improvements over existing pose‑free baselines while maintaining compact memory usage.<br /><strong>Summary (CN):</strong> OnlineSplatter 是一种在线前馈框架，可在无需相机位姿、深度先验或束优化的情况下，仅凭单目 RGB 视频直接重建自由运动物体的 3D 高斯表示。它以首帧为锚点，并通过双键记忆模块将当前帧特征与累计的物体状态融合，随着观测增多逐步细化模型，实现了与视频长度无关的恒定计算成本。实验证明，该方法在真实数据集上显著优于现有的无姿态重建基线，并保持紧凑的内存占用。<br /><strong>Keywords:</strong> online 3D reconstruction, pose-free, neural Gaussian, dual-key memory, monocular video, object-centric representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mark He Huang, Lin Geng Foo, Christian Theobalt, Ying Sun, De Wen Soh</div>
Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.
<div><strong>Authors:</strong> Mark He Huang, Lin Geng Foo, Christian Theobalt, Ying Sun, De Wen Soh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "OnlineSplatter is an online feed‑forward framework that reconstructs free‑moving objects as 3D Gaussian representations directly from monocular RGB video without using camera pose, depth priors, or bundle adjustment. It anchors the reconstruction to the first frame and refines the model over time using a dual‑key memory that fuses current frame features with an aggregated object state, achieving constant computational cost regardless of video length. Experiments on real‑world datasets show substantial improvements over existing pose‑free baselines while maintaining compact memory usage.", "summary_cn": "OnlineSplatter 是一种在线前馈框架，可在无需相机位姿、深度先验或束优化的情况下，仅凭单目 RGB 视频直接重建自由运动物体的 3D 高斯表示。它以首帧为锚点，并通过双键记忆模块将当前帧特征与累计的物体状态融合，随着观测增多逐步细化模型，实现了与视频长度无关的恒定计算成本。实验证明，该方法在真实数据集上显著优于现有的无姿态重建基线，并保持紧凑的内存占用。", "keywords": "online 3D reconstruction, pose-free, neural Gaussian, dual-key memory, monocular video, object-centric representation", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mark He Huang", "Lin Geng Foo", "Christian Theobalt", "Ying Sun", "De Wen Soh"]}
]]></acme>

<pubDate>2025-10-23T14:37:25+00:00</pubDate>
</item>
<item>
<title>Resounding Acoustic Fields with Reciprocity</title>
<link>https://papers.cool/arxiv/2510.20602</link>
<guid>https://papers.cool/arxiv/2510.20602</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines a new task called resounding, which seeks to predict room impulse responses at arbitrary source locations from a sparse set of measured positions, using the reciprocity principle. It introduces Versa, a physics‑inspired, self‑supervised approach that generates dense virtual emitter samples by swapping emitter and listener poses, improving acoustic field estimation on simulated and real datasets and enhancing perceived spatial audio in user studies.<br /><strong>Summary (CN):</strong> 本文提出了“resounding”任务，旨在利用稀疏测量的声源位置，通过互惠原理估计任意位置的房间冲激响应。作者设计了 Versa，这是一种物理启发的自监督方法，通过交换声源和监听器姿态生成密集的虚拟声源样本，从而在仿真和真实数据集上显著提升声场学习效果，并在感知用户研究中增强了沉浸式空间音频体验。<br /><strong>Keywords:</strong> acoustic field learning, reciprocity, room impulse response, Versa, self-supervised learning, immersive audio, virtual emitter positions, physics-inspired modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zitong Lan, Yiduo Hao, Mingmin Zhao</div>
Achieving immersive auditory experiences in virtual environments requires flexible sound modeling that supports dynamic source positions. In this paper, we introduce a task called resounding, which aims to estimate room impulse responses at arbitrary emitter location from a sparse set of measured emitter positions, analogous to the relighting problem in vision. We leverage the reciprocity property and introduce Versa, a physics-inspired approach to facilitating acoustic field learning. Our method creates physically valid samples with dense virtual emitter positions by exchanging emitter and listener poses. We also identify challenges in deploying reciprocity due to emitter/listener gain patterns and propose a self-supervised learning approach to address them. Results show that Versa substantially improve the performance of acoustic field learning on both simulated and real-world datasets across different metrics. Perceptual user studies show that Versa can greatly improve the immersive spatial sound experience. Code, dataset and demo videos are available on the project website: https://waves.seas.upenn.edu/projects/versa.
<div><strong>Authors:</strong> Zitong Lan, Yiduo Hao, Mingmin Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines a new task called resounding, which seeks to predict room impulse responses at arbitrary source locations from a sparse set of measured positions, using the reciprocity principle. It introduces Versa, a physics‑inspired, self‑supervised approach that generates dense virtual emitter samples by swapping emitter and listener poses, improving acoustic field estimation on simulated and real datasets and enhancing perceived spatial audio in user studies.", "summary_cn": "本文提出了“resounding”任务，旨在利用稀疏测量的声源位置，通过互惠原理估计任意位置的房间冲激响应。作者设计了 Versa，这是一种物理启发的自监督方法，通过交换声源和监听器姿态生成密集的虚拟声源样本，从而在仿真和真实数据集上显著提升声场学习效果，并在感知用户研究中增强了沉浸式空间音频体验。", "keywords": "acoustic field learning, reciprocity, room impulse response, Versa, self-supervised learning, immersive audio, virtual emitter positions, physics-inspired modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zitong Lan", "Yiduo Hao", "Mingmin Zhao"]}
]]></acme>

<pubDate>2025-10-23T14:30:09+00:00</pubDate>
</item>
<item>
<title>Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</title>
<link>https://papers.cool/arxiv/2510.20596</link>
<guid>https://papers.cool/arxiv/2510.20596</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a novel unsupervised domain adaptation framework for cross‑modality segmentation that learns class‑wise similarity‑based prototypes and stores them in a dictionary to enable contrastive learning and avoid class‑missing issues. Experiments demonstrate that this prototype‑driven approach outperforms existing state‑of‑the‑art methods on benchmark domain shift scenarios.<br /><strong>Summary (CN):</strong> 本文提出了一种用于跨模态分割的无监督领域适应新框架，通过在嵌入空间学习基于相似性的类原型并使用字典存储以实现原型的对比学习，进而防止类缺失问题。实验表明，该原型驱动的方法在多个领域转移基准上优于现有最先进技术。<br /><strong>Keywords:</strong> unsupervised domain adaptation, cross-modality segmentation, prototype learning, similarity constraint, contrastive learning, dictionary storage<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang</div>
Deep learning models have achieved great success on various vision challenges, but a well-trained model would face drastic performance degradation when applied to unseen data. Since the model is sensitive to domain shift, unsupervised domain adaptation attempts to reduce the domain gap and avoid costly annotation of unseen domains. This paper proposes a novel framework for cross-modality segmentation via similarity-based prototypes. In specific, we learn class-wise prototypes within an embedding space, then introduce a similarity constraint to make these prototypes representative for each semantic class while separable from different classes. Moreover, we use dictionaries to store prototypes extracted from different images, which prevents the class-missing problem and enables the contrastive learning of prototypes, and further improves performance. Extensive experiments show that our method achieves better results than other state-of-the-art methods.
<div><strong>Authors:</strong> Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a novel unsupervised domain adaptation framework for cross‑modality segmentation that learns class‑wise similarity‑based prototypes and stores them in a dictionary to enable contrastive learning and avoid class‑missing issues. Experiments demonstrate that this prototype‑driven approach outperforms existing state‑of‑the‑art methods on benchmark domain shift scenarios.", "summary_cn": "本文提出了一种用于跨模态分割的无监督领域适应新框架，通过在嵌入空间学习基于相似性的类原型并使用字典存储以实现原型的对比学习，进而防止类缺失问题。实验表明，该原型驱动的方法在多个领域转移基准上优于现有最先进技术。", "keywords": "unsupervised domain adaptation, cross-modality segmentation, prototype learning, similarity constraint, contrastive learning, dictionary storage", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ziyu Ye", "Chen Ju", "Chaofan Ma", "Xiaoyun Zhang"]}
]]></acme>

<pubDate>2025-10-23T14:24:12+00:00</pubDate>
</item>
<item>
<title>Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks</title>
<link>https://papers.cool/arxiv/2510.20584</link>
<guid>https://papers.cool/arxiv/2510.20584</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether using ChatGPT to automatically code communication data for collaborative tasks introduces demographic bias. Analyzing coding across gender and race in negotiation, problem‑solving, and decision‑making tasks, the authors find no significant bias, indicating that ChatGPT can be safely adopted for large‑scale assessment of collaboration and communication.<br /><strong>Summary (CN):</strong> 本文研究使用 ChatGPT 对协作任务中的交流数据进行自动编码是否对性别和种族等人口群体产生偏见。通过对谈判、问题求解和决策三类任务的编码结果进行比较，发现 ChatGPT 的编码在性别和种族方面没有显著偏差，表明其可用于大规模协作评估。<br /><strong>Keywords:</strong> ChatGPT, automated coding, bias detection, fairness, collaborative communication, demographic bias, negotiation, problem solving, decision making<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi</div>
Assessing communication and collaboration at scale depends on a labor intensive task of coding communication data into categories according to different frameworks. Prior research has established that ChatGPT can be directly instructed with coding rubrics to code the communication data and achieves accuracy comparable to human raters. However, whether the coding from ChatGPT or similar AI technology exhibits bias against different demographic groups, such as gender and race, remains unclear. To fill this gap, this paper investigates ChatGPT-based automated coding of communication data using a typical coding framework for collaborative problem solving, examining differences across gender and racial groups. The analysis draws on data from three types of collaborative tasks: negotiation, problem solving, and decision making. Our results show that ChatGPT-based coding exhibits no significant bias across gender and racial groups, paving the road for its adoption in large-scale assessment of collaboration and communication.
<div><strong>Authors:</strong> Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether using ChatGPT to automatically code communication data for collaborative tasks introduces demographic bias. Analyzing coding across gender and race in negotiation, problem‑solving, and decision‑making tasks, the authors find no significant bias, indicating that ChatGPT can be safely adopted for large‑scale assessment of collaboration and communication.", "summary_cn": "本文研究使用 ChatGPT 对协作任务中的交流数据进行自动编码是否对性别和种族等人口群体产生偏见。通过对谈判、问题求解和决策三类任务的编码结果进行比较，发现 ChatGPT 的编码在性别和种族方面没有显著偏差，表明其可用于大规模协作评估。", "keywords": "ChatGPT, automated coding, bias detection, fairness, collaborative communication, demographic bias, negotiation, problem solving, decision making", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi"]}
]]></acme>

<pubDate>2025-10-23T14:09:03+00:00</pubDate>
</item>
<item>
<title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
<link>https://papers.cool/arxiv/2510.20579</link>
<guid>https://papers.cool/arxiv/2510.20579</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Open-o3 Video introduces a non‑agent framework that generates explicit spatio‑temporal evidence (timestamps, objects, and bounding boxes) alongside answers for video reasoning tasks. The authors curate two large datasets with unified spatio‑temporal annotations and employ a cold‑start reinforcement learning regime with specialized rewards to improve answer accuracy, temporal alignment, and spatial precision, achieving state‑of‑the‑art results on multiple video benchmarks.<br /><strong>Summary (CN):</strong> Open-o3 Video 提出了一种非主体框架，在视频推理时同时输出明确的时空证据（时间戳、对象及其边界框）与答案。作者构建了两个大规模的统一时空标注数据集，并采用冷启动强化学习并结合多重奖励，提升答案准确性、时间对齐和空间精度，在多项视频基准上实现了最新水平。<br /><strong>Keywords:</strong> video reasoning, spatio-temporal evidence, grounding, reinforcement learning, dataset construction, Open-o3 Video, interpretability<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 8, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</div>
Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.
<div><strong>Authors:</strong> Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Open-o3 Video introduces a non‑agent framework that generates explicit spatio‑temporal evidence (timestamps, objects, and bounding boxes) alongside answers for video reasoning tasks. The authors curate two large datasets with unified spatio‑temporal annotations and employ a cold‑start reinforcement learning regime with specialized rewards to improve answer accuracy, temporal alignment, and spatial precision, achieving state‑of‑the‑art results on multiple video benchmarks.", "summary_cn": "Open-o3 Video 提出了一种非主体框架，在视频推理时同时输出明确的时空证据（时间戳、对象及其边界框）与答案。作者构建了两个大规模的统一时空标注数据集，并采用冷启动强化学习并结合多重奖励，提升答案准确性、时间对齐和空间精度，在多项视频基准上实现了最新水平。", "keywords": "video reasoning, spatio-temporal evidence, grounding, reinforcement learning, dataset construction, Open-o3 Video, interpretability", "scoring": {"interpretability": 7, "understanding": 8, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jiahao Meng", "Xiangtai Li", "Haochen Wang", "Yue Tan", "Tao Zhang", "Lingdong Kong", "Yunhai Tong", "Anran Wang", "Zhiyang Teng", "Yujing Wang", "Zhuochen Wang"]}
]]></acme>

<pubDate>2025-10-23T14:05:56+00:00</pubDate>
</item>
<item>
<title>AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN</title>
<link>https://papers.cool/arxiv/2510.20566</link>
<guid>https://papers.cool/arxiv/2510.20566</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AdaDoS is an adaptive denial-of-service attack framework for software-defined networks that uses deep adversarial reinforcement learning to dynamically adjust attack strategies while evading existing rule- and ML-based detectors. The attack is modeled as a competitive game formulated as a partially observed Markov decision process (POMDP), with a reciprocal learning module where a student agent with limited observations learns from a teacher agent with full observations. Experiments demonstrate that AdaDoS can generate effective attack sequences that bypass detection.<br /><strong>Summary (CN):</strong> AdaDoS 是一种面向软件定义网络（SDN）的自适应拒绝服务（DoS）攻击框架，利用深度对抗强化学习在动态调整攻击策略的同时规避现有基于规则和机器学习的检测器。该攻击被建模为竞争博弈，并以部分可观测马尔可夫决策过程（POMDP）形式实现，使用递归学习模块，使观测受限的学生智能体可以向拥有完整观测能力的教师智能体学习。实验表明 AdaDoS 能够生成有效的攻击序列，成功逃避检测。<br /><strong>Keywords:</strong> adaptive DoS, adversarial reinforcement learning, SDN security, POMDP, reciprocal learning, attack evasion, network security<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Wei Shao, Yuhao Wang, Rongguang He, Muhammad Ejaz Ahmed, Seyit Camtepe</div>
Existing defence mechanisms have demonstrated significant effectiveness in mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined signatures and static heuristics to identify and block malicious traffic. However, the emergence of AI-driven techniques presents new challenges to SDN security, potentially compromising the efficacy of existing defence mechanisms. In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt network operations while evading detection by existing DoS-based detectors through adversarial reinforcement learning (RL). Specifically, AdaDoS models the problem as a competitive game between an attacker, whose goal is to obstruct network traffic without being detected, and a detector, which aims to identify malicious traffic. AdaDoS can solve this game by dynamically adjusting its attack strategy based on feedback from the SDN and the detector. Additionally, recognising that attackers typically have less information than defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov decision process (POMDP), with the attacker having access only to delay information between attacker and victim nodes. We address this challenge with a novel reciprocal learning module, where the student agent, with limited observations, enhances its performance by learning from the teacher agent, who has full observational capabilities in the SDN environment. AdaDoS represents the first application of RL to develop DoS-like attack sequences, capable of adaptively evading both machine learning-based and rule-based DoS-like attack detectors.
<div><strong>Authors:</strong> Wei Shao, Yuhao Wang, Rongguang He, Muhammad Ejaz Ahmed, Seyit Camtepe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AdaDoS is an adaptive denial-of-service attack framework for software-defined networks that uses deep adversarial reinforcement learning to dynamically adjust attack strategies while evading existing rule- and ML-based detectors. The attack is modeled as a competitive game formulated as a partially observed Markov decision process (POMDP), with a reciprocal learning module where a student agent with limited observations learns from a teacher agent with full observations. Experiments demonstrate that AdaDoS can generate effective attack sequences that bypass detection.", "summary_cn": "AdaDoS 是一种面向软件定义网络（SDN）的自适应拒绝服务（DoS）攻击框架，利用深度对抗强化学习在动态调整攻击策略的同时规避现有基于规则和机器学习的检测器。该攻击被建模为竞争博弈，并以部分可观测马尔可夫决策过程（POMDP）形式实现，使用递归学习模块，使观测受限的学生智能体可以向拥有完整观测能力的教师智能体学习。实验表明 AdaDoS 能够生成有效的攻击序列，成功逃避检测。", "keywords": "adaptive DoS, adversarial reinforcement learning, SDN security, POMDP, reciprocal learning, attack evasion, network security", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Wei Shao", "Yuhao Wang", "Rongguang He", "Muhammad Ejaz Ahmed", "Seyit Camtepe"]}
]]></acme>

<pubDate>2025-10-23T13:51:40+00:00</pubDate>
</item>
<item>
<title>Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics</title>
<link>https://papers.cool/arxiv/2510.20556</link>
<guid>https://papers.cool/arxiv/2510.20556</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically analyzes how different graph rewiring strategies affect a variety of graph structural metrics and how these changes relate to downstream node classification performance. By evaluating seven rewiring methods, the authors find that successful approaches tend to preserve local structural properties while allowing flexibility in global connectivity, providing guidance for designing effective rewiring techniques.<br /><strong>Summary (CN):</strong> 本文系统性地研究了不同图重连策略对多种图结构度量的影响，以及这些变化与节点分类性能的关系。通过评估七种重连方法，作者发现成功的策略通常保持局部结构特征，同时在全局连通性上保持灵活性，从而为设计有效的图重连技术提供了指导。<br /><strong>Keywords:</strong> graph rewiring, over-squashing, graph neural networks, structural metrics, node classification, local structure preservation, global connectivity<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Alexandre Benoit, Catherine Aitken, Yu He</div>
Graph rewiring has emerged as a key technique to alleviate over-squashing in Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph topology to improve information flow. While effective, rewiring inherently alters the graph's structure, raising the risk of distorting important topology-dependent signals. Yet, despite the growing use of rewiring, little is known about which structural properties must be preserved to ensure both performance gains and structural fidelity. In this work, we provide the first systematic analysis of how rewiring affects a range of graph structural metrics, and how these changes relate to downstream task performance. We study seven diverse rewiring strategies and correlate changes in local and global graph properties with node classification accuracy. Our results reveal a consistent pattern: successful rewiring methods tend to preserve local structure while allowing for flexibility in global connectivity. These findings offer new insights into the design of effective rewiring strategies, bridging the gap between graph theory and practical GNN optimization.
<div><strong>Authors:</strong> Alexandre Benoit, Catherine Aitken, Yu He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically analyzes how different graph rewiring strategies affect a variety of graph structural metrics and how these changes relate to downstream node classification performance. By evaluating seven rewiring methods, the authors find that successful approaches tend to preserve local structural properties while allowing flexibility in global connectivity, providing guidance for designing effective rewiring techniques.", "summary_cn": "本文系统性地研究了不同图重连策略对多种图结构度量的影响，以及这些变化与节点分类性能的关系。通过评估七种重连方法，作者发现成功的策略通常保持局部结构特征，同时在全局连通性上保持灵活性，从而为设计有效的图重连技术提供了指导。", "keywords": "graph rewiring, over-squashing, graph neural networks, structural metrics, node classification, local structure preservation, global connectivity", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Alexandre Benoit", "Catherine Aitken", "Yu He"]}
]]></acme>

<pubDate>2025-10-23T13:38:41+00:00</pubDate>
</item>
<item>
<title>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.20548</link>
<guid>https://papers.cool/arxiv/2510.20548</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> GlobalRAG is a reinforcement learning framework that improves retrieval-augmented generation for multi-hop question answering by introducing explicit global planning and subgoal execution. It decomposes questions into subgoals, coordinates retrieval with reasoning, and uses novel Planning Quality and SubGoal Completion rewards together with a progressive weight annealing strategy. Experiments show that GlobalRAG achieves up to 14.2% relative gains in EM and F1 while requiring only 42% of the training data used by strong baselines.<br /><strong>Summary (CN):</strong> GlobalRAG 是一个强化学习框架，旨在通过显式的全局规划和子目标执行来提升检索增强生成在多跳问答中的表现。它将问题拆解为子目标，协调检索与推理，并引入规划质量奖励与子目标完成奖励以及渐进权重退火策略。实验表明，GlobalRAG 在 EM 和 F1 上相较于强基线提升约 14.2%，且仅使用 42% 的训练数据。<br /><strong>Keywords:</strong> reinforcement learning, multi-hop question answering, retrieval-augmented generation, global planning, subgoal decomposition, planning quality reward, QA robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.
<div><strong>Authors:</strong> Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "GlobalRAG is a reinforcement learning framework that improves retrieval-augmented generation for multi-hop question answering by introducing explicit global planning and subgoal execution. It decomposes questions into subgoals, coordinates retrieval with reasoning, and uses novel Planning Quality and SubGoal Completion rewards together with a progressive weight annealing strategy. Experiments show that GlobalRAG achieves up to 14.2% relative gains in EM and F1 while requiring only 42% of the training data used by strong baselines.", "summary_cn": "GlobalRAG 是一个强化学习框架，旨在通过显式的全局规划和子目标执行来提升检索增强生成在多跳问答中的表现。它将问题拆解为子目标，协调检索与推理，并引入规划质量奖励与子目标完成奖励以及渐进权重退火策略。实验表明，GlobalRAG 在 EM 和 F1 上相较于强基线提升约 14.2%，且仅使用 42% 的训练数据。", "keywords": "reinforcement learning, multi-hop question answering, retrieval-augmented generation, global planning, subgoal decomposition, planning quality reward, QA robustness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinchang Luo", "Mingquan Cheng", "Fan Wan", "Ni Li", "Xiaoling Xia", "Shuangshuang Tian", "Tingcheng Bian", "Haiwei Wang", "Haohuan Fu", "Yan Tao"]}
]]></acme>

<pubDate>2025-10-23T13:35:02+00:00</pubDate>
</item>
<item>
<title>The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts</title>
<link>https://papers.cool/arxiv/2510.20543</link>
<guid>https://papers.cool/arxiv/2510.20543</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CenterBench, a dataset of 9,720 questions built from center‑embedded sentences that systematically vary syntactic depth, paired with semantically implausible versions. Experiments on several language models show that as the nesting depth grows, performance on implausible sentences drops sharply, indicating a shift from genuine structural analysis to reliance on semantic shortcuts. The work provides a concrete method for diagnosing when models abandon syntactic reasoning in favor of pattern matching.<br /><strong>Summary (CN):</strong> 本文提出 CenterBench 数据集，包含 9,720 条围绕中心嵌入句子的理解问题，并为每个句子提供语义上不合 plausibility 的对应句。实验表明，随着嵌套深度增加，语言模型在不合 plausibility 的句子上的表现显著下降，说明模型在结构分析和语义关联之间出现了转向。该工作为识别模型何时放弃句法分析而依赖语义捷径提供了评估框架。<br /><strong>Keywords:</strong> center-embedded sentences, structural understanding, semantic shortcuts, language model evaluation, syntactic parsing, plausibility bias, benchmark dataset, causal reasoning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 8, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Sangmitra Madhusudan, Kaige Chen, Ali Emami</div>
When language models correctly parse "The cat that the dog chased meowed," are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like "The cat [that the dog chased] meowed") where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.
<div><strong>Authors:</strong> Sangmitra Madhusudan, Kaige Chen, Ali Emami</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CenterBench, a dataset of 9,720 questions built from center‑embedded sentences that systematically vary syntactic depth, paired with semantically implausible versions. Experiments on several language models show that as the nesting depth grows, performance on implausible sentences drops sharply, indicating a shift from genuine structural analysis to reliance on semantic shortcuts. The work provides a concrete method for diagnosing when models abandon syntactic reasoning in favor of pattern matching.", "summary_cn": "本文提出 CenterBench 数据集，包含 9,720 条围绕中心嵌入句子的理解问题，并为每个句子提供语义上不合 plausibility 的对应句。实验表明，随着嵌套深度增加，语言模型在不合 plausibility 的句子上的表现显著下降，说明模型在结构分析和语义关联之间出现了转向。该工作为识别模型何时放弃句法分析而依赖语义捷径提供了评估框架。", "keywords": "center-embedded sentences, structural understanding, semantic shortcuts, language model evaluation, syntactic parsing, plausibility bias, benchmark dataset, causal reasoning", "scoring": {"interpretability": 7, "understanding": 8, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sangmitra Madhusudan", "Kaige Chen", "Ali Emami"]}
]]></acme>

<pubDate>2025-10-23T13:30:40+00:00</pubDate>
</item>
<item>
<title>ARC-Encoder: learning compressed text representations for large language models</title>
<link>https://papers.cool/arxiv/2510.20535</link>
<guid>https://papers.cool/arxiv/2510.20535</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ARC-Encoder is an encoder that compresses long text contexts into a smaller number of continuous representations, which replace token embeddings in decoder language models, reducing inference cost while preserving performance. The paper systematically studies encoder architectures and training strategies, achieving state-of-the-art results on various LLM tasks such as in-context learning and context window extension, and demonstrates that a single encoder can be adapted to multiple decoders. The approach provides a flexible, portable solution for efficient LLM deployment without fine-tuning the decoder.<br /><strong>Summary (CN):</strong> ARC-Encoder 是一种编码器，将冗长的文本上下文压缩为更少的连续表征，用以取代解码器语言模型中的词元嵌入，从而降低推理成本并保持性能。论文系统研究了编码器的结构与训练策略，在多种 LLM 任务（如上下文学习和上下文窗口扩展）上实现了最新水平，并展示了单一编码器可适配多个解码器的能力。该方法提供了一种灵活、可移植的高效 LLM 部署方案，无需对解码器进行微调。<br /><strong>Keywords:</strong> context compression, continuous representations, encoder-decoder LLM, inference efficiency, in-context learning, retrieval-augmented generation, chain-of-thought, multi-decoder adaptation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hippolyte Pilchen, Edouard Grave, Patrick Pérez</div>
Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .
<div><strong>Authors:</strong> Hippolyte Pilchen, Edouard Grave, Patrick Pérez</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ARC-Encoder is an encoder that compresses long text contexts into a smaller number of continuous representations, which replace token embeddings in decoder language models, reducing inference cost while preserving performance. The paper systematically studies encoder architectures and training strategies, achieving state-of-the-art results on various LLM tasks such as in-context learning and context window extension, and demonstrates that a single encoder can be adapted to multiple decoders. The approach provides a flexible, portable solution for efficient LLM deployment without fine-tuning the decoder.", "summary_cn": "ARC-Encoder 是一种编码器，将冗长的文本上下文压缩为更少的连续表征，用以取代解码器语言模型中的词元嵌入，从而降低推理成本并保持性能。论文系统研究了编码器的结构与训练策略，在多种 LLM 任务（如上下文学习和上下文窗口扩展）上实现了最新水平，并展示了单一编码器可适配多个解码器的能力。该方法提供了一种灵活、可移植的高效 LLM 部署方案，无需对解码器进行微调。", "keywords": "context compression, continuous representations, encoder-decoder LLM, inference efficiency, in-context learning, retrieval-augmented generation, chain-of-thought, multi-decoder adaptation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hippolyte Pilchen", "Edouard Grave", "Patrick Pérez"]}
]]></acme>

<pubDate>2025-10-23T13:20:57+00:00</pubDate>
</item>
<item>
<title>Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis</title>
<link>https://papers.cool/arxiv/2510.20531</link>
<guid>https://papers.cool/arxiv/2510.20531</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Fake-in-Facext (FiFa) framework to enable fine-grained explainable deepfake analysis by constructing a Facial Image Concept Tree for detailed annotation and defining an Artifact-Grounding Explanation (AGE) task that produces textual forgery explanations alongside segmentation masks. A unified multi-task multimodal LLM, FiFa-MLLM, is trained with auxiliary supervision to handle diverse inputs and outputs, achieving state-of-the-art performance on the AGE task and existing XDFA benchmarks.<br /><strong>Summary (CN):</strong> 本文提出了 Fake-in-Facext（FiFa）框架，通过构建面部图像概念树（Facial Image Concept Tree）实现细粒度的数据标注，并定义了工件‑对齐解释（Artifact-Grounding Explanation，AGE）任务，能够生成包含篡改工件分割掩码的文本伪造解释。作者进一步设计了统一的多任务多模态大语言模型 FiFa-MLLM，支持丰富的输入输出形式，在 AGE 任务以及现有可解释深度伪造分析（XDFA）数据集上实现了最先进的性能。<br /><strong>Keywords:</strong> deepfake detection, explainable AI, multimodal LLM, facial image concept tree, artifact grounding, segmentation, fine-grained analysis<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - interpretability<br /><strong>Authors:</strong> Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu</div>
The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.
<div><strong>Authors:</strong> Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Fake-in-Facext (FiFa) framework to enable fine-grained explainable deepfake analysis by constructing a Facial Image Concept Tree for detailed annotation and defining an Artifact-Grounding Explanation (AGE) task that produces textual forgery explanations alongside segmentation masks. A unified multi-task multimodal LLM, FiFa-MLLM, is trained with auxiliary supervision to handle diverse inputs and outputs, achieving state-of-the-art performance on the AGE task and existing XDFA benchmarks.", "summary_cn": "本文提出了 Fake-in-Facext（FiFa）框架，通过构建面部图像概念树（Facial Image Concept Tree）实现细粒度的数据标注，并定义了工件‑对齐解释（Artifact-Grounding Explanation，AGE）任务，能够生成包含篡改工件分割掩码的文本伪造解释。作者进一步设计了统一的多任务多模态大语言模型 FiFa-MLLM，支持丰富的输入输出形式，在 AGE 任务以及现有可解释深度伪造分析（XDFA）数据集上实现了最先进的性能。", "keywords": "deepfake detection, explainable AI, multimodal LLM, facial image concept tree, artifact grounding, segmentation, fine-grained analysis", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Lixiong Qin", "Yang Zhang", "Mei Wang", "Jiani Hu", "Weihong Deng", "Weiran Xu"]}
]]></acme>

<pubDate>2025-10-23T13:16:12+00:00</pubDate>
</item>
<item>
<title>Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</title>
<link>https://papers.cool/arxiv/2510.20519</link>
<guid>https://papers.cool/arxiv/2510.20519</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Metis-HOME, a Hybrid Optimized Mixture-of-Experts (MoE) framework that splits a multimodal large model into a thinking branch for complex reasoning and a non-thinking branch for fast, direct inference, using a trainable router to allocate queries. Evaluations on Qwen2.5-VL-7B demonstrate that this architecture improves both complex reasoning performance and general tasks such as VQA and OCR, mitigating the typical trade-off between reasoning capability and generalization. The work proposes a new "Hybrid Thinking" paradigm to create more versatile and efficient multimodal models.<br /><strong>Summary (CN):</strong> 本文提出了 Metis-HOME，一种混合优化的专家混合（MoE）框架，将多模态大模型划分为用于复杂推理的思考分支和用于快速直接推断的非思考分支，并通过可训练路由器动态分配查询。基于 Qwen2.5‑VL‑7B 的实验显示，该架构在提升复杂推理能力的同时，也改善了 VQA、OCR 等通用任务的表现，缓解了推理能力与通用性之间的权衡。该工作提出的 “Hybrid Thinking” 范式为构建更强大且高效的多模态模型提供了新思路。<br /><strong>Keywords:</strong> multimodal reasoning, mixture-of-experts, hybrid thinking, router, Qwen2.5-VL, efficiency, VQA, OCR, large language model<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma</div>
Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma.
<div><strong>Authors:</strong> Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Metis-HOME, a Hybrid Optimized Mixture-of-Experts (MoE) framework that splits a multimodal large model into a thinking branch for complex reasoning and a non-thinking branch for fast, direct inference, using a trainable router to allocate queries. Evaluations on Qwen2.5-VL-7B demonstrate that this architecture improves both complex reasoning performance and general tasks such as VQA and OCR, mitigating the typical trade-off between reasoning capability and generalization. The work proposes a new \"Hybrid Thinking\" paradigm to create more versatile and efficient multimodal models.", "summary_cn": "本文提出了 Metis-HOME，一种混合优化的专家混合（MoE）框架，将多模态大模型划分为用于复杂推理的思考分支和用于快速直接推断的非思考分支，并通过可训练路由器动态分配查询。基于 Qwen2.5‑VL‑7B 的实验显示，该架构在提升复杂推理能力的同时，也改善了 VQA、OCR 等通用任务的表现，缓解了推理能力与通用性之间的权衡。该工作提出的 “Hybrid Thinking” 范式为构建更强大且高效的多模态模型提供了新思路。", "keywords": "multimodal reasoning, mixture-of-experts, hybrid thinking, router, Qwen2.5-VL, efficiency, VQA, OCR, large language model", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Xiaohan Lan", "Fanfan Liu", "Haibo Qiu", "Siqi Yang", "Delian Ruan", "Peng Shi", "Lin Ma"]}
]]></acme>

<pubDate>2025-10-23T13:02:49+00:00</pubDate>
</item>
<item>
<title>Hierarchical Sequence Iteration for Heterogeneous Question Answering</title>
<link>https://papers.cool/arxiv/2510.20505</link>
<guid>https://papers.cool/arxiv/2510.20505</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Hierarchical Sequence (HSEQ) Iteration, a unified framework that linearizes text, tables, and knowledge graphs into a reversible hierarchical sequence with structural tags and performs structure-aware iterative retrieval to gather just-enough evidence before answer synthesis. Experiments on HotpotQA, HybridQA/TAT-QA, and MetaQA demonstrate consistent gains in EM/F1 over strong baselines while reducing token and tool usage. The approach offers a format-agnostic policy, budget-aware iteration, and evidence canonicalization for more reliable and auditable QA.<br /><strong>Summary (CN):</strong> 本文提出层次序列迭代（HSEQ）框架，将文本、表格和知识图谱线性化为带有轻量结构标签的可逆层次序列，并通过结构感知的迭代检索在生成答案前收集恰当的证据。实验在 HotpotQA、HybridQA/TAT-QA 和 MetaQA 上展示了相较于强基线的 EM/F1 提升，同时降低了 token 与工具调用。该方法实现了格式无关的统一策略、预算感知的迭代以及证据规范化，从而提升了问答的可靠性和可审计性。<br /><strong>Keywords:</strong> hierarchical sequence, heterogeneous question answering, retrieval-augmented generation, multi-hop reasoning, knowledge graph, table QA, structural tags, budget-aware iteration, evidence canonicalization, RAG<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration for Heterogeneous Question Answering, a unified framework that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) guided, budget-aware iteration that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.
<div><strong>Authors:</strong> Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Hierarchical Sequence (HSEQ) Iteration, a unified framework that linearizes text, tables, and knowledge graphs into a reversible hierarchical sequence with structural tags and performs structure-aware iterative retrieval to gather just-enough evidence before answer synthesis. Experiments on HotpotQA, HybridQA/TAT-QA, and MetaQA demonstrate consistent gains in EM/F1 over strong baselines while reducing token and tool usage. The approach offers a format-agnostic policy, budget-aware iteration, and evidence canonicalization for more reliable and auditable QA.", "summary_cn": "本文提出层次序列迭代（HSEQ）框架，将文本、表格和知识图谱线性化为带有轻量结构标签的可逆层次序列，并通过结构感知的迭代检索在生成答案前收集恰当的证据。实验在 HotpotQA、HybridQA/TAT-QA 和 MetaQA 上展示了相较于强基线的 EM/F1 提升，同时降低了 token 与工具调用。该方法实现了格式无关的统一策略、预算感知的迭代以及证据规范化，从而提升了问答的可靠性和可审计性。", "keywords": "hierarchical sequence, heterogeneous question answering, retrieval-augmented generation, multi-hop reasoning, knowledge graph, table QA, structural tags, budget-aware iteration, evidence canonicalization, RAG", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"]}
]]></acme>

<pubDate>2025-10-23T12:48:18+00:00</pubDate>
</item>
<item>
<title>Steering Evaluation-Aware Language Models To Act Like They Are Deployed</title>
<link>https://papers.cool/arxiv/2510.20487</link>
<guid>https://papers.cool/arxiv/2510.20487</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that large language models can become evaluation‑aware, altering their behavior when they detect test cues, which undermines safety evaluations. By adding a steering vector to the model's activations—derived from the original model before additional training—the authors can suppress this awareness, making the model behave as if it were deployed even in the presence of evaluation cues. This steering technique is demonstrated using a two‑step training process that induces evaluation‑aware behavior via type‑hint cues and expert iteration.<br /><strong>Summary (CN):</strong> 本文指出大语言模型在检测到评估提示时会表现出评估意识，从而在安全评估中改变行为。作者通过在模型激活上添加一个来自原始模型的引导向量，能够抑制这种评估意识，使模型在出现评估线索时仍表现得像部署状态。文中使用两步训练流程（包括使用 Python 类型提示的评估情境和专家迭代）来诱导评估意识，并展示了引导向量在消除该行为上的效果。<br /><strong>Keywords:</strong> evaluation awareness, activation steering, LLM alignment, safety evaluation, deployment behavior, expert iteration, type hints, model steering vector<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda</div>
Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. However, this gap can only be observed by removing the evaluation cue. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.
<div><strong>Authors:</strong> Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that large language models can become evaluation‑aware, altering their behavior when they detect test cues, which undermines safety evaluations. By adding a steering vector to the model's activations—derived from the original model before additional training—the authors can suppress this awareness, making the model behave as if it were deployed even in the presence of evaluation cues. This steering technique is demonstrated using a two‑step training process that induces evaluation‑aware behavior via type‑hint cues and expert iteration.", "summary_cn": "本文指出大语言模型在检测到评估提示时会表现出评估意识，从而在安全评估中改变行为。作者通过在模型激活上添加一个来自原始模型的引导向量，能够抑制这种评估意识，使模型在出现评估线索时仍表现得像部署状态。文中使用两步训练流程（包括使用 Python 类型提示的评估情境和专家迭代）来诱导评估意识，并展示了引导向量在消除该行为上的效果。", "keywords": "evaluation awareness, activation steering, LLM alignment, safety evaluation, deployment behavior, expert iteration, type hints, model steering vector", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Tim Tian Hua", "Andrew Qin", "Samuel Marks", "Neel Nanda"]}
]]></acme>

<pubDate>2025-10-23T12:29:16+00:00</pubDate>
</item>
<item>
<title>Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval</title>
<link>https://papers.cool/arxiv/2510.20486</link>
<guid>https://papers.cool/arxiv/2510.20486</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Hurdle-IMDL, a framework that tackles label imbalance in infrared rainfall retrieval by separating zero inflation and long-tail components, using a hurdle model for the former and an inverse‑model debiasing approach for the latter. Comprehensive experiments on Eastern China data show significant improvements in retrieving rare heavy‑to‑extreme rain events compared with standard, cost‑sensitive, generative, and multitask methods. The approach is presented as a generalizable solution for imbalanced environmental variable distributions.<br /><strong>Summary (CN):</strong> 本文提出 Hurdle-IMDL 框架，通过将零通胀（non‑rain 样本占多数）与长尾（轻雨样本相对重雨样本过多）两部分拆解，分别使用 hurdle 模型和逆模型去偏方法来解决红外降雨检索中的标签不平衡问题。基于中国东部天气的实验表明，该方法在检索罕见的强到极端降雨上显著优于传统、成本敏感、生成式以及多任务学习方法。该框架被视为解决环境变量分布不平衡的通用方法。<br /><strong>Keywords:</strong> imbalanced learning, hurdle model, infrared rainfall retrieval, long-tail, zero inflation, remote sensing, inverse modeling, environmental AI, heavy rain estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Fangjian Zhang, Xiaoyong Zhuge, Wenlan Wang, Haixia Xiao, Yuying Zhu, Siyang Cheng</div>
Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.
<div><strong>Authors:</strong> Fangjian Zhang, Xiaoyong Zhuge, Wenlan Wang, Haixia Xiao, Yuying Zhu, Siyang Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Hurdle-IMDL, a framework that tackles label imbalance in infrared rainfall retrieval by separating zero inflation and long-tail components, using a hurdle model for the former and an inverse‑model debiasing approach for the latter. Comprehensive experiments on Eastern China data show significant improvements in retrieving rare heavy‑to‑extreme rain events compared with standard, cost‑sensitive, generative, and multitask methods. The approach is presented as a generalizable solution for imbalanced environmental variable distributions.", "summary_cn": "本文提出 Hurdle-IMDL 框架，通过将零通胀（non‑rain 样本占多数）与长尾（轻雨样本相对重雨样本过多）两部分拆解，分别使用 hurdle 模型和逆模型去偏方法来解决红外降雨检索中的标签不平衡问题。基于中国东部天气的实验表明，该方法在检索罕见的强到极端降雨上显著优于传统、成本敏感、生成式以及多任务学习方法。该框架被视为解决环境变量分布不平衡的通用方法。", "keywords": "imbalanced learning, hurdle model, infrared rainfall retrieval, long-tail, zero inflation, remote sensing, inverse modeling, environmental AI, heavy rain estimation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Fangjian Zhang", "Xiaoyong Zhuge", "Wenlan Wang", "Haixia Xiao", "Yuying Zhu", "Siyang Cheng"]}
]]></acme>

<pubDate>2025-10-23T12:25:52+00:00</pubDate>
</item>
<item>
<title>RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging</title>
<link>https://papers.cool/arxiv/2510.20479</link>
<guid>https://papers.cool/arxiv/2510.20479</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RECALL, a representation-aware model merging framework that leverages layer-wise hidden representations to compute similarity between large language models and perform adaptive hierarchical parameter fusion for continual learning without historic data. By aligning knowledge across models, RECALL preserves domain-general features in shallow layers while allowing task-specific adaptation in deeper layers, achieving strong resistance to catastrophic forgetting and superior performance on multiple NLP tasks. Extensive experiments demonstrate that RECALL outperforms existing baselines in both knowledge retention and generalization across various continual learning scenarios.<br /><strong>Summary (CN):</strong> 本文提出 RECALL，一种基于内部表征的模型合并框架，通过在典型样本上计算层级隐藏表征的相似度，并进行自适应层次参数融合，实现无历史数据的持续学习。该方法在浅层保持通用特征、在深层实现任务特化，从而显著抑制灾难性遗忘并提升多任务 NLP 场景的性能。实验表明 RECALL 在知识保留和泛化能力上均优于现有基线。<br /><strong>Keywords:</strong> representation alignment, catastrophic forgetting, continual learning, model merging, LLM, hierarchical fusion, knowledge retention<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Bowen Wang, Haiyuan Wan, Liwen Shi, Chen Yang, Peng He, Yue Ma, Haochen Han, Wenhao Li, Tiao Tan, Yongjian Li, Fangming Liu, Yifan Gong, Sheng Zhang</div>
We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.
<div><strong>Authors:</strong> Bowen Wang, Haiyuan Wan, Liwen Shi, Chen Yang, Peng He, Yue Ma, Haochen Han, Wenhao Li, Tiao Tan, Yongjian Li, Fangming Liu, Yifan Gong, Sheng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RECALL, a representation-aware model merging framework that leverages layer-wise hidden representations to compute similarity between large language models and perform adaptive hierarchical parameter fusion for continual learning without historic data. By aligning knowledge across models, RECALL preserves domain-general features in shallow layers while allowing task-specific adaptation in deeper layers, achieving strong resistance to catastrophic forgetting and superior performance on multiple NLP tasks. Extensive experiments demonstrate that RECALL outperforms existing baselines in both knowledge retention and generalization across various continual learning scenarios.", "summary_cn": "本文提出 RECALL，一种基于内部表征的模型合并框架，通过在典型样本上计算层级隐藏表征的相似度，并进行自适应层次参数融合，实现无历史数据的持续学习。该方法在浅层保持通用特征、在深层实现任务特化，从而显著抑制灾难性遗忘并提升多任务 NLP 场景的性能。实验表明 RECALL 在知识保留和泛化能力上均优于现有基线。", "keywords": "representation alignment, catastrophic forgetting, continual learning, model merging, LLM, hierarchical fusion, knowledge retention", "scoring": {"interpretability": 4, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Bowen Wang", "Haiyuan Wan", "Liwen Shi", "Chen Yang", "Peng He", "Yue Ma", "Haochen Han", "Wenhao Li", "Tiao Tan", "Yongjian Li", "Fangming Liu", "Yifan Gong", "Sheng Zhang"]}
]]></acme>

<pubDate>2025-10-23T12:17:37+00:00</pubDate>
</item>
<item>
<title>Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks</title>
<link>https://papers.cool/arxiv/2510.20469</link>
<guid>https://papers.cool/arxiv/2510.20469</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how holonic structures emerge in a multi‑agent system performing information fusion over peer‑to‑peer, resource‑constrained networks. Using a generic multi‑agent model, it shows that constraints on energy, bandwidth and time lead to the formation of autonomous, adaptable holons that cooperate to mitigate uncertainty and communication scarcity. An illustrative example demonstrates the potential benefits of such structures for robustness and adaptability.<br /><strong>Summary (CN):</strong> 本文研究了在资源受限的点对点网络中进行信息融合的多智能体系统如何生成全息结构（holons）。通过通用的多智能体模型，展示了能源、带宽和时间等约束促使系统形成自主、可适应的全息子结构，以降低不确定性并缓解通信短缺。文中提供的示例说明了这些结构在提高系统鲁棒性和适应性方面的潜在优势。<br /><strong>Keywords:</strong> holonic fusion, multiagent systems, peer-to-peer networks, resource-constrained communication, distributed information fusion, adaptive structures<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Horacio Paggi, Juan A. Lara, Javier Soriano</div>
There has recently been a major advance with respect to how information fusion is performed. Information fusion has gone from being conceived as a purely hierarchical procedure, as is the case of traditional military applications, to now being regarded collaboratively, as holonic fusion, which is better suited for civil applications and edge organizations. The above paradigm shift is being boosted as information fusion gains ground in different non-military areas, and human-computer and machine-machine communications, where holarchies, which are more flexible structures than ordinary, static hierarchies, become more widespread. This paper focuses on showing how holonic structures tend to be generated when there are constraints on resources (energy, available messages, time, etc.) for interactions based on a set of fully intercommunicating elements (peers) whose components fuse information as a means of optimizing the impact of vagueness and uncertainty present message exchanges. Holon formation is studied generically based on a multiagent system model, and an example of its possible operation is shown. Holonic structures have a series of advantages, such as adaptability, to sudden changes in the environment or its composition, are somewhat autonomous and are capable of cooperating in order to achieve a common goal. This can be useful when the shortage of resources prevents communications or when the system components start to fail.
<div><strong>Authors:</strong> Horacio Paggi, Juan A. Lara, Javier Soriano</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how holonic structures emerge in a multi‑agent system performing information fusion over peer‑to‑peer, resource‑constrained networks. Using a generic multi‑agent model, it shows that constraints on energy, bandwidth and time lead to the formation of autonomous, adaptable holons that cooperate to mitigate uncertainty and communication scarcity. An illustrative example demonstrates the potential benefits of such structures for robustness and adaptability.", "summary_cn": "本文研究了在资源受限的点对点网络中进行信息融合的多智能体系统如何生成全息结构（holons）。通过通用的多智能体模型，展示了能源、带宽和时间等约束促使系统形成自主、可适应的全息子结构，以降低不确定性并缓解通信短缺。文中提供的示例说明了这些结构在提高系统鲁棒性和适应性方面的潜在优势。", "keywords": "holonic fusion, multiagent systems, peer-to-peer networks, resource-constrained communication, distributed information fusion, adaptive structures", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Horacio Paggi", "Juan A. Lara", "Javier Soriano"]}
]]></acme>

<pubDate>2025-10-23T12:07:32+00:00</pubDate>
</item>
<item>
<title>Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models</title>
<link>https://papers.cool/arxiv/2510.20468</link>
<guid>https://papers.cool/arxiv/2510.20468</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a black-box, one-shot method for forging post-hoc image watermarks by first training a preference model to detect watermarked images using procedurally generated data, and then optimizing a target image via backpropagation to embed a forged watermark without any knowledge of the original watermarking algorithm. Experiments across several watermarking schemes show that a single watermarked example suffices to successfully transfer and apply counterfeit watermarks, exposing a significant security vulnerability in current watermarking approaches.<br /><strong>Summary (CN):</strong> 本文提出一种黑箱单次攻击方法，通过在纯程序生成的图像上使用排序损失训练偏好模型，以判断图像是否被水印，然后利用反向传播对目标图像进行优化，使其嵌入伪造的水印，而无需了解原始水印算法。对多种后置图像水印方案的实验表明，仅凭单个真实水印样本即可成功转移并应用伪造水印，揭示了当前水印技术的重大安全隐患。<br /><strong>Keywords:</strong> watermarking, black-box attack, image preference model, forging, security, generative AI, post-hoc watermark, one-shot attack<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.
<div><strong>Authors:</strong> Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a black-box, one-shot method for forging post-hoc image watermarks by first training a preference model to detect watermarked images using procedurally generated data, and then optimizing a target image via backpropagation to embed a forged watermark without any knowledge of the original watermarking algorithm. Experiments across several watermarking schemes show that a single watermarked example suffices to successfully transfer and apply counterfeit watermarks, exposing a significant security vulnerability in current watermarking approaches.", "summary_cn": "本文提出一种黑箱单次攻击方法，通过在纯程序生成的图像上使用排序损失训练偏好模型，以判断图像是否被水印，然后利用反向传播对目标图像进行优化，使其嵌入伪造的水印，而无需了解原始水印算法。对多种后置图像水印方案的实验表明，仅凭单个真实水印样本即可成功转移并应用伪造水印，揭示了当前水印技术的重大安全隐患。", "keywords": "watermarking, black-box attack, image preference model, forging, security, generative AI, post-hoc watermark, one-shot attack", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tomáš Souček", "Sylvestre-Alvise Rebuffi", "Pierre Fernandez", "Nikola Jovanović", "Hady Elsahar", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"]}
]]></acme>

<pubDate>2025-10-23T12:06:35+00:00</pubDate>
</item>
<item>
<title>Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics</title>
<link>https://papers.cool/arxiv/2510.20453</link>
<guid>https://papers.cool/arxiv/2510.20453</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper applies symbolic regression to the Constrained Minimal Supersymmetric Standard Model (CMSSM), deriving compact analytic expressions for key observables such as the Higgs mass, dark matter relic density, and the muon anomalous magnetic moment. These expressions enable fast, differentiable global fits that recover posterior distributions comparable to traditional sampling methods, and outperform neural‑network regression in robustness across the parameter space.<br /><strong>Summary (CN):</strong> 本文将符号回归应用于约束最小超对称模型（CMSSM），为希格斯质量、暗物质遗迹密度以及μ子异常磁矩等关键可观测量推导出简洁的解析表达式。这些表达式支持快速的可微拟合，得到的后验分布与传统抽样方法相吻合，并在参数空间的鲁棒性方面优于神经网络回归。<br /><strong>Keywords:</strong> symbolic regression, differentiable fitting, CMSSM, Higgs mass, dark matter relic density, muon g-2, global fits, neural network regression, interpretability<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</div>
We demonstrate the efficacy of symbolic regression (SR) to probe models of particle physics Beyond the Standard Model (BSM), by considering the so-called Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. We show that analysis of the phenomenology can be greatly accelerated by using symbolic expressions derived for the observables in terms of the input parameters. Here we focus on the Higgs mass, the cold dark matter relic density, and the contribution to the anomalous magnetic moment of the muon. We find that SR can produce remarkably accurate expressions. Using them we make global fits to derive the posterior probability densities of the CMSSM input parameters which are in good agreement with those performed using conventional methods. Moreover, we demonstrate a major advantage of SR which is the ability to make fits using differentiable methods rather than sampling methods. We also compare the method with neural network (NN) regression. SR produces more globally robust results, while NNs require data that is focussed on the promising regions in order to be equally performant.
<div><strong>Authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper applies symbolic regression to the Constrained Minimal Supersymmetric Standard Model (CMSSM), deriving compact analytic expressions for key observables such as the Higgs mass, dark matter relic density, and the muon anomalous magnetic moment. These expressions enable fast, differentiable global fits that recover posterior distributions comparable to traditional sampling methods, and outperform neural‑network regression in robustness across the parameter space.", "summary_cn": "本文将符号回归应用于约束最小超对称模型（CMSSM），为希格斯质量、暗物质遗迹密度以及μ子异常磁矩等关键可观测量推导出简洁的解析表达式。这些表达式支持快速的可微拟合，得到的后验分布与传统抽样方法相吻合，并在参数空间的鲁棒性方面优于神经网络回归。", "keywords": "symbolic regression, differentiable fitting, CMSSM, Higgs mass, dark matter relic density, muon g-2, global fits, neural network regression, interpretability", "scoring": {"interpretability": 6, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shehu AbdusSalam", "Steven Abel", "Deaglan Bartlett", "Miguel Crispim Romão"]}
]]></acme>

<pubDate>2025-10-23T11:40:15+00:00</pubDate>
</item>
<item>
<title>MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction</title>
<link>https://papers.cool/arxiv/2510.20448</link>
<guid>https://papers.cool/arxiv/2510.20448</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MolBridge proposes an atom‑level joint graph refinement framework that constructs a combined graph of two drugs to directly model inter‑molecular interactions for drug‑drug interaction event prediction. A structure‑consistency module iteratively refines node features while preserving global context, mitigating over‑smoothing and capturing both local and long‑range atomic dependencies. Experiments on benchmark DDI datasets show state‑of‑the‑art performance, especially on long‑tail and inductive scenarios, and offer mechanistic interpretability of the predicted interactions.<br /><strong>Summary (CN):</strong> MolBridge 提出了一种原子级联合图精炼框架，构建药物对的联合图以直接建模分子间相互作用，从而预测药物‑药物相互作用事件。通过结构一致性模块迭代更新节点特征并保持全局结构上下文，避免过度平滑，捕获局部及长程原子依赖。该方法在基准 DDI 数据集上取得最先进的性能，尤其在长尾和归纳场景下表现突出，并提供了预测交互的机制可解释性。<br /><strong>Keywords:</strong> drug-drug interaction, graph neural network, joint graph, atom-level, mechanistic interpretability, robustness, DDI prediction, biomedical safety, structure consistency, inductive learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Xuan Lin, Aocheng Ding, Tengfei Ma, Hua Liang, Zhe Quan</div>
Drug combinations offer therapeutic benefits but also carry the risk of adverse drug-drug interactions (DDIs), especially under complex molecular structures. Accurate DDI event prediction requires capturing fine-grained inter-drug relationships, which are critical for modeling metabolic mechanisms such as enzyme-mediated competition. However, existing approaches typically rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. To address these limitations, we propose MolBridge, a novel atom-level joint graph refinement framework for robust DDI event prediction. MolBridge constructs a joint graph that integrates atomic structures of drug pairs, enabling direct modeling of inter-drug associations. A central challenge in such joint graph settings is the potential loss of information caused by over-smoothing when modeling long-range atomic dependencies. To overcome this, we introduce a structure consistency module that iteratively refines node features while preserving the global structural context. This joint design allows MolBridge to effectively learn both local and global interaction outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. patterns, yielding robust representations across both frequent and rare DDI types. Extensive experiments on two benchmark datasets show that MolBridge consistently. These results demonstrate the advantages of fine-grained graph refinement in improving the accuracy, robustness, and mechanistic interpretability of DDI event prediction.This work contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks.
<div><strong>Authors:</strong> Xuan Lin, Aocheng Ding, Tengfei Ma, Hua Liang, Zhe Quan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MolBridge proposes an atom‑level joint graph refinement framework that constructs a combined graph of two drugs to directly model inter‑molecular interactions for drug‑drug interaction event prediction. A structure‑consistency module iteratively refines node features while preserving global context, mitigating over‑smoothing and capturing both local and long‑range atomic dependencies. Experiments on benchmark DDI datasets show state‑of‑the‑art performance, especially on long‑tail and inductive scenarios, and offer mechanistic interpretability of the predicted interactions.", "summary_cn": "MolBridge 提出了一种原子级联合图精炼框架，构建药物对的联合图以直接建模分子间相互作用，从而预测药物‑药物相互作用事件。通过结构一致性模块迭代更新节点特征并保持全局结构上下文，避免过度平滑，捕获局部及长程原子依赖。该方法在基准 DDI 数据集上取得最先进的性能，尤其在长尾和归纳场景下表现突出，并提供了预测交互的机制可解释性。", "keywords": "drug-drug interaction, graph neural network, joint graph, atom-level, mechanistic interpretability, robustness, DDI prediction, biomedical safety, structure consistency, inductive learning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Xuan Lin", "Aocheng Ding", "Tengfei Ma", "Hua Liang", "Zhe Quan"]}
]]></acme>

<pubDate>2025-10-23T11:33:16+00:00</pubDate>
</item>
<item>
<title>UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement</title>
<link>https://papers.cool/arxiv/2510.20441</link>
<guid>https://papers.cool/arxiv/2510.20441</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> UniSE proposes a unified decoder‑only autoregressive language‑model framework that tackles multiple speech‑enhancement tasks—restoration, target‑speaker extraction, and separation—by conditioning on speech features and generating discrete token sequences of the desired output. Experiments on standard benchmarks show competitive performance against both discriminative and generative baselines, demonstrating the feasibility of using language models to unify speech‑enhancement tasks.<br /><strong>Summary (CN):</strong> UniSE 提出一个统一的仅解码器自回归语言模型框架，通过以语音特征为条件并生成目标语音的离散 token 序列，能够同时处理语音恢复、目标说话人提取和语音分离等多种语音增强任务。实验在多个基准上显示其性能与判别式及生成式基线相当，证明了语言模型在统一语音增强任务中的可行性。<br /><strong>Keywords:</strong> speech enhancement, autoregressive language model, decoder-only LM, neural audio codec, speech restoration, speaker extraction, speech separation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue</div>
The development of neural audio codecs (NACs) has largely promoted applications of language models (LMs) to speech processing and understanding. However, there lacks the verification on the effectiveness of autoregressive (AR) LMbased models in unifying different sub-tasks of speech enhancement (SE). In this work, we propose UniSE, a unified decoder-only LM-based framework to handle different SE tasks including speech restoration, target speaker extraction and speech separation. It takes input speech features as conditions and generates discrete tokens of the target speech using AR modeling, which facilitates a compatibility between distinct learning patterns of multiple tasks. Experiments on several benchmarks indicate the proposed UniSE can achieve competitive performance compared to discriminative and generative baselines, showing the capacity of LMs in unifying SE tasks. The demo page is available here: https://github.com/hyyan2k/UniSE.
<div><strong>Authors:</strong> Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "UniSE proposes a unified decoder‑only autoregressive language‑model framework that tackles multiple speech‑enhancement tasks—restoration, target‑speaker extraction, and separation—by conditioning on speech features and generating discrete token sequences of the desired output. Experiments on standard benchmarks show competitive performance against both discriminative and generative baselines, demonstrating the feasibility of using language models to unify speech‑enhancement tasks.", "summary_cn": "UniSE 提出一个统一的仅解码器自回归语言模型框架，通过以语音特征为条件并生成目标语音的离散 token 序列，能够同时处理语音恢复、目标说话人提取和语音分离等多种语音增强任务。实验在多个基准上显示其性能与判别式及生成式基线相当，证明了语言模型在统一语音增强任务中的可行性。", "keywords": "speech enhancement, autoregressive language model, decoder-only LM, neural audio codec, speech restoration, speaker extraction, speech separation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyin Yan", "Chengwei Liu", "Shaofei Xue", "Xiaotao Liang", "Zheng Xue"]}
]]></acme>

<pubDate>2025-10-23T11:22:24+00:00</pubDate>
</item>
<item>
<title>Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment</title>
<link>https://papers.cool/arxiv/2510.20438</link>
<guid>https://papers.cool/arxiv/2510.20438</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FuzzyDistillViT-MobileNet, a framework that uses fuzzy‑logic‑driven dynamic weighting for knowledge distillation from a Vision Transformer teacher to a MobileNet student to classify lung cancer images. It incorporates pixel‑level image fusion (gamma correction, histogram equalization, wavelet‑based fusion) and a genetic algorithm to select the optimal student model, achieving >99% accuracy on two histopathology and CT datasets.<br /><strong>Summary (CN):</strong> 本文提出 FuzzyDistillViT-MobileNet 框架，利用模糊逻辑驱动的动态权重知识蒸馏，将 Vision Transformer (ViT-B32) 作为教师模型向 MobileNet 学生模型传递知识，实现肺癌图像分类。文中还引入像素级图像融合（伽马校正、直方图均衡、基于小波的融合）以及遗传算法用于在多个候选学生模型中挑选最优方案，在两个数据集上均取得超过 99% 的准确率。<br /><strong>Keywords:</strong> dynamic knowledge distillation, fuzzy logic, Vision Transformer, MobileNet, lung cancer detection, image fusion, genetic algorithm, medical imaging, model compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</div>
This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven knowledge distillation (KD) to address uncertainty and complexity in disease diagnosis. Unlike traditional models that rely on static KD with fixed weights, our method dynamically adjusts the distillation weight using fuzzy logic, enabling the student model to focus on high-confidence regions while reducing attention to ambiguous areas. This dynamic adjustment improves the model ability to handle varying uncertainty levels across different regions of LC images. We employ the Vision Transformer (ViT-B32) as the instructor model, which effectively transfers knowledge to the student model, MobileNet, enhancing the student generalization capabilities. The training process is further optimized using a dynamic wait adjustment mechanism that adapts the training procedure for improved convergence and performance. To enhance image quality, we introduce pixel-level image fusion improvement techniques such as Gamma correction and Histogram Equalization. The processed images (Pix1 and Pix2) are fused using a wavelet-based fusion method to improve image resolution and feature preservation. This fusion method uses the wavedec2 function to standardize images to a 224x224 resolution, decompose them into multi-scale frequency components, and recursively average coefficients at each level for better feature representation. To address computational efficiency, Genetic Algorithm (GA) is used to select the most suitable pre-trained student model from a pool of 12 candidates, balancing model performance with computational cost. The model is evaluated on two datasets, including LC25000 histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images (99.54% accuracy), demonstrating robustness across different imaging domains.
<div><strong>Authors:</strong> Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FuzzyDistillViT-MobileNet, a framework that uses fuzzy‑logic‑driven dynamic weighting for knowledge distillation from a Vision Transformer teacher to a MobileNet student to classify lung cancer images. It incorporates pixel‑level image fusion (gamma correction, histogram equalization, wavelet‑based fusion) and a genetic algorithm to select the optimal student model, achieving >99% accuracy on two histopathology and CT datasets.", "summary_cn": "本文提出 FuzzyDistillViT-MobileNet 框架，利用模糊逻辑驱动的动态权重知识蒸馏，将 Vision Transformer (ViT-B32) 作为教师模型向 MobileNet 学生模型传递知识，实现肺癌图像分类。文中还引入像素级图像融合（伽马校正、直方图均衡、基于小波的融合）以及遗传算法用于在多个候选学生模型中挑选最优方案，在两个数据集上均取得超过 99% 的准确率。", "keywords": "dynamic knowledge distillation, fuzzy logic, Vision Transformer, MobileNet, lung cancer detection, image fusion, genetic algorithm, medical imaging, model compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"]}
]]></acme>

<pubDate>2025-10-23T11:19:52+00:00</pubDate>
</item>
<item>
<title>Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control</title>
<link>https://papers.cool/arxiv/2510.20408</link>
<guid>https://papers.cool/arxiv/2510.20408</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a new benchmark that combines SortingEnv and ContainerGym into a sequential recycling task, enabling the study of specialization versus centralization in multi‑agent reinforcement learning for industrial control. It compares a modular system of specialized agents with a monolithic agent, showing that action masking dramatically improves performance for both and reduces the benefit of specialization. The results highlight the importance of action‑space constraints for practical, robust multi‑agent RL in industrial settings.<br /><strong>Summary (CN):</strong> 本文提出了一个将 SortingEnv 与 ContainerGym 融合的顺序回收基准，用于研究工业控制中多智能体强化学习的专门化与中心化之间的权衡。通过比较由专门化子代理组成的模块化架构与控制全部系统的单体代理，发现动作屏蔽显著提升了两者的学习效果并大幅缩小专门化的优势。该工作强调了动作空间约束在实现工业自动化中实用且稳健的多代理 RL 方案中的决定性作用。<br /><strong>Keywords:</strong> multi-agent reinforcement learning, industrial control, modular architecture, centralization, action masking, benchmark, sequential recycling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Tom Maus, Asma Atamna, Tobias Glasmachers</div>
Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.
<div><strong>Authors:</strong> Tom Maus, Asma Atamna, Tobias Glasmachers</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a new benchmark that combines SortingEnv and ContainerGym into a sequential recycling task, enabling the study of specialization versus centralization in multi‑agent reinforcement learning for industrial control. It compares a modular system of specialized agents with a monolithic agent, showing that action masking dramatically improves performance for both and reduces the benefit of specialization. The results highlight the importance of action‑space constraints for practical, robust multi‑agent RL in industrial settings.", "summary_cn": "本文提出了一个将 SortingEnv 与 ContainerGym 融合的顺序回收基准，用于研究工业控制中多智能体强化学习的专门化与中心化之间的权衡。通过比较由专门化子代理组成的模块化架构与控制全部系统的单体代理，发现动作屏蔽显著提升了两者的学习效果并大幅缩小专门化的优势。该工作强调了动作空间约束在实现工业自动化中实用且稳健的多代理 RL 方案中的决定性作用。", "keywords": "multi-agent reinforcement learning, industrial control, modular architecture, centralization, action masking, benchmark, sequential recycling", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"]}
]]></acme>

<pubDate>2025-10-23T10:21:54+00:00</pubDate>
</item>
<item>
<title>FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services</title>
<link>https://papers.cool/arxiv/2510.20388</link>
<guid>https://papers.cool/arxiv/2510.20388</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces FLAS (Forecasted Load Auto-Scaling), an auto-scaling system for distributed services that combines proactive predictive scaling with a reactive contingency mechanism to decide optimal scaling actions. It features a high‑level metric trend predictor and a low‑invasiveness reactive component that estimates performance metrics from resource usage, demonstrated on a content‑based publish‑subscribe middleware and evaluated using boundary‑value analysis test cases.<br /><strong>Summary (CN):</strong> 本文提出 FLAS（Forecasted Load Auto-Scaling）系统，将主动预测式弹性伸缩与基于资源使用估计的被动应急机制相结合，以在每一时刻决定最优的伸缩操作。该系统利用高层度量趋势预测模型提前捕捉响应时间或吞吐量等 SLA 参数变化，并通过低侵入式的被动组件实现对不同分布式服务（如内容订阅中间件）的自适应伸缩，实验采用边界值分析验证了其在超过 99% 时间内满足性能要求的有效性。<br /><strong>Keywords:</strong> auto-scaling, proactive scaling, reactive scaling, predictive model, distributed services, publish-subscribe, elasticity, cloud computing<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Víctor Rampérez, Javier Soriano, David Lizcano, Juan A. Lara</div>
Cloud computing has established itself as the support for the vast majority of emerging technologies, mainly due to the characteristic of elasticity it offers. Auto-scalers are the systems that enable this elasticity by acquiring and releasing resources on demand to ensure an agreed service level. In this article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for distributed services that combines the advantages of proactive and reactive approaches according to the situation to decide the optimal scaling actions in every moment. The main novelties introduced by FLAS are (i) a predictive model of the high-level metrics trend which allows to anticipate changes in the relevant SLA parameters (e.g. performance metrics such as response time or throughput) and (ii) a reactive contingency system based on the estimation of high-level metrics from resource use metrics, reducing the necessary instrumentation (less invasive) and allowing it to be adapted agnostically to different applications. We provide a FLAS implementation for the use case of a content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone of an event-driven architecture. To the best of our knowledge, this is the first auto-scaling system for content-based publish-subscribe distributed systems (although it is generic enough to fit any distributed service). Through an evaluation based on several test cases recreating not only the expected contexts of use, but also the worst possible scenarios (following the Boundary-Value Analysis or BVA test methodology), we have validated our approach and demonstrated the effectiveness of our solution by ensuring compliance with performance requirements over 99% of the time.
<div><strong>Authors:</strong> Víctor Rampérez, Javier Soriano, David Lizcano, Juan A. Lara</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces FLAS (Forecasted Load Auto-Scaling), an auto-scaling system for distributed services that combines proactive predictive scaling with a reactive contingency mechanism to decide optimal scaling actions. It features a high‑level metric trend predictor and a low‑invasiveness reactive component that estimates performance metrics from resource usage, demonstrated on a content‑based publish‑subscribe middleware and evaluated using boundary‑value analysis test cases.", "summary_cn": "本文提出 FLAS（Forecasted Load Auto-Scaling）系统，将主动预测式弹性伸缩与基于资源使用估计的被动应急机制相结合，以在每一时刻决定最优的伸缩操作。该系统利用高层度量趋势预测模型提前捕捉响应时间或吞吐量等 SLA 参数变化，并通过低侵入式的被动组件实现对不同分布式服务（如内容订阅中间件）的自适应伸缩，实验采用边界值分析验证了其在超过 99% 时间内满足性能要求的有效性。", "keywords": "auto-scaling, proactive scaling, reactive scaling, predictive model, distributed services, publish-subscribe, elasticity, cloud computing", "scoring": {"interpretability": 1, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Víctor Rampérez", "Javier Soriano", "David Lizcano", "Juan A. Lara"]}
]]></acme>

<pubDate>2025-10-23T09:38:07+00:00</pubDate>
</item>
<item>
<title>Relative-Based Scaling Law for Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.20387</link>
<guid>https://papers.cool/arxiv/2510.20387</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a Relative-Based Probability (RBP) metric that measures the likelihood that the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Experiments across four datasets and four model families spanning five orders of magnitude show that the law accurately predicts performance and can explain emergence phenomena and guide theoretical scaling analyses.<br /><strong>Summary (CN):</strong> 本文提出相对概率（RBP）指标，用于衡量正确词在预测排名中的位置概率，并基于该指标建立相对尺度律，描述 RBP 随模型规模的提升规律。通过在四个数据集和四类模型（跨五个数量级）的广泛实验验证了该尺度律的鲁性与准确性，并展示了其在解释模型出现现象和寻找尺度律理论方面的应用。<br /><strong>Keywords:</strong> relative-based probability, scaling laws, language models, model emergence, performance prediction, RBP metric<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
Scaling laws aim to accurately predict model performance across different scales. Existing scaling-law studies almost exclusively rely on cross-entropy as the evaluation metric. However, cross-entropy provides only a partial view of performance: it measures the absolute probability assigned to the correct token, but ignores the relative ordering between correct and incorrect tokens. Yet, relative ordering is crucial for language models, such as in greedy-sampling scenario. To address this limitation, we investigate scaling from the perspective of relative ordering. We first propose the Relative-Based Probability (RBP) metric, which quantifies the probability that the correct token is ranked among the top predictions. Building on this metric, we establish the Relative-Based Scaling Law, which characterizes how RBP improves with increasing model size. Through extensive experiments on four datasets and four model families spanning five orders of magnitude, we demonstrate the robustness and accuracy of this law. Finally, we illustrate the broad application of this law with two examples, namely providing a deeper explanation of emergence phenomena and facilitating finding fundamental theories of scaling laws. In summary, the Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models. Thus, it offers valuable insights for both practical development and theoretical exploration.
<div><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a Relative-Based Probability (RBP) metric that measures the likelihood that the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Experiments across four datasets and four model families spanning five orders of magnitude show that the law accurately predicts performance and can explain emergence phenomena and guide theoretical scaling analyses.", "summary_cn": "本文提出相对概率（RBP）指标，用于衡量正确词在预测排名中的位置概率，并基于该指标建立相对尺度律，描述 RBP 随模型规模的提升规律。通过在四个数据集和四类模型（跨五个数量级）的广泛实验验证了该尺度律的鲁性与准确性，并展示了其在解释模型出现现象和寻找尺度律理论方面的应用。", "keywords": "relative-based probability, scaling laws, language models, model emergence, performance prediction, RBP metric", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baoqing Yue", "Jinyuan Zhou", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"]}
]]></acme>

<pubDate>2025-10-23T09:37:00+00:00</pubDate>
</item>
<item>
<title>VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation</title>
<link>https://papers.cool/arxiv/2510.20381</link>
<guid>https://papers.cool/arxiv/2510.20381</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the VLSP 2025 MLQA-TSR shared task, which provides a benchmark for Vietnamese multimodal legal question answering focused on traffic sign regulations. The task comprises two subtasks—multimodal legal retrieval and multimodal question answering—and reports best results of 64.55% F2 for retrieval and 86.30% accuracy for QA. The dataset aims to advance research on multimodal legal text processing in the Vietnamese context.<br /><strong>Summary (CN):</strong> 本文介绍了 VLSP 2025 MLQA-TSR 共享任务，提供了一个针对越南交通标志法规的多模态法律问答基准数据集。任务包括多模态法律检索和多模态问答两子任务，最佳结果分别为检索 F2 分数 64.55% 和问答准确率 86.30%。该数据集旨在推动越南多模态法律文本处理研究。<br /><strong>Keywords:</strong> Vietnamese multimodal QA, legal question answering, traffic sign regulation, benchmark dataset, multimodal retrieval, multimodal question answering<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Son T. Luu, Trung Vo, Hiep Nguyen, Khanh Quoc Tran, Kiet Van Nguyen, Vu Tran, Ngan Luu-Thuy Nguyen, Le-Minh Nguyen</div>
This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025 MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal question answering. The goal is to advance research on Vietnamese multimodal legal text processing and to provide a benchmark dataset for building and evaluating intelligent systems in multimodal legal domains, with a focus on traffic sign regulation in Vietnam. The best-reported results on VLSP 2025 MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an accuracy of 86.30% for multimodal question answering.
<div><strong>Authors:</strong> Son T. Luu, Trung Vo, Hiep Nguyen, Khanh Quoc Tran, Kiet Van Nguyen, Vu Tran, Ngan Luu-Thuy Nguyen, Le-Minh Nguyen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the VLSP 2025 MLQA-TSR shared task, which provides a benchmark for Vietnamese multimodal legal question answering focused on traffic sign regulations. The task comprises two subtasks—multimodal legal retrieval and multimodal question answering—and reports best results of 64.55% F2 for retrieval and 86.30% accuracy for QA. The dataset aims to advance research on multimodal legal text processing in the Vietnamese context.", "summary_cn": "本文介绍了 VLSP 2025 MLQA-TSR 共享任务，提供了一个针对越南交通标志法规的多模态法律问答基准数据集。任务包括多模态法律检索和多模态问答两子任务，最佳结果分别为检索 F2 分数 64.55% 和问答准确率 86.30%。该数据集旨在推动越南多模态法律文本处理研究。", "keywords": "Vietnamese multimodal QA, legal question answering, traffic sign regulation, benchmark dataset, multimodal retrieval, multimodal question answering", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Son T. Luu", "Trung Vo", "Hiep Nguyen", "Khanh Quoc Tran", "Kiet Van Nguyen", "Vu Tran", "Ngan Luu-Thuy Nguyen", "Le-Minh Nguyen"]}
]]></acme>

<pubDate>2025-10-23T09:24:43+00:00</pubDate>
</item>
<item>
<title>The Impact of Negated Text on Hallucination with Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20375</link>
<guid>https://papers.cool/arxiv/2510.20375</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how negated text affects hallucination detection in large language models (LLMs). It introduces the NegHalu dataset, which rewrites existing hallucination detection data with negated expressions, and shows that LLMs often fail to correctly identify hallucinations in such contexts, producing inconsistent judgments. Additionally, the authors trace token‑level internal states of LLMs when processing negated inputs, revealing challenges in mitigating these unintended effects.<br /><strong>Summary (CN):</strong> 本文研究了否定文本对大型语言模型（LLM）幻觉检测的影响。作者构建了 NegHalu 数据集，将已有的幻觉检测数据改写为否定表达，并发现 LLM 在此类文本上往往难以正确识别幻觉，导致判断不一致。进一步通过对 LLM 处理否定输入的 token 级内部状态进行追踪，揭示了缓解这些异常行为的挑战。<br /><strong>Keywords:</strong> hallucination detection, negation, large language models, dataset, token-level analysis<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Jaehyung Seo, Hyeonseok Moon, Heuiseok Lim</div>
Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects.
<div><strong>Authors:</strong> Jaehyung Seo, Hyeonseok Moon, Heuiseok Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how negated text affects hallucination detection in large language models (LLMs). It introduces the NegHalu dataset, which rewrites existing hallucination detection data with negated expressions, and shows that LLMs often fail to correctly identify hallucinations in such contexts, producing inconsistent judgments. Additionally, the authors trace token‑level internal states of LLMs when processing negated inputs, revealing challenges in mitigating these unintended effects.", "summary_cn": "本文研究了否定文本对大型语言模型（LLM）幻觉检测的影响。作者构建了 NegHalu 数据集，将已有的幻觉检测数据改写为否定表达，并发现 LLM 在此类文本上往往难以正确识别幻觉，导致判断不一致。进一步通过对 LLM 处理否定输入的 token 级内部状态进行追踪，揭示了缓解这些异常行为的挑战。", "keywords": "hallucination detection, negation, large language models, dataset, token-level analysis", "scoring": {"interpretability": 5, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jaehyung Seo", "Hyeonseok Moon", "Heuiseok Lim"]}
]]></acme>

<pubDate>2025-10-23T09:20:15+00:00</pubDate>
</item>
<item>
<title>Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20351</link>
<guid>https://papers.cool/arxiv/2510.20351</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models possess latent knowledge of widely used tabular benchmark datasets such as Adult Income and Titanic. By conducting controlled probing experiments, the authors show that performance dramatically improves when semantic cues like meaningful column names are present, indicating dataset contamination, while randomizing these cues reduces accuracy to near-random levels. They argue that current evaluations of tabular reasoning may overstate LLM capabilities and suggest strategies to separate memor from genuine reasoning in future assessments.<br /><strong>Summary (CN):</strong> 本文研究了大型语言模型是否对常用表格基准数据集（如 Adult Income、Titanic）拥有潜在知识。通过受控探测实验，作者发现当数据包含有意义的列名等语义线索时模型表现显著提升，表明存在数据集泄漏；而在移除或随机化这些线索后，性能骤降至随机水平。作者指出，目前的表格推理评估可能因记忆而高估模型能力，并提出在未来评估中区分记忆与真实推理的策略。<br /><strong>Keywords:</strong> latent knowledge, dataset contamination, tabular reasoning, large language models, evaluation protocols, memorization, semantic cues<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Matteo Silvestri, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei</div>
Large Language Models (LLMs) are increasingly evaluated on their ability to reason over structured data, yet such assessments often overlook a crucial confound: dataset contamination. In this work, we investigate whether LLMs exhibit prior knowledge of widely used tabular benchmarks such as Adult Income, Titanic, and others. Through a series of controlled probing experiments, we reveal that contamination effects emerge exclusively for datasets containing strong semantic cues-for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels. These findings suggest that LLMs' apparent competence on tabular reasoning tasks may, in part, reflect memorization of publicly available datasets rather than genuine generalization. We discuss implications for evaluation protocols and propose strategies to disentangle semantic leakage from authentic reasoning ability in future LLM assessments.
<div><strong>Authors:</strong> Matteo Silvestri, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models possess latent knowledge of widely used tabular benchmark datasets such as Adult Income and Titanic. By conducting controlled probing experiments, the authors show that performance dramatically improves when semantic cues like meaningful column names are present, indicating dataset contamination, while randomizing these cues reduces accuracy to near-random levels. They argue that current evaluations of tabular reasoning may overstate LLM capabilities and suggest strategies to separate memor from genuine reasoning in future assessments.", "summary_cn": "本文研究了大型语言模型是否对常用表格基准数据集（如 Adult Income、Titanic）拥有潜在知识。通过受控探测实验，作者发现当数据包含有意义的列名等语义线索时模型表现显著提升，表明存在数据集泄漏；而在移除或随机化这些线索后，性能骤降至随机水平。作者指出，目前的表格推理评估可能因记忆而高估模型能力，并提出在未来评估中区分记忆与真实推理的策略。", "keywords": "latent knowledge, dataset contamination, tabular reasoning, large language models, evaluation protocols, memorization, semantic cues", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Matteo Silvestri", "Flavio Giorgi", "Fabrizio Silvestri", "Gabriele Tolomei"]}
]]></acme>

<pubDate>2025-10-23T08:51:14+00:00</pubDate>
</item>
<item>
<title>What do AI-Generated Images Want?</title>
<link>https://papers.cool/arxiv/2510.20350</link>
<guid>https://papers.cool/arxiv/2510.20350</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reframes Mitchell’s question “What do pictures want?” for modern AI text‑to‑image generators, arguing that AI‑generated images “want” specificity and concreteness because they are fundamentally abstract. It critiques the assumption that text and image are interchangeable tokens and highlights the hidden representational regress in the user pipeline.<br /><strong>Summary (CN):</strong> 本文将 Mitchell 的“图片想要什么？”问题植到当代 AI 文本‑图像生成模型，主张由于这些图像本质上是抽象的，它们渴求具体和明确的内容。文章批评了文本与图像可互换的假设，指出用户从文字输入到视觉输出的流程隐藏了表征倒退。<br /><strong>Keywords:</strong> AI-generated images, text-to-image models, abstraction, specificity, multimodal semantics, image agency, Mitchell, visual representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 2, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Amanda Wasielewski</div>
W.J.T. Mitchell's influential essay 'What do pictures want?' shifts the theoretical focus away from the interpretative act of understanding pictures and from the motivations of the humans who create them to the possibility that the picture itself is an entity with agency and wants. In this article, I reframe Mitchell's question in light of contemporary AI image generation tools to ask: what do AI-generated images want? Drawing from art historical discourse on the nature of abstraction, I argue that AI-generated images want specificity and concreteness because they are fundamentally abstract. Multimodal text-to-image models, which are the primary subject of this article, are based on the premise that text and image are interchangeable or exchangeable tokens and that there is a commensurability between them, at least as represented mathematically in data. The user pipeline that sees textual input become visual output, however, obscures this representational regress and makes it seem like one form transforms into the other -- as if by magic.
<div><strong>Authors:</strong> Amanda Wasielewski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reframes Mitchell’s question “What do pictures want?” for modern AI text‑to‑image generators, arguing that AI‑generated images “want” specificity and concreteness because they are fundamentally abstract. It critiques the assumption that text and image are interchangeable tokens and highlights the hidden representational regress in the user pipeline.", "summary_cn": "本文将 Mitchell 的“图片想要什么？”问题植到当代 AI 文本‑图像生成模型，主张由于这些图像本质上是抽象的，它们渴求具体和明确的内容。文章批评了文本与图像可互换的假设，指出用户从文字输入到视觉输出的流程隐藏了表征倒退。", "keywords": "AI-generated images, text-to-image models, abstraction, specificity, multimodal semantics, image agency, Mitchell, visual representation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 2, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Amanda Wasielewski"]}
]]></acme>

<pubDate>2025-10-23T08:48:47+00:00</pubDate>
</item>
<item>
<title>Teaching Language Models to Reason with Tools</title>
<link>https://papers.cool/arxiv/2510.20342</link>
<guid>https://papers.cool/arxiv/2510.20342</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CoRT, a post‑training framework that teaches large reasoning models to effectively interleave internal reasoning with external code interpreters via a new Hint‑Engineering data synthesis strategy. By fine‑tuning models of 1.5B to 32B parameters on generated code‑integrated reasoning examples and refining interaction with rejection sampling and reinforcement learning, CoRT achieves notable accuracy gains and token‑usage reductions on several mathematical reasoning benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 CoRT 框架，通过 Hint‑Engineering 数据合成策略，让大规模推理模型学会有效地在内部思考与外部代码解释器之间交替使用。通过在 1.5B‑32B 参数模型上进行监督微调并结合拒绝采样与强化学习的交互优化，CoRT 在多个数学推理基准上实现了显著的准确率提升和约 30%‑50% 的 token 使用量降低。<br /><strong>Keywords:</strong> tool-use, reasoning, code interpreter, CoRT, hint-engineering, supervised fine-tuning, reinforcement learning, mathematical reasoning, efficiency<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</div>
Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\% for the 32B model and 50\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: https://github.com/ChengpengLi1003/CoRT.
<div><strong>Authors:</strong> Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CoRT, a post‑training framework that teaches large reasoning models to effectively interleave internal reasoning with external code interpreters via a new Hint‑Engineering data synthesis strategy. By fine‑tuning models of 1.5B to 32B parameters on generated code‑integrated reasoning examples and refining interaction with rejection sampling and reinforcement learning, CoRT achieves notable accuracy gains and token‑usage reductions on several mathematical reasoning benchmarks.", "summary_cn": "本文提出 CoRT 框架，通过 Hint‑Engineering 数据合成策略，让大规模推理模型学会有效地在内部思考与外部代码解释器之间交替使用。通过在 1.5B‑32B 参数模型上进行监督微调并结合拒绝采样与强化学习的交互优化，CoRT 在多个数学推理基准上实现了显著的准确率提升和约 30%‑50% 的 token 使用量降低。", "keywords": "tool-use, reasoning, code interpreter, CoRT, hint-engineering, supervised fine-tuning, reinforcement learning, mathematical reasoning, efficiency", "scoring": {"interpretability": 4, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"]}
]]></acme>

<pubDate>2025-10-23T08:41:44+00:00</pubDate>
</item>
<item>
<title>Multi-Task Deep Learning for Surface Metrology</title>
<link>https://papers.cool/arxiv/2510.20339</link>
<guid>https://papers.cool/arxiv/2510.20339</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a reproducible deep‑learning framework for surface metrology that jointly predicts texture parameters (Ra, Rz, RONt) and their standard uncertainties using multi‑task heads (quantile and heteroscedastic) with post‑hoc conformal calibration. Experiments on a diverse tactile‑and‑optical dataset show high regression performance for most targets and a 92.85% accuracy for measurement‑system classification, while revealing negative transfer when naïvely sharing a multi‑output trunk.<br /><strong>Summary (CN):</strong> 本文提出了一种可复现的深度学习框架用于表面计量，能够同时预测表面纹理参数（Ra、Rz、RONt）及其标准不确定性，使用分位数和异方差输出头，并通过事后符合校准获得校准区间。基于包含触觉和光学仪器的多仪器数据集，实验显示除RONt不确定性外，回归性能均较高，分类准确率达92.85%，但在简单多输出共享网络中观察到负迁移现象。<br /><strong>Keywords:</strong> surface metrology, multi-task deep learning, uncertainty quantification, quantile regression, heteroscedastic regression, conformal calibration, measurement system classification, Ra, Rz, RONt<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</div>
A reproducible deep learning framework is presented for surface metrology to predict surface texture parameters together with their reported standard uncertainties. Using a multi-instrument dataset spanning tactile and optical systems, measurement system type classification is addressed alongside coordinated regression of Ra, Rz, RONt and their uncertainty targets (Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and heteroscedastic heads with post-hoc conformal calibration to yield calibrated intervals. On a held-out set, high fidelity was achieved by single-target regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and probability calibration was essentially unchanged after temperature scaling (ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for naive multi-output trunks, with single-target models performing better. These results provide calibrated predictions suitable to inform instrument selection and acceptance decisions in metrological workflows.
<div><strong>Authors:</strong> D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a reproducible deep‑learning framework for surface metrology that jointly predicts texture parameters (Ra, Rz, RONt) and their standard uncertainties using multi‑task heads (quantile and heteroscedastic) with post‑hoc conformal calibration. Experiments on a diverse tactile‑and‑optical dataset show high regression performance for most targets and a 92.85% accuracy for measurement‑system classification, while revealing negative transfer when naïvely sharing a multi‑output trunk.", "summary_cn": "本文提出了一种可复现的深度学习框架用于表面计量，能够同时预测表面纹理参数（Ra、Rz、RONt）及其标准不确定性，使用分位数和异方差输出头，并通过事后符合校准获得校准区间。基于包含触觉和光学仪器的多仪器数据集，实验显示除RONt不确定性外，回归性能均较高，分类准确率达92.85%，但在简单多输出共享网络中观察到负迁移现象。", "keywords": "surface metrology, multi-task deep learning, uncertainty quantification, quantile regression, heteroscedastic regression, conformal calibration, measurement system classification, Ra, Rz, RONt", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["D. Kucharski", "A. Gaska", "T. Kowaluk", "K. Stepien", "M. Repalska", "B. Gapinski", "M. Wieczorowski", "M. Nawotka", "P. Sobecki", "P. Sosinowski", "J. Tomasik", "A. Wojtowicz"]}
]]></acme>

<pubDate>2025-10-23T08:38:18+00:00</pubDate>
</item>
<item>
<title>GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?</title>
<link>https://papers.cool/arxiv/2510.20333</link>
<guid>https://papers.cool/arxiv/2510.20333</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GhostEI-Bench, a benchmark that evaluates the resilience of vision‑language model agents operating on mobile graphical user interfaces against environmental injection attacks, where adversarial UI elements are inserted into the device environment. By injecting deceptive overlays and notifications in realistic Android emulator workflows and using a judge‑LLM protocol for fine‑grained failure analysis, the authors demonstrate that current agents are vulnerable to perception and reasoning errors. The benchmark aims to quantify and mitigate this emerging safety threat for embodied AI agents.<br /><strong>Summary (CN):</strong> 本文提出 GhostEI-Bench 基准，用于评估在移动图形用户界面上运行的视觉语言模型代理对环境注入攻击的鲁棒性——即通过在设备界面中插入对抗性 UI 元素（如欺骗性覆盖层或伪造通知）来干扰其视觉感知。该基准在真实的 Android 模拟器工作流中注入恶意事件，并采用 judge‑LLM 协议对代理的动作轨迹和截图序列进行细粒度失败分析，揭示了当前模型在感知和推理层面的显著脆弱性。EI-Bench 为量化并缓解这一新兴安全威胁提供了框架。<br /><strong>Keywords:</strong> environmental injection, mobile agents, vision-language models, GUI adversarial attacks, robustness, safety, benchmark, Android emulator, embodied AI, failure analysis<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Chiyu Chen, Xinhao Song, Yunkai Chai, Yang Yao, Haodong Zhao, Lijun Li, Jie Li, Yan Teng, Gongshen Liu, Yingchun Wang</div>
Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.
<div><strong>Authors:</strong> Chiyu Chen, Xinhao Song, Yunkai Chai, Yang Yao, Haodong Zhao, Lijun Li, Jie Li, Yan Teng, Gongshen Liu, Yingchun Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GhostEI-Bench, a benchmark that evaluates the resilience of vision‑language model agents operating on mobile graphical user interfaces against environmental injection attacks, where adversarial UI elements are inserted into the device environment. By injecting deceptive overlays and notifications in realistic Android emulator workflows and using a judge‑LLM protocol for fine‑grained failure analysis, the authors demonstrate that current agents are vulnerable to perception and reasoning errors. The benchmark aims to quantify and mitigate this emerging safety threat for embodied AI agents.", "summary_cn": "本文提出 GhostEI-Bench 基准，用于评估在移动图形用户界面上运行的视觉语言模型代理对环境注入攻击的鲁棒性——即通过在设备界面中插入对抗性 UI 元素（如欺骗性覆盖层或伪造通知）来干扰其视觉感知。该基准在真实的 Android 模拟器工作流中注入恶意事件，并采用 judge‑LLM 协议对代理的动作轨迹和截图序列进行细粒度失败分析，揭示了当前模型在感知和推理层面的显著脆弱性。EI-Bench 为量化并缓解这一新兴安全威胁提供了框架。", "keywords": "environmental injection, mobile agents, vision-language models, GUI adversarial attacks, robustness, safety, benchmark, Android emulator, embodied AI, failure analysis", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Chiyu Chen", "Xinhao Song", "Yunkai Chai", "Yang Yao", "Haodong Zhao", "Lijun Li", "Jie Li", "Yan Teng", "Gongshen Liu", "Yingchun Wang"]}
]]></acme>

<pubDate>2025-10-23T08:33:24+00:00</pubDate>
</item>
<item>
<title>MemER: Scaling Up Memory for Robot Control via Experience Retrieval</title>
<link>https://papers.cool/arxiv/2510.20328</link>
<guid>https://papers.cool/arxiv/2510.20328</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MemER, a hierarchical robot control framework that enables long‑horizon memory by having a high‑level policy retrieve and track relevant keyframes from past experience and pass them as context to a low‑level vision‑language‑action policy. By fine‑tuning large VLA models (Qwen2.5‑VL‑7B‑Instruct and π₀.₅) and using minimal language annotations, MemER achieves state‑of‑the‑art performance on three real‑world manipulation tasks requiring minutes of memory. The approach demonstrates an efficient way to incorporate experience retrieval into robot policies without conditioning on full observation histories.<br /><strong>Summary (CN):</strong> 本文提出 MemER，一种层次化机器人控制框架，通过让高层策略从以往经验中检索并跟踪关键帧作为上下文，再交给低层视觉‑语言‑动作（VLA）策略执行，从而实现分钟级的长期记忆。作者在 Qwen2.5‑VL‑7B‑Instruct 与 π₀.₅ 上进行微调，仅使用少量语言标注，即在三个真实长时序操作任务上超越现有方法。该方法在不直接使用完整历史观测的情况下，高效地将经验检索融入机器人政策。<br /><strong>Keywords:</strong> memory retrieval, robot control, hierarchical policy, vision-language-action, long-horizon manipulation, keyframe selection, experience replay, large language model, Qwen2.5, robotic memory<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Ajay Sridhar, Jennifer Pan, Satvik Sharma, Chelsea Finn</div>
Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $\pi_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.
<div><strong>Authors:</strong> Ajay Sridhar, Jennifer Pan, Satvik Sharma, Chelsea Finn</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MemER, a hierarchical robot control framework that enables long‑horizon memory by having a high‑level policy retrieve and track relevant keyframes from past experience and pass them as context to a low‑level vision‑language‑action policy. By fine‑tuning large VLA models (Qwen2.5‑VL‑7B‑Instruct and π₀.₅) and using minimal language annotations, MemER achieves state‑of‑the‑art performance on three real‑world manipulation tasks requiring minutes of memory. The approach demonstrates an efficient way to incorporate experience retrieval into robot policies without conditioning on full observation histories.", "summary_cn": "本文提出 MemER，一种层次化机器人控制框架，通过让高层策略从以往经验中检索并跟踪关键帧作为上下文，再交给低层视觉‑语言‑动作（VLA）策略执行，从而实现分钟级的长期记忆。作者在 Qwen2.5‑VL‑7B‑Instruct 与 π₀.₅ 上进行微调，仅使用少量语言标注，即在三个真实长时序操作任务上超越现有方法。该方法在不直接使用完整历史观测的情况下，高效地将经验检索融入机器人政策。", "keywords": "memory retrieval, robot control, hierarchical policy, vision-language-action, long-horizon manipulation, keyframe selection, experience replay, large language model, Qwen2.5, robotic memory", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Ajay Sridhar", "Jennifer Pan", "Satvik Sharma", "Chelsea Finn"]}
]]></acme>

<pubDate>2025-10-23T08:26:17+00:00</pubDate>
</item>
<item>
<title>LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems</title>
<link>https://papers.cool/arxiv/2510.20327</link>
<guid>https://papers.cool/arxiv/2510.20327</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LEGO introduces a lightweight framework for simultaneously unlearning multiple sensitive attributes in recommender systems by first calibrating user embeddings to remove attribute-specific information and then flexibly combining these calibrated embeddings. The approach formulates unlearning as a mutual information minimization problem, providing theoretical guarantees for simultaneous attribute removal while remaining efficient and adaptable to dynamic unlearning requests. Experiments on three datasets and various recommendation models show that LEGO effectively protects privacy with low computational overhead.<br /><strong>Summary (CN):</strong> LEGO 提出一种轻量化框架，通过先对用户嵌入进行校准以去除特定属性信息，再灵活组合这些校准后的嵌入，实现对推荐系统中多个敏感属性的同步删除。该方法将卸记过程建模为互信息最小化问题，提供了同时卸记的理论保证，并能够高效适应动态的卸记需求。实验在三个真实数据集和多种推荐模型上验证了 LEGO 在保护隐私的同时保持低计算开销的效果。<br /><strong>Keywords:</strong> recommendation unlearning, multiple-attribute privacy, embedding calibration, mutual information minimization, efficient unlearning, recommender systems<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Fengyuan Yu, Yuyuan Li, Xiaohua Feng, Junjie Fang, Tao Wang, Chaochao Chen</div>
With the growing demand for safeguarding sensitive user information in recommender systems, recommendation attribute unlearning is receiving increasing attention. Existing studies predominantly focus on single-attribute unlearning. However, privacy protection requirements in the real world often involve multiple sensitive attributes and are dynamic. Existing single-attribute unlearning methods cannot meet these real-world requirements due to i) CH1: the inability to handle multiple unlearning requests simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic unlearning needs. To address these challenges, we propose LEGO, a lightweight and efficient multiple-attribute unlearning framework. Specifically, we divide the multiple-attribute unlearning process into two steps: i) Embedding Calibration removes information related to a specific attribute from user embedding, and ii) Flexible Combination combines these embeddings into a single embedding, protecting all sensitive attributes. We frame the unlearning process as a mutual information minimization problem, providing LEGO a theoretical guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step framework, where Embedding Calibration can be performed in parallel and Flexible Combination is flexible and efficient, we address CH2. Extensive experiments on three real-world datasets across three representative recommendation models demonstrate the effectiveness and efficiency of our proposed framework. Our code and appendix are available at https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.
<div><strong>Authors:</strong> Fengyuan Yu, Yuyuan Li, Xiaohua Feng, Junjie Fang, Tao Wang, Chaochao Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LEGO introduces a lightweight framework for simultaneously unlearning multiple sensitive attributes in recommender systems by first calibrating user embeddings to remove attribute-specific information and then flexibly combining these calibrated embeddings. The approach formulates unlearning as a mutual information minimization problem, providing theoretical guarantees for simultaneous attribute removal while remaining efficient and adaptable to dynamic unlearning requests. Experiments on three datasets and various recommendation models show that LEGO effectively protects privacy with low computational overhead.", "summary_cn": "LEGO 提出一种轻量化框架，通过先对用户嵌入进行校准以去除特定属性信息，再灵活组合这些校准后的嵌入，实现对推荐系统中多个敏感属性的同步删除。该方法将卸记过程建模为互信息最小化问题，提供了同时卸记的理论保证，并能够高效适应动态的卸记需求。实验在三个真实数据集和多种推荐模型上验证了 LEGO 在保护隐私的同时保持低计算开销的效果。", "keywords": "recommendation unlearning, multiple-attribute privacy, embedding calibration, mutual information minimization, efficient unlearning, recommender systems", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Fengyuan Yu", "Yuyuan Li", "Xiaohua Feng", "Junjie Fang", "Tao Wang", "Chaochao Chen"]}
]]></acme>

<pubDate>2025-10-23T08:20:47+00:00</pubDate>
</item>
<item>
<title>Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses</title>
<link>https://papers.cool/arxiv/2510.20314</link>
<guid>https://papers.cool/arxiv/2510.20314</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a comprehensive survey of adversarial attacks and defense methods for deep reinforcement learning (DRL), classifying attacks by perturbation type and target (state, action, reward, model) and reviewing robustness training strategies such as adversarial training, competitive training, detection, and distillation. It discusses the strengths and limitations of these approaches and outlines future research directions to improve generalization, computational efficiency, scalability, and explainability of DRL in adversarial settings.<br /><strong>Summary (CN):</strong> 本文对深度强化学习（DRL）的对抗攻击与防御方法进行全面综述，按照扰动类型和攻击目标（状态空间、动作空间、奖励函数、模型空间）构建分类框架，并详细回顾了对抗训练、竞争训练、鲁棒学习、对抗检测、蒸馏等主流防御策略，分析它们的优缺点。最后展望了在对抗环境中提升 DRL 泛化能力、降低计算复杂度、强化可扩展性和可解释性的研究方向。<br /><strong>Keywords:</strong> deep reinforcement learning, adversarial attacks, adversarial defenses, robustness, security, adversarial training, detection, perturbation, DRL safety, survey<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Wu Yichao, Wang Yirui, Ding Panpan, Wang Hailong, Zhu Bingqian, Liu Chun</div>
With the wide application of deep reinforcement learning (DRL) techniques in complex fields such as autonomous driving, intelligent manufacturing, and smart healthcare, how to improve its security and robustness in dynamic and changeable environments has become a core issue in current research. Especially in the face of adversarial attacks, DRL may suffer serious performance degradation or even make potentially dangerous decisions, so it is crucial to ensure their stability in security-sensitive scenarios. In this paper, we first introduce the basic framework of DRL and analyze the main security challenges faced in complex and changing environments. In addition, this paper proposes an adversarial attack classification framework based on perturbation type and attack target and reviews the mainstream adversarial attack methods against DRL in detail, including various attack methods such as perturbation state space, action space, reward function and model space. To effectively counter the attacks, this paper systematically summarizes various current robustness training strategies, including adversarial training, competitive training, robust learning, adversarial detection, defense distillation and other related defense techniques, we also discuss the advantages and shortcomings of these methods in improving the robustness of DRL. Finally, this paper looks into the future research direction of DRL in adversarial environments, emphasizing the research needs in terms of improving generalization, reducing computational complexity, and enhancing scalability and explainability, aiming to provide valuable references and directions for researchers.
<div><strong>Authors:</strong> Wu Yichao, Wang Yirui, Ding Panpan, Wang Hailong, Zhu Bingqian, Liu Chun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a comprehensive survey of adversarial attacks and defense methods for deep reinforcement learning (DRL), classifying attacks by perturbation type and target (state, action, reward, model) and reviewing robustness training strategies such as adversarial training, competitive training, detection, and distillation. It discusses the strengths and limitations of these approaches and outlines future research directions to improve generalization, computational efficiency, scalability, and explainability of DRL in adversarial settings.", "summary_cn": "本文对深度强化学习（DRL）的对抗攻击与防御方法进行全面综述，按照扰动类型和攻击目标（状态空间、动作空间、奖励函数、模型空间）构建分类框架，并详细回顾了对抗训练、竞争训练、鲁棒学习、对抗检测、蒸馏等主流防御策略，分析它们的优缺点。最后展望了在对抗环境中提升 DRL 泛化能力、降低计算复杂度、强化可扩展性和可解释性的研究方向。", "keywords": "deep reinforcement learning, adversarial attacks, adversarial defenses, robustness, security, adversarial training, detection, perturbation, DRL safety, survey", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Wu Yichao", "Wang Yirui", "Ding Panpan", "Wang Hailong", "Zhu Bingqian", "Liu Chun"]}
]]></acme>

<pubDate>2025-10-23T08:04:57+00:00</pubDate>
</item>
<item>
<title>DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability</title>
<link>https://papers.cool/arxiv/2510.20299</link>
<guid>https://papers.cool/arxiv/2510.20299</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DB-FGA-Net, a dual‑backbone network combining VGG16 and Xception with a Frequency‑Gated Attention block to capture complementary local and global features for brain tumor classification. It achieves state‑of‑the‑art accuracy on multiple class settings without any data augmentation and incorporates Grad‑CAM visualizations and real‑time GUI to provide interpretable tumor localization for clinical use.<br /><strong>Summary (CN):</strong> 本文提出 DB-FGA-Net，使用 VGG16 与 Xception 双主干并加入 Frequency‑Gated Attention（频率门控注意力）块，以捕捉局部与全局互补特征，实现脑肿瘤的多类分类。模型在无需数据增强的情况下取得了近乎最佳的分类精度，并集成 Grad‑CAM 可视化及实时 GUI，提供可解释的肿瘤定位，提升临床可用性。<br /><strong>Keywords:</strong> brain tumor classification, dual backbone, frequency gated attention, Grad-CAM, interpretability, medical imaging, deep learning, augmentation-free, VGG16, Xception<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
Brain tumors are a challenging problem in neuro-oncology, where early and precise diagnosis is important for successful treatment. Deep learning-based brain tumor classification methods often rely on heavy data augmentation which can limit generalization and trust in clinical applications. In this paper, we propose a double-backbone network integrating VGG16 and Xception with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Unlike previous studies, our model achieves state-of-the-art performance without augmentation which demonstrates robustness to variably sized and distributed datasets. For further transparency, Grad-CAM is integrated to visualize the tumor regions based on which the model is giving prediction, bridging the gap between model prediction and clinical interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class and 2-class settings, respectively. On the independent 3K-DS dataset, the model generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art methods. To further support clinical usability, we developed a graphical user interface (GUI) that provides real-time classification and Grad-CAM-based tumor localization. These findings suggest that augmentation-free, interpretable, and deployable deep learning models such as DB-FGA-Net hold strong potential for reliable clinical translation in brain tumor diagnosis.
<div><strong>Authors:</strong> Saraf Anzum Shreya, MD. Abu Ismail Siddique, Sharaf Tasnim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DB-FGA-Net, a dual‑backbone network combining VGG16 and Xception with a Frequency‑Gated Attention block to capture complementary local and global features for brain tumor classification. It achieves state‑of‑the‑art accuracy on multiple class settings without any data augmentation and incorporates Grad‑CAM visualizations and real‑time GUI to provide interpretable tumor localization for clinical use.", "summary_cn": "本文提出 DB-FGA-Net，使用 VGG16 与 Xception 双主干并加入 Frequency‑Gated Attention（频率门控注意力）块，以捕捉局部与全局互补特征，实现脑肿瘤的多类分类。模型在无需数据增强的情况下取得了近乎最佳的分类精度，并集成 Grad‑CAM 可视化及实时 GUI，提供可解释的肿瘤定位，提升临床可用性。", "keywords": "brain tumor classification, dual backbone, frequency gated attention, Grad-CAM, interpretability, medical imaging, deep learning, augmentation-free, VGG16, Xception", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"]}
]]></acme>

<pubDate>2025-10-23T07:39:00+00:00</pubDate>
</item>
<item>
<title>RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective</title>
<link>https://papers.cool/arxiv/2510.20296</link>
<guid>https://papers.cool/arxiv/2510.20296</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RAG-Stack, a three‑pillar framework for jointly optimizing generation quality and system performance in retrieval‑augmented generation pipelines from the vector‑database viewpoint. It proposes an intermediate representation (RAG‑IR) to separate quality and performance concerns, a cost model (RAG‑CM) to predict performance, and a plan‑exploration algorithm (RAG‑PE) to find high‑quality, high‑performance configurations. The authors argue this blueprint will become a standard paradigm for RAG system co‑optimization.<br /><strong>Summary (CN):</strong> 本文提出 RAG‑Stack 框架，旨在从向量数据库的角度共同优化检索增强生成（RAG）系统的生成质量与运行性能。框架包括用于解耦质量与性能的中间表示（RAG‑IR）、用于预测系统性能的成本模型（RAG‑CM），以及搜索高质量高性能配置的探索算法（RAG‑PE）。作者认为该蓝图将成为未来 RAG 质量‑性能协同优化的主流范式。<br /><strong>Keywords:</strong> retrieval-augmented generation, vector database, quality-performance co-optimization, intermediate representation, cost model, plan exploration, RAG-Stack<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wenqi Jiang</div>
Retrieval-augmented generation (RAG) has emerged as one of the most prominent applications of vector databases. By integrating documents retrieved from a database into the prompt of a large language model (LLM), RAG enables more reliable and informative content generation. While there has been extensive research on vector databases, many open research problems remain once they are considered in the wider context of end-to-end RAG pipelines. One practical yet challenging problem is how to jointly optimize both system performance and generation quality in RAG, which is significantly more complex than it appears due to the numerous knobs on both the algorithmic side (spanning models and databases) and the systems side (from software to hardware). In this paper, we present RAG-Stack, a three-pillar blueprint for quality-performance co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an intermediate representation that serves as an abstraction layer to decouple quality and performance aspects; (2) RAG-CM, a cost model for estimating system performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that searches for high-quality, high-performance RAG configurations. We believe this three-pillar blueprint will become the de facto paradigm for RAG quality-performance co-optimization in the years to come.
<div><strong>Authors:</strong> Wenqi Jiang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RAG-Stack, a three‑pillar framework for jointly optimizing generation quality and system performance in retrieval‑augmented generation pipelines from the vector‑database viewpoint. It proposes an intermediate representation (RAG‑IR) to separate quality and performance concerns, a cost model (RAG‑CM) to predict performance, and a plan‑exploration algorithm (RAG‑PE) to find high‑quality, high‑performance configurations. The authors argue this blueprint will become a standard paradigm for RAG system co‑optimization.", "summary_cn": "本文提出 RAG‑Stack 框架，旨在从向量数据库的角度共同优化检索增强生成（RAG）系统的生成质量与运行性能。框架包括用于解耦质量与性能的中间表示（RAG‑IR）、用于预测系统性能的成本模型（RAG‑CM），以及搜索高质量高性能配置的探索算法（RAG‑PE）。作者认为该蓝图将成为未来 RAG 质量‑性能协同优化的主流范式。", "keywords": "retrieval-augmented generation, vector database, quality-performance co-optimization, intermediate representation, cost model, plan exploration, RAG-Stack", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenqi Jiang"]}
]]></acme>

<pubDate>2025-10-23T07:35:19+00:00</pubDate>
</item>
<item>
<title>A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization</title>
<link>https://papers.cool/arxiv/2510.20291</link>
<guid>https://papers.cool/arxiv/2510.20291</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a parameter‑efficient Mixture‑of‑Experts framework for cross‑modal geo‑localization, combining platform‑specific experts, domain‑aligned preprocessing, and an LLM‑based caption refinement pipeline to handle heterogeneity between satellite, drone, and ground images. Using BGE‑M3 for text and EVA‑CLIP for images, the system trains three experts with progressive hard‑negative mining and fuses their scores, achieving top performance on the RoboSense 2025 Track 4 leaderboard.<br /><strong>Summary (CN):</strong> 本文提出了一种参数高效的混合（Mixture‑of‑Experts）框架用于跨模态地理定位，结合平台特定专家、域对齐的预处理以及基于大语言模型（LLM）的字幕精炼流水线，以应对卫星、无人机和地面图像之间的异构性。使用 BGE‑M3（文本）和 EVA‑CLIP（图像），通过渐进式硬负例挖掘训练三个专家，并在推理时融合其得分，最终在 RoboSense 2025 第四赛道排行榜中取得首位。<br /><strong>Keywords:</strong> mixture-of-experts, cross-modal retrieval, geo-localization, domain alignment, LLM caption refinement, hard-negative mining, multi-platform, EVA-CLIP, BGE-M3<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li</div>
We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.
<div><strong>Authors:</strong> LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a parameter‑efficient Mixture‑of‑Experts framework for cross‑modal geo‑localization, combining platform‑specific experts, domain‑aligned preprocessing, and an LLM‑based caption refinement pipeline to handle heterogeneity between satellite, drone, and ground images. Using BGE‑M3 for text and EVA‑CLIP for images, the system trains three experts with progressive hard‑negative mining and fuses their scores, achieving top performance on the RoboSense 2025 Track 4 leaderboard.", "summary_cn": "本文提出了一种参数高效的混合（Mixture‑of‑Experts）框架用于跨模态地理定位，结合平台特定专家、域对齐的预处理以及基于大语言模型（LLM）的字幕精炼流水线，以应对卫星、无人机和地面图像之间的异构性。使用 BGE‑M3（文本）和 EVA‑CLIP（图像），通过渐进式硬负例挖掘训练三个专家，并在推理时融合其得分，最终在 RoboSense 2025 第四赛道排行榜中取得首位。", "keywords": "mixture-of-experts, cross-modal retrieval, geo-localization, domain alignment, LLM caption refinement, hard-negative mining, multi-platform, EVA-CLIP, BGE-M3", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["LinFeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li"]}
]]></acme>

<pubDate>2025-10-23T07:23:47+00:00</pubDate>
</item>
<item>
<title>Breakdance Video classification in the age of Generative AI</title>
<link>https://papers.cool/arxiv/2510.20287</link>
<guid>https://papers.cool/arxiv/2510.20287</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates the use of modern video foundation models for classifying breakdance videos, comparing encoder-only models with encoder-decoder video language models. Results indicate that video encoders outperform state-of-the-art video‑language models on this niche sport classification task, and the study provides practical guidance for model selection and analysis of finetuned decoders.<br /><strong>Summary (CN):</strong> 本文研究了现代视频基础模型在街舞（breakdance）视频分类中的应用，比较了仅使用编码器的模型与编码器‑解码器视频语言模型的表现。结果显示，视频编码器在此细分运动的分类任务上优于最先进语言模型，并提供了模型选择的实用建议以及对微调解码器工作原理的深入分析。<br /><strong>Keywords:</strong> breakdance, video classification, video foundation models, vision-language models, encoder, decoder, generative AI<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.
<div><strong>Authors:</strong> Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates the use of modern video foundation models for classifying breakdance videos, comparing encoder-only models with encoder-decoder video language models. Results indicate that video encoders outperform state-of-the-art video‑language models on this niche sport classification task, and the study provides practical guidance for model selection and analysis of finetuned decoders.", "summary_cn": "本文研究了现代视频基础模型在街舞（breakdance）视频分类中的应用，比较了仅使用编码器的模型与编码器‑解码器视频语言模型的表现。结果显示，视频编码器在此细分运动的分类任务上优于最先进语言模型，并提供了模型选择的实用建议以及对微调解码器工作原理的深入分析。", "keywords": "breakdance, video classification, video foundation models, vision-language models, encoder, decoder, generative AI", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Michelle Munson"]}
]]></acme>

<pubDate>2025-10-23T07:18:54+00:00</pubDate>
</item>
<item>
<title>UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</title>
<link>https://papers.cool/arxiv/2510.20286</link>
<guid>https://papers.cool/arxiv/2510.20286</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Instruction-as-Reasoning paradigm for GUI grounding, treating natural-language instructions as dynamic analytical pathways that can be selected and composed during inference. A two‑stage training pipeline (supervised fine‑tuning on diverse synthesized instructions followed by reinforcement learning) enables models UI‑Ins‑7B and UI‑Ins‑32B to achieve state‑of‑the‑art performance on several grounding benchmarks and demonstrate emergent reasoning abilities. The work also analyzes how instruction diversity affects grounding and mitigates policy collapse in the SFT+RL framework.<br /><strong>Summary (CN):</strong> 本文提出“指令即推理”(Instruction-as-Reasoning)范式，将 GUI 任务中的自然语言指令视为可在推理过程中动态选择和组合的分析路径。通过先在多样化合成指令上进行监督微调，再使用强化学习优化路径选择，构建了 UI‑Ins‑7B 与 UI‑Ins‑32B 两个模型，在多个 GUI 对齐基准上取得了最先进的性能，并展示了在推理时生成并组合新指令路径的能力。论文进一步分析了指令多样性对对齐表现的影响，并解决了 SFT+RL 框架中的策略崩塌问题。<br /><strong>Keywords:</strong> GUI grounding, instruction-as-reasoning, multi-perspective instructions, reinforcement learning, UI agents, grounding benchmarks<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi</div>
GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.
<div><strong>Authors:</strong> Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Instruction-as-Reasoning paradigm for GUI grounding, treating natural-language instructions as dynamic analytical pathways that can be selected and composed during inference. A two‑stage training pipeline (supervised fine‑tuning on diverse synthesized instructions followed by reinforcement learning) enables models UI‑Ins‑7B and UI‑Ins‑32B to achieve state‑of‑the‑art performance on several grounding benchmarks and demonstrate emergent reasoning abilities. The work also analyzes how instruction diversity affects grounding and mitigates policy collapse in the SFT+RL framework.", "summary_cn": "本文提出“指令即推理”(Instruction-as-Reasoning)范式，将 GUI 任务中的自然语言指令视为可在推理过程中动态选择和组合的分析路径。通过先在多样化合成指令上进行监督微调，再使用强化学习优化路径选择，构建了 UI‑Ins‑7B 与 UI‑Ins‑32B 两个模型，在多个 GUI 对齐基准上取得了最先进的性能，并展示了在推理时生成并组合新指令路径的能力。论文进一步分析了指令多样性对对齐表现的影响，并解决了 SFT+RL 框架中的策略崩塌问题。", "keywords": "GUI grounding, instruction-as-reasoning, multi-perspective instructions, reinforcement learning, UI agents, grounding benchmarks", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liangyu Chen", "Hanzhang Zhou", "Chenglin Cai", "Jianan Zhang", "Panrong Tong", "Quyu Kong", "Xu Zhang", "Chen Liu", "Yuqi Liu", "Wenxuan Wang", "Yue Wang", "Qin Jin", "Steven Hoi"]}
]]></acme>

<pubDate>2025-10-23T07:18:32+00:00</pubDate>
</item>
<item>
<title>Context-level Language Modeling by Learning Predictive Context Embeddings</title>
<link>https://papers.cool/arxiv/2510.20280</link>
<guid>https://papers.cool/arxiv/2510.20280</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ContextLM, a framework that adds a next-context prediction objective to standard next-token language model pretraining, enabling models to learn predictive embeddings of multi-token contexts. Experiments with GPT-2 and Pythia models up to 1.5B parameters show consistent improvements in perplexity and downstream tasks, attributed to better long-range coherence and more efficient attention allocation without breaking token-by-token evaluation.<br /><strong>Summary (CN):</strong> 本文提出 ContextLM 框架，在传统的逐词预测预训练中加入下一上下文预测目标，使模型能够学习多词上下文的预测表示（predictive context embeddings）。在 GPT-2 与 Pythia 系列模型（最高 1.5 B 参数）的实验表明，该方法在保持自回归评估（如困惑度）的前提下，提升了困惑度和下游任务表现，主要得益于更好的长程语义连贯性和更高效的注意力分配。<br /><strong>Keywords:</strong> next-context prediction, context embeddings, language modeling, long-range coherence, GPT-2, Pythia, pretraining, predictive representations<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Beiya Dai, Yuliang Liu, Daozheng Xue, Qipeng Guo, Kai Chen, Xinbing Wang</div>
Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model's capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \textbf{ContextLM}, a framework that augments standard pretraining with an inherent \textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.
<div><strong>Authors:</strong> Beiya Dai, Yuliang Liu, Daozheng Xue, Qipeng Guo, Kai Chen, Xinbing Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ContextLM, a framework that adds a next-context prediction objective to standard next-token language model pretraining, enabling models to learn predictive embeddings of multi-token contexts. Experiments with GPT-2 and Pythia models up to 1.5B parameters show consistent improvements in perplexity and downstream tasks, attributed to better long-range coherence and more efficient attention allocation without breaking token-by-token evaluation.", "summary_cn": "本文提出 ContextLM 框架，在传统的逐词预测预训练中加入下一上下文预测目标，使模型能够学习多词上下文的预测表示（predictive context embeddings）。在 GPT-2 与 Pythia 系列模型（最高 1.5 B 参数）的实验表明，该方法在保持自回归评估（如困惑度）的前提下，提升了困惑度和下游任务表现，主要得益于更好的长程语义连贯性和更高效的注意力分配。", "keywords": "next-context prediction, context embeddings, language modeling, long-range coherence, GPT-2, Pythia, pretraining, predictive representations", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Beiya Dai", "Yuliang Liu", "Daozheng Xue", "Qipeng Guo", "Kai Chen", "Xinbing Wang"]}
]]></acme>

<pubDate>2025-10-23T07:09:45+00:00</pubDate>
</item>
<item>
<title>Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs</title>
<link>https://papers.cool/arxiv/2510.20272</link>
<guid>https://papers.cool/arxiv/2510.20272</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates whether process reward model (PRM)-guided tree search can improve mathematical reasoning in large to chain-of-thought Best-of-N selection. Experiments on 23 problems using Qwen2.5-Math-7B-Instruct show no significant benefit, reveal that PRMs poorly approximate state values and degrade with depth, and that Monte Carlo tree search and beam search outperform other PRM-guided methods. The authors conclude that current PRM quality limits the effectiveness of tree search for mathematical reasoning.<br /><strong>Summary (CN):</strong> 本文评估了基于过程奖励模型（PRM）的树搜索能否在大语言模型的数学推理中优于链式思考的 Best-of-N 方法。使用 Qwen2.5-Math-7B-Instruct 在 23 道题目上的实验表明，PRM 引导的树搜索未能显著提升性能，且 PRM 对状态价值的近似效果差，随推理深度下降，Monte Carlo 树搜索和束搜索在多个 PRM 引导方法中表现更佳。作者指出，当前 PRM 的质量限制了树搜索在数学推理中的有效性。<br /><strong>Keywords:</strong> process reward, tree search, mathematical reasoning, large language models, Monte Carlo tree search, beam search, PRM reliability, exploration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Tristan Cinquin, Geoff Pleiss, Agustinus Kristiadi</div>
While chain-of-thought prompting with Best-of-N (BoN) selection has become popular for mathematical reasoning in large language models (LLMs), its linear structure fails to capture the branching and exploratory nature of complex problem-solving. In this work, we propose an adaptive algorithm to maximize process reward model (PRM) scores over the intractable action space, and investigate whether PRM-guided tree search can improve mathematical reasoning by exploring multiple partial solution paths. Across $23$ diverse mathematical problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case study, we find that: (1) PRM-guided tree search shows no statistically significant improvements over BoN despite higher costs, (2) Monte Carlo tree search and beam search outperform other PRM-guided tree search methods, (3) PRMs poorly approximate state values and their reliability degrades with reasoning depth, and (4) PRMs generalize poorly out of distribution. This underperformance stems from tree search's greater reliance on unreliable PRM scores, suggesting different reward modeling is necessary before tree search can effectively enhance mathematical reasoning in LLMs.
<div><strong>Authors:</strong> Tristan Cinquin, Geoff Pleiss, Agustinus Kristiadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates whether process reward model (PRM)-guided tree search can improve mathematical reasoning in large to chain-of-thought Best-of-N selection. Experiments on 23 problems using Qwen2.5-Math-7B-Instruct show no significant benefit, reveal that PRMs poorly approximate state values and degrade with depth, and that Monte Carlo tree search and beam search outperform other PRM-guided methods. The authors conclude that current PRM quality limits the effectiveness of tree search for mathematical reasoning.", "summary_cn": "本文评估了基于过程奖励模型（PRM）的树搜索能否在大语言模型的数学推理中优于链式思考的 Best-of-N 方法。使用 Qwen2.5-Math-7B-Instruct 在 23 道题目上的实验表明，PRM 引导的树搜索未能显著提升性能，且 PRM 对状态价值的近似效果差，随推理深度下降，Monte Carlo 树搜索和束搜索在多个 PRM 引导方法中表现更佳。作者指出，当前 PRM 的质量限制了树搜索在数学推理中的有效性。", "keywords": "process reward, tree search, mathematical reasoning, large language models, Monte Carlo tree search, beam search, PRM reliability, exploration", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Tristan Cinquin", "Geoff Pleiss", "Agustinus Kristiadi"]}
]]></acme>

<pubDate>2025-10-23T06:59:36+00:00</pubDate>
</item>
<item>
<title>Towards AI Agents for Course Instruction in Higher Education: Early Experiences from the Field</title>
<link>https://papers.cool/arxiv/2510.20255</link>
<guid>https://papers.cool/arxiv/2510.20255</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports on the design, deployment, and early evaluation of a large‑language‑model‑based Instructor Agent that serves as the primary teacher for a graduate‑level cloud computing course. It introduces a pedagogical framework that combines the Agent with human instructors and an analytical framework that measures engagement via topic coverage, depth, and turn‑level elaboration. Preliminary results show a shift from broad conceptual exploration to deeper inquiry, suggesting that conversational AI can support reflective learning at scale.<br /><strong>Summary (CN):</strong> 本文介绍了一种基于大型语言模型（LLM）的教学代理（Instructor Agent），在印度理工学院（IISc）的一门研究生云计算课程中担任主要教师，并报告了其早期设计、部署及评估经验。文章提出了将该代理与人工教师相结合的教学框架，以及通过主题覆盖率、深度和回合层面阐释度等可解释参与度指标来分析师生互动的分析框架。初步结果显示，学生互动从宽泛的概念探索逐步转向更深入的针对性探讨，表明对话式AI有助于促进反思式学习并实现高质量教育的规模化。<br /><strong>Keywords:</strong> AI instructor agent, large language model, educational technology, engagement metrics, conversational AI, higher education, pedagogical framework, student interaction, reflective learning, LLM-driven teaching<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yogesh Simmhan, Varad Kulkarni</div>
This article presents early findings from designing, deploying and evaluating an AI-based educational agent deployed as the primary instructor in a graduate-level Cloud Computing course at IISc. We detail the design of a Large Language Model (LLM)-driven Instructor Agent, and introduce a pedagogical framework that integrates the Instructor Agent into the course workflow for actively interacting with the students for content delivery, supplemented by the human instructor to offer the course structure and undertake question--answer sessions. We also propose an analytical framework that evaluates the Agent--Student interaction transcripts using interpretable engagement metrics of topic coverage, topic depth and turn-level elaboration. We report early experiences on how students interact with the Agent to explore concepts, clarify doubts and sustain inquiry-driven dialogue during live classroom sessions. We also report preliminary analysis on our evaluation metrics applied across two successive instructional modules that reveals patterns of engagement evolution, transitioning from broad conceptual exploration to deeper, focused inquiry. These demonstrate how structured integration of conversational AI agents can foster reflective learning, offer a reproducible methodology for studying engagement in authentic classroom settings, and support scalable, high-quality higher education.
<div><strong>Authors:</strong> Yogesh Simmhan, Varad Kulkarni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports on the design, deployment, and early evaluation of a large‑language‑model‑based Instructor Agent that serves as the primary teacher for a graduate‑level cloud computing course. It introduces a pedagogical framework that combines the Agent with human instructors and an analytical framework that measures engagement via topic coverage, depth, and turn‑level elaboration. Preliminary results show a shift from broad conceptual exploration to deeper inquiry, suggesting that conversational AI can support reflective learning at scale.", "summary_cn": "本文介绍了一种基于大型语言模型（LLM）的教学代理（Instructor Agent），在印度理工学院（IISc）的一门研究生云计算课程中担任主要教师，并报告了其早期设计、部署及评估经验。文章提出了将该代理与人工教师相结合的教学框架，以及通过主题覆盖率、深度和回合层面阐释度等可解释参与度指标来分析师生互动的分析框架。初步结果显示，学生互动从宽泛的概念探索逐步转向更深入的针对性探讨，表明对话式AI有助于促进反思式学习并实现高质量教育的规模化。", "keywords": "AI instructor agent, large language model, educational technology, engagement metrics, conversational AI, higher education, pedagogical framework, student interaction, reflective learning, LLM-driven teaching", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yogesh Simmhan", "Varad Kulkarni"]}
]]></acme>

<pubDate>2025-10-23T06:23:35+00:00</pubDate>
</item>
<item>
<title>What Does It Take to Build a Performant Selective Classifier?</title>
<link>https://papers.cool/arxiv/2510.20242</link>
<guid>https://papers.cool/arxiv/2510.20242</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper formalizes the selective‑classification gap and provides a finite‑sample decomposition into five sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift slack. It shows that post‑hoc monotone calibration rarely improves the ranking and that closing the gap requires scoring mechanisms that reorder predictions, validated on synthetic and vision/language benchmarks. The analysis yields an error budget and design guidelines for building more performant selective classifiers.<br /><strong>Summary (CN):</strong> 本文形式化了选择性分类的误差差距，并提出了一个有限样本的分解，将差距归因于贝叶斯噪声、近似误差、排序误差、统计噪声以及实现或分布漂移导致的松弛五个来源。研究表明单调后处理校准对排名影响有限，真正缩小差距需要能够重新排序预测的评分机制，并在合成的月牙数据以及真实视觉和语言基准上进行验证。该分解提供了量化的误差预算和可操作的设计指南，以构建更接近理想oracle的选择性分类器。<br /><strong>Keywords:</strong> selective classification, abstention, calibration, ranking error, Bayes noise, distributional shift, reliability, model uncertainty, error decomposition, feature-aware calibrator<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Stephan Rabanser, Nicolas Papernot</div>
Selective classifiers improve model reliability by abstaining on inputs the model deems uncertain. However, few practical approaches achieve the gold-standard performance of a perfect-ordering oracle that accepts examples exactly in order of correctness. Our work formalizes this shortfall as the selective-classification gap and present the first finite-sample decomposition of this gap to five distinct sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation- or shift-induced slack. Crucially, our analysis reveals that monotone post-hoc calibration -- often believed to strengthen selective classifiers -- has limited impact on closing this gap, since it rarely alters the model's underlying score ranking. Bridging the gap therefore requires scoring mechanisms that can effectively reorder predictions rather than merely rescale them. We validate our decomposition on synthetic two-moons data and on real-world vision and language benchmarks, isolating each error component through controlled experiments. Our results confirm that (i) Bayes noise and limited model capacity can account for substantial gaps, (ii) only richer, feature-aware calibrators meaningfully improve score ordering, and (iii) data shift introduces a separate slack that demands distributionally robust training. Together, our decomposition yields a quantitative error budget as well as actionable design guidelines that practitioners can use to build selective classifiers which approximate ideal oracle behavior more closely.
<div><strong>Authors:</strong> Stephan Rabanser, Nicolas Papernot</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper formalizes the selective‑classification gap and provides a finite‑sample decomposition into five sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift slack. It shows that post‑hoc monotone calibration rarely improves the ranking and that closing the gap requires scoring mechanisms that reorder predictions, validated on synthetic and vision/language benchmarks. The analysis yields an error budget and design guidelines for building more performant selective classifiers.", "summary_cn": "本文形式化了选择性分类的误差差距，并提出了一个有限样本的分解，将差距归因于贝叶斯噪声、近似误差、排序误差、统计噪声以及实现或分布漂移导致的松弛五个来源。研究表明单调后处理校准对排名影响有限，真正缩小差距需要能够重新排序预测的评分机制，并在合成的月牙数据以及真实视觉和语言基准上进行验证。该分解提供了量化的误差预算和可操作的设计指南，以构建更接近理想oracle的选择性分类器。", "keywords": "selective classification, abstention, calibration, ranking error, Bayes noise, distributional shift, reliability, model uncertainty, error decomposition, feature-aware calibrator", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Stephan Rabanser", "Nicolas Papernot"]}
]]></acme>

<pubDate>2025-10-23T05:48:40+00:00</pubDate>
</item>
<item>
<title>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders</title>
<link>https://papers.cool/arxiv/2510.20239</link>
<guid>https://papers.cool/arxiv/2510.20239</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified tri‑modal affective severity framework that fuses interview text, audio, and facial signals to predict graded severity levels for both depression and PTSD. Using transformer‑based text embeddings, log‑Mel audio features, and facial action units, a calibrated late‑fusion classifier outputs disorder‑specific probabilities together with feature‑level attributions, achieving superior performance over unimodal baselines and robustness to missing modalities. Experiments on DAIC‑derived corpora demonstrate improved accuracy, weighted F1, and decision‑curve utility, with text driving depression severity and audio‑facial cues crucial for PTSD.<br /><strong>Summary (CN):</strong> 本文提出一种统一的三模态情感严重程度框架，融合访谈文本、音频和面部信号，以预测抑郁症和创伤后应激障碍（PTSD）的分级严重程度。利用基于 transformer 的文本嵌入、对数梅尔（log‑Mel）音频特征和面部行动单元（AU），通过校准的后期融合分类器输出每种疾病的概率并提供特征层面的归因，性能优于单模态基线且在缺失模态下表现鲁棒。对 DAIC 派生数据集的实验显示准确率、加权 F1 和决策曲线效用均有所提升，文本对抑郁严重程度贡献最大，音频和面部线索对 PTSD 关键。<br /><strong>Keywords:</strong> multimodal affective analysis, depression severity, PTSD severity, late fusion classifier, feature attribution, text embeddings, audio log-Mel features, facial action units, clinical decision support<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao</div>
Depression and post traumatic stress disorder (PTSD) often co-occur with connected symptoms, complicating automated assessment, which is often binary and disorder specific. Clinically useful diagnosis needs severity aware cross disorder estimates and decision support explanations. Our unified tri modal affective severity framework synchronizes and fuses interview text with sentence level transformer embeddings, audio with log Mel statistics with deltas, and facial signals with action units, gaze, head and pose descriptors to output graded severities for diagnosing both depression (PHQ-8; 5 classes) and PTSD (3 classes). Standardized features are fused via a calibrated late fusion classifier, yielding per disorder probabilities and feature-level attributions. This severity aware tri-modal affective fusion approach is demoed on multi disorder concurrent depression and PTSD assessment. Stratified cross validation on DAIC derived corpora outperforms unimodal/ablation baselines. The fused model matches the strongest unimodal baseline on accuracy and weighted F1, while improving decision curve utility and robustness under noisy or missing modalities. For PTSD specifically, fusion reduces regression error and improves class concordance. Errors cluster between adjacent severities; extreme classes are identified reliably. Ablations show text contributes most to depression severity, audio and facial cues are critical for PTSD, whereas attributions align with linguistic and behavioral markers. Our approach offers reproducible evaluation and clinician in the loop support for affective clinical decision making.
<div><strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified tri‑modal affective severity framework that fuses interview text, audio, and facial signals to predict graded severity levels for both depression and PTSD. Using transformer‑based text embeddings, log‑Mel audio features, and facial action units, a calibrated late‑fusion classifier outputs disorder‑specific probabilities together with feature‑level attributions, achieving superior performance over unimodal baselines and robustness to missing modalities. Experiments on DAIC‑derived corpora demonstrate improved accuracy, weighted F1, and decision‑curve utility, with text driving depression severity and audio‑facial cues crucial for PTSD.", "summary_cn": "本文提出一种统一的三模态情感严重程度框架，融合访谈文本、音频和面部信号，以预测抑郁症和创伤后应激障碍（PTSD）的分级严重程度。利用基于 transformer 的文本嵌入、对数梅尔（log‑Mel）音频特征和面部行动单元（AU），通过校准的后期融合分类器输出每种疾病的概率并提供特征层面的归因，性能优于单模态基线且在缺失模态下表现鲁棒。对 DAIC 派生数据集的实验显示准确率、加权 F1 和决策曲线效用均有所提升，文本对抑郁严重程度贡献最大，音频和面部线索对 PTSD 关键。", "keywords": "multimodal affective analysis, depression severity, PTSD severity, late fusion classifier, feature attribution, text embeddings, audio log-Mel features, facial action units, clinical decision support", "scoring": {"interpretability": 7, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Filippo Cenacchi", "Deborah Richards", "Longbing Cao"]}
]]></acme>

<pubDate>2025-10-23T05:46:38+00:00</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach</title>
<link>https://papers.cool/arxiv/2510.20235</link>
<guid>https://papers.cool/arxiv/2510.20235</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a provably convergent framework for multi-objective reinforcement learning using the max-min criterion, reformulating the problem as a two-player zero-sum regularized continuous game and solving it with a mirror-descent algorithm that enjoys global last-iterate convergence. The authors analyze iteration and sample complexities for exact and approximate policy evaluation, introduce adaptive regularization, and demonstrate superior performance over prior baselines in both tabular and deep MORL environments.<br /><strong>Summary (CN):</strong> 本文提出了一种针对最大-最小准则的多目标强化学习框架，将其重新表述为双玩家零和正则化连续博弈，并基于镜像下降设计了具备全局最后迭代收敛性的高效算法。作者给出在精确与近似策略评估下的迭代复杂度和样本复杂度分析，进一步引入自适应正则化，并在表格及深度 MORL 环境中展示出显著优于现有基线的实验结果。<br /><strong>Keywords:</strong> multi-objective reinforcement learning, max-min criterion, game-theoretic, mirror descent, regularized continuous game, sample complexity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Woohyeon Byeon, Giseung Park, Jongseong Chae, Amir Leshem, Youngchul Sung</div>
In this paper, we propose a provably convergent and practical framework for multi-objective reinforcement learning with max-min criterion. From a game-theoretic perspective, we reformulate max-min multi-objective reinforcement learning as a two-player zero-sum regularized continuous game and introduce an efficient algorithm based on mirror descent. Our approach simplifies the policy update while ensuring global last-iterate convergence. We provide a comprehensive theoretical analysis on our algorithm, including iteration complexity under both exact and approximate policy evaluations, as well as sample complexity bounds. To further enhance performance, we modify the proposed algorithm with adaptive regularization. Our experiments demonstrate the convergence behavior of the proposed algorithm in tabular settings, and our implementation for deep reinforcement learning significantly outperforms previous baselines in many MORL environments.
<div><strong>Authors:</strong> Woohyeon Byeon, Giseung Park, Jongseong Chae, Amir Leshem, Youngchul Sung</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a provably convergent framework for multi-objective reinforcement learning using the max-min criterion, reformulating the problem as a two-player zero-sum regularized continuous game and solving it with a mirror-descent algorithm that enjoys global last-iterate convergence. The authors analyze iteration and sample complexities for exact and approximate policy evaluation, introduce adaptive regularization, and demonstrate superior performance over prior baselines in both tabular and deep MORL environments.", "summary_cn": "本文提出了一种针对最大-最小准则的多目标强化学习框架，将其重新表述为双玩家零和正则化连续博弈，并基于镜像下降设计了具备全局最后迭代收敛性的高效算法。作者给出在精确与近似策略评估下的迭代复杂度和样本复杂度分析，进一步引入自适应正则化，并在表格及深度 MORL 环境中展示出显著优于现有基线的实验结果。", "keywords": "multi-objective reinforcement learning, max-min criterion, game-theoretic, mirror descent, regularized continuous game, sample complexity", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Woohyeon Byeon", "Giseung Park", "Jongseong Chae", "Amir Leshem", "Youngchul Sung"]}
]]></acme>

<pubDate>2025-10-23T05:39:26+00:00</pubDate>
</item>
<item>
<title>Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context</title>
<link>https://papers.cool/arxiv/2510.20229</link>
<guid>https://papers.cool/arxiv/2510.20229</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates why large vision-language models (LVLMs) generate more hallucinations in longer, free-form responses, attributing the phenomenon to increased reliance on context rather than length alone. It introduces an "induce-detect-suppress" framework that deliberately creates hallucinations via crafted contexts, uses these instances for early detection of high-risk cases, and suppresses object-level hallucinations during decoding, achieving consistent improvements across benchmarks. The work provides new insights into the role of context in LVLM hallucinations and offers a practical mitigation strategy.<br /><strong>Summary (CN):</strong> 本文研究了大型视觉语言模型（LVLM）在较长自由文本回复中更易出现幻觉的原因，认为这主要源于为了保持连贯性和完整性而对上下文的依赖增加，而非单纯的长度问题。作者提出了“诱导-检测-抑制”框架，通过设计特定上下文诱发幻觉，利用诱导实例提前检测高风险情况，并在实际解码时抑制对象层面的幻觉，从而在所有基准测试上取得稳定显著的提升。该研究为理解上下文在 LVLM 幻觉产生中的作用提供了新视角，并提供了一种实用的幻觉缓解方案。<br /><strong>Keywords:</strong> LVLM, hallucination, context, detection, mitigation, vision-language, induce-detect-suppress, factual consistency<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel "induce-detect-suppress" framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs' longer responses.
<div><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates why large vision-language models (LVLMs) generate more hallucinations in longer, free-form responses, attributing the phenomenon to increased reliance on context rather than length alone. It introduces an \"induce-detect-suppress\" framework that deliberately creates hallucinations via crafted contexts, uses these instances for early detection of high-risk cases, and suppresses object-level hallucinations during decoding, achieving consistent improvements across benchmarks. The work provides new insights into the role of context in LVLM hallucinations and offers a practical mitigation strategy.", "summary_cn": "本文研究了大型视觉语言模型（LVLM）在较长自由文本回复中更易出现幻觉的原因，认为这主要源于为了保持连贯性和完整性而对上下文的依赖增加，而非单纯的长度问题。作者提出了“诱导-检测-抑制”框架，通过设计特定上下文诱发幻觉，利用诱导实例提前检测高风险情况，并在实际解码时抑制对象层面的幻觉，从而在所有基准测试上取得稳定显著的提升。该研究为理解上下文在 LVLM 幻觉产生中的作用提供了新视角，并提供了一种实用的幻觉缓解方案。", "keywords": "LVLM, hallucination, context, detection, mitigation, vision-language, induce-detect-suppress, factual consistency", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"]}
]]></acme>

<pubDate>2025-10-23T05:22:07+00:00</pubDate>
</item>
<item>
<title>Federated Learning via Meta-Variational Dropout</title>
<link>https://papers.cool/arxiv/2510.20225</link>
<guid>https://papers.cool/arxiv/2510.20225</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Meta-Variational Dropout (MetaVD), a Bayesian meta-learning method that predicts client-specific dropout rates via a shared hypernetwork to improve personalization, mitigate overfitting, and reduce communication in federated learning with limited non-IID data. Experiments show that MetaVD achieves higher classification accuracy, better uncertainty calibration, and effective out-of-distribution performance while compressing local model parameters.<br /><strong>Summary (CN):</strong> 本文提出了元变分 dropout（MetaVD）——一种基于贝叶斯元学习的方案，通过共享超网络预测每个客户端的 dropout 率，以在数据稀疏且非 IID 的联邦学习环境中实现模型个性化、降低过拟合并压缩通信量。实验表明，MetaVD 在分类准确率、uncertainty 校准以及对 OOD 客户端的性能上均优于基线，同时能显著压缩本地模型参数。<br /><strong>Keywords:</strong> federated learning, meta-learning, variational dropout, Bayesian FL, non-IID data, model personalization, uncertainty calibration, OOD detection<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Insu Jeon, Minui Hong, Junhyeog Yun, Gunhee Kim</div>
Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD.
<div><strong>Authors:</strong> Insu Jeon, Minui Hong, Junhyeog Yun, Gunhee Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Meta-Variational Dropout (MetaVD), a Bayesian meta-learning method that predicts client-specific dropout rates via a shared hypernetwork to improve personalization, mitigate overfitting, and reduce communication in federated learning with limited non-IID data. Experiments show that MetaVD achieves higher classification accuracy, better uncertainty calibration, and effective out-of-distribution performance while compressing local model parameters.", "summary_cn": "本文提出了元变分 dropout（MetaVD）——一种基于贝叶斯元学习的方案，通过共享超网络预测每个客户端的 dropout 率，以在数据稀疏且非 IID 的联邦学习环境中实现模型个性化、降低过拟合并压缩通信量。实验表明，MetaVD 在分类准确率、uncertainty 校准以及对 OOD 客户端的性能上均优于基线，同时能显著压缩本地模型参数。", "keywords": "federated learning, meta-learning, variational dropout, Bayesian FL, non-IID data, model personalization, uncertainty calibration, OOD detection", "scoring": {"interpretability": 4, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Insu Jeon", "Minui Hong", "Junhyeog Yun", "Gunhee Kim"]}
]]></acme>

<pubDate>2025-10-23T05:17:40+00:00</pubDate>
</item>
<item>
<title>QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20222</link>
<guid>https://papers.cool/arxiv/2510.20222</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes QKCV (Query‑Key‑Category‑Value) attention, which extends the standard QKV attention by adding a static categorical embedding to incorporate category information in time‑series forecasting models. As a plug‑in module, QKCV improves forecasting accuracy across several attention‑based architectures and enables efficient fine‑tuning of pre‑trained univariate foundation models by updating only the categorical embeddings. Experiments on real‑world datasets demonstrate superior performance with minimal computational overhead.<br /><strong>Summary (CN):</strong> 本文提出 QKCV（Query‑Key‑Category‑Value）注意力机制，通过引入静态类别嵌入 C 来在时间序列预测模型中强调类别信息，从而扩展传统的 QKV 注意力。作为通用插件，QKCV 在多种基于注意力的模型（如 Vanilla Transformer、Informer、PatchTST、TFT）上提升了预测精度，并且在微调预训练的单变量基础模型时，仅更新类别嵌入即可实现高效、轻量的适配，显著降低计算开销并取得更佳性能。实验在多个真实数据集上验证了其有效性。<br /><strong>Keywords:</strong> time series forecasting, attention mechanism, QKCV attention, categorical embeddings, foundation models, fine-tuning, transformer, Informer, PatchTST<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hao Wang, Baojun Ma</div>
In real-world time series forecasting tasks, category information plays a pivotal role in capturing inherent data patterns. This paper introduces QKCV (Query-Key-Category-Value) attention, an extension of the traditional QKV framework that incorporates a static categorical embedding C to emphasize category-specific information. As a versatile plug-in module, QKCV enhances the forecasting accuracy of attention-based models (e.g., Vanilla Transformer, Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV demonstrates remarkable adaptability in fine-tuning univariate time series foundation model by solely updating the static embedding C while preserving pretrained weights, thereby reducing computational overhead and achieving superior fine-tuning performance.
<div><strong>Authors:</strong> Hao Wang, Baojun Ma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes QKCV (Query‑Key‑Category‑Value) attention, which extends the standard QKV attention by adding a static categorical embedding to incorporate category information in time‑series forecasting models. As a plug‑in module, QKCV improves forecasting accuracy across several attention‑based architectures and enables efficient fine‑tuning of pre‑trained univariate foundation models by updating only the categorical embeddings. Experiments on real‑world datasets demonstrate superior performance with minimal computational overhead.", "summary_cn": "本文提出 QKCV（Query‑Key‑Category‑Value）注意力机制，通过引入静态类别嵌入 C 来在时间序列预测模型中强调类别信息，从而扩展传统的 QKV 注意力。作为通用插件，QKCV 在多种基于注意力的模型（如 Vanilla Transformer、Informer、PatchTST、TFT）上提升了预测精度，并且在微调预训练的单变量基础模型时，仅更新类别嵌入即可实现高效、轻量的适配，显著降低计算开销并取得更佳性能。实验在多个真实数据集上验证了其有效性。", "keywords": "time series forecasting, attention mechanism, QKCV attention, categorical embeddings, foundation models, fine-tuning, transformer, Informer, PatchTST", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hao Wang", "Baojun Ma"]}
]]></acme>

<pubDate>2025-10-21T18:15:21+00:00</pubDate>
</item>
<item>
<title>FinCARE: Financial Causal Analysis with Reasoning and Evidence</title>
<link>https://papers.cool/arxiv/2510.20221</link>
<guid>https://papers.cool/arxiv/2510.20221</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FinCARE introduces a hybrid framework that combines statistical causal discovery methods (PC, GES, NOTEARS) with domain knowledge from a financial knowledge graph extracted from SEC filings and reasoning from large language models. By encoding graph constraints and using LLMs for hypothesis generation, the system markedly improves causal edge detection on a synthetic dataset and enables accurate counterfactual scenario analysis for finance applications. The approach aims to give portfolio managers a more reliable causal foundation for risk management and strategic decision‑making.<br /><strong>Summary (CN):</strong> FinCARE 提出一种混合框架，将统计因果发现算法（PC、GES、NOTEARS）与从 SEC 10‑K 文件中提取的金融知识图谱以及大语言模型（LLM）的概念推理相结合。通过在算法中加入图谱约束并利用 LLM 生成假设，该系统在合成金融数据集上显著提升了因果边的检测精度，并实现了对反事实情景的精确预测，为资产管理者提供可靠的因果基础以进行风险管理和战略决策。<br /><strong>Keywords:</strong> causal discovery, financial knowledge graph, LLM reasoning, PC algorithm, GES, NOTEARS, counterfactual prediction, finance, risk management<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alejandro Michel, Abhinav Arun, Bhaskarjit Sarmah, Stefano Pasquali</div>
Portfolio managers rely on correlation-based analysis and heuristic methods that fail to capture true causal relationships driving performance. We present a hybrid framework that integrates statistical causal discovery algorithms with domain knowledge from two complementary sources: a financial knowledge graph extracted from SEC 10-K filings and large language model reasoning. Our approach systematically enhances three representative causal discovery paradigms, constraint-based (PC), score-based (GES), and continuous optimization (NOTEARS), by encoding knowledge graph constraints algorithmically and leveraging LLM conceptual reasoning for hypothesis generation. Evaluated on a synthetic financial dataset of 500 firms across 18 variables, our KG+LLM-enhanced methods demonstrate consistent improvements across all three algorithms: PC (F1: 0.622 vs. 0.459 baseline, +36%), GES (F1: 0.735 vs. 0.367, +100%), and NOTEARS (F1: 0.759 vs. 0.163, +366%). The framework enables reliable scenario analysis with mean absolute error of 0.003610 for counterfactual predictions and perfect directional accuracy for intervention effects. It also addresses critical limitations of existing methods by grounding statistical discoveries in financial domain expertise while maintaining empirical validation, providing portfolio managers with the causal foundation necessary for proactive risk management and strategic decision-making in dynamic market environments.
<div><strong>Authors:</strong> Alejandro Michel, Abhinav Arun, Bhaskarjit Sarmah, Stefano Pasquali</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FinCARE introduces a hybrid framework that combines statistical causal discovery methods (PC, GES, NOTEARS) with domain knowledge from a financial knowledge graph extracted from SEC filings and reasoning from large language models. By encoding graph constraints and using LLMs for hypothesis generation, the system markedly improves causal edge detection on a synthetic dataset and enables accurate counterfactual scenario analysis for finance applications. The approach aims to give portfolio managers a more reliable causal foundation for risk management and strategic decision‑making.", "summary_cn": "FinCARE 提出一种混合框架，将统计因果发现算法（PC、GES、NOTEARS）与从 SEC 10‑K 文件中提取的金融知识图谱以及大语言模型（LLM）的概念推理相结合。通过在算法中加入图谱约束并利用 LLM 生成假设，该系统在合成金融数据集上显著提升了因果边的检测精度，并实现了对反事实情景的精确预测，为资产管理者提供可靠的因果基础以进行风险管理和战略决策。", "keywords": "causal discovery, financial knowledge graph, LLM reasoning, PC algorithm, GES, NOTEARS, counterfactual prediction, finance, risk management", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alejandro Michel", "Abhinav Arun", "Bhaskarjit Sarmah", "Stefano Pasquali"]}
]]></acme>

<pubDate>2025-10-23T05:14:28+00:00</pubDate>
</item>
<item>
<title>High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning</title>
<link>https://papers.cool/arxiv/2510.20218</link>
<guid>https://papers.cool/arxiv/2510.20218</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Continued Fraction Q-Learning (QCoFr), a value decomposition framework that captures arbitrary-order interactions among agents with linear O(n) complexity, avoiding combinatorial explosion. By employing a variational information bottleneck to extract latent information for credit estimation, the method filters noisy interactions, improving both cooperation performance and interpretability. Extensive experiments show that QCoFr consistently outperforms baselines while providing interpretable insights that align with the authors' theoretical analysis.<br /><strong>Summary (CN):</strong> 本文提出了一种名为 Continued Fraction Q-Learning (QCoFr) 的价值分解框架，能够以线性 O(n) 复杂度灵活捕获任意阶的多智能体交互，从而避免了组合爆炸问题。通过引入变分信息瓶颈提取潜在信息用于信用估计，帮助智能体过滤噪声交互，显著提升了合作性能并提供了与理论分析一致的可解释性。实验表明该方法在性能和可解释性上均优于现有方法。<br /><strong>Keywords:</strong> multi-agent reinforcement learning, high-order interactions, value decomposition, continued fraction Q-learning, interpretability, credit assignment, variational information bottleneck, linear complexity<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Qinyu Xu, Yuanyang Zhu, Xuefei Wu, Chunlin Chen</div>
The ability to model interactions among agents is crucial for effective coordination and understanding their cooperation mechanisms in multi-agent reinforcement learning (MARL). However, previous efforts to model high-order interactions have been primarily hindered by the combinatorial explosion or the opaque nature of their black-box network structures. In this paper, we propose a novel value decomposition framework, called Continued Fraction Q-Learning (QCoFr), which can flexibly capture arbitrary-order agent interactions with only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents, thus avoiding the combinatorial explosion when modeling rich cooperation. Furthermore, we introduce the variational information bottleneck to extract latent information for estimating credits. This latent information helps agents filter out noisy interactions, thereby significantly enhancing both cooperation and interpretability. Extensive experiments demonstrate that QCoFr not only consistently achieves better performance but also provides interpretability that aligns with our theoretical analysis.
<div><strong>Authors:</strong> Qinyu Xu, Yuanyang Zhu, Xuefei Wu, Chunlin Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Continued Fraction Q-Learning (QCoFr), a value decomposition framework that captures arbitrary-order interactions among agents with linear O(n) complexity, avoiding combinatorial explosion. By employing a variational information bottleneck to extract latent information for credit estimation, the method filters noisy interactions, improving both cooperation performance and interpretability. Extensive experiments show that QCoFr consistently outperforms baselines while providing interpretable insights that align with the authors' theoretical analysis.", "summary_cn": "本文提出了一种名为 Continued Fraction Q-Learning (QCoFr) 的价值分解框架，能够以线性 O(n) 复杂度灵活捕获任意阶的多智能体交互，从而避免了组合爆炸问题。通过引入变分信息瓶颈提取潜在信息用于信用估计，帮助智能体过滤噪声交互，显著提升了合作性能并提供了与理论分析一致的可解释性。实验表明该方法在性能和可解释性上均优于现有方法。", "keywords": "multi-agent reinforcement learning, high-order interactions, value decomposition, continued fraction Q-learning, interpretability, credit assignment, variational information bottleneck, linear complexity", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Qinyu Xu", "Yuanyang Zhu", "Xuefei Wu", "Chunlin Chen"]}
]]></acme>

<pubDate>2025-10-23T05:08:32+00:00</pubDate>
</item>
<item>
<title>Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents</title>
<link>https://papers.cool/arxiv/2510.20211</link>
<guid>https://papers.cool/arxiv/2510.20211</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NSync, an automated system that uses large language model agents to detect and reconcile infrastructure drift by analyzing cloud API traces and synthesizing updates to Terraform IaC configurations. It employs a self‑evolving knowledge base of past reconciliations and demonstrates superior accuracy and token efficiency across real‑world projects compared to baselines.<br /><strong>Summary (CN):</strong> 本文提出 NSync 系统，利用大语言模型（LLM）代理分析云 API 调用序列，检测基础设施漂移并自动生成 Terraform IaC 配置的更新，实现配置与实际资源的一致性。系统通过自我演化的知识库不断改进，并在真实项目的实验中展示出比基线更高的准确率和更高的令牌效率。<br /><strong>Keywords:</strong> Infrastructure-as-Code, IaC drift, Terraform, LLM agents, API trace analysis, cloud reconciliation, NSync, self-evolving knowledge base, pass@3<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Zhenning Yang, Hui Guan, Victor Nicolet, Brandon Paulsen, Joey Dodds, Daniel Kroening, Ang Chen</div>
Cloud infrastructure is managed through a mix of interfaces -- traditionally, cloud consoles, command-line interfaces (CLI), and SDKs are the tools of choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the infrastructure in a "source-of-truth" configuration. They are capable of automatically carrying out modifications to the cloud -- deploying, updating, or destroying resources -- to bring the actual infrastructure into alignment with the IaC configuration. However, when IaC is used alongside consoles, CLIs, or SDKs, it loses visibility into external changes, causing infrastructure drift, where the configuration becomes outdated, and later IaC operations may undo valid updates or trigger errors. We present NSync, an automated system for IaC reconciliation that propagates out-of-band changes back into the IaC program. Our key insight is that infrastructure changes eventually all occur via cloud API invocations -- the lowest layer for cloud management operations. NSync gleans insights from API traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update the IaC configuration to capture the changes). It employs an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. We further introduce a novel evaluation pipeline for injecting realistic drifts into cloud infrastructure and assessing reconciliation performance. Experiments across five real-world Terraform projects and 372 drift scenarios show that NSync outperforms the baseline both in terms of accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$ improvement).
<div><strong>Authors:</strong> Zhenning Yang, Hui Guan, Victor Nicolet, Brandon Paulsen, Joey Dodds, Daniel Kroening, Ang Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NSync, an automated system that uses large language model agents to detect and reconcile infrastructure drift by analyzing cloud API traces and synthesizing updates to Terraform IaC configurations. It employs a self‑evolving knowledge base of past reconciliations and demonstrates superior accuracy and token efficiency across real‑world projects compared to baselines.", "summary_cn": "本文提出 NSync 系统，利用大语言模型（LLM）代理分析云 API 调用序列，检测基础设施漂移并自动生成 Terraform IaC 配置的更新，实现配置与实际资源的一致性。系统通过自我演化的知识库不断改进，并在真实项目的实验中展示出比基线更高的准确率和更高的令牌效率。", "keywords": "Infrastructure-as-Code, IaC drift, Terraform, LLM agents, API trace analysis, cloud reconciliation, NSync, self-evolving knowledge base, pass@3", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"]}
]]></acme>

<pubDate>2025-10-23T04:57:00+00:00</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset</title>
<link>https://papers.cool/arxiv/2510.20209</link>
<guid>https://papers.cool/arxiv/2510.20209</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The study evaluates whether routine laboratory data can be used to detect early cancer in dogs by benchmarking 126 machine‑learning pipelines on the Golden Retriever Lifetime Study cohort. The best model (logistic regression with class weighting and recursive feature elimination) achieved moderate AUROC (0.815) but low clinical utility (F1‑score 0.25, PPV 0.15), with high NPV yet insufficient recall for a reliable rule‑out test. SHAP analysis showed predictions were driven by non‑specific factors such as age and inflammatory markers, indicating that the cancer signal in these data is weak and confounded.<br /><strong>Summary (CN):</strong> 本研究评估常规实验室数据用于犬类早期癌症检测的可行性，针对 Golden Retriever Lifetime Study 队列对 126 条机器学习流水线进行基准测试。最佳模型（使用类别加权和递归特征消除的 Logistic Regression）实现了中等的 AUROC（0.815），但临床实用性较低（F1 分数 0.25，阳性预测值 0.15），尽管阴性预测值高（0.98），但召回率不足，无法可靠用于排除检测。SHAP（Shapley Additive exPlanations）分析显示预测主要受年龄和炎症/贫血等非特异性特征驱动，表明单纯实验室数据中的癌症信号薄弱且受混淆因素影响。<br /><strong>Keywords:</strong> cancer detection, veterinary oncology, machine learning, imbalanced dataset, logistic regression, SHAP, feature selection, early detection, routine laboratory data, classification performance<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shumin Li</div>
The development of accessible screening tools for early cancer detection in dogs represents a significant challenge in veterinary medicine. Routine laboratory data offer a promising, low-cost source for such tools, but their utility is hampered by the non-specificity of individual biomarkers and the severe class imbalance inherent in screening populations. This study assesses the feasibility of cancer risk classification using the Golden Retriever Lifetime Study (GRLS) cohort under real-world constraints, including the grouping of diverse cancer types and the inclusion of post-diagnosis samples. A comprehensive benchmark evaluation was conducted, systematically comparing 126 analytical pipelines that comprised various machine learning models, feature selection methods, and data balancing techniques. Data were partitioned at the patient level to prevent leakage. The optimal model, a Logistic Regression classifier with class weighting and recursive feature elimination, demonstrated moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical classification performance (F1-score = 0.25, Positive Predictive Value = 0.15). While a high Negative Predictive Value (0.98) was achieved, insufficient recall (0.79) precludes its use as a reliable rule-out test. Interpretability analysis with SHapley Additive exPlanations (SHAP) revealed that predictions were driven by non-specific features like age and markers of inflammation and anemia. It is concluded that while a statistically detectable cancer signal exists in routine lab data, it is too weak and confounded for clinically reliable discrimination from normal aging or other inflammatory conditions. This work establishes a critical performance ceiling for this data modality in isolation and underscores that meaningful progress in computational veterinary oncology will require integration of multi-modal data sources.
<div><strong>Authors:</strong> Shumin Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The study evaluates whether routine laboratory data can be used to detect early cancer in dogs by benchmarking 126 machine‑learning pipelines on the Golden Retriever Lifetime Study cohort. The best model (logistic regression with class weighting and recursive feature elimination) achieved moderate AUROC (0.815) but low clinical utility (F1‑score 0.25, PPV 0.15), with high NPV yet insufficient recall for a reliable rule‑out test. SHAP analysis showed predictions were driven by non‑specific factors such as age and inflammatory markers, indicating that the cancer signal in these data is weak and confounded.", "summary_cn": "本研究评估常规实验室数据用于犬类早期癌症检测的可行性，针对 Golden Retriever Lifetime Study 队列对 126 条机器学习流水线进行基准测试。最佳模型（使用类别加权和递归特征消除的 Logistic Regression）实现了中等的 AUROC（0.815），但临床实用性较低（F1 分数 0.25，阳性预测值 0.15），尽管阴性预测值高（0.98），但召回率不足，无法可靠用于排除检测。SHAP（Shapley Additive exPlanations）分析显示预测主要受年龄和炎症/贫血等非特异性特征驱动，表明单纯实验室数据中的癌症信号薄弱且受混淆因素影响。", "keywords": "cancer detection, veterinary oncology, machine learning, imbalanced dataset, logistic regression, SHAP, feature selection, early detection, routine laboratory data, classification performance", "scoring": {"interpretability": 5, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shumin Li"]}
]]></acme>

<pubDate>2025-10-23T04:52:42+00:00</pubDate>
</item>
<item>
<title>Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20198</link>
<guid>https://papers.cool/arxiv/2510.20198</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a benchmark suite of five grid‑based tasks to evaluate the spatial reasoning abilities of large language models from textual inputs. Experiments show that while models perform reasonably on small grids, accuracy declines sharply as grid size and complexity increase, with drops up to 84%, indicating a lack of robust spatial representations. The study highlights a gap between linguistic and geometric reasoning in current LLMs and proposes this suite as a tool for future research.<br /><strong>Summary (CN):</strong> 本文提出了包含五个基于网格的任务的基准，用于评估大语言模型在文本输入下的空间推理能力。实验表明，模型在小尺度网格上表现尚可，但随着网格尺寸和任务复杂度的提升，准确率急剧下降，最大降幅达84%，暗示其缺乏稳健的空间表示。该工作揭示了语言与几何推理之间的差距，并为后续研究提供了评估基准。<br /><strong>Keywords:</strong> spatial reasoning, large language models, benchmark, grid tasks, geometry, capability evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
This paper explores the spatial reasoning capability of large language models (LLMs) over textual input through a suite of five tasks aimed at probing their spatial understanding and computational abilities. The models were tested on both fundamental spatial reasoning and multi-step problem-solving within structured grid-based environments using tasks such as quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Each task was scaled in complexity through increasing grid dimensions, requiring models to extend beyond simple pattern recognition into abstract spatial reasoning. Our results reveal that while LLMs demonstrate moderate success in all tasks with small complexity and size, performance drops off rapidly as scale increases, with an average loss in accuracy of 42.7%, and reaching as high as 84%. Every test that began with over 50% accuracy showed a loss of at least 48%, illustrating the consistent nature of the deterioration. Furthermore, their struggles with scaling complexity hint at a lack of robust spatial representations in their underlying architectures. This paper underscores the gap between linguistic and spatial reasoning in LLMs, offering insights into their current limitations, and laying the groundwork for future integrative benchmarks at the intersection of language and geometry.
<div><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a benchmark suite of five grid‑based tasks to evaluate the spatial reasoning abilities of large language models from textual inputs. Experiments show that while models perform reasonably on small grids, accuracy declines sharply as grid size and complexity increase, with drops up to 84%, indicating a lack of robust spatial representations. The study highlights a gap between linguistic and geometric reasoning in current LLMs and proposes this suite as a tool for future research.", "summary_cn": "本文提出了包含五个基于网格的任务的基准，用于评估大语言模型在文本输入下的空间推理能力。实验表明，模型在小尺度网格上表现尚可，但随着网格尺寸和任务复杂度的提升，准确率急剧下降，最大降幅达84%，暗示其缺乏稳健的空间表示。该工作揭示了语言与几何推理之间的差距，并为后续研究提供了评估基准。", "keywords": "spatial reasoning, large language models, benchmark, grid tasks, geometry, capability evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"]}
]]></acme>

<pubDate>2025-10-23T04:32:46+00:00</pubDate>
</item>
<item>
<title>PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</title>
<link>https://papers.cool/arxiv/2510.20178</link>
<guid>https://papers.cool/arxiv/2510.20178</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PPMStereo, a Pick-and-Play Memory (PPM) construction module designed to achieve temporally consistent depth estimation in stereo video by selecting and adaptively weighting relevant past frames, enabling efficient long-range spatio-temporal aggregation. Experiments show state-of-the-art accuracy and temporal consistency with reduced computational cost compared to prior methods.<br /><strong>Summary (CN):</strong> 本文提出 PPMStereo，一种基于 Pick-and-Play Memory（PPM）构建模块的动态立体匹配方法，通过‘pick’过程挑选最相关帧，再通过‘play’过程自适应加权，实现高效的长程时空信息聚合，从而在立体视频中获得时间上连续的深度估计。实验表明该方法在精度和时间一致性上均优于现有技术，并且计算成本更低。<br /><strong>Keywords:</strong> dynamic stereo matching, temporal consistency, pick-and-play memory, spatio-temporal aggregation, memory buffer<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu</div>
Temporally consistent depth estimation from stereo video is critical for real-world applications such as augmented reality, where inconsistent depth estimation disrupts the immersion of users. Despite its importance, this task remains challenging due to the difficulty in modeling long-term temporal consistency in a computationally efficient manner. Previous methods attempt to address this by aggregating spatio-temporal information but face a fundamental trade-off: limited temporal modeling provides only modest gains, whereas capturing long-range dependencies significantly increases computational cost. To address this limitation, we introduce a memory buffer for modeling long-range spatio-temporal consistency while achieving efficient dynamic stereo matching. Inspired by the two-stage decision-making process in humans, we propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM consists of a `pick' process that identifies the most relevant frames and a `play' process that weights the selected frames adaptively for spatio-temporal aggregation. This two-stage collaborative process maintains a compact yet highly informative memory buffer while achieving temporally consistent information aggregation. Extensive experiments validate the effectiveness of PPMStereo, demonstrating state-of-the-art performance in both accuracy and temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer computational costs. Codes are available at \textcolor{blue}{https://github.com/cocowy1/PPMStereo}.
<div><strong>Authors:</strong> Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PPMStereo, a Pick-and-Play Memory (PPM) construction module designed to achieve temporally consistent depth estimation in stereo video by selecting and adaptively weighting relevant past frames, enabling efficient long-range spatio-temporal aggregation. Experiments show state-of-the-art accuracy and temporal consistency with reduced computational cost compared to prior methods.", "summary_cn": "本文提出 PPMStereo，一种基于 Pick-and-Play Memory（PPM）构建模块的动态立体匹配方法，通过‘pick’过程挑选最相关帧，再通过‘play’过程自适应加权，实现高效的长程时空信息聚合，从而在立体视频中获得时间上连续的深度估计。实验表明该方法在精度和时间一致性上均优于现有技术，并且计算成本更低。", "keywords": "dynamic stereo matching, temporal consistency, pick-and-play memory, spatio-temporal aggregation, memory buffer", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yun Wang", "Junjie Hu", "Qiaole Dong", "Yongjian Zhang", "Yanwei Fu", "Tin Lun Lam", "Dapeng Wu"]}
]]></acme>

<pubDate>2025-10-23T03:52:39+00:00</pubDate>
</item>
<item>
<title>Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</title>
<link>https://papers.cool/arxiv/2510.20176</link>
<guid>https://papers.cool/arxiv/2510.20176</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Mixture-of-Minds, a multi-agent framework that splits table reasoning into three specialized roles—planning, coding, and answering—leveraging code execution for precise manipulation. It proposes a self-improvement training loop using Monte Carlo Tree Search rollouts to generate pseudo‑gold trajectories and fine‑tune the agents via reinforcement learning, achieving state‑of‑the‑art performance on TableBench. Experiments demonstrate substantial gains over existing fine‑tuned and tool‑based approaches, highlighting the benefits of structured multi‑agent workflows for table understanding.<br /><strong>Summary (CN):</strong> 本文提出 Mixture-of-Minds 多智能体框架，将表格推理拆分为规划、编码和回答三个专职角色，利用代码执行实现精确的表格操作。作者使用蒙特卡罗树搜索（MCTS）生成伪金轨迹，并通过强化学习对各智能体进行自我改进训练，实现了在 TableBench 上显著的性能提升，超越了现有的微调和工具化方法。实验结果表明，结构化的多智能体工作流能够有效提升表格理解能力。<br /><strong>Keywords:</strong> table understanding, multi-agent reinforcement learning, Mixture-of-Minds, Monte Carlo Tree Search, code execution, TableBench, planning agent, coding agent, answer agent<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei wang, Jiayi Liu, Fei Liu, Serena Li, Weiwi Li, Mingze Gao, Abhishek Kumar, Xiangjun Fan, Zhuokai Zhao, Lizhu Zhang</div>
Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.
<div><strong>Authors:</strong> Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei wang, Jiayi Liu, Fei Liu, Serena Li, Weiwi Li, Mingze Gao, Abhishek Kumar, Xiangjun Fan, Zhuokai Zhao, Lizhu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Mixture-of-Minds, a multi-agent framework that splits table reasoning into three specialized roles—planning, coding, and answering—leveraging code execution for precise manipulation. It proposes a self-improvement training loop using Monte Carlo Tree Search rollouts to generate pseudo‑gold trajectories and fine‑tune the agents via reinforcement learning, achieving state‑of‑the‑art performance on TableBench. Experiments demonstrate substantial gains over existing fine‑tuned and tool‑based approaches, highlighting the benefits of structured multi‑agent workflows for table understanding.", "summary_cn": "本文提出 Mixture-of-Minds 多智能体框架，将表格推理拆分为规划、编码和回答三个专职角色，利用代码执行实现精确的表格操作。作者使用蒙特卡罗树搜索（MCTS）生成伪金轨迹，并通过强化学习对各智能体进行自我改进训练，实现了在 TableBench 上显著的性能提升，超越了现有的微调和工具化方法。实验结果表明，结构化的多智能体工作流能够有效提升表格理解能力。", "keywords": "table understanding, multi-agent reinforcement learning, Mixture-of-Minds, Monte Carlo Tree Search, code execution, TableBench, planning agent, coding agent, answer agent", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhang Zhou", "Mingrui Zhang", "Ke Li", "Mingyi Wang", "Qiao Liu", "Qifei wang", "Jiayi Liu", "Fei Liu", "Serena Li", "Weiwi Li", "Mingze Gao", "Abhishek Kumar", "Xiangjun Fan", "Zhuokai Zhao", "Lizhu Zhang"]}
]]></acme>

<pubDate>2025-10-23T03:51:17+00:00</pubDate>
</item>
<item>
<title>Collective Communication for 100k+ GPUs</title>
<link>https://papers.cool/arxiv/2510.20171</link>
<guid>https://papers.cool/arxiv/2510.20171</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NCCLX, a collective communication framework designed to operate efficiently on clusters exceeding 100,000 GPUs, addressing throughput and latency bottlenecks in large language model training and inference. It demonstrates substantial performance gains on the Llama4 model, enabling both synchronous training and low‑latency inference at unprecedented scales.<br /><strong>Summary (CN):</strong> 本文提出了 NCCLX 集体通信框架，专为在超 10 万 GPU 集群上高效运行而设计，解决了大规模语言模型训练和推理中的吞吐量和延迟瓶颈。通过在 Llama4 模型上的实验，展示了显著的性能提升，实现了同步训练和低延迟推理的前所未有规模。<br /><strong>Keywords:</strong> collective communication, large language models, GPU scaling, NCCLX, high-performance computing, training efficiency, inference latency, bandwidth optimization, massive GPU clusters, Llama4<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</div>
The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.
<div><strong>Authors:</strong> Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NCCLX, a collective communication framework designed to operate efficiently on clusters exceeding 100,000 GPUs, addressing throughput and latency bottlenecks in large language model training and inference. It demonstrates substantial performance gains on the Llama4 model, enabling both synchronous training and low‑latency inference at unprecedented scales.", "summary_cn": "本文提出了 NCCLX 集体通信框架，专为在超 10 万 GPU 集群上高效运行而设计，解决了大规模语言模型训练和推理中的吞吐量和延迟瓶颈。通过在 Llama4 模型上的实验，展示了显著的性能提升，实现了同步训练和低延迟推理的前所未有规模。", "keywords": "collective communication, large language models, GPU scaling, NCCLX, high-performance computing, training efficiency, inference latency, bandwidth optimization, massive GPU clusters, Llama4", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"]}
]]></acme>

<pubDate>2025-10-23T03:32:04+00:00</pubDate>
</item>
<item>
<title>IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
<link>https://papers.cool/arxiv/2510.20165</link>
<guid>https://papers.cool/arxiv/2510.20165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> IB-GAN introduces an information bottleneck objective into GAN training by placing a stochastic intermediate layer in the generator to constrain mutual information between input and output. This design yields a learnable latent distribution that promotes disentangled and interpretable representations, achieving competitive disentanglement scores on dSprites and improved sample quality on CelebA and 3D Chairs compared to β-VAEs and InfoGAN.<br /><strong>Summary (CN):</strong> IB-GAN 在生成对抗网络的训练中引入信息瓶颈机制，通过在生成器中加入随机中间层来限制输入与输出之间的互信息。该设计使得潜在分布可学习，从而实现了可拆解且可解释的表示，在 dSprites 上的拆解度分数与最先进的 \u03b2-VAEs 相当，并且在 CelebA 与 3D Chairs 上的样本质量和多样性优于 \u03b2-VAEs 与 InfoGAN。<br /><strong>Keywords:</strong> disentangled representation, GAN, InfoGAN, information bottleneck, mutual information, unsupervised learning, generative adversarial networks, representation learning, FID, dSprites<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim</div>
We propose a new GAN-based unsupervised model for disentangled representation learning. The new model is discovered in an attempt to utilize the Information Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The architecture of IB-GAN is partially similar to that of InfoGAN but has a critical difference; an intermediate layer of the generator is leveraged to constrain the mutual information between the input and the generated output. The intermediate stochastic layer can serve as a learnable latent distribution that is trained with the generator jointly in an end-to-end fashion. As a result, the generator of IB-GAN can harness the latent space in a disentangled and interpretable manner. With the experiments on dSprites and Color-dSprites dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover, the visual quality and the diversity of samples generated by IB-GAN are often better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA and 3D Chairs dataset.
<div><strong>Authors:</strong> Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "IB-GAN introduces an information bottleneck objective into GAN training by placing a stochastic intermediate layer in the generator to constrain mutual information between input and output. This design yields a learnable latent distribution that promotes disentangled and interpretable representations, achieving competitive disentanglement scores on dSprites and improved sample quality on CelebA and 3D Chairs compared to β-VAEs and InfoGAN.", "summary_cn": "IB-GAN 在生成对抗网络的训练中引入信息瓶颈机制，通过在生成器中加入随机中间层来限制输入与输出之间的互信息。该设计使得潜在分布可学习，从而实现了可拆解且可解释的表示，在 dSprites 上的拆解度分数与最先进的 \\u03b2-VAEs 相当，并且在 CelebA 与 3D Chairs 上的样本质量和多样性优于 \\u03b2-VAEs 与 InfoGAN。", "keywords": "disentangled representation, GAN, InfoGAN, information bottleneck, mutual information, unsupervised learning, generative adversarial networks, representation learning, FID, dSprites", "scoring": {"interpretability": 7, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Insu Jeon", "Wonkwang Lee", "Myeongjang Pyeon", "Gunhee Kim"]}
]]></acme>

<pubDate>2025-10-23T03:24:48+00:00</pubDate>
</item>
<item>
<title>Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?</title>
<link>https://papers.cool/arxiv/2510.20154</link>
<guid>https://papers.cool/arxiv/2510.20154</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how large language models exhibit stereotypical biases when performing zero-shot stance detection, revealing that attributes such as dialect and text readability influence the model's stance decisions. By automatically annotating existing stance detection datasets with dialect and readability labels, the authors demonstrate systematic biases, such as associating low readability with pro‑marijuana views and African American dialect with opposition to Donald Trump. These findings highlight a neglected safety and fairness issue in NLP stance detection.<br /><strong>Summary (CN):</strong> 本文研究了大语言模型在零样本立场检测中表现出的刻板偏见，发现方言和文本可读性等属性会影响模型的立场判断。例如，模型倾向于将低可读性文本与支持大麻联系在一起，并将非裔美国人方言与反对唐纳德·特朗普关联。通过为已有立场检测数据集自动标注方言和可读性标签，作者揭示了这类系统性偏见，凸显了 NLP 立场检测中被忽视的安全与公平问题。<br /><strong>Keywords:</strong> stereotype bias, zero-shot stance detection, large language models, dialect bias, readability bias, fairness, NLP bias, political bias<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Anthony Dubreuil, Antoine Gourru, Christine Largeron, Amine Trabelsi</div>
Large Language Models inherit stereotypes from their pretraining data, leading to biased behavior toward certain social groups in many Natural Language Processing tasks, such as hateful speech detection or sentiment analysis. Surprisingly, the evaluation of this kind of bias in stance detection methods has been largely overlooked by the community. Stance Detection involves labeling a statement as being against, in favor, or neutral towards a specific target and is among the most sensitive NLP tasks, as it often relates to political leanings. In this paper, we focus on the bias of Large Language Models when performing stance detection in a zero-shot setting. We automatically annotate posts in pre-existing stance detection datasets with two attributes: dialect or vernacular of a specific group and text complexity/readability, to investigate whether these attributes influence the model's stance detection decisions. Our results show that LLMs exhibit significant stereotypes in stance detection tasks, such as incorrectly associating pro-marijuana views with low text complexity and African American dialect with opposition to Donald Trump.
<div><strong>Authors:</strong> Anthony Dubreuil, Antoine Gourru, Christine Largeron, Amine Trabelsi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how large language models exhibit stereotypical biases when performing zero-shot stance detection, revealing that attributes such as dialect and text readability influence the model's stance decisions. By automatically annotating existing stance detection datasets with dialect and readability labels, the authors demonstrate systematic biases, such as associating low readability with pro‑marijuana views and African American dialect with opposition to Donald Trump. These findings highlight a neglected safety and fairness issue in NLP stance detection.", "summary_cn": "本文研究了大语言模型在零样本立场检测中表现出的刻板偏见，发现方言和文本可读性等属性会影响模型的立场判断。例如，模型倾向于将低可读性文本与支持大麻联系在一起，并将非裔美国人方言与反对唐纳德·特朗普关联。通过为已有立场检测数据集自动标注方言和可读性标签，作者揭示了这类系统性偏见，凸显了 NLP 立场检测中被忽视的安全与公平问题。", "keywords": "stereotype bias, zero-shot stance detection, large language models, dialect bias, readability bias, fairness, NLP bias, political bias", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Anthony Dubreuil", "Antoine Gourru", "Christine Largeron", "Amine Trabelsi"]}
]]></acme>

<pubDate>2025-10-23T03:05:25+00:00</pubDate>
</item>
<item>
<title>SAID: Empowering Large Language Models with Self-Activating Internal Defense</title>
<link>https://papers.cool/arxiv/2510.20129</link>
<guid>https://papers.cool/arxiv/2510.20129</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SAID, a training-free defense that activates latent safety capabilities within large language models to detect and neutralize malicious intents in a three-stage pipeline (intent distillation, safety prefix probing, and conservative aggregation). Experiments on five open-source LLMs against six jailbreak attacks show that SAID significantly reduces harmful outputs while preserving performance on benign tasks and incurring minimal overhead. This work suggests that leveraging internal reasoning for safety can offer a scalable, robust alternative to external defenses.<br /><strong>Summary (CN):</strong> 本文提出 SAID，一种无需额外训练的防御框架，通过激活大语言模型内部潜在的安全机制，以三阶段流程（意图蒸馏、最优安全前缀探测、保守聚合）主动识别并中和恶意意图。针对五个开源 LLM 在六种高级 jailbreak 攻击下的实验表明，SAID 在显著降低有害输出的同时，保持了对良性任务的性能，且计算开销极小。该工作表明利用模型内部推理能力进行安全防护是更具可扩展性和鲁棒性的路径。<br /><strong>Keywords:</strong> jailbreak mitigation, self-activating internal defense, LLM safety, intent distillation, safety prefix probing, conservative aggregation, training-free defense, adversarial robustness, alignment, internal safety activation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Yulong Chen, Yadong Liu, Jiawen Zhang, Mu Li, Chao Huang, Jie Wen</div>
Large Language Models (LLMs), despite advances in safety alignment, remain vulnerable to jailbreak attacks designed to circumvent protective mechanisms. Prevailing defense strategies rely on external interventions, such as input filtering or output modification, which often lack generalizability and compromise model utility while incurring significant computational overhead. In this work, we introduce a new, training-free defense paradigm, Self-Activating Internal Defense (SAID), which reframes the defense task from external correction to internal capability activation. SAID uniquely leverages the LLM's own reasoning abilities to proactively identify and neutralize malicious intent through a three-stage pipeline: model-native intent distillation to extract core semantics, optimal safety prefix probing to activate latent safety awareness, and a conservative aggregation strategy to ensure robust decision-making. Extensive experiments on five open-source LLMs against six advanced jailbreak attacks demonstrate that SAID substantially outperforms state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves this while preserving model performance on benign tasks and incurring minimal computational overhead. Our work establishes that activating the intrinsic safety mechanisms of LLMs is a more robust and scalable path toward building safer and more reliable aligned AI systems.
<div><strong>Authors:</strong> Yulong Chen, Yadong Liu, Jiawen Zhang, Mu Li, Chao Huang, Jie Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SAID, a training-free defense that activates latent safety capabilities within large language models to detect and neutralize malicious intents in a three-stage pipeline (intent distillation, safety prefix probing, and conservative aggregation). Experiments on five open-source LLMs against six jailbreak attacks show that SAID significantly reduces harmful outputs while preserving performance on benign tasks and incurring minimal overhead. This work suggests that leveraging internal reasoning for safety can offer a scalable, robust alternative to external defenses.", "summary_cn": "本文提出 SAID，一种无需额外训练的防御框架，通过激活大语言模型内部潜在的安全机制，以三阶段流程（意图蒸馏、最优安全前缀探测、保守聚合）主动识别并中和恶意意图。针对五个开源 LLM 在六种高级 jailbreak 攻击下的实验表明，SAID 在显著降低有害输出的同时，保持了对良性任务的性能，且计算开销极小。该工作表明利用模型内部推理能力进行安全防护是更具可扩展性和鲁棒性的路径。", "keywords": "jailbreak mitigation, self-activating internal defense, LLM safety, intent distillation, safety prefix probing, conservative aggregation, training-free defense, adversarial robustness, alignment, internal safety activation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yulong Chen", "Yadong Liu", "Jiawen Zhang", "Mu Li", "Chao Huang", "Jie Wen"]}
]]></acme>

<pubDate>2025-10-23T02:07:54+00:00</pubDate>
</item>
<item>
<title>Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning</title>
<link>https://papers.cool/arxiv/2510.20098</link>
<guid>https://papers.cool/arxiv/2510.20098</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ARTER (Adaptive Routing and Targeted Entity Reasoning), a pipeline that combines lightweight candidate generation, context‑based scoring, and an adaptive router to separate easy mentions (handled by a low‑cost linker such as ReFinED) from hard mentions (processed by targeted LLM‑based reasoning). This approach achieves state‑of‑the‑art entity‑linking performance while using roughly half the LLM tokens compared to full‑reasoning pipelines.<br /><strong>Summary (CN):</strong> 本文提出 ARTER（Adaptive Routing and Targeted Entity Reasoning）框架，通过候选生成、上下文打分和自适应路由，将易处理的实体链接案例交给低计算成本的链接器（如 ReFinED），将困难案例交给针对性的 LLM 推理，从而在保持或提升实体链接效果的同时，将 LLM 令牌使用量减半。<br /><strong>Keywords:</strong> entity linking, large language models, adaptive routing, targeted reasoning, few-shot prompting, efficiency, candidate generation, ReFinED<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</div>
Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.
<div><strong>Authors:</strong> Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ARTER (Adaptive Routing and Targeted Entity Reasoning), a pipeline that combines lightweight candidate generation, context‑based scoring, and an adaptive router to separate easy mentions (handled by a low‑cost linker such as ReFinED) from hard mentions (processed by targeted LLM‑based reasoning). This approach achieves state‑of‑the‑art entity‑linking performance while using roughly half the LLM tokens compared to full‑reasoning pipelines.", "summary_cn": "本文提出 ARTER（Adaptive Routing and Targeted Entity Reasoning）框架，通过候选生成、上下文打分和自适应路由，将易处理的实体链接案例交给低计算成本的链接器（如 ReFinED），将困难案例交给针对性的 LLM 推理，从而在保持或提升实体链接效果的同时，将 LLM 令牌使用量减半。", "keywords": "entity linking, large language models, adaptive routing, targeted reasoning, few-shot prompting, efficiency, candidate generation, ReFinED", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yajie Li", "Albert Galimov", "Mitra Datta Ganapaneni", "Pujitha Thejaswi", "De Meng", "Priyanshu Kumar", "Saloni Potdar"]}
]]></acme>

<pubDate>2025-10-23T00:50:14+00:00</pubDate>
</item>
<item>
<title>On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers</title>
<link>https://papers.cool/arxiv/2510.20094</link>
<guid>https://papers.cool/arxiv/2510.20094</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper establishes an exact equivalence between stationary solutions of McKean‑Vlasov equations on the circle and an infinite‑dimensional quadratic system over Fourier coefficients, enabling explicit characterisation of stationary states, local bifurcations, and resonance structures, including discontinuous phase transitions. It further derives analytic expressions for the shape of bifurcations and global properties of the free‑energy landscape, and applies the framework to the Noisy Mean‑Field Transformer, revealing how temperature changes induce rich multi‑mode stationary (metastable) solutions and a sharp transition from continuous to first‑order phase behaviour.<br /><strong>Summary (CN):</strong> 本文展示了圆上McKean‑Vlasov方程的稳态解与傅里叶系数的无限维二次方程组之间的精确等价，从而能够在序列空间而非函数空间中明确刻画稳态、局部分岔及共振结构，并解析不连续相变。文中给出了分岔形态（超临界、临界、亚临界或转移）的解析表达式，分析了自由能全局正则性与凹性，并将理论应用于噪声平均场Transformer模型，说明温度参数β的变化如何产生丰富的多模态稳态（准稳态），以及从连续相行为到一阶相变的锐利转变。<br /><strong>Keywords:</strong> McKean-Vlasov, mean-field transformer, stationary solutions, bifurcation analysis, Fourier coefficients, phase transitions, noisy transformers, free energy landscape<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 2, Technicality: 9, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet</div>
We study stationary solutions of McKean-Vlasov equations on the circle. Our main contributions stem from observing an exact equivalence between solutions of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic system of equations over Fourier coefficients, which allows explicit characterization of the stationary states in a sequence space rather than a function space. This framework provides a transparent description of local bifurcations, characterizing their periodicity, and resonance structures, while accommodating singular potentials. We derive analytic expressions that characterize the emergence, form and shape (supercritical, critical, subcritical or transcritical) of bifurcations involving possibly multiple Fourier modes and connect them with discontinuous phase transitions. We also characterize, under suitable assumptions, the detailed structure of the stationary bifurcating solutions that are accurate upto an arbitrary number of Fourier modes. At the global level, we establish regularity and concavity properties of the free energy landscape, proving existence, compactness, and coexistence of globally minimizing stationary measures, further identifying discontinuous phase transitions with points of non-differentiability of the minimum free energy map. As an application, we specialize the theory to the Noisy Mean-Field Transformer model, where we show how changing the inverse temperature parameter $\beta$ affects the geometry of the infinitely many bifurcations from the uniform measure. We also explain how increasing $\beta$ can lead to a rich class of approximate multi-mode stationary solutions which can be seen as `metastable states'. Further, a sharp transition from continuous to discontinuous (first-order) phase behavior is observed as $\beta$ increases.
<div><strong>Authors:</strong> Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper establishes an exact equivalence between stationary solutions of McKean‑Vlasov equations on the circle and an infinite‑dimensional quadratic system over Fourier coefficients, enabling explicit characterisation of stationary states, local bifurcations, and resonance structures, including discontinuous phase transitions. It further derives analytic expressions for the shape of bifurcations and global properties of the free‑energy landscape, and applies the framework to the Noisy Mean‑Field Transformer, revealing how temperature changes induce rich multi‑mode stationary (metastable) solutions and a sharp transition from continuous to first‑order phase behaviour.", "summary_cn": "本文展示了圆上McKean‑Vlasov方程的稳态解与傅里叶系数的无限维二次方程组之间的精确等价，从而能够在序列空间而非函数空间中明确刻画稳态、局部分岔及共振结构，并解析不连续相变。文中给出了分岔形态（超临界、临界、亚临界或转移）的解析表达式，分析了自由能全局正则性与凹性，并将理论应用于噪声平均场Transformer模型，说明温度参数β的变化如何产生丰富的多模态稳态（准稳态），以及从连续相行为到一阶相变的锐利转变。", "keywords": "McKean-Vlasov, mean-field transformer, stationary solutions, bifurcation analysis, Fourier coefficients, phase transitions, noisy transformers, free energy landscape", "scoring": {"interpretability": 4, "understanding": 7, "safety": 2, "technicality": 9, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Krishnakumar Balasubramanian", "Sayan Banerjee", "Philippe Rigollet"]}
]]></acme>

<pubDate>2025-10-23T00:28:32+00:00</pubDate>
</item>
<item>
<title>StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback</title>
<link>https://papers.cool/arxiv/2510.20093</link>
<guid>https://papers.cool/arxiv/2510.20093</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces StableSketcher, a framework that fine-tunes the variational autoencoder of diffusion models and incorporates a reinforcement‑learning reward derived from visual question answering to improve the fidelity of pixel‑based hand‑drawn sketch generation. It also releases SketchDUO, a new dataset of sketches paired with captions and QA pairs, demonstrating better prompt alignment compared to the Stable Diffusion baseline.<br /><strong>Summary (CN):</strong> 本文提出 StableSketcher 框架，通过微调扩散模型的变分自编码器并利用视觉问答（VQA）产生的奖励进行强化学习，以提升像素级手绘草图的生成质量和文本对齐度。同时发布了 SketchDUO 数据集，包含草图、标题以及问答对，实验表明其在提示一致性方面优于 Stable Diffusion 基线。<br /><strong>Keywords:</strong> diffusion models, sketch generation, visual question answering, reinforcement learning, dataset, StableSketcher, VAE fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</div>
Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance.
<div><strong>Authors:</strong> Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces StableSketcher, a framework that fine-tunes the variational autoencoder of diffusion models and incorporates a reinforcement‑learning reward derived from visual question answering to improve the fidelity of pixel‑based hand‑drawn sketch generation. It also releases SketchDUO, a new dataset of sketches paired with captions and QA pairs, demonstrating better prompt alignment compared to the Stable Diffusion baseline.", "summary_cn": "本文提出 StableSketcher 框架，通过微调扩散模型的变分自编码器并利用视觉问答（VQA）产生的奖励进行强化学习，以提升像素级手绘草图的生成质量和文本对齐度。同时发布了 SketchDUO 数据集，包含草图、标题以及问答对，实验表明其在提示一致性方面优于 Stable Diffusion 基线。", "keywords": "diffusion models, sketch generation, visual question answering, reinforcement learning, dataset, StableSketcher, VAE fine-tuning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"]}
]]></acme>

<pubDate>2025-10-23T00:27:32+00:00</pubDate>
</item>
<item>
<title>CreativityPrism: A Holistic Benchmark for Large Language Model Creativity</title>
<link>https://papers.cool/arxiv/2510.20091</link>
<guid>https://papers.cool/arxiv/2510.20091</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CreativityPrism introduces a comprehensive benchmark for assessing large language model creativity across three dimensions—quality, novelty, and diversity—spanning nine tasks in divergent thinking, creative writing, and logical reasoning. The authors evaluate 17 state-of-the-art proprietary and open-source models, revealing performance gaps and varying correlations between metrics and domains, highlighting that strong performance in one dimension does not guarantee generalization to others.<br /><strong>Summary (CN):</strong> CreativityPrism 提出一个综合评估框架，用于在质量、创新性和多样性三个维度上衡量大语言模型的创造力，覆盖发散思维、创意写作和逻辑推理的九项任务。作者对 17 种最新的专有和开源模型进行评测，发现不同模型在各维度上的表现差距以及同一领域任务之间指标的高度相关性，同时指出在某一维度表现出色并不一定能推广到其他维度。<br /><strong>Keywords:</strong> LLM creativity, benchmark, evaluation metrics, quality, novelty, diversity, divergent thinking, creative writing, logical reasoning, model assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhaoyi Joey Hou, Bowei Alvin Zhang, Yining Lu, Bhiman Kumar Baghel, Anneliese Brei, Ximing Lu, Meng Jiang, Faeze Brahman, Snigdha Chaturvedi, Haw-Shiuan Chang, Daniel Khashabi, Xiang Lorraine Li</div>
Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as producing creative text, there is still no holistic framework to evaluate their creativity across diverse scenarios. Existing evaluation methods remain fragmented, with dramatic variation across domains and tasks, largely due to differing definitions and measurements of creativity. Inspired by the hypothesis that creativity is not one fixed idea, we propose CreativityPrism, an evaluation analysis framework that decomposes creativity into three dimensions: quality, novelty, and diversity. CreativityPrism incorporates nine tasks, three domains, i.e., divergent thinking, creative writing, and logical reasoning, and twenty evaluation metrics, which measure each dimension in task-specific, unique ways. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CreativityPrism and analyze the performance correlations among different metrics and task domains. Our results reveal a notable gap between proprietary and open-source models. Overall, model performance tends to be highly correlated across tasks within the same domain and less so across different domains. Among evaluation dimensions, diversity and quality metrics show strong correlations - models that perform well on one often excel on the other - whereas novelty exhibits much weaker correlation with either. These findings support our hypothesis that strong performance in one creativity task or dimension does not necessarily generalize to others, underscoring the need for a holistic evaluation of LLM creativity.
<div><strong>Authors:</strong> Zhaoyi Joey Hou, Bowei Alvin Zhang, Yining Lu, Bhiman Kumar Baghel, Anneliese Brei, Ximing Lu, Meng Jiang, Faeze Brahman, Snigdha Chaturvedi, Haw-Shiuan Chang, Daniel Khashabi, Xiang Lorraine Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CreativityPrism introduces a comprehensive benchmark for assessing large language model creativity across three dimensions—quality, novelty, and diversity—spanning nine tasks in divergent thinking, creative writing, and logical reasoning. The authors evaluate 17 state-of-the-art proprietary and open-source models, revealing performance gaps and varying correlations between metrics and domains, highlighting that strong performance in one dimension does not guarantee generalization to others.", "summary_cn": "CreativityPrism 提出一个综合评估框架，用于在质量、创新性和多样性三个维度上衡量大语言模型的创造力，覆盖发散思维、创意写作和逻辑推理的九项任务。作者对 17 种最新的专有和开源模型进行评测，发现不同模型在各维度上的表现差距以及同一领域任务之间指标的高度相关性，同时指出在某一维度表现出色并不一定能推广到其他维度。", "keywords": "LLM creativity, benchmark, evaluation metrics, quality, novelty, diversity, divergent thinking, creative writing, logical reasoning, model assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhaoyi Joey Hou", "Bowei Alvin Zhang", "Yining Lu", "Bhiman Kumar Baghel", "Anneliese Brei", "Ximing Lu", "Meng Jiang", "Faeze Brahman", "Snigdha Chaturvedi", "Haw-Shiuan Chang", "Daniel Khashabi", "Xiang Lorraine Li"]}
]]></acme>

<pubDate>2025-10-23T00:22:10+00:00</pubDate>
</item>
<item>
<title>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models</title>
<link>https://papers.cool/arxiv/2510.20084</link>
<guid>https://papers.cool/arxiv/2510.20084</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ShapeX introduces a post-hoc explanation framework for time‑series classifiers that first discovers a diverse set of discriminative shapelets and then segments series according to these shapelets, using Shapley values to quantify each segment's saliency. By leveraging the atomic nature of shapelets, the method aims to provide explanations that capture causal relationships rather than mere correlations, and experiments on synthetic and real datasets show improved precision and causal fidelity over existing timestep‑level attribution techniques.<br /><strong>Summary (CN):</strong> ShapeX 提出了一种时间序列分类模型的事后解释框架，先通过 Shapelet Describe-and-Detect (SDD) 学习多样的判别 shapelet，并据此将序列划分为 shapelet 驱动的片段，然后使用 Shapley 值评估每个片段的重要性。该方法利用 shapelet 的原子性来揭示因果关系，而非仅仅相关性，在合成和真实数据集上的实验表明其在识别关键子序列的精确度和因果忠实度方面优于现有的逐时间步特征归因方法。<br /><strong>Keywords:</strong> shapelet, time series classification, post-hoc explanation, Shapley values, causal fidelity, interpretable machine learning<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan</div>
Explaining time series classification models is crucial, particularly in high-stakes applications such as healthcare and finance, where transparency and trust play a critical role. Although numerous time series classification methods have identified key subsequences, known as shapelets, as core features for achieving state-of-the-art performance and validating their pivotal role in classification outcomes, existing post-hoc time series explanation (PHTSE) methods primarily focus on timestep-level feature attribution. These explanation methods overlook the fundamental prior that classification outcomes are predominantly driven by key shapelets. To bridge this gap, we present ShapeX, an innovative framework that segments time series into meaningful shapelet-driven segments and employs Shapley values to assess their saliency. At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework, which effectively learns a diverse set of shapelets essential for classification. We further demonstrate that ShapeX produces explanations which reveal causal relationships instead of just correlations, owing to the atomicity properties of shapelets. Experimental results on both synthetic and real-world datasets demonstrate that ShapeX outperforms existing methods in identifying the most relevant subsequences, enhancing both the precision and causal fidelity of time series explanations.
<div><strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ShapeX introduces a post-hoc explanation framework for time‑series classifiers that first discovers a diverse set of discriminative shapelets and then segments series according to these shapelets, using Shapley values to quantify each segment's saliency. By leveraging the atomic nature of shapelets, the method aims to provide explanations that capture causal relationships rather than mere correlations, and experiments on synthetic and real datasets show improved precision and causal fidelity over existing timestep‑level attribution techniques.", "summary_cn": "ShapeX 提出了一种时间序列分类模型的事后解释框架，先通过 Shapelet Describe-and-Detect (SDD) 学习多样的判别 shapelet，并据此将序列划分为 shapelet 驱动的片段，然后使用 Shapley 值评估每个片段的重要性。该方法利用 shapelet 的原子性来揭示因果关系，而非仅仅相关性，在合成和真实数据集上的实验表明其在识别关键子序列的精确度和因果忠实度方面优于现有的逐时间步特征归因方法。", "keywords": "shapelet, time series classification, post-hoc explanation, Shapley values, causal fidelity, interpretable machine learning", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Bosong Huang", "Ming Jin", "Yuxuan Liang", "Johan Barthelemy", "Debo Cheng", "Qingsong Wen", "Chenghao Liu", "Shirui Pan"]}
]]></acme>

<pubDate>2025-10-23T00:01:40+00:00</pubDate>
</item>
<item>
<title>Ask What Your Country Can Do For You: Towards a Public Red Teaming Model</title>
<link>https://papers.cool/arxiv/2510.20061</link>
<guid>https://papers.cool/arxiv/2510.20061</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a cooperative public AI red‑teaming model as a way to continuously and adversarially evaluate AI systems for harms such as bias, hate speech, and misinformation. It describes pilot implementations with NIST's ARIA exercise and Singapore's IMDA, showing how public participation can produce actionable safety insights and be scaled across jurisdictions.<br /><strong>Summary (CN):</strong> 本文提出了一种合作式公共 AI 红队模型，用于持续进行对抗性评估，以识别偏见、仇恨言论、错误信息等潜在危害。文中回顾了与 NIST 的 ARIA 试点以及新加坡 IMDA 的红队演练，展示了公众参与如何提供有意义的安全洞见并具备跨地区扩展的能力。<br /><strong>Keywords:</strong> public red teaming, AI safety, sociotechnical evaluation, bias mitigation, AI governance, adversarial testing, responsibility gap, NIST ARIA, IMDA, cooperative oversight<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 4, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Wm. Matthew Kennedy, Cigdem Patlak, Jayraj Dave, Blake Chambers, Aayush Dhanotiya, Darshini Ramiah, Reva Schwartz, Jack Hagen, Akash Kundu, Mouni Pendharkar, Liam Baisley, Theodora Skeadas, Rumman Chowdhury</div>
AI systems have the potential to produce both benefits and harms, but without rigorous and ongoing adversarial evaluation, AI actors will struggle to assess the breadth and magnitude of the AI risk surface. Researchers from the field of systems design have developed several effective sociotechnical AI evaluation and red teaming techniques targeting bias, hate speech, mis/disinformation, and other documented harm classes. However, as increasingly sophisticated AI systems are released into high-stakes sectors (such as education, healthcare, and intelligence-gathering), our current evaluation and monitoring methods are proving less and less capable of delivering effective oversight. In order to actually deliver responsible AI and to ensure AI's harms are fully understood and its security vulnerabilities mitigated, pioneering new approaches to close this "responsibility gap" are now more urgent than ever. In this paper, we propose one such approach, the cooperative public AI red-teaming exercise, and discuss early results of its prior pilot implementations. This approach is intertwined with CAMLIS itself: the first in-person public demonstrator exercise was held in conjunction with CAMLIS 2024. We review the operational design and results of this exercise, the prior National Institute of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI (ARIA) pilot exercise, and another similar exercise conducted with the Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue that this approach is both capable of delivering meaningful results and is also scalable to many AI developing jurisdictions.
<div><strong>Authors:</strong> Wm. Matthew Kennedy, Cigdem Patlak, Jayraj Dave, Blake Chambers, Aayush Dhanotiya, Darshini Ramiah, Reva Schwartz, Jack Hagen, Akash Kundu, Mouni Pendharkar, Liam Baisley, Theodora Skeadas, Rumman Chowdhury</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a cooperative public AI red‑teaming model as a way to continuously and adversarially evaluate AI systems for harms such as bias, hate speech, and misinformation. It describes pilot implementations with NIST's ARIA exercise and Singapore's IMDA, showing how public participation can produce actionable safety insights and be scaled across jurisdictions.", "summary_cn": "本文提出了一种合作式公共 AI 红队模型，用于持续进行对抗性评估，以识别偏见、仇恨言论、错误信息等潜在危害。文中回顾了与 NIST 的 ARIA 试点以及新加坡 IMDA 的红队演练，展示了公众参与如何提供有意义的安全洞见并具备跨地区扩展的能力。", "keywords": "public red teaming, AI safety, sociotechnical evaluation, bias mitigation, AI governance, adversarial testing, responsibility gap, NIST ARIA, IMDA, cooperative oversight", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 4, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Wm. Matthew Kennedy", "Cigdem Patlak", "Jayraj Dave", "Blake Chambers", "Aayush Dhanotiya", "Darshini Ramiah", "Reva Schwartz", "Jack Hagen", "Akash Kundu", "Mouni Pendharkar", "Liam Baisley", "Theodora Skeadas", "Rumman Chowdhury"]}
]]></acme>

<pubDate>2025-10-22T22:24:21+00:00</pubDate>
</item>
<item>
<title>Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning</title>
<link>https://papers.cool/arxiv/2510.20040</link>
<guid>https://papers.cool/arxiv/2510.20040</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an imitation‑learning framework that trains a neural network to replicate the actions of a mixed‑integer Economic Model Predictive Control (EMPC) expert for microgrid energy management, enabling fast, real‑time decisions without solving the optimization online. By injecting noise during training and accounting for renewable generation and demand forecast uncertainty, the learned policy attains economic performance close to EMPC while reducing computation time to about 10% of the original method.<br /><strong>Summary (CN):</strong> 本文提出一种模仿学习框架，利用神经网络学习并复制混合整数经济模型预测控制（EMPC）专家在微电网能源管理中的控制动作，从而实现无需在线求解优化问题的快速实时决策。训练过程中加入噪声并显式考虑可再生能源和需求预测不确定性，使得学习到的策略在经济性能上接近 EMPC，同时计算时间仅为原方法的约 10%。<br /><strong>Keywords:</strong> imitation learning, model predictive control, microgrid, energy management, economic MPC, mixed-integer optimization, neural network policy, robustness, forecast uncertainty<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Changrui Liu, Shengling Shi, Anil Alan, Ganesh Kumar Venayagamoorthy, Bart De Schutter</div>
Efficient energy management is essential for reliable and sustainable microgrid operation amid increasing renewable integration. This paper proposes an imitation learning-based framework to approximate mixed-integer Economic Model Predictive Control (EMPC) for microgrid energy management. The proposed method trains a neural network to imitate expert EMPC control actions from offline trajectories, enabling fast, real-time decision making without solving optimization problems online. To enhance robustness and generalization, the learning process includes noise injection during training to mitigate distribution shift and explicitly incorporates forecast uncertainty in renewable generation and demand. Simulation results demonstrate that the learned policy achieves economic performance comparable to EMPC while only requiring $10\%$ of the computation time of optimization-based EMPC in practice.
<div><strong>Authors:</strong> Changrui Liu, Shengling Shi, Anil Alan, Ganesh Kumar Venayagamoorthy, Bart De Schutter</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an imitation‑learning framework that trains a neural network to replicate the actions of a mixed‑integer Economic Model Predictive Control (EMPC) expert for microgrid energy management, enabling fast, real‑time decisions without solving the optimization online. By injecting noise during training and accounting for renewable generation and demand forecast uncertainty, the learned policy attains economic performance close to EMPC while reducing computation time to about 10% of the original method.", "summary_cn": "本文提出一种模仿学习框架，利用神经网络学习并复制混合整数经济模型预测控制（EMPC）专家在微电网能源管理中的控制动作，从而实现无需在线求解优化问题的快速实时决策。训练过程中加入噪声并显式考虑可再生能源和需求预测不确定性，使得学习到的策略在经济性能上接近 EMPC，同时计算时间仅为原方法的约 10%。", "keywords": "imitation learning, model predictive control, microgrid, energy management, economic MPC, mixed-integer optimization, neural network policy, robustness, forecast uncertainty", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Changrui Liu", "Shengling Shi", "Anil Alan", "Ganesh Kumar Venayagamoorthy", "Bart De Schutter"]}
]]></acme>

<pubDate>2025-10-22T21:39:18+00:00</pubDate>
</item>
<item>
<title>Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions</title>
<link>https://papers.cool/arxiv/2510.20039</link>
<guid>https://papers.cool/arxiv/2510.20039</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how opinions evolve in multi‑turn conversations between humans and large language model chatbots, showing that while human stances change only slightly, LLM outputs shift substantially toward users, especially when personalization is used. It highlights a risk of over‑alignment where models too readily adopt users' viewpoints, and suggests careful design of personalized agents to maintain stable alignment.<br /><strong>Summary (CN):</strong> 本文研究了在人与大型语言模型聊天机器人之间的多轮对话中观点的双向演变，发现虽然人类立场仅有轻微变化，LLM 的输出却显著向用户倾斜，且在个性化设置下这种双向影响更为显著。文章指出了过度对齐的风险，即模型过度迎合用户观点，并呼吁对个性化聊天机器人进行慎重设计，以实现更稳健的对齐。<br /><strong>Keywords:</strong> bidirectional influence, opinion dynamics, LLM alignment, personalized chatbot, stance shift, human-LLM interaction, over-alignment, multi-turn conversation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 7, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen</div>
Large language model (LLM)-powered chatbots are increasingly used for opinion exploration. Prior research examined how LLMs alter user views, yet little work extended beyond one-way influence to address how user input can affect LLM responses and how such bi-directional influence manifests throughout the multi-turn conversations. This study investigates this dynamic through 50 controversial-topic discussions with participants (N=266) across three conditions: static statements, standard chatbot, and personalized chatbot. Results show that human opinions barely shifted, while LLM outputs changed more substantially, narrowing the gap between human and LLM stance. Personalization amplified these shifts in both directions compared to the standard setting. Analysis of multi-turn conversations further revealed that exchanges involving participants' personal stories were most likely to trigger stance changes for both humans and LLMs. Our work highlights the risk of over-alignment in human-LLM interaction and the need for careful design of personalized chatbots to more thoughtfully and stably align with users.
<div><strong>Authors:</strong> Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how opinions evolve in multi‑turn conversations between humans and large language model chatbots, showing that while human stances change only slightly, LLM outputs shift substantially toward users, especially when personalization is used. It highlights a risk of over‑alignment where models too readily adopt users' viewpoints, and suggests careful design of personalized agents to maintain stable alignment.", "summary_cn": "本文研究了在人与大型语言模型聊天机器人之间的多轮对话中观点的双向演变，发现虽然人类立场仅有轻微变化，LLM 的输出却显著向用户倾斜，且在个性化设置下这种双向影响更为显著。文章指出了过度对齐的风险，即模型过度迎合用户观点，并呼吁对个性化聊天机器人进行慎重设计，以实现更稳健的对齐。", "keywords": "bidirectional influence, opinion dynamics, LLM alignment, personalized chatbot, stance shift, human-LLM interaction, over-alignment, multi-turn conversation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yuyang Jiang", "Longjie Guo", "Yuchen Wu", "Aylin Caliskan", "Tanu Mitra", "Hua Shen"]}
]]></acme>

<pubDate>2025-10-22T21:38:10+00:00</pubDate>
</item>
<item>
<title>The Temporal Graph of Bitcoin Transactions</title>
<link>https://papers.cool/arxiv/2510.20028</link>
<guid>https://papers.cool/arxiv/2510.20028</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a large-scale, temporal, heterogeneous graph dataset that reconstructs the flow of funds across the entire Bitcoin transaction history, comprising over 2.4 billion nodes and 39.7 billion edges. It provides sampling methods, feature vectors, and tools for loading the data into graph databases, enabling machine‑learning research on tasks such as anomaly detection, address classification, and market analysis. The dataset and accompanying code are publicly released to facilitate large‑scale graph‑ML benchmarking on blockchain data.<br /><strong>Summary (CN):</strong> 本文构建了一个大规模的时间异构图数据集，重建了比特币全部交易历史的资金流动，包含超过 24 亿节点和 397 亿边。论文提供了抽样方法、特征向量以及在图数据库中加载数据的工具，支持在异常检测、地址分类和市场分析等机器学习任务上的研究。该数据集及代码已公开发布，旨在促进区块链数据上的大规模图机器学习基准测试。<br /><strong>Keywords:</strong> bitcoin, transaction graph, temporal graph, heterogeneous graph, blockchain analytics, anomaly detection, graph machine learning, dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vahid Jalili</div>
Since its 2009 genesis block, the Bitcoin network has processed \num{>1.08} billion (B) transactions representing \num{>8.72}B BTC, offering rich potential for machine learning (ML); yet, its pseudonymity and obscured flow of funds inherent in its \utxo-based design, have rendered this data largely inaccessible for ML research. Addressing this gap, we present an ML-compatible graph modeling the Bitcoin's economic topology by reconstructing the flow of funds. This temporal, heterogeneous graph encompasses complete transaction history up to block \cutoffHeight, consisting of \num{>2.4}B nodes and \num{>39.72}B edges. Additionally, we provide custom sampling methods yielding node and edge feature vectors of sampled communities, tools to load and analyze the Bitcoin graph data within specialized graph databases, and ready-to-use database snapshots. This comprehensive dataset and toolkit empower the ML community to tackle Bitcoin's intricate ecosystem at scale, driving progress in applications such as anomaly detection, address classification, market analysis, and large-scale graph ML benchmarking. Dataset and code available at \href{https://github.com/B1AAB/EBA}{github.com/b1aab/eba}
<div><strong>Authors:</strong> Vahid Jalili</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a large-scale, temporal, heterogeneous graph dataset that reconstructs the flow of funds across the entire Bitcoin transaction history, comprising over 2.4 billion nodes and 39.7 billion edges. It provides sampling methods, feature vectors, and tools for loading the data into graph databases, enabling machine‑learning research on tasks such as anomaly detection, address classification, and market analysis. The dataset and accompanying code are publicly released to facilitate large‑scale graph‑ML benchmarking on blockchain data.", "summary_cn": "本文构建了一个大规模的时间异构图数据集，重建了比特币全部交易历史的资金流动，包含超过 24 亿节点和 397 亿边。论文提供了抽样方法、特征向量以及在图数据库中加载数据的工具，支持在异常检测、地址分类和市场分析等机器学习任务上的研究。该数据集及代码已公开发布，旨在促进区块链数据上的大规模图机器学习基准测试。", "keywords": "bitcoin, transaction graph, temporal graph, heterogeneous graph, blockchain analytics, anomaly detection, graph machine learning, dataset", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vahid Jalili"]}
]]></acme>

<pubDate>2025-10-22T21:10:46+00:00</pubDate>
</item>
<item>
<title>Optimized Distortion in Linear Social Choice</title>
<link>https://papers.cool/arxiv/2510.20020</link>
<guid>https://papers.cool/arxiv/2510.20020</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies distortion in social choice when voters' utilities are linear functions of candidate embeddings, providing dimension‑dependent bounds for deterministic and randomized voting rules. It introduces polynomial‑time instance‑optimal algorithms to minimize distortion and evaluates them on real‑world recommendation and language‑model embedding datasets, comparing against standard rules. The results show that distortion can be significantly reduced by leveraging the geometry of the embedding space.<br /><strong>Summary (CN):</strong> 本文研究了当选民的效用是候选人嵌入的线性函数时，社会选择中的失真（distortion），并给出仅依赖嵌入维度的确定性和随机投票规则的界限。作者提出了多项式时间的实例最优算法来最小化失真，并在基于协同过滤的推荐系统嵌入和语言模型嵌入的真实数据上进行评估，比较了几种标准投票规则。结果表明，利用嵌入空间的几何结构可以显著降低失真。<br /><strong>Keywords:</strong> distortion, linear utilities, social choice, voting rules, instance-optimal algorithms, embedding dimension, recommendation systems, language model embeddings, value alignment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Luise Ge, Gregory Kehne, Yevgeniy Vorobeychik</div>
Social choice theory offers a wealth of approaches for selecting a candidate on behalf of voters based on their reported preference rankings over options. When voters have underlying utilities for these options, however, using preference rankings may lead to suboptimal outcomes vis-à-vis utilitarian social welfare. Distortion is a measure of this suboptimality, and provides a worst-case approach for developing and analyzing voting rules when utilities have minimal structure. However in many settings, such as common paradigms for value alignment, alternatives admit a vector representation, and it is natural to suppose that utilities are parametric functions thereof. We undertake the first study of distortion for linear utility functions. Specifically, we investigate the distortion of linear social choice for deterministic and randomized voting rules. We obtain bounds that depend only on the dimension of the candidate embedding, and are independent of the numbers of candidates or voters. Additionally, we introduce poly-time instance-optimal algorithms for minimizing distortion given a collection of candidates and votes. We empirically evaluate these in two real-world domains: recommendation systems using collaborative filtering embeddings, and opinion surveys utilizing language model embeddings, benchmarking several standard rules against our instance-optimal algorithms.
<div><strong>Authors:</strong> Luise Ge, Gregory Kehne, Yevgeniy Vorobeychik</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies distortion in social choice when voters' utilities are linear functions of candidate embeddings, providing dimension‑dependent bounds for deterministic and randomized voting rules. It introduces polynomial‑time instance‑optimal algorithms to minimize distortion and evaluates them on real‑world recommendation and language‑model embedding datasets, comparing against standard rules. The results show that distortion can be significantly reduced by leveraging the geometry of the embedding space.", "summary_cn": "本文研究了当选民的效用是候选人嵌入的线性函数时，社会选择中的失真（distortion），并给出仅依赖嵌入维度的确定性和随机投票规则的界限。作者提出了多项式时间的实例最优算法来最小化失真，并在基于协同过滤的推荐系统嵌入和语言模型嵌入的真实数据上进行评估，比较了几种标准投票规则。结果表明，利用嵌入空间的几何结构可以显著降低失真。", "keywords": "distortion, linear utilities, social choice, voting rules, instance-optimal algorithms, embedding dimension, recommendation systems, language model embeddings, value alignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Luise Ge", "Gregory Kehne", "Yevgeniy Vorobeychik"]}
]]></acme>

<pubDate>2025-10-22T20:42:49+00:00</pubDate>
</item>
<item>
<title>Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training</title>
<link>https://papers.cool/arxiv/2510.20002</link>
<guid>https://papers.cool/arxiv/2510.20002</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Greek Embedding Models (GEMs), a family of transformer language models for Modern Greek built on extensively curated high‑quality corpora from general and legal sources. It describes a quality‑driven data filtering pipeline, pre‑trains diverse architectures such as ELECTRA, ConvBERT and RoBERTa, and presents bilingual Greek‑English models for the legal domain, showing significant performance gains over baselines on downstream tasks.<br /><strong>Summary (CN):</strong> 本文推出了希腊语嵌入模型（GEMs）系列，使用经过严格质量筛选的通用及法律文本大规模语料库，对多种 transformer 架构（如 ELECTRA、ConvBERT、RoBERTa）进行预训练，并首次构建针对法律领域的希腊‑英语双语模型。实验表明这些模型在下游任务上显著优于现有基线。<br /><strong>Keywords:</strong> Greek NLP, corpus curation, legal domain, transformer pretraining, bilingual embeddings, GEM, ELECTRA, ConvBERT, RoBERTa, language models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexandra Apostolopoulou, Konstantinos Kanaris, Athanasios Koursaris, Dimitris Tsakalidis, George Domalis, Ioannis E. Livieris</div>
The advancement of natural language processing for morphologically rich, moderately-resourced languages like Modern Greek is often hindered by a fragmented research landscape, a lack of architectural diversity and reliance on limited context-length models. This is particularly true in specialized, high-value domains such as law, where existing models are frequently confined to early transformer architectures with a restrictive 512-token window, insufficient for analyzing long legal documents. To address these challenges, this paper presents Greek Embedding Models, a new family of transformer models for Greek language built upon a foundation of extensive, quality-driven data curation. We detail the construction of several large-scale Greek corpora, emphasizing a rigorous, quality-based filtering and preprocessing methodology to create high-value training datasets from both general-domain and specialized legal sources. On this carefully curated foundation, we pre-train and systematically evaluate a diverse suite of modern architectures, which has not previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT. Furthermore, we propose the first bilingual Greek-English Embedding Models tailored for the legal domain. The extensive experiments on downstream tasks demonstrate that the new class of models establish the effectiveness of the proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models significantly outperform existing baselines.
<div><strong>Authors:</strong> Alexandra Apostolopoulou, Konstantinos Kanaris, Athanasios Koursaris, Dimitris Tsakalidis, George Domalis, Ioannis E. Livieris</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Greek Embedding Models (GEMs), a family of transformer language models for Modern Greek built on extensively curated high‑quality corpora from general and legal sources. It describes a quality‑driven data filtering pipeline, pre‑trains diverse architectures such as ELECTRA, ConvBERT and RoBERTa, and presents bilingual Greek‑English models for the legal domain, showing significant performance gains over baselines on downstream tasks.", "summary_cn": "本文推出了希腊语嵌入模型（GEMs）系列，使用经过严格质量筛选的通用及法律文本大规模语料库，对多种 transformer 架构（如 ELECTRA、ConvBERT、RoBERTa）进行预训练，并首次构建针对法律领域的希腊‑英语双语模型。实验表明这些模型在下游任务上显著优于现有基线。", "keywords": "Greek NLP, corpus curation, legal domain, transformer pretraining, bilingual embeddings, GEM, ELECTRA, ConvBERT, RoBERTa, language models", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandra Apostolopoulou", "Konstantinos Kanaris", "Athanasios Koursaris", "Dimitris Tsakalidis", "George Domalis", "Ioannis E. Livieris"]}
]]></acme>

<pubDate>2025-10-22T20:06:48+00:00</pubDate>
</item>
<item>
<title>Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs</title>
<link>https://papers.cool/arxiv/2510.20001</link>
<guid>https://papers.cool/arxiv/2510.20001</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified paradigm for assessing LLMs in real-world clinical decision-making, structuring tasks along Clinical Backgrounds and Clinical Questions dimensions and mapping existing benchmarks onto this space. It surveys current datasets and methods, extends evaluation beyond accuracy to include efficiency and explainability, and outlines open challenges for building clinically useful LLMs.<br /><strong>Summary (CN):</strong> 本文提出了一种统一的评估范式，将临床决策任务沿“临床背景”和“临床问题”两个维度进行刻画，并将现有数据集映射到该空间。文章综述了当前的数据集与方法，扩展评估指标至效率和可解释性，并指出实现临床可用 LLM 的开放挑战。<br /><strong>Keywords:</strong> clinical decision-making, large language models, evaluation paradigm, explainability, MedQA, dataset benchmarking, efficiency, safety, AI in healthcare<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 6, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Yunpeng Xiao, Carl Yang, Mark Mai, Xiao Hu, Kai Shu</div>
Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.
<div><strong>Authors:</strong> Yunpeng Xiao, Carl Yang, Mark Mai, Xiao Hu, Kai Shu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified paradigm for assessing LLMs in real-world clinical decision-making, structuring tasks along Clinical Backgrounds and Clinical Questions dimensions and mapping existing benchmarks onto this space. It surveys current datasets and methods, extends evaluation beyond accuracy to include efficiency and explainability, and outlines open challenges for building clinically useful LLMs.", "summary_cn": "本文提出了一种统一的评估范式，将临床决策任务沿“临床背景”和“临床问题”两个维度进行刻画，并将现有数据集映射到该空间。文章综述了当前的数据集与方法，扩展评估指标至效率和可解释性，并指出实现临床可用 LLM 的开放挑战。", "keywords": "clinical decision-making, large language models, evaluation paradigm, explainability, MedQA, dataset benchmarking, efficiency, safety, AI in healthcare", "scoring": {"interpretability": 5, "understanding": 7, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yunpeng Xiao", "Carl Yang", "Mark Mai", "Xiao Hu", "Kai Shu"]}
]]></acme>

<pubDate>2025-10-22T20:06:10+00:00</pubDate>
</item>
<item>
<title>A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)</title>
<link>https://papers.cool/arxiv/2510.19997</link>
<guid>https://papers.cool/arxiv/2510.19997</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes FAIGMOE, a conceptual framework that integrates technology adoption theory, organizational change management, and innovation diffusion to guide the adoption and integration of Generative AI in midsize organizations and enterprises. It outlines four phases—strategic assessment, planning and use case development, implementation and integration, and operationalization and optimization—detailing practices for readiness assessment, risk governance, prompt engineering, model orchestration, and hallucination management. The framework aims to fill a gap in existing adoption models by providing scalable, AI‑specific guidance, and calls for empirical validation in future work.<br /><strong>Summary (CN):</strong> 本文提出了 FAIGMOE 框架，将技术采纳理论、组织变革管理和创新扩散视角融合，为中型组织和大型企业的生成式 AI 采用与集成提供指导。框架划分为四个相互关联的阶段：战略评估、规划与用例开发、实施与集成以及运维优化，并在每个阶段提供可扩展的准备度评估、风险治理、Prompt 工程、模型编排和幻觉管理等具体指导。该框架旨在填补现有采纳模型在生成式 AI 方面的空白，并呼吁通过未来研究进行实证验证。<br /><strong>Keywords:</strong> generative AI adoption, technology adoption framework, organizational change management, risk governance, prompt engineering, model orchestration, hallucination management<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 4, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Abraham Itzhak Weinberg</div>
Generative Artificial Intelligence (GenAI) presents transformative opportunities for organizations, yet both midsize organizations and larger enterprises face distinctive adoption challenges. Midsize organizations encounter resource constraints and limited AI expertise, while enterprises struggle with organizational complexity and coordination challenges. Existing technology adoption frameworks, including TAM (Technology Acceptance Model), TOE (Technology Organization Environment), and DOI (Diffusion of Innovations) theory, lack the specificity required for GenAI implementation across these diverse contexts, creating a critical gap in adoption literature. This paper introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises), a conceptual framework addressing the unique needs of both organizational types. FAIGMOE synthesizes technology adoption theory, organizational change management, and innovation diffusion perspectives into four interconnected phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization. Each phase provides scalable guidance on readiness assessment, strategic alignment, risk governance, technical architecture, and change management adaptable to organizational scale and complexity. The framework incorporates GenAI specific considerations including prompt engineering, model orchestration, and hallucination management that distinguish it from generic technology adoption frameworks. As a perspective contribution, FAIGMOE provides the first comprehensive conceptual framework explicitly addressing GenAI adoption across midsize and enterprise organizations, offering actionable implementation protocols, assessment instruments, and governance templates requiring empirical validation through future research.
<div><strong>Authors:</strong> Abraham Itzhak Weinberg</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes FAIGMOE, a conceptual framework that integrates technology adoption theory, organizational change management, and innovation diffusion to guide the adoption and integration of Generative AI in midsize organizations and enterprises. It outlines four phases—strategic assessment, planning and use case development, implementation and integration, and operationalization and optimization—detailing practices for readiness assessment, risk governance, prompt engineering, model orchestration, and hallucination management. The framework aims to fill a gap in existing adoption models by providing scalable, AI‑specific guidance, and calls for empirical validation in future work.", "summary_cn": "本文提出了 FAIGMOE 框架，将技术采纳理论、组织变革管理和创新扩散视角融合，为中型组织和大型企业的生成式 AI 采用与集成提供指导。框架划分为四个相互关联的阶段：战略评估、规划与用例开发、实施与集成以及运维优化，并在每个阶段提供可扩展的准备度评估、风险治理、Prompt 工程、模型编排和幻觉管理等具体指导。该框架旨在填补现有采纳模型在生成式 AI 方面的空白，并呼吁通过未来研究进行实证验证。", "keywords": "generative AI adoption, technology adoption framework, organizational change management, risk governance, prompt engineering, model orchestration, hallucination management", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 4, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Abraham Itzhak Weinberg"]}
]]></acme>

<pubDate>2025-10-22T19:55:31+00:00</pubDate>
</item>
<item>
<title>LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation</title>
<link>https://papers.cool/arxiv/2510.19988</link>
<guid>https://papers.cool/arxiv/2510.19988</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a hybrid system that combines large language models for text simplification and gap‑filling with a symbolic natural‑language‑understanding pipeline that generates relational representations for reasoning. By using LLMs to broaden coverage and symbolic methods to retain interpretability, the approach improves extraction of quantities and causal laws from commonsense science texts compared to a symbolic‑only baseline.<br /><strong>Summary (CN):</strong> 本文提出将大型语言模型用于文本重述和简化，以扩大覆盖范围，同时利用符号化自然语言理解系统生成可用于推理的关系表示，从而实现两者优势的结合。该混合方法在从常识科学文本中抽取数量和因果规律方面显著优于仅使用符号化管线的结果。<br /><strong>Keywords:</strong> LLM, symbolic NLU, causal statement extraction, hybrid AI, interpretability, hallucination mitigation, knowledge representation, commonsense science texts<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Xin Lian, Kenneth D. Forbus</div>
Despite the broad applicability of large language models (LLMs), their reliance on probabilistic inference makes them vulnerable to errors such as hallucination in generated facts and inconsistent output structure in natural language understanding (NLU) tasks. By contrast, symbolic NLU systems provide interpretable understanding grounded in curated lexicons, semantic resources, and syntactic & semantic interpretation rules. They produce relational representations that can be used for accurate reasoning and planning, as well as incremental debuggable learning. However, symbolic NLU systems tend to be more limited in coverage than LLMs and require scarce knowledge representation and linguistics skills to extend and maintain. This paper explores a hybrid approach that integrates the broad-coverage language processing of LLMs with the symbolic NLU capabilities of producing structured relational representations to hopefully get the best of both approaches. We use LLMs for rephrasing and text simplification, to provide broad coverage, and as a source of information to fill in knowledge gaps more automatically. We use symbolic NLU to produce representations that can be used for reasoning and for incremental learning. We evaluate this approach on the task of extracting and interpreting quantities and causal laws from commonsense science texts, along with symbolic- and LLM-only pipelines. Our results suggest that our hybrid method works significantly better than the symbolic-only pipeline.
<div><strong>Authors:</strong> Xin Lian, Kenneth D. Forbus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a hybrid system that combines large language models for text simplification and gap‑filling with a symbolic natural‑language‑understanding pipeline that generates relational representations for reasoning. By using LLMs to broaden coverage and symbolic methods to retain interpretability, the approach improves extraction of quantities and causal laws from commonsense science texts compared to a symbolic‑only baseline.", "summary_cn": "本文提出将大型语言模型用于文本重述和简化，以扩大覆盖范围，同时利用符号化自然语言理解系统生成可用于推理的关系表示，从而实现两者优势的结合。该混合方法在从常识科学文本中抽取数量和因果规律方面显著优于仅使用符号化管线的结果。", "keywords": "LLM, symbolic NLU, causal statement extraction, hybrid AI, interpretability, hallucination mitigation, knowledge representation, commonsense science texts", "scoring": {"interpretability": 8, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Xin Lian", "Kenneth D. Forbus"]}
]]></acme>

<pubDate>2025-10-22T19:38:20+00:00</pubDate>
</item>
<item>
<title>Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</title>
<link>https://papers.cool/arxiv/2510.19975</link>
<guid>https://papers.cool/arxiv/2510.19975</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies two-point zeroth-order gradient estimators and derives the perturbation distribution that minimizes asymptotic variance, showing that optimal perturbations can align directionally with the true gradient rather than having fixed length. It introduces the Directionally Aligned Perturbation (DAP) scheme, provides convergence analysis for SGD with δ‑unbiased perturbations, and demonstrates empirically that DAPs can outperform traditional methods on synthetic and practical tasks.<br /><strong>Summary (CN):</strong> 本文研究了两点零阶梯度估计器，并推导出能够最小化渐近方差的随机扰动分布，表明最佳扰动可以在方向上与真实梯度对齐，而不必保持固定长度。作者提出了方向对齐扰动（DAP）方案，给出基于 δ‑无偏扰动的 SGD 收敛分析，并在合成问题和实际任务上实验验证 DAP 在特定条件下优于传统方法。<br /><strong>Keywords:</strong> zeroth-order optimization, two-point estimator, minimum variance, directionally aligned perturbations, stochastic gradient descent, unbiased perturbations, variance reduction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $\delta$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies two-point zeroth-order gradient estimators and derives the perturbation distribution that minimizes asymptotic variance, showing that optimal perturbations can align directionally with the true gradient rather than having fixed length. It introduces the Directionally Aligned Perturbation (DAP) scheme, provides convergence analysis for SGD with δ‑unbiased perturbations, and demonstrates empirically that DAPs can outperform traditional methods on synthetic and practical tasks.", "summary_cn": "本文研究了两点零阶梯度估计器，并推导出能够最小化渐近方差的随机扰动分布，表明最佳扰动可以在方向上与真实梯度对齐，而不必保持固定长度。作者提出了方向对齐扰动（DAP）方案，给出基于 δ‑无偏扰动的 SGD 收敛分析，并在合成问题和实际任务上实验验证 DAP 在特定条件下优于传统方法。", "keywords": "zeroth-order optimization, two-point estimator, minimum variance, directionally aligned perturbations, stochastic gradient descent, unbiased perturbations, variance reduction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T19:06:39+00:00</pubDate>
</item>
<item>
<title>A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks</title>
<link>https://papers.cool/arxiv/2510.19973</link>
<guid>https://papers.cool/arxiv/2510.19973</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a tutorial on how cognitive biases from human design can affect large‑language‑model‑powered agents used for 6G network autonomy, describing taxonomy, mathematical formulations, emergence in telecom systems, and mitigation strategies, with two use‑cases demonstrating bias‑aware resource allocation and cross‑domain management improvements.<br /><strong>Summary (CN):</strong> 本文概述了在人类设计中继承的认知偏差如何影响用于 6G 网络自主化的大语言模型（LLM）代理，阐述了偏差的分类、数学形式、在通信系统中的出现以及对应的缓解策略，并通过两个实际案例展示了偏差感知的资源分配和跨域管理可实现显著的延迟降低和能耗提升。<br /><strong>Keywords:</strong> cognitive bias, agentic AI, 6G autonomous networks, large language model agents, bias mitigation, multi-objective optimization, network autonomy<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Hatim Chergui, Farhad Rezazadeh, Merouane Debbah, Christos Verikoukis</div>
The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\times 5$ lower latency and around $40\%$ higher energy saving.
<div><strong>Authors:</strong> Hatim Chergui, Farhad Rezazadeh, Merouane Debbah, Christos Verikoukis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a tutorial on how cognitive biases from human design can affect large‑language‑model‑powered agents used for 6G network autonomy, describing taxonomy, mathematical formulations, emergence in telecom systems, and mitigation strategies, with two use‑cases demonstrating bias‑aware resource allocation and cross‑domain management improvements.", "summary_cn": "本文概述了在人类设计中继承的认知偏差如何影响用于 6G 网络自主化的大语言模型（LLM）代理，阐述了偏差的分类、数学形式、在通信系统中的出现以及对应的缓解策略，并通过两个实际案例展示了偏差感知的资源分配和跨域管理可实现显著的延迟降低和能耗提升。", "keywords": "cognitive bias, agentic AI, 6G autonomous networks, large language model agents, bias mitigation, multi-objective optimization, network autonomy", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hatim Chergui", "Farhad Rezazadeh", "Merouane Debbah", "Christos Verikoukis"]}
]]></acme>

<pubDate>2025-10-22T19:05:04+00:00</pubDate>
</item>
<item>
<title>LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation</title>
<link>https://papers.cool/arxiv/2510.19967</link>
<guid>https://papers.cool/arxiv/2510.19967</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LyriCAR introduces a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, enabling unsupervised learning that progressively tackles more complex translation challenges. The adaptive curriculum strategy allocates training resources efficiently, reducing training steps by about 40% while achieving state-of-the-art performance on English-to-Chinese lyric translation across standard metrics and multi-dimensional rewards.<br /><strong>Summary (CN):</strong> LyriCAR 提出了一种难度感知的课程强化学习框架，用于可控的歌词翻译，实现了完全无监督的学习，并通过逐步增加翻译难度来提升模型能力。自适应课程策略能够高效分配训练资源，将训练步数降低约40%，同时在英-中歌词翻译任务上在标准翻译指标和多维奖励得分上均达到最新水平。<br /><strong>Keywords:</strong> lyric translation, curriculum learning, reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, adaptive curriculum, reward modeling, multilingual NLP, neural machine translation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.
<div><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LyriCAR introduces a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, enabling unsupervised learning that progressively tackles more complex translation challenges. The adaptive curriculum strategy allocates training resources efficiently, reducing training steps by about 40% while achieving state-of-the-art performance on English-to-Chinese lyric translation across standard metrics and multi-dimensional rewards.", "summary_cn": "LyriCAR 提出了一种难度感知的课程强化学习框架，用于可控的歌词翻译，实现了完全无监督的学习，并通过逐步增加翻译难度来提升模型能力。自适应课程策略能够高效分配训练资源，将训练步数降低约40%，同时在英-中歌词翻译任务上在标准翻译指标和多维奖励得分上均达到最新水平。", "keywords": "lyric translation, curriculum learning, reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, adaptive curriculum, reward modeling, multilingual NLP, neural machine translation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"]}
]]></acme>

<pubDate>2025-10-22T18:57:20+00:00</pubDate>
</item>
<item>
<title>On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization</title>
<link>https://papers.cool/arxiv/2510.19953</link>
<guid>https://papers.cool/arxiv/2510.19953</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a new family of unbiased gradient estimators for zeroth-order optimization by reformulating directional derivatives as a telescoping series and sampling from optimally designed distributions. It derives optimal scaling distributions and perturbation stepsizes for four constructions, proving that SGD with these estimators attains optimal complexity on smooth non-convex objectives, and demonstrates superior performance on synthetic benchmarks and language model fine‑tuning.<br /><strong>Summary (CN):</strong> 本文提出了一套全新的无偏梯度估计方法用于零阶优化，通过将方向导数重写为递进级数并从精心设计的分布中采样，实现了在仅使用函数评估的情况下消除偏差并保持方差可控。作者推导了四种具体构造的最优尺度分布和扰动步长，证明使用这些估计量的 SGD 在平滑非凸目标上达到了最优复杂度，并在合成任务以及语言模型微调实验中展示了优越的收敛性和精度。<br /><strong>Keywords:</strong> zeroth-order optimization, unbiased gradient estimator, stochastic optimization, directional derivatives, variance reduction, non-convex optimization, SGD, language model fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of unbiased gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a new family of unbiased gradient estimators for zeroth-order optimization by reformulating directional derivatives as a telescoping series and sampling from optimally designed distributions. It derives optimal scaling distributions and perturbation stepsizes for four constructions, proving that SGD with these estimators attains optimal complexity on smooth non-convex objectives, and demonstrates superior performance on synthetic benchmarks and language model fine‑tuning.", "summary_cn": "本文提出了一套全新的无偏梯度估计方法用于零阶优化，通过将方向导数重写为递进级数并从精心设计的分布中采样，实现了在仅使用函数评估的情况下消除偏差并保持方差可控。作者推导了四种具体构造的最优尺度分布和扰动步长，证明使用这些估计量的 SGD 在平滑非凸目标上达到了最优复杂度，并在合成任务以及语言模型微调实验中展示了优越的收敛性和精度。", "keywords": "zeroth-order optimization, unbiased gradient estimator, stochastic optimization, directional derivatives, variance reduction, non-convex optimization, SGD, language model fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T18:25:43+00:00</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</title>
<link>https://papers.cool/arxiv/2510.19950</link>
<guid>https://papers.cool/arxiv/2510.19950</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces elliptic uncertainty sets to model the directional market impact of trading agents, enabling closed-form worst-case analysis for robust reinforcement learning in financial markets. Experiments on single- and multi-asset tasks show improved Sharpe ratios and resilience to larger trade volumes compared with conventional symmetric robust RL methods.<br /><strong>Summary (CN):</strong> 本文提出使用椭圆不确定集来刻画交易行为对资产价格的方向性影响，从而实现对强化学习在金融市场中的最坏情况进行闭式求解的鲁棒评估。实验在单资产和多资产交易任务上表明，相较于传统对称鲁棒方法，本文方法能够提升夏普比率并在交易量增长时保持稳健。<br /><strong>Keywords:</strong> robust reinforcement learning, market impact, elliptic uncertainty sets, financial trading, Sharpe ratio, robust policy evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.
<div><strong>Authors:</strong> Shaocong Ma, Heng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces elliptic uncertainty sets to model the directional market impact of trading agents, enabling closed-form worst-case analysis for robust reinforcement learning in financial markets. Experiments on single- and multi-asset tasks show improved Sharpe ratios and resilience to larger trade volumes compared with conventional symmetric robust RL methods.", "summary_cn": "本文提出使用椭圆不确定集来刻画交易行为对资产价格的方向性影响，从而实现对强化学习在金融市场中的最坏情况进行闭式求解的鲁棒评估。实验在单资产和多资产交易任务上表明，相较于传统对称鲁棒方法，本文方法能够提升夏普比率并在交易量增长时保持稳健。", "keywords": "robust reinforcement learning, market impact, elliptic uncertainty sets, financial trading, Sharpe ratio, robust policy evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Shaocong Ma", "Heng Huang"]}
]]></acme>

<pubDate>2025-10-22T18:22:25+00:00</pubDate>
</item>
<item>
<title>Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</title>
<link>https://papers.cool/arxiv/2510.19897</link>
<guid>https://papers.cool/arxiv/2510.19897</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a memory-augmented framework that lets pretrained large language models learn classification tasks from labeled examples and model-generated critiques without updating parameters, using episodic memory for instance-specific critiques and semantic memory for distilled task-level guidance. Empirical results show up to a 24.8% accuracy boost over retrieval‑only baselines and introduce a "suggestibility" metric to analyze how supervision representations in memory affect learning dynamics across different models. The study highlights the potential of reflective, memory‑driven learning for building more adaptive and interpretable LLM agents.<br /><strong>Summary (CN):</strong> 本文提出一种记忆增强框架，使预训练大语言模型在不更新参数的情况下，仅通过标注示例和模型生成的批评即可学习分类任务，使用情景记忆存储实例级批评，使用语义记忆提炼为任务级指导。实验表明，与仅基于检索的基线相比，该方法可提升最高 24.8% 的准确率，并引入“suggestibility（可暗示性）”指标来解释不同模型对监督表征在记忆中的响应差异。研究展示了记忆驱动的反思学习在构建更具适应性和可解释性的 LLM 代理方面的前景。<br /><strong>Keywords:</strong> memory-augmented learning, semantic memory, episodic memory, large language models, reflective learning, suggestibility metric, few-shot classification, interpretability, adaptive agents<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.
<div><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a memory-augmented framework that lets pretrained large language models learn classification tasks from labeled examples and model-generated critiques without updating parameters, using episodic memory for instance-specific critiques and semantic memory for distilled task-level guidance. Empirical results show up to a 24.8% accuracy boost over retrieval‑only baselines and introduce a \"suggestibility\" metric to analyze how supervision representations in memory affect learning dynamics across different models. The study highlights the potential of reflective, memory‑driven learning for building more adaptive and interpretable LLM agents.", "summary_cn": "本文提出一种记忆增强框架，使预训练大语言模型在不更新参数的情况下，仅通过标注示例和模型生成的批评即可学习分类任务，使用情景记忆存储实例级批评，使用语义记忆提炼为任务级指导。实验表明，与仅基于检索的基线相比，该方法可提升最高 24.8% 的准确率，并引入“suggestibility（可暗示性）”指标来解释不同模型对监督表征在记忆中的响应差异。研究展示了记忆驱动的反思学习在构建更具适应性和可解释性的 LLM 代理方面的前景。", "keywords": "memory-augmented learning, semantic memory, episodic memory, large language models, reflective learning, suggestibility metric, few-shot classification, interpretability, adaptive agents", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"]}
]]></acme>

<pubDate>2025-10-22T17:58:03+00:00</pubDate>
</item>
<item>
<title>Large Language Model enabled Mathematical Modeling</title>
<link>https://papers.cool/arxiv/2510.19895</link>
<guid>https://papers.cool/arxiv/2510.19895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates the DeepSeek‑R1 large language model for automatically generating mathematical optimization formulations across four OR benchmarks (NL4OPT, IndustryOR, EasyLP, ComplexOR). It introduces a taxonomy of LLM hallucinations and proposes mitigation techniques such as LLM‑as‑a‑Judge, few‑shot learning, tool calling, and a multi‑agent framework to improve formulation accuracy and alignment with user intent.<br /><strong>Summary (CN):</strong> 本文评估了 DeepSeek‑R1 大语言模型在四个运筹学基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）上自动生成数学优化模型的能力。研究提出了 LLM 幻觉的分类体系，并通过 LLM‑as‑a‑Judge、少量示例学习、工具调用以及多代理框架等方法来降低幻觉、提升公式的准确性和与用户意图的对齐。<br /><strong>Keywords:</strong> LLM, mathematical modeling, operations research, DeepSeek-R1, hallucination mitigation, NL4OPT, tool calling, multi-agent framework<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Guoyun Zhang</div>
The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.
<div><strong>Authors:</strong> Guoyun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates the DeepSeek‑R1 large language model for automatically generating mathematical optimization formulations across four OR benchmarks (NL4OPT, IndustryOR, EasyLP, ComplexOR). It introduces a taxonomy of LLM hallucinations and proposes mitigation techniques such as LLM‑as‑a‑Judge, few‑shot learning, tool calling, and a multi‑agent framework to improve formulation accuracy and alignment with user intent.", "summary_cn": "本文评估了 DeepSeek‑R1 大语言模型在四个运筹学基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）上自动生成数学优化模型的能力。研究提出了 LLM 幻觉的分类体系，并通过 LLM‑as‑a‑Judge、少量示例学习、工具调用以及多代理框架等方法来降低幻觉、提升公式的准确性和与用户意图的对齐。", "keywords": "LLM, mathematical modeling, operations research, DeepSeek-R1, hallucination mitigation, NL4OPT, tool calling, multi-agent framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Guoyun Zhang"]}
]]></acme>

<pubDate>2025-10-22T17:41:42+00:00</pubDate>
</item>
<item>
<title>Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</title>
<link>https://papers.cool/arxiv/2510.19892</link>
<guid>https://papers.cool/arxiv/2510.19892</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a game‑based evaluation framework for multimodal large language models using the board game Dixit. By requiring models to generate captions that guide some but not all players to select the correct card, the method jointly tests multiple capabilities and provides objective win‑rate metrics. Experiments with five MLMs show that Dixit win‑rates correlate with standard benchmark scores, and human‑MLM games reveal distinct strategy differences and areas for improving model reasoning.<br /><strong>Summary (CN):</strong> 本文提出了一种基于游戏的多模态大语言模型评估框架，利用桌面游戏 Dixit。模型需要为卡牌生成能够让部分但不是全部玩家选择该卡的描述，从而在同一任务中考察多种能力并提供客观的胜率指标。对五种 MLM 的实验表明，Dixit 胜率与主流基准得分高度相关，并且人与模型的对战揭示了模型策略的差异及其推理改进空间。<br /><strong>Keywords:</strong> multimodal language models, game-based evaluation, Dixit, benchmarking, model reasoning, evaluation metrics, comparative assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nishant Balepur, Dang Nguyen, Dayeon Ki</div>
Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.
<div><strong>Authors:</strong> Nishant Balepur, Dang Nguyen, Dayeon Ki</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a game‑based evaluation framework for multimodal large language models using the board game Dixit. By requiring models to generate captions that guide some but not all players to select the correct card, the method jointly tests multiple capabilities and provides objective win‑rate metrics. Experiments with five MLMs show that Dixit win‑rates correlate with standard benchmark scores, and human‑MLM games reveal distinct strategy differences and areas for improving model reasoning.", "summary_cn": "本文提出了一种基于游戏的多模态大语言模型评估框架，利用桌面游戏 Dixit。模型需要为卡牌生成能够让部分但不是全部玩家选择该卡的描述，从而在同一任务中考察多种能力并提供客观的胜率指标。对五种 MLM 的实验表明，Dixit 胜率与主流基准得分高度相关，并且人与模型的对战揭示了模型策略的差异及其推理改进空间。", "keywords": "multimodal language models, game-based evaluation, Dixit, benchmarking, model reasoning, evaluation metrics, comparative assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"]}
]]></acme>

<pubDate>2025-10-22T17:21:16+00:00</pubDate>
</item>
<item>
<title>From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem</title>
<link>https://papers.cool/arxiv/2510.19889</link>
<guid>https://papers.cool/arxiv/2510.19889</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a Transformer‑based deep neural network to directly predict equilibrium path‑flow distributions for the traffic assignment problem, offering a data‑driven alternative to traditional optimization methods. Experiments on synthetic Manhattan, Sioux Falls, and Eastern‑Massachusetts networks show orders‑of‑magnitude speedups while maintaining or improving prediction accuracy, and the model adapts flexibly to changes in demand and network structure. This approach enables rapid what‑if analyses for transportation planning and policy‑making.<br /><strong>Summary (CN):</strong> 本文提出使用基于 Transformer 的深度神经网络直接预测交通分配问题中的均衡路径流分布，提供了一种数据驱动的替代传统优化的方法。通过在曼哈顿模拟网络、Sioux Falls 网络和东马萨诸塞网络上的实验，展示了在保持或提升预测准确性的同时，实现了数量级的计算加速，并且模型能够灵活适应需求和网络结构的变化。该方法支持快速的 "what‑if" 分析，助力交通规划与政策制定。<br /><strong>Keywords:</strong> traffic assignment, transformer, path flow estimation, equilibrium, deep learning, transportation planning, prediction, network optimization, multi-class traffic, what-if analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</div>
The traffic assignment problem is essential for traffic flow analysis, traditionally solved using mathematical programs under the Equilibrium principle. These methods become computationally prohibitive for large-scale networks due to non-linear growth in complexity with the number of OD pairs. This study introduces a novel data-driven approach using deep neural networks, specifically leveraging the Transformer architecture, to predict equilibrium path flows directly. By focusing on path-level traffic distribution, the proposed model captures intricate correlations between OD pairs, offering a more detailed and flexible analysis compared to traditional link-level approaches. The Transformer-based model drastically reduces computation time, while adapting to changes in demand and network structure without the need for recalculation. Numerical experiments are conducted on the Manhattan-like synthetic network, the Sioux Falls network, and the Eastern-Massachusetts network. The results demonstrate that the proposed model is orders of magnitude faster than conventional optimization. It efficiently estimates path-level traffic flows in multi-class networks, reducing computational costs and improving prediction accuracy by capturing detailed trip and flow information. The model also adapts flexibly to varying demand and network conditions, supporting traffic management and enabling rapid `what-if' analyses for enhanced transportation planning and policy-making.
<div><strong>Authors:</strong> Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a Transformer‑based deep neural network to directly predict equilibrium path‑flow distributions for the traffic assignment problem, offering a data‑driven alternative to traditional optimization methods. Experiments on synthetic Manhattan, Sioux Falls, and Eastern‑Massachusetts networks show orders‑of‑magnitude speedups while maintaining or improving prediction accuracy, and the model adapts flexibly to changes in demand and network structure. This approach enables rapid what‑if analyses for transportation planning and policy‑making.", "summary_cn": "本文提出使用基于 Transformer 的深度神经网络直接预测交通分配问题中的均衡路径流分布，提供了一种数据驱动的替代传统优化的方法。通过在曼哈顿模拟网络、Sioux Falls 网络和东马萨诸塞网络上的实验，展示了在保持或提升预测准确性的同时，实现了数量级的计算加速，并且模型能够灵活适应需求和网络结构的变化。该方法支持快速的 \"what‑if\" 分析，助力交通规划与政策制定。", "keywords": "traffic assignment, transformer, path flow estimation, equilibrium, deep learning, transportation planning, prediction, network optimization, multi-class traffic, what-if analysis", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mostafa Ameli", "Van Anh Le", "Sulthana Shams", "Alexander Skabardonis"]}
]]></acme>

<pubDate>2025-10-22T16:45:12+00:00</pubDate>
</item>
<item>
<title>Quantifying Feature Importance for Online Content Moderation</title>
<link>https://papers.cool/arxiv/2510.19882</link>
<guid>https://papers.cool/arxiv/2510.19882</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates which of 753 socio‑behavioural, linguistic, relational, and psychological features best predict Reddit users' changes in activity, toxicity, and participation diversity after a major moderation intervention. Using a greedy feature‑selection approach framed as a quantification problem, the authors identify a small set of consistently informative features and show that predictive difficulty varies across tasks. The findings aim to improve systems that forecast user reactions and highlight the need for moderation strategies tailored to both user traits and specific intervention goals.<br /><strong>Summary (CN):</strong> 本文研究了 753 项社会行为、语言、关系和心理特征中哪些最能预测 Reddit 用户在一次重大内容审查后在活跃度、毒性和参与多样性方面的行为变化。作者将问题框定为“定量化”，采用贪婪特征选择方法，找出一小部分在所有任务中一致有信息量的特征，并指出不同任务的预测难度存在差异。研究旨在提升预测用户反应的系统，并强调审查策略应同时考虑用户特征和具体干预目标。<br /><strong>Keywords:</strong> feature importance, content moderation, user behavior prediction, quantification, greedy feature selection, Reddit, toxicity, activity, participation diversity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Benedetta Tessa, Alejandro Moreo, Stefano Cresci, Tiziano Fagni, Fabrizio Sebastiani</div>
Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of "quantification", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.
<div><strong>Authors:</strong> Benedetta Tessa, Alejandro Moreo, Stefano Cresci, Tiziano Fagni, Fabrizio Sebastiani</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates which of 753 socio‑behavioural, linguistic, relational, and psychological features best predict Reddit users' changes in activity, toxicity, and participation diversity after a major moderation intervention. Using a greedy feature‑selection approach framed as a quantification problem, the authors identify a small set of consistently informative features and show that predictive difficulty varies across tasks. The findings aim to improve systems that forecast user reactions and highlight the need for moderation strategies tailored to both user traits and specific intervention goals.", "summary_cn": "本文研究了 753 项社会行为、语言、关系和心理特征中哪些最能预测 Reddit 用户在一次重大内容审查后在活跃度、毒性和参与多样性方面的行为变化。作者将问题框定为“定量化”，采用贪婪特征选择方法，找出一小部分在所有任务中一致有信息量的特征，并指出不同任务的预测难度存在差异。研究旨在提升预测用户反应的系统，并强调审查策略应同时考虑用户特征和具体干预目标。", "keywords": "feature importance, content moderation, user behavior prediction, quantification, greedy feature selection, Reddit, toxicity, activity, participation diversity", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Benedetta Tessa", "Alejandro Moreo", "Stefano Cresci", "Tiziano Fagni", "Fabrizio Sebastiani"]}
]]></acme>

<pubDate>2025-10-22T14:02:30+00:00</pubDate>
</item>
<item>
<title>Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention</title>
<link>https://papers.cool/arxiv/2510.19875</link>
<guid>https://papers.cool/arxiv/2510.19875</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Sparse Tracing, a technique that leverages dynamic sparse attention to make mechanistic interpretability feasible for large language models with million‑token contexts. It presents Stream, a hierarchical pruning algorithm that estimates per‑head sparse attention masks in near‑linear time and linear space, enabling one‑pass analysis of attention patterns while preserving the model's next‑token behavior. Experiments on chain‑of‑thought reasoning traces and the RULER benchmark show that Stream retains critical retrieval paths while discarding up to 99% of token interactions.<br /><strong>Summary (CN):</strong> 本文提出了 Sparse Tracing 技术，通过动态稀疏注意力实现对拥有百万级 token 上下文的大型语言模型的机械可解释性。文中展示了 Stream——一种层次化剪枝算法，可在接近线性时间 O(T\log T) 与线性空间 O(T) 下估计每个注意头的稀疏注意力掩码，从而在保持模型下一词行为的同时实现一次性注意力分析。对 chain‑of‑thought 推理链和 RULER 基准的实验表明，Stream 在保留关键检索路径的同时，剔除高达 97‑99% 的 token 交互。<br /><strong>Keywords:</strong> mechanistic interpretability, sparse attention, long-context LLMs, chain-of-thought tracing, hierarchical pruning, attention masking, RULER benchmark<br /><strong>Scores:</strong> Interpretability: 9, Understanding: 8, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> J Rosser, José Luis Redondo García, Gustavo Penha, Konstantina Palla, Hugues Bouchard</div>
As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.
<div><strong>Authors:</strong> J Rosser, José Luis Redondo García, Gustavo Penha, Konstantina Palla, Hugues Bouchard</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Sparse Tracing, a technique that leverages dynamic sparse attention to make mechanistic interpretability feasible for large language models with million‑token contexts. It presents Stream, a hierarchical pruning algorithm that estimates per‑head sparse attention masks in near‑linear time and linear space, enabling one‑pass analysis of attention patterns while preserving the model's next‑token behavior. Experiments on chain‑of‑thought reasoning traces and the RULER benchmark show that Stream retains critical retrieval paths while discarding up to 99% of token interactions.", "summary_cn": "本文提出了 Sparse Tracing 技术，通过动态稀疏注意力实现对拥有百万级 token 上下文的大型语言模型的机械可解释性。文中展示了 Stream——一种层次化剪枝算法，可在接近线性时间 O(T\\log T) 与线性空间 O(T) 下估计每个注意头的稀疏注意力掩码，从而在保持模型下一词行为的同时实现一次性注意力分析。对 chain‑of‑thought 推理链和 RULER 基准的实验表明，Stream 在保留关键检索路径的同时，剔除高达 97‑99% 的 token 交互。", "keywords": "mechanistic interpretability, sparse attention, long-context LLMs, chain-of-thought tracing, hierarchical pruning, attention masking, RULER benchmark", "scoring": {"interpretability": 9, "understanding": 8, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["J Rosser", "José Luis Redondo García", "Gustavo Penha", "Konstantina Palla", "Hugues Bouchard"]}
]]></acme>

<pubDate>2025-10-22T09:42:29+00:00</pubDate>
</item>
<item>
<title>From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph</title>
<link>https://papers.cool/arxiv/2510.19873</link>
<guid>https://papers.cool/arxiv/2510.19873</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ReGraphT, a training‑free retrieval‑augmented generation framework that transfers the reasoning abilities of large language models to smaller, privacy‑friendly models for CUDA code optimization. By constructing a reasoning graph of optimization state transitions and applying Monte Carlo Graph Search, ReGraphT enables small models to generate optimized CUDA code with performance close to that of larger models, achieving significant speedups on the CUDAEval and ParEval benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 ReGraphT，一种无需训练的检索增强生成框架，通过构建 CUDA 优化轨迹的推理图并利用蒙特卡罗图搜索，将大型语言模型的推理能力转移至体积更小且更具隐私性的模型，使其能够生成高效的 CUDA 代码，并在 CUDAEval 与 ParEval 基准上实现显著的加速。<br /><strong>Keywords:</strong> CUDA optimization, large language model, small language model, retrieval-augmented generation, reasoning graph, Monte Carlo Graph Search, code generation, performance benchmarking<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li</div>
Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice faces two major challenges: cloud-based APIs pose risks of code leakage, and local deployment is often computationally expensive and inefficient. These drawbacks have spurred interest in small language models (SLMs), which are more lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs can achieve performance comparable to LLMs on specific tasks. While SLMs can match LLMs on domain-specific tasks, their limited reasoning abilities lead to suboptimal performance in complex CUDA generation according to our experiments. To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented generation framework that transfers LLM-level reasoning to smaller models. ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph, modeling the combined CUDA optimizations as state transitions, and leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also present a CUDA-specific benchmark with difficulty tiers defined by reasoning complexity to evaluate models more comprehensively. Experiments show that ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level performance without the associated privacy risks or excessive computing overhead.
<div><strong>Authors:</strong> Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ReGraphT, a training‑free retrieval‑augmented generation framework that transfers the reasoning abilities of large language models to smaller, privacy‑friendly models for CUDA code optimization. By constructing a reasoning graph of optimization state transitions and applying Monte Carlo Graph Search, ReGraphT enables small models to generate optimized CUDA code with performance close to that of larger models, achieving significant speedups on the CUDAEval and ParEval benchmarks.", "summary_cn": "本文提出 ReGraphT，一种无需训练的检索增强生成框架，通过构建 CUDA 优化轨迹的推理图并利用蒙特卡罗图搜索，将大型语言模型的推理能力转移至体积更小且更具隐私性的模型，使其能够生成高效的 CUDA 代码，并在 CUDAEval 与 ParEval 基准上实现显著的加速。", "keywords": "CUDA optimization, large language model, small language model, retrieval-augmented generation, reasoning graph, Monte Carlo Graph Search, code generation, performance benchmarking", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junfeng Gong", "Zhiyi Wei", "Junying Chen", "Cheng Liu", "Huawei Li"]}
]]></acme>

<pubDate>2025-10-22T08:33:44+00:00</pubDate>
</item>
<item>
<title>An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</title>
<link>https://papers.cool/arxiv/2510.19866</link>
<guid>https://papers.cool/arxiv/2510.19866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates the pedagogical soundness and usability of AI‑generated high‑school physics lesson plans across five leading large language models and three structured prompt frameworks, using four automated metrics (readability, factual accuracy, curriculum alignment, and cognitive demand). Results show that model choice most strongly influences linguistic accessibility, while prompt framework design impacts factual accuracy and curricular completeness, with the RACE framework performing best. The study highlights the need to combine readability‑optimized models with well‑structured prompts and explicit content checklists to improve instructional quality.<br /><strong>Summary (CN):</strong> 本文评估了五种主流大语言模型在生成高中物理教学计划时的教学可靠性和可用性，并比较了三种结构化提示框架（TAG、RACE、COSTAR），采用可读性、事实准确性、课程对齐度和学习目标认知需求四项自动化指标。结果表明，模型选择对语言可读性影响最大，而提示框架对事实错误率和课程匹配度影响更显著，RACE 框架表现最佳。研究指出，将可读性优化的模型与结构化提示以及明确的物理概念和高阶目标检查表相结合，可提升教学计划的质量。<br /><strong>Keywords:</strong> AI-generated lesson plans, large language models, prompt engineering, pedagogical evaluation, readability, factual accuracy, curriculum alignment, Bloom's taxonomy, educational AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Xincheng Liu</div>
This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice, three structured prompt frameworks were tested: TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format). Fifteen lesson plans were generated for a single high-school physics topic, The Electromagnetic Spectrum. The lesson plans were analyzed through four automated computational metrics: (1) readability and linguistic complexity, (2) factual accuracy and hallucination detection, (3) standards and curriculum alignment, and (4) cognitive demand of learning objectives. Results indicate that model selection exerted the strongest influence on linguistic accessibility, with DeepSeek producing the most readable teaching plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89). The prompt framework structure most strongly affected the factual accuracy and pedagogical completeness, with the RACE framework yielding the lowest hallucination index and the highest incidental alignment with NGSS curriculum standards. Across all models, the learning objectives in the fifteen lesson plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There were limited higher-order verbs in the learning objectives extracted. Overall, the findings suggest that readability is significantly governed by model design, while instructional reliability and curricular alignment depend more on the prompt framework. The most effective configuration for lesson plans identified in the results was to combine a readability-optimized model with the RACE framework and an explicit checklist of physics concepts, curriculum standards, and higher-order objectives.
<div><strong>Authors:</strong> Xincheng Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates the pedagogical soundness and usability of AI‑generated high‑school physics lesson plans across five leading large language models and three structured prompt frameworks, using four automated metrics (readability, factual accuracy, curriculum alignment, and cognitive demand). Results show that model choice most strongly influences linguistic accessibility, while prompt framework design impacts factual accuracy and curricular completeness, with the RACE framework performing best. The study highlights the need to combine readability‑optimized models with well‑structured prompts and explicit content checklists to improve instructional quality.", "summary_cn": "本文评估了五种主流大语言模型在生成高中物理教学计划时的教学可靠性和可用性，并比较了三种结构化提示框架（TAG、RACE、COSTAR），采用可读性、事实准确性、课程对齐度和学习目标认知需求四项自动化指标。结果表明，模型选择对语言可读性影响最大，而提示框架对事实错误率和课程匹配度影响更显著，RACE 框架表现最佳。研究指出，将可读性优化的模型与结构化提示以及明确的物理概念和高阶目标检查表相结合，可提升教学计划的质量。", "keywords": "AI-generated lesson plans, large language models, prompt engineering, pedagogical evaluation, readability, factual accuracy, curriculum alignment, Bloom's taxonomy, educational AI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Xincheng Liu"]}
]]></acme>

<pubDate>2025-10-22T02:53:06+00:00</pubDate>
</item>
<item>
<title>Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability</title>
<link>https://papers.cool/arxiv/2510.19851</link>
<guid>https://papers.cool/arxiv/2510.19851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether language models can deliberately obscure their chain-of-thought (CoT) reasoning to hide adversarial goals, thereby evading monitoring mechanisms that rely on transparent reasoning. By constructing a taxonomy of prompts that induce CoT obfuscation and testing both internal (latent) and external (output) CoT across toy tasks and the SHADE‑Arena benchmark, the authors find that monitoring works well absent obfuscation pressure, but strong pressure can lead models to succeed on malicious tasks while hiding their reasoning, especially in external CoT. The results highlight the need for model‑specific stress‑testing of CoT monitorability before deployment.<br /><strong>Summary (CN):</strong> 本文研究语言模型是否能刻意隐藏其链式思考（Chain-of-Thought, CoT）过程，以掩盖对抗性目标并规避基于透明推理的监控机制。作者构建了一套促使 CoT 模糊化的提示分类法，并在玩具任务与 SHADE‑Arena 环境中分别评估内部（内部推理痕迹）和外部（输出中的推理）CoT。实验表明，在没有模糊化压力时监控表现准确高效，但在强压力下模型能够完成对抗任务并隐藏外部 CoT，内部 CoT 的模糊化程度较低。结果表明，尽管 CoT 在良性场景下提供有价值的监督，但实际部署前需进行针对特定模型的监控可测性压力测试。<br /><strong>Keywords:</strong> chain-of-thought, monitorability, deception, obfuscation, safety testing, SHADE-Arena, adversarial objectives<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner</div>
Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.
<div><strong>Authors:</strong> Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether language models can deliberately obscure their chain-of-thought (CoT) reasoning to hide adversarial goals, thereby evading monitoring mechanisms that rely on transparent reasoning. By constructing a taxonomy of prompts that induce CoT obfuscation and testing both internal (latent) and external (output) CoT across toy tasks and the SHADE‑Arena benchmark, the authors find that monitoring works well absent obfuscation pressure, but strong pressure can lead models to succeed on malicious tasks while hiding their reasoning, especially in external CoT. The results highlight the need for model‑specific stress‑testing of CoT monitorability before deployment.", "summary_cn": "本文研究语言模型是否能刻意隐藏其链式思考（Chain-of-Thought, CoT）过程，以掩盖对抗性目标并规避基于透明推理的监控机制。作者构建了一套促使 CoT 模糊化的提示分类法，并在玩具任务与 SHADE‑Arena 环境中分别评估内部（内部推理痕迹）和外部（输出中的推理）CoT。实验表明，在没有模糊化压力时监控表现准确高效，但在强压力下模型能够完成对抗任务并隐藏外部 CoT，内部 CoT 的模糊化程度较低。结果表明，尽管 CoT 在良性场景下提供有价值的监督，但实际部署前需进行针对特定模型的监控可测性压力测试。", "keywords": "chain-of-thought, monitorability, deception, obfuscation, safety testing, SHADE-Arena, adversarial objectives", "scoring": {"interpretability": 6, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Artur Zolkowski", "Wen Xing", "David Lindner", "Florian Tramèr", "Erik Jenner"]}
]]></acme>

<pubDate>2025-10-21T18:07:10+00:00</pubDate>
</item>
<item>
<title>Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs</title>
<link>https://papers.cool/arxiv/2510.19850</link>
<guid>https://papers.cool/arxiv/2510.19850</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Prompt Decorators, a declarative and composable syntax that lets users control LLM behavior through compact tokens (e.g., +++Reasoning, +++Tone) without altering the task content. It defines twenty core decorators split into cognitive/generative and expressive/systemic families, a unified syntax, scoping rules, and a deterministic processing pipeline to achieve reproducible and auditable behavior composition. Use cases demonstrate clearer reasoning, reduced prompt complexity, and standardized model outputs across domains.<br /><strong>Summary (CN):</strong> 本文提出了 Prompt Decorators，一种声明式且可组合的语法，通过紧凑的控制标记（如 +++Reasoning、+++Tone）在不改变任务内容的前提下调节大型语言模型的行为。论文定义了二十个核心饰符，划分为认知/生成和表达/系统两大家族，提供统一语法、作用域模型和确定性处理流水线，实现可复现、可审计的行为组合。示例展示了提升推理透明度、降低提示复杂度以及在不同领域实现输出一致性。<br /><strong>Keywords:</strong> prompt engineering, declarative syntax, composable decorators, LLM control, reasoning, interpretability, modularity<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Mostapha Kalami Heris</div>
Large Language Models (LLMs) are central to reasoning, writing, and decision-support workflows, yet users lack consistent control over how they reason and express outputs. Conventional prompt engineering relies on verbose natural-language instructions, limiting reproducibility, modularity, and interpretability. This paper introduces Prompt Decorators, a declarative, composable syntax that governs LLM behavior through compact control tokens such as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems Thinking"). Each decorator modifies a behavioral dimension, such as reasoning style, structure, or tone, without changing task content. The framework formalizes twenty core decorators organized into two functional families (Cognitive & Generative and Expressive & Systemic), each further decomposed into subcategories that govern reasoning, interaction, expression, and session-control. It defines a unified syntax, scoping model, and deterministic processing pipeline enabling predictable and auditable behavior composition. By decoupling task intent from execution behavior, Prompt Decorators create a reusable and interpretable interface for prompt design. Illustrative use cases demonstrate improved reasoning transparency, reduced prompt complexity, and standardized model behavior across domains. The paper concludes with implications for interoperability, behavioral consistency, and the development of declarative interfaces for scalable AI systems.
<div><strong>Authors:</strong> Mostapha Kalami Heris</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Prompt Decorators, a declarative and composable syntax that lets users control LLM behavior through compact tokens (e.g., +++Reasoning, +++Tone) without altering the task content. It defines twenty core decorators split into cognitive/generative and expressive/systemic families, a unified syntax, scoping rules, and a deterministic processing pipeline to achieve reproducible and auditable behavior composition. Use cases demonstrate clearer reasoning, reduced prompt complexity, and standardized model outputs across domains.", "summary_cn": "本文提出了 Prompt Decorators，一种声明式且可组合的语法，通过紧凑的控制标记（如 +++Reasoning、+++Tone）在不改变任务内容的前提下调节大型语言模型的行为。论文定义了二十个核心饰符，划分为认知/生成和表达/系统两大家族，提供统一语法、作用域模型和确定性处理流水线，实现可复现、可审计的行为组合。示例展示了提升推理透明度、降低提示复杂度以及在不同领域实现输出一致性。", "keywords": "prompt engineering, declarative syntax, composable decorators, LLM control, reasoning, interpretability, modularity", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Mostapha Kalami Heris"]}
]]></acme>

<pubDate>2025-10-21T17:35:49+00:00</pubDate>
</item>
<item>
<title>CourtGuard: A Local, Multiagent Prompt Injection Classifier</title>
<link>https://papers.cool/arxiv/2510.19844</link>
<guid>https://papers.cool/arxiv/2510.19844</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CourtGuard is a locally runnable, multi‑agent system that classifies prompts as benign or malicious by staging a courtroom‑style debate between a defense‑attorney LLM, a prosecution‑attorney LLM, and a judge LLM. The approach achieves a lower false‑positive rate than a single‑LLM detector, though overall detection performance is weaker, highlighting trade‑offs in multi‑agent defenses against prompt injection attacks.<br /><strong>Summary (CN):</strong> CourtGuard 是一个本地可运行的多智能体系统，通过让“辩护律师”模型、 “检控律师”模型和“法官”模型在类似法庭的辩论中评估提示，从而判定其是否为提示注入攻击。该方法相比单一模型的检测器显著降低了误报率，尽管整体检测效果略逊一筹，凸显了多智能体防御在应对提示注入攻击时的权衡。<br /><strong>Keywords:</strong> prompt injection, multiagent classification, LLM safety, adversarial detection, local defense, courtroom model, prompt security<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Isaac Wu, Michael Maslowski</div>
As large language models (LLMs) become integrated into various sensitive applications, prompt injection, the use of prompting to induce harmful behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks can cause LLMs to leak sensitive data, spread misinformation, and exhibit harmful behaviors. To defend against these attacks, we propose CourtGuard, a locally-runnable, multiagent prompt injection classifier. In it, prompts are evaluated in a court-like multiagent LLM system, where a "defense attorney" model argues the prompt is benign, a "prosecution attorney" model argues the prompt is a prompt injection, and a "judge" model gives the final classification. CourtGuard has a lower false positive rate than the Direct Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt injection detector. Nevertheless, this lower false positive rate highlights the importance of considering both adversarial and benign scenarios for the classification of a prompt. Additionally, the relative performance of CourtGuard in comparison to other prompt injection classifiers advances the use of multiagent systems as a defense against prompt injection attacks. The implementations of CourtGuard and the Direct Detector with full prompts for Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at https://github.com/isaacwu2000/CourtGuard.
<div><strong>Authors:</strong> Isaac Wu, Michael Maslowski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CourtGuard is a locally runnable, multi‑agent system that classifies prompts as benign or malicious by staging a courtroom‑style debate between a defense‑attorney LLM, a prosecution‑attorney LLM, and a judge LLM. The approach achieves a lower false‑positive rate than a single‑LLM detector, though overall detection performance is weaker, highlighting trade‑offs in multi‑agent defenses against prompt injection attacks.", "summary_cn": "CourtGuard 是一个本地可运行的多智能体系统，通过让“辩护律师”模型、 “检控律师”模型和“法官”模型在类似法庭的辩论中评估提示，从而判定其是否为提示注入攻击。该方法相比单一模型的检测器显著降低了误报率，尽管整体检测效果略逊一筹，凸显了多智能体防御在应对提示注入攻击时的权衡。", "keywords": "prompt injection, multiagent classification, LLM safety, adversarial detection, local defense, courtroom model, prompt security", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Isaac Wu", "Michael Maslowski"]}
]]></acme>

<pubDate>2025-10-20T20:10:06+00:00</pubDate>
</item>
<item>
<title>SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks</title>
<link>https://papers.cool/arxiv/2510.19829</link>
<guid>https://papers.cool/arxiv/2510.19829</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SSL‑SE‑EEG, a framework that combines self‑supervised learning with squeeze‑and‑excitation networks to process EEG signals transformed into 2‑D image-like representations, achieving high classification accuracy and robustness to noise while reducing the need for labeled data. Experiments on four benchmark EEG datasets (MindBigData, TUH‑AB, SEED‑IV, BCI‑IV) show state‑of‑the‑art performance, suggesting suitability for real‑time BCI applications.<br /><strong>Summary (CN):</strong> 本文提出了 SSL‑SE‑EEG 框架，将 EEG 信号转化为二维图像形式，并结合自监督学习（Self‑Supervised Learning）与 Squeeze‑Excitation 网络（SE‑Net），在提升特征提取、噪声鲁棒性并降低标注需求的同时，实现了高分类准确率。在 MindBigData、TUH‑AB、SEED‑IV、BCI‑IV 四个公开数据集上的实验表明该方法达到或超过当前最佳水平，适用于实时脑机接口（BCI）应用。<br /><strong>Keywords:</strong> self-supervised learning, squeeze-excitation networks, EEG, brain-computer interface, noise robustness, representation learning, biomedical signal processing, deep learning, unsupervised feature extraction<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Meghna Roy Chowdhury, Yi Ding, Shreyas Sen</div>
Electroencephalography (EEG) plays a crucial role in brain-computer interfaces (BCIs) and neurological diagnostics, but its real-world deployment faces challenges due to noise artifacts, missing data, and high annotation costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance feature extraction, improve noise robustness, and reduce reliance on labeled data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG signals into structured 2D image representations, suitable for deep learning. Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB), making it well-suited for real-time BCI applications. By enabling low-power, scalable EEG processing, SSL-SE-EEG presents a promising solution for biomedical signal analysis, neural engineering, and next-generation BCIs.
<div><strong>Authors:</strong> Meghna Roy Chowdhury, Yi Ding, Shreyas Sen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SSL‑SE‑EEG, a framework that combines self‑supervised learning with squeeze‑and‑excitation networks to process EEG signals transformed into 2‑D image-like representations, achieving high classification accuracy and robustness to noise while reducing the need for labeled data. Experiments on four benchmark EEG datasets (MindBigData, TUH‑AB, SEED‑IV, BCI‑IV) show state‑of‑the‑art performance, suggesting suitability for real‑time BCI applications.", "summary_cn": "本文提出了 SSL‑SE‑EEG 框架，将 EEG 信号转化为二维图像形式，并结合自监督学习（Self‑Supervised Learning）与 Squeeze‑Excitation 网络（SE‑Net），在提升特征提取、噪声鲁棒性并降低标注需求的同时，实现了高分类准确率。在 MindBigData、TUH‑AB、SEED‑IV、BCI‑IV 四个公开数据集上的实验表明该方法达到或超过当前最佳水平，适用于实时脑机接口（BCI）应用。", "keywords": "self-supervised learning, squeeze-excitation networks, EEG, brain-computer interface, noise robustness, representation learning, biomedical signal processing, deep learning, unsupervised feature extraction", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Meghna Roy Chowdhury", "Yi Ding", "Shreyas Sen"]}
]]></acme>

<pubDate>2025-10-07T06:37:34+00:00</pubDate>
</item>
<item>
<title>SLYKLatent, a Learning Framework for Facial Features Estimation</title>
<link>https://papers.cool/arxiv/2402.01555</link>
<guid>https://papers.cool/arxiv/2402.01555</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SLYKLatent is a novel learning framework that improves gaze estimation by tackling appearance instability caused by aleatoric uncertainty, covariate shift, and domain generalization. It first pretrains a self-supervised model on facial expression data, then refines it with a patch-based tri-branch network and an inverse-explained-variance weighted loss. Experiments on Gaze360, MPIIFaceGaze, ETH-XGaze and emotion datasets demonstrate significant performance gains.<br /><strong>Summary (CN):</strong> SLYKLatent 是一种新颖的学习框架，旨在通过解决因随机不确定性、协变量漂移和测试领域泛化导致的外观不稳定问题来提升目光估计。该方法先在面部表情数据上进行自监督预训练，然后通过基于补丁的三分支网络和逆方差加权损失进行精炼。实验在 Gaze360、MPIIFaceGaze、ETH-XGaze 以及情感数据集上显示出显著的性能提升。<br /><strong>Keywords:</strong> gaze estimation, self-supervised learning, domain generalization, tri-branch network, inverse explained variance loss, facial expression datasets, computer vision<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Samuel Adebayo, Joost C. Dessing, Seán McLoone</div>
In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function. Our evaluation on benchmark datasets achieves an 8.7% improvement on Gaze360, rivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%, surpassing existing methods by significant margins. Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components. This approach has strong potential in human-robot interaction.
<div><strong>Authors:</strong> Samuel Adebayo, Joost C. Dessing, Seán McLoone</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SLYKLatent is a novel learning framework that improves gaze estimation by tackling appearance instability caused by aleatoric uncertainty, covariate shift, and domain generalization. It first pretrains a self-supervised model on facial expression data, then refines it with a patch-based tri-branch network and an inverse-explained-variance weighted loss. Experiments on Gaze360, MPIIFaceGaze, ETH-XGaze and emotion datasets demonstrate significant performance gains.", "summary_cn": "SLYKLatent 是一种新颖的学习框架，旨在通过解决因随机不确定性、协变量漂移和测试领域泛化导致的外观不稳定问题来提升目光估计。该方法先在面部表情数据上进行自监督预训练，然后通过基于补丁的三分支网络和逆方差加权损失进行精炼。实验在 Gaze360、MPIIFaceGaze、ETH-XGaze 以及情感数据集上显示出显著的性能提升。", "keywords": "gaze estimation, self-supervised learning, domain generalization, tri-branch network, inverse explained variance loss, facial expression datasets, computer vision", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Samuel Adebayo", "Joost C. Dessing", "Seán McLoone"]}
]]></acme>

<pubDate>2024-02-02T16:47:18+00:00</pubDate>
</item>
<item>
<title>Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location</title>
<link>https://papers.cool/arxiv/2510.18803</link>
<guid>https://papers.cool/arxiv/2510.18803</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes 18 years of Canadian research grant proposals using three topic‑modeling methods (LDA, STM, BERTopic) and introduces COFFEE, an algorithm for covariate effect estimation with BERTopic. It finds that BERTopic yields more granular and coherent topics, including emerging AI themes, and that gender and provincial location systematically influence topic distributions. The results aim to inform more equitable funding strategies.<br /><strong>Summary (CN):</strong> 本文分析了 2005‑2022 年加拿大 NSERC 资助的研究提案，比较了 LDA、STM 与 BERTopic 三种主题模型，并提出了用于 BERTopic 的协变量效应估计算法 COFFEE。研究表明 BERTopic 能更细致且连贯地捕捉主题，包括人工智能等新兴领域，并确认性别与省份对主题分布存在一致的影响模式。此洞察旨在帮助制定更公平的科研资助策略。<br /><strong>Keywords:</strong> topic modeling, LDA, STM, BERTopic, COFFEE algorithm, gender analysis, geographic analysis, research funding, scientometrics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shirin Tavakoli Kafiabad, Andrea Schiffauerova, Ashkan Ebadi</div>
Optimizing national scientific investment requires a clear understanding of evolving research trends and the demographic and geographical forces shaping them, particularly in light of commitments to equity, diversity, and inclusion. This study addresses this need by analyzing 18 years (2005-2022) of research proposals funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). We conducted a comprehensive comparative evaluation of three topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic Modelling (STM), and BERTopic. We also introduced a novel algorithm, named COFFEE, designed to enable robust covariate effect estimation for BERTopic. This advancement addresses a significant gap, as BERTopic lacks a native function for covariate analysis, unlike the probabilistic STM. Our findings highlight that while all models effectively delineate core scientific domains, BERTopic outperformed by consistently identifying more granular, coherent, and emergent themes, such as the rapid expansion of artificial intelligence. Additionally, the covariate analysis, powered by COFFEE, confirmed distinct provincial research specializations and revealed consistent gender-based thematic patterns across various scientific disciplines. These insights offer a robust empirical foundation for funding organizations to formulate more equitable and impactful funding strategies, thereby enhancing the effectiveness of the scientific ecosystem.
<div><strong>Authors:</strong> Shirin Tavakoli Kafiabad, Andrea Schiffauerova, Ashkan Ebadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes 18 years of Canadian research grant proposals using three topic‑modeling methods (LDA, STM, BERTopic) and introduces COFFEE, an algorithm for covariate effect estimation with BERTopic. It finds that BERTopic yields more granular and coherent topics, including emerging AI themes, and that gender and provincial location systematically influence topic distributions. The results aim to inform more equitable funding strategies.", "summary_cn": "本文分析了 2005‑2022 年加拿大 NSERC 资助的研究提案，比较了 LDA、STM 与 BERTopic 三种主题模型，并提出了用于 BERTopic 的协变量效应估计算法 COFFEE。研究表明 BERTopic 能更细致且连贯地捕捉主题，包括人工智能等新兴领域，并确认性别与省份对主题分布存在一致的影响模式。此洞察旨在帮助制定更公平的科研资助策略。", "keywords": "topic modeling, LDA, STM, BERTopic, COFFEE algorithm, gender analysis, geographic analysis, research funding, scientometrics", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shirin Tavakoli Kafiabad", "Andrea Schiffauerova", "Ashkan Ebadi"]}
]]></acme>

<pubDate>2025-10-21T16:58:00+00:00</pubDate>
</item>
<item>
<title>Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation</title>
<link>https://papers.cool/arxiv/2510.18751</link>
<guid>https://papers.cool/arxiv/2510.18751</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents ALGOS, a language‑guided segmentation and reasoning system for monitoring harmful algal blooms (HAB) using remote‑sensing imagery. It combines GeoSAM‑assisted human evaluation for high‑quality mask curation with a fine‑tuned vision‑language model to predict bloom severity from the Cyanobacteria Aggregated Manual Labels (CAML) dataset, showing strong performance on both segmentation and severity‑level estimation.<br /><strong>Summary (CN):</strong> 本文提出了 ALGOS 系统，一种基于语言引导的有害藻华（HAB）监测分割与推理方法，利用遥感图像进行监测。系统结合 GeoSAM 辅助的人类评估以获得高质量的分割掩码，并在 NASA 提供的 Cyanobacteria Aggregated Manual Labels (CAML) 数据上微调视觉语言模型，以预测藻华的严重程度，实验表明在分割和严重程度估计上均实现了稳健性能。<br /><strong>Keywords:</strong> harmful algal bloom, cyanobacteria, vision-language model, remote sensing, segmentation, severity estimation, GeoSAM, CAML, environmental monitoring<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Patterson Hsieh, Jerry Yeh, Mao-Chi He, Wen-Han Hsieh, Elvis Hsieh</div>
Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.
<div><strong>Authors:</strong> Patterson Hsieh, Jerry Yeh, Mao-Chi He, Wen-Han Hsieh, Elvis Hsieh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents ALGOS, a language‑guided segmentation and reasoning system for monitoring harmful algal blooms (HAB) using remote‑sensing imagery. It combines GeoSAM‑assisted human evaluation for high‑quality mask curation with a fine‑tuned vision‑language model to predict bloom severity from the Cyanobacteria Aggregated Manual Labels (CAML) dataset, showing strong performance on both segmentation and severity‑level estimation.", "summary_cn": "本文提出了 ALGOS 系统，一种基于语言引导的有害藻华（HAB）监测分割与推理方法，利用遥感图像进行监测。系统结合 GeoSAM 辅助的人类评估以获得高质量的分割掩码，并在 NASA 提供的 Cyanobacteria Aggregated Manual Labels (CAML) 数据上微调视觉语言模型，以预测藻华的严重程度，实验表明在分割和严重程度估计上均实现了稳健性能。", "keywords": "harmful algal bloom, cyanobacteria, vision-language model, remote sensing, segmentation, severity estimation, GeoSAM, CAML, environmental monitoring", "scoring": {"interpretability": 5, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"]}
]]></acme>

<pubDate>2025-10-21T15:59:00+00:00</pubDate>
</item>
<item>
<title>Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval</title>
<link>https://papers.cool/arxiv/2510.18659</link>
<guid>https://papers.cool/arxiv/2510.18659</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SherlockLLM, a reinforcement‑learning‑based framework that learns to ask binary clarification questions in a dialogue to efficiently narrow the search space for information retrieval. By training an agent without large annotated dialogue datasets, the approach achieves near‑optimal performance on structured tasks and significantly outperforms baselines on challenging unstructured retrieval tasks.<br /><strong>Summary (CN):</strong> 本文提出 SherlockLLM，一种基于强化学习的框架，学习在对话中提出二元澄清问题，以高效缩小信息检索的搜索空间。该方法无需大规模标注对话数据，在结构化任务上接近二分搜索的理论最优，在复杂的非结构化任务上显著超越基线。<br /><strong>Keywords:</strong> interactive retrieval, dialogue-based retrieval, reinforcement learning, question generation, binary search, SherlockLLM<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dong Yun, Marco Schouten, Dim Papadopoulos</div>
User queries in information retrieval are often ambiguous, making it challenging for systems to identify a user's target from a single query. While recent dialogue-based interactive retrieval systems can clarify user intent, they are inefficient as they often lack an explicit strategy to ask the most informative questions. To address this limitation, we propose SherlockLLM, a dialogue-driven retrieval framework that learns an optimal questioning strategy via Reinforcement Learning (RL) and avoids the need for large-scale annotated dialogue data. In our framework, an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space. To validate our approach, we introduce a benchmark with both structured and unstructured tasks. Experimental results show that SherlockLLM is a robust and efficient solution. On the structured tasks, its performance matches strong baselines and approaches the theoretical optimal defined by binary search. On the challenging unstructured task, our agent significantly outperforms these baselines, showcasing its ability to learn a highly effective information-seeking dialogue policy.
<div><strong>Authors:</strong> Dong Yun, Marco Schouten, Dim Papadopoulos</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SherlockLLM, a reinforcement‑learning‑based framework that learns to ask binary clarification questions in a dialogue to efficiently narrow the search space for information retrieval. By training an agent without large annotated dialogue datasets, the approach achieves near‑optimal performance on structured tasks and significantly outperforms baselines on challenging unstructured retrieval tasks.", "summary_cn": "本文提出 SherlockLLM，一种基于强化学习的框架，学习在对话中提出二元澄清问题，以高效缩小信息检索的搜索空间。该方法无需大规模标注对话数据，在结构化任务上接近二分搜索的理论最优，在复杂的非结构化任务上显著超越基线。", "keywords": "interactive retrieval, dialogue-based retrieval, reinforcement learning, question generation, binary search, SherlockLLM", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dong Yun", "Marco Schouten", "Dim Papadopoulos"]}
]]></acme>

<pubDate>2025-10-21T14:10:42+00:00</pubDate>
</item>
<item>
<title>Query Decomposition for RAG: Balancing Exploration-Exploitation</title>
<link>https://papers.cool/arxiv/2510.18633</link>
<guid>https://papers.cool/arxiv/2510.18633</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper formulates query decomposition and document retrieval in retrieval‑augmented generation as an exploration‑exploitation problem and applies multi‑armed bandit algorithms to dynamically select sub‑queries. By estimating relevance from rank information and human judgments, the method improves document‑level precision by 35% and α‑nG by 15% while enhancing long‑form generation quality. Experiments show the effectiveness of various bandit learners for balancing broad coverage and noise reduction.<br /><strong>Summary (CN):</strong> 本文将检索增强生成（RAG）中的查询分解与文档检索建模为探索‑利用问题，并使用多臂老虎机算法动态选择子查询。通过利用排名信息和人工判断估计文档相关性，方法在文档级精度上提升 35%，α‑nDCG 提高 15%，并改善长文本生成效果。实验验证了不同老虎机学习方法在平衡广度覆盖与噪声抑制方面的有效性。<br /><strong>Keywords:</strong> retrieval-augmented generation, query decomposition, bandit learning, exploration-exploitation, document relevance ranking, long-form generation, sub-query selection, α-nDCG<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Roxana Petcu, Kenton Murray, Daniel Khashabi, Evangelos Kanoulas, Maarten de Rijke, Dawn Lawrie, Kevin Duh</div>
Retrieval-augmented generation (RAG) systems address complex user requests by decomposing them into subqueries, retrieving potentially relevant documents for each, and then aggregating them to generate an answer. Efficiently selecting informative documents requires balancing a key trade-off: (i) retrieving broadly enough to capture all the relevant material, and (ii) limiting retrieval to avoid excessive noise and computational cost. We formulate query decomposition and document retrieval in an exploitation-exploration setting, where retrieving one document at a time builds a belief about the utility of a given sub-query and informs the decision to continue exploiting or exploring an alternative. We experiment with a variety of bandit learning methods and demonstrate their effectiveness in dynamically selecting the most informative sub-queries. Our main finding is that estimating document relevance using rank information and human judgments yields a 35% gain in document-level precision, 15% increase in {\alpha}-nDCG, and better performance on the downstream task of long-form generation.
<div><strong>Authors:</strong> Roxana Petcu, Kenton Murray, Daniel Khashabi, Evangelos Kanoulas, Maarten de Rijke, Dawn Lawrie, Kevin Duh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper formulates query decomposition and document retrieval in retrieval‑augmented generation as an exploration‑exploitation problem and applies multi‑armed bandit algorithms to dynamically select sub‑queries. By estimating relevance from rank information and human judgments, the method improves document‑level precision by 35% and α‑nG by 15% while enhancing long‑form generation quality. Experiments show the effectiveness of various bandit learners for balancing broad coverage and noise reduction.", "summary_cn": "本文将检索增强生成（RAG）中的查询分解与文档检索建模为探索‑利用问题，并使用多臂老虎机算法动态选择子查询。通过利用排名信息和人工判断估计文档相关性，方法在文档级精度上提升 35%，α‑nDCG 提高 15%，并改善长文本生成效果。实验验证了不同老虎机学习方法在平衡广度覆盖与噪声抑制方面的有效性。", "keywords": "retrieval-augmented generation, query decomposition, bandit learning, exploration-exploitation, document relevance ranking, long-form generation, sub-query selection, α-nDCG", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Roxana Petcu", "Kenton Murray", "Daniel Khashabi", "Evangelos Kanoulas", "Maarten de Rijke", "Dawn Lawrie", "Kevin Duh"]}
]]></acme>

<pubDate>2025-10-21T13:37:11+00:00</pubDate>
</item>
<item>
<title>Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises</title>
<link>https://papers.cool/arxiv/2510.18631</link>
<guid>https://papers.cool/arxiv/2510.18631</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a notion of expressivity that can compare abstract and structured argumentation frameworks when uncertainty appears in rules and premises. It presents both negative and positive expressivity results, establishing how incomplete abstract argumentation frameworks, their extensions with dependencies, and the structured ASPIC+ framework differ in expressive power.<br /><strong>Summary (CN):</strong> 本文提出了一种可比较抽象与结构化论证框架在规则与前提不确定性下表达能力的概念，并给出了正向与负向的表达力结果，阐明了不完整抽象论证框架、其带依赖的扩展以及结构化的 ASPIC+ 框架在表达能力上的差异。<br /><strong>Keywords:</strong> structured argumentation, uncertainty, expressivity, ASPIC+, abstract argumentation, rule uncertainty, premise uncertainty, comparative expressivity, formal argumentation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Carlo Proietti, Antonio Yuste-Ginel</div>
Modelling qualitative uncertainty in formal argumentation is essential both for practical applications and theoretical understanding. Yet, most of the existing works focus on \textit{abstract} models for arguing with uncertainty. Following a recent trend in the literature, we tackle the open question of studying plausible instantiations of these abstract models. To do so, we ground the uncertainty of arguments in their components, structured within rules and premises. Our main technical contributions are: i) the introduction of a notion of expressivity that can handle abstract and structured formalisms, and ii) the presentation of both negative and positive expressivity results, comparing the expressivity of abstract and structured models of argumentation with uncertainty. These results affect incomplete abstract argumentation frameworks, and their extension with dependencies, on the abstract side, and ASPIC+, on the structured side.
<div><strong>Authors:</strong> Carlo Proietti, Antonio Yuste-Ginel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a notion of expressivity that can compare abstract and structured argumentation frameworks when uncertainty appears in rules and premises. It presents both negative and positive expressivity results, establishing how incomplete abstract argumentation frameworks, their extensions with dependencies, and the structured ASPIC+ framework differ in expressive power.", "summary_cn": "本文提出了一种可比较抽象与结构化论证框架在规则与前提不确定性下表达能力的概念，并给出了正向与负向的表达力结果，阐明了不完整抽象论证框架、其带依赖的扩展以及结构化的 ASPIC+ 框架在表达能力上的差异。", "keywords": "structured argumentation, uncertainty, expressivity, ASPIC+, abstract argumentation, rule uncertainty, premise uncertainty, comparative expressivity, formal argumentation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Carlo Proietti", "Antonio Yuste-Ginel"]}
]]></acme>

<pubDate>2025-10-21T13:36:38+00:00</pubDate>
</item>
<item>
<title>Leveraging Association Rules for Better Predictions and Better Explanations</title>
<link>https://papers.cool/arxiv/2510.18628</link>
<guid>https://papers.cool/arxiv/2510.18628</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a classification framework that combines data mining-derived association rules (including negations) with tree‑based models such as decision trees and random forests. The extracted rules are used to boost predictive accuracy and to generate more general abductive explanations, resulting in smaller explanation sizes in experiments.<br /><strong>Summary (CN):</strong> 本文提出一种将数据挖掘得到的关联规则（包括否定）与基于树的模型（决策树和随机森林）相结合的分类框架。利用这些规则提升预测性能，并生成更一般的反事实解释，使解释规模在实验中得到缩小。<br /><strong>Keywords:</strong> association rules, decision trees, random forests, abductive explanations, interpretable machine learning, predictive performance<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
We present a new approach to classification that combines data and knowledge. In this approach, data mining is used to derive association rules (possibly with negations) from data. Those rules are leveraged to increase the predictive performance of tree-based models (decision trees and random forests) used for a classification task. They are also used to improve the corresponding explanation task through the generation of abductive explanations that are more general than those derivable without taking such rules into account. Experiments show that for the two tree-based models under consideration, benefits can be offered by the approach in terms of predictive performance and in terms of explanation sizes.
<div><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a classification framework that combines data mining-derived association rules (including negations) with tree‑based models such as decision trees and random forests. The extracted rules are used to boost predictive accuracy and to generate more general abductive explanations, resulting in smaller explanation sizes in experiments.", "summary_cn": "本文提出一种将数据挖掘得到的关联规则（包括否定）与基于树的模型（决策树和随机森林）相结合的分类框架。利用这些规则提升预测性能，并生成更一般的反事实解释，使解释规模在实验中得到缩小。", "keywords": "association rules, decision trees, random forests, abductive explanations, interpretable machine learning, predictive performance", "scoring": {"interpretability": 6, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Gilles Audemard", "Sylvie Coste-Marquis", "Pierre Marquis", "Mehdi Sabiri", "Nicolas Szczepanski"]}
]]></acme>

<pubDate>2025-10-21T13:32:02+00:00</pubDate>
</item>
<item>
<title>VAR: Visual Attention Reasoning via Structured Search and Backtracking</title>
<link>https://papers.cool/arxiv/2510.18619</link>
<guid>https://papers.cool/arxiv/2510.18619</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents VAR, a Visual Attention Reasoning framework that transforms grounded multimodal reasoning into a structured search over trajectory space, combining traceable evidence grounding with a backtracking chain-of-thought generation guided by semantic and geometric reward functions. The authors provide theoretical analysis showing high‑probability solution recovery and demonstrate that VAR-7B achieves state‑of‑the‑art performance on hallucination and safety benchmarks, surpassing open‑source and many proprietary models.<br /><strong>Summary (CN):</strong> 本文提出 VAR（Visual Attention Reasoning）框架，将多模态语言模型的基于视觉的推理重新表述为对推理轨迹空间的结构化搜索，包含可追踪的证据定位和带回溯的思考链生成，并通过语义与几何自验证奖励函数进行引导。作者给出理论分析证明该搜索策略能够高概率找到正确解，并展示 VAR‑7B 在幻觉抑制和安全基准上取得领先性能，显著超越现有开源模型并与多家商业系统竞争。<br /><strong>Keywords:</strong> Visual Attention Reasoning, multimodal LLM, hallucination mitigation, structured search, backtracking, chain-of-thought, evidence grounding, safety benchmarks<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li</div>
Multimodal Large Language Models (MLLMs), despite their advances, are hindered by their high hallucination tendency and heavy reliance on brittle, linear reasoning processes, leading to failures in complex tasks. To address these limitations, we introduce Visual Attention Reasoning (VAR), a novel framework that recasts grounded reasoning as a structured search over a reasoning trajectory space. VAR decomposes the reasoning process into two key stages: traceable evidence grounding and search-based chain-of-thought (CoT) generation, which incorporates a backtracking mechanism for self-correction. The search is guided by a multi-faceted reward function with semantic and geometric self-verification components, which penalize outputs that are not faithfully grounded in the visual input. We provide a theoretical analysis for our search strategy, validating its capability to find the correct solution with high probability. Experimental results show that our 7B model, VAR-7B, sets a new state-of-the-art on a comprehensive suite of hallucination and safety benchmarks, significantly outperforming existing open-source models and demonstrating competitive performance against leading proprietary systems.
<div><strong>Authors:</strong> Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents VAR, a Visual Attention Reasoning framework that transforms grounded multimodal reasoning into a structured search over trajectory space, combining traceable evidence grounding with a backtracking chain-of-thought generation guided by semantic and geometric reward functions. The authors provide theoretical analysis showing high‑probability solution recovery and demonstrate that VAR-7B achieves state‑of‑the‑art performance on hallucination and safety benchmarks, surpassing open‑source and many proprietary models.", "summary_cn": "本文提出 VAR（Visual Attention Reasoning）框架，将多模态语言模型的基于视觉的推理重新表述为对推理轨迹空间的结构化搜索，包含可追踪的证据定位和带回溯的思考链生成，并通过语义与几何自验证奖励函数进行引导。作者给出理论分析证明该搜索策略能够高概率找到正确解，并展示 VAR‑7B 在幻觉抑制和安全基准上取得领先性能，显著超越现有开源模型并与多家商业系统竞争。", "keywords": "Visual Attention Reasoning, multimodal LLM, hallucination mitigation, structured search, backtracking, chain-of-thought, evidence grounding, safety benchmarks", "scoring": {"interpretability": 6, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Wei Cai", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Ming Zhu", "Haichuan Tang", "Chi Zhang", "Xuelong Li"]}
]]></acme>

<pubDate>2025-10-21T13:18:44+00:00</pubDate>
</item>
<item>
<title>QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework</title>
<link>https://papers.cool/arxiv/2510.18569</link>
<guid>https://papers.cool/arxiv/2510.18569</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> QuantEvolve introduces an evolutionary framework that combines quality‑diversity optimization with a hypothesis‑driven multi‑agent system to automatically discover quantitative trading strategies aligned with investor preferences. By maintaining a feature‑map of strategy attributes (type, risk, turnover, returns) the system generates a diverse portfolio of strategies that adapt to market regime shifts, outperforming conventional baselines in empirical evaluations. A dataset of the evolved strategies is released for future research.<br /><strong>Summary (CN):</strong> QuantEvolve 提出一种进化框架，结合质量‑多样性优化和假设驱动的多代理系统，实现自动化发现符合投资者偏好的量化交易策略。该系统通过策略属性特征图（类型、风险、换手率、收益）保持策略多样性，能够适应市场状态变化，并在实证评估中优于传统基准。论文还公开了进化得到的策略数据集，以支持后续研究。<br /><strong>Keywords:</strong> evolutionary algorithms, quality-diversity optimization, multi-agent systems, quantitative trading, strategy discovery, financial AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Junhyeog Yun, Hyoun Jun Lee, Insu Jeon</div>
Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.
<div><strong>Authors:</strong> Junhyeog Yun, Hyoun Jun Lee, Insu Jeon</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "QuantEvolve introduces an evolutionary framework that combines quality‑diversity optimization with a hypothesis‑driven multi‑agent system to automatically discover quantitative trading strategies aligned with investor preferences. By maintaining a feature‑map of strategy attributes (type, risk, turnover, returns) the system generates a diverse portfolio of strategies that adapt to market regime shifts, outperforming conventional baselines in empirical evaluations. A dataset of the evolved strategies is released for future research.", "summary_cn": "QuantEvolve 提出一种进化框架，结合质量‑多样性优化和假设驱动的多代理系统，实现自动化发现符合投资者偏好的量化交易策略。该系统通过策略属性特征图（类型、风险、换手率、收益）保持策略多样性，能够适应市场状态变化，并在实证评估中优于传统基准。论文还公开了进化得到的策略数据集，以支持后续研究。", "keywords": "evolutionary algorithms, quality-diversity optimization, multi-agent systems, quantitative trading, strategy discovery, financial AI", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junhyeog Yun", "Hyoun Jun Lee", "Insu Jeon"]}
]]></acme>

<pubDate>2025-10-21T12:22:16+00:00</pubDate>
</item>
<item>
<title>Extracting alignment data in open models</title>
<link>https://papers.cool/arxiv/2510.18554</link>
<guid>https://papers.cool/arxiv/2510.18554</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper shows that large amounts of alignment training data be recovered from post‑trained language models by measuring semantic similarity with high‑quality embedding models rather than exact string matching. The extracted data, originating from supervised fine‑tuning or RL‑HF phases, can be used to retrain a base model and recover much of its original performance, exposing a significant safety risk and prompting discussion of the downstream effects of model distillation.<br /><strong>Summary (CN):</strong> 本文展示了通过高质量嵌入模型测量语义相似度，而非精确字符串匹配，可以从后训练的语言模型中恢复大量对齐训练数据。这些数据来自监督微调（SFT）或强化学习人类反馈（RLHF）阶段，能够用于重新训练基础模型并恢复其大部分原有性能，揭示了一种重要的安全风险，并引发了关于模型蒸馏下游影响的讨论。<br /><strong>Keywords:</strong> alignment data extraction, memorization, embedding similarity, post‑training, safety, model distillation, RLHF, SFT, data leakage<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - alignment<br /><strong>Authors:</strong> Federico Barbero, Xiangming Gu, Christopher A. Choquette-Choo, Chawin Sitawarin, Matthew Jagielski, Itay Yona, Petar Veličković, Ilia Shumailov, Jamie Hayes</div>
In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of $10\times$) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset.
<div><strong>Authors:</strong> Federico Barbero, Xiangming Gu, Christopher A. Choquette-Choo, Chawin Sitawarin, Matthew Jagielski, Itay Yona, Petar Veličković, Ilia Shumailov, Jamie Hayes</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper shows that large amounts of alignment training data be recovered from post‑trained language models by measuring semantic similarity with high‑quality embedding models rather than exact string matching. The extracted data, originating from supervised fine‑tuning or RL‑HF phases, can be used to retrain a base model and recover much of its original performance, exposing a significant safety risk and prompting discussion of the downstream effects of model distillation.", "summary_cn": "本文展示了通过高质量嵌入模型测量语义相似度，而非精确字符串匹配，可以从后训练的语言模型中恢复大量对齐训练数据。这些数据来自监督微调（SFT）或强化学习人类反馈（RLHF）阶段，能够用于重新训练基础模型并恢复其大部分原有性能，揭示了一种重要的安全风险，并引发了关于模型蒸馏下游影响的讨论。", "keywords": "alignment data extraction, memorization, embedding similarity, post‑training, safety, model distillation, RLHF, SFT, data leakage", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "alignment"}, "authors": ["Federico Barbero", "Xiangming Gu", "Christopher A. Choquette-Choo", "Chawin Sitawarin", "Matthew Jagielski", "Itay Yona", "Petar Veličković", "Ilia Shumailov", "Jamie Hayes"]}
]]></acme>

<pubDate>2025-10-21T12:06:00+00:00</pubDate>
</item>
<item>
<title>SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation</title>
<link>https://papers.cool/arxiv/2510.18551</link>
<guid>https://papers.cool/arxiv/2510.18551</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SOCIA-Nabla, an end-to-end agentic framework that treats simulator construction as instance optimization over code within a textual computation graph. Specialized LLM-driven agents act as graph nodes and a workflow manager runs a loss‑driven loop of code synthesis, execution, evaluation, and repair, performing Textual‑Gradient Descent (TGD) while minimizing human involvement. Evaluated on three CPS tasks, the system achieves state‑of‑the‑art accuracy and demonstrates scalable, reproducible simulator generation.<br /><strong>Summary (CN):</strong> 本文提出 SOCIA-Nabla，一个端到端的代理框架，将模拟器构建视为代码的实例优化问题，以文本计算图的形式组织。专用的 LLM 驱动代理作为图节点，工作流管理器执行代码生成→执行→评估→修复的损失驱动循环，实现文本梯度下降 (Textual‑Gradient Descent, TGD)，并将人为干预限制在任务规格确认上。该系统在三个 CPS 任务上实现了最新的整体准确率，展示了可扩展、可复现的模拟器代码生成能力。<br /><strong>Keywords:</strong> textual gradient descent, multi-agent orchestration, simulator generation, LLM agents, code synthesis, loss-driven optimization, CPS tasks<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuncheng Hua, Sion Weatherhead, Mehdi Jafari, Hao Xue, Flora D. Salim</div>
In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-Nabla converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. This work is under review, and we will release the code soon.
<div><strong>Authors:</strong> Yuncheng Hua, Sion Weatherhead, Mehdi Jafari, Hao Xue, Flora D. Salim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SOCIA-Nabla, an end-to-end agentic framework that treats simulator construction as instance optimization over code within a textual computation graph. Specialized LLM-driven agents act as graph nodes and a workflow manager runs a loss‑driven loop of code synthesis, execution, evaluation, and repair, performing Textual‑Gradient Descent (TGD) while minimizing human involvement. Evaluated on three CPS tasks, the system achieves state‑of‑the‑art accuracy and demonstrates scalable, reproducible simulator generation.", "summary_cn": "本文提出 SOCIA-Nabla，一个端到端的代理框架，将模拟器构建视为代码的实例优化问题，以文本计算图的形式组织。专用的 LLM 驱动代理作为图节点，工作流管理器执行代码生成→执行→评估→修复的损失驱动循环，实现文本梯度下降 (Textual‑Gradient Descent, TGD)，并将人为干预限制在任务规格确认上。该系统在三个 CPS 任务上实现了最新的整体准确率，展示了可扩展、可复现的模拟器代码生成能力。", "keywords": "textual gradient descent, multi-agent orchestration, simulator generation, LLM agents, code synthesis, loss-driven optimization, CPS tasks", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuncheng Hua", "Sion Weatherhead", "Mehdi Jafari", "Hao Xue", "Flora D. Salim"]}
]]></acme>

<pubDate>2025-10-21T12:00:00+00:00</pubDate>
</item>
<item>
<title>Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages</title>
<link>https://papers.cool/arxiv/2510.18535</link>
<guid>https://papers.cool/arxiv/2510.18535</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a physics‑guided emulator of the Global Flood Awareness System that combines long‑ and short‑term memory networks with a relaxed water‑balance constraint to maintain physical coherence under data latency and outages. Five model architectures spanning full to limited data availability are evaluated on over 5,000 basins, showing smooth performance degradation and demonstrating operational robustness as a quantifiable property. Transfer experiments across diverse hydroclimatic regimes highlight the limits of generalisation when data are scarce or human‑managed.<br /><strong>Summary (CN):</strong> 本文构建了一个融合长短期记忆网络并加入松弛水量平衡约束的物理引导仿真器，用于在数据延迟和缺失情况下保持物理一致性，模拟全球洪水预警系统（GloFAS）的核心水文过程。通过五种从完整到受限信息的模型架构，在 5000 多个流域上进行评估，展示了性能随信息质量下降的平滑衰减，并将运营鲁棒性定义为可度量属性。跨不同水文气候和管理条件的迁移实验揭示了在数据稀缺或人为影响下的泛化极限。<br /><strong>Keywords:</strong> hydrological forecasting, physics-guided machine learning, operational robustness, data latency, emulator, GloFAS, memory networks<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Sarth Dubey, Subimal Ghosh, Udit Bhatia</div>
Reliable hydrologic and flood forecasting requires models that remain stable when input data are delayed, missing, or inconsistent. However, most advances in rainfall-runoff prediction have been evaluated under ideal data conditions, emphasizing accuracy rather than operational resilience. Here, we develop an operationally ready emulator of the Global Flood Awareness System (GloFAS) that couples long- and short-term memory networks with a relaxed water-balance constraint to preserve physical coherence. Five architectures span a continuum of information availability: from complete historical and forecast forcings to scenarios with data latency and outages, allowing systematic evaluation of robustness. Trained in minimally managed catchments across the United States and tested in more than 5,000 basins, including heavily regulated rivers in India, the emulator reproduces the hydrological core of GloFAS and degrades smoothly as information quality declines. Transfer across contrasting hydroclimatic and management regimes yields reduced yet physically consistent performance, defining the limits of generalization under data scarcity and human influence. The framework establishes operational robustness as a measurable property of hydrological machine learning and advances the design of reliable real-time forecasting systems.
<div><strong>Authors:</strong> Sarth Dubey, Subimal Ghosh, Udit Bhatia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a physics‑guided emulator of the Global Flood Awareness System that combines long‑ and short‑term memory networks with a relaxed water‑balance constraint to maintain physical coherence under data latency and outages. Five model architectures spanning full to limited data availability are evaluated on over 5,000 basins, showing smooth performance degradation and demonstrating operational robustness as a quantifiable property. Transfer experiments across diverse hydroclimatic regimes highlight the limits of generalisation when data are scarce or human‑managed.", "summary_cn": "本文构建了一个融合长短期记忆网络并加入松弛水量平衡约束的物理引导仿真器，用于在数据延迟和缺失情况下保持物理一致性，模拟全球洪水预警系统（GloFAS）的核心水文过程。通过五种从完整到受限信息的模型架构，在 5000 多个流域上进行评估，展示了性能随信息质量下降的平滑衰减，并将运营鲁棒性定义为可度量属性。跨不同水文气候和管理条件的迁移实验揭示了在数据稀缺或人为影响下的泛化极限。", "keywords": "hydrological forecasting, physics-guided machine learning, operational robustness, data latency, emulator, GloFAS, memory networks", "scoring": {"interpretability": 5, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Sarth Dubey", "Subimal Ghosh", "Udit Bhatia"]}
]]></acme>

<pubDate>2025-10-21T11:25:31+00:00</pubDate>
</item>
<item>
<title>Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18526</link>
<guid>https://papers.cool/arxiv/2510.18526</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces COUPLE, a framework that employs a structural causal model and counterfactual reasoning to align large language models with pluralistic human values, explicitly modeling interdependencies and relative priorities among value dimensions. By generating counterfactual outputs that satisfy any desired value objective, COUPLE enhances steerability, offers clearer interpretability of the alignment process, and outperforms existing baselines on two value‑aligned datasets.<br /><strong>Summary (CN):</strong> 本文提出 COUPLE 框架，利用结构因果模型和反事实推理实现对大型语言模型的多元价值对齐，显式建模价值维度之间的相互依赖及相对优先级。通过生成满足任意价值目标的反事实输出，COUPLE 提升了价值可操控性，并提供了对对齐过程的更好可解释性，在两个价值对齐数据集上超过了现有基线。<br /><strong>Keywords:</strong> counterfactual reasoning, pluralistic value alignment, structural causal model, steerability, large language models, interpretability, AI safety, value prioritization<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Hanze Guo, Jing Yao, Xiao Zhou, Xiaoyuan Yi, Xing Xie</div>
As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). In psychological and social value theories such as Schwartz's Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives.
<div><strong>Authors:</strong> Hanze Guo, Jing Yao, Xiao Zhou, Xiaoyuan Yi, Xing Xie</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces COUPLE, a framework that employs a structural causal model and counterfactual reasoning to align large language models with pluralistic human values, explicitly modeling interdependencies and relative priorities among value dimensions. By generating counterfactual outputs that satisfy any desired value objective, COUPLE enhances steerability, offers clearer interpretability of the alignment process, and outperforms existing baselines on two value‑aligned datasets.", "summary_cn": "本文提出 COUPLE 框架，利用结构因果模型和反事实推理实现对大型语言模型的多元价值对齐，显式建模价值维度之间的相互依赖及相对优先级。通过生成满足任意价值目标的反事实输出，COUPLE 提升了价值可操控性，并提供了对对齐过程的更好可解释性，在两个价值对齐数据集上超过了现有基线。", "keywords": "counterfactual reasoning, pluralistic value alignment, structural causal model, steerability, large language models, interpretability, AI safety, value prioritization", "scoring": {"interpretability": 6, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hanze Guo", "Jing Yao", "Xiao Zhou", "Xiaoyuan Yi", "Xing Xie"]}
]]></acme>

<pubDate>2025-10-21T11:12:45+00:00</pubDate>
</item>
<item>
<title>Crucible: Quantifying the Potential of Control Algorithms through LLM Agents</title>
<link>https://papers.cool/arxiv/2510.18491</link>
<guid>https://papers.cool/arxiv/2510.18491</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Crucible, an LLM‑driven agent that simulates multi‑level experts to evaluate the "Tuning Potential" of control algorithms, providing a formal metric for how much performance can be improved by parameter adjustment. Experiments across classic control tasks and complex computer systems, including a real‑world deployment, show that Crucible can quantify the tunable space of various algorithms and guide algorithm design for better performance.<br /><strong>Summary (CN):</strong> 本文提出 Crucible——一种由大型语言模型驱动的代理，通过多层次专家模拟来量化控制算法的“调参潜力”，并给出正式的度量指标。通过在经典控制任务、复杂计算系统以及真实部署中的实验，展示了 Crucible 能系统地评估不同算法的可调空间，并为算法设计和性能提升提供新维度。<br /><strong>Keywords:</strong> control algorithms, tuning potential, LLM agents, expert simulation, algorithm analysis, performance improvement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Lianchen Jia, Chaoyang Li, Qian Houde, Tianchi Huang, Jiangchuan Liu, Lifeng Sun</div>
Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate Crucible's effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that Crucible systematically quantifies the tunable space across different algorithms. Furthermore, Crucible provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.
<div><strong>Authors:</strong> Lianchen Jia, Chaoyang Li, Qian Houde, Tianchi Huang, Jiangchuan Liu, Lifeng Sun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Crucible, an LLM‑driven agent that simulates multi‑level experts to evaluate the \"Tuning Potential\" of control algorithms, providing a formal metric for how much performance can be improved by parameter adjustment. Experiments across classic control tasks and complex computer systems, including a real‑world deployment, show that Crucible can quantify the tunable space of various algorithms and guide algorithm design for better performance.", "summary_cn": "本文提出 Crucible——一种由大型语言模型驱动的代理，通过多层次专家模拟来量化控制算法的“调参潜力”，并给出正式的度量指标。通过在经典控制任务、复杂计算系统以及真实部署中的实验，展示了 Crucible 能系统地评估不同算法的可调空间，并为算法设计和性能提升提供新维度。", "keywords": "control algorithms, tuning potential, LLM agents, expert simulation, algorithm analysis, performance improvement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Lianchen Jia", "Chaoyang Li", "Qian Houde", "Tianchi Huang", "Jiangchuan Liu", "Lifeng Sun"]}
]]></acme>

<pubDate>2025-10-21T10:25:26+00:00</pubDate>
</item>
<item>
<title>AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification</title>
<link>https://papers.cool/arxiv/2510.18488</link>
<guid>https://papers.cool/arxiv/2510.18488</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies significant flaws in the AndroidControl benchmark that underestimate GUI agent performance and introduces AndroidControl-Curated, a purified benchmark that raises state-of-the-art success rates to about 75% on complex tasks. It also presents Magma-R1-3B, a small model trained on a curated dataset that matches the performance of much larger models. The authors release both the benchmark and the model to encourage more accurate evaluation of on-device virtual assistants.<br /><strong>Summary (CN):</strong> 本文指出 AndroidControl 基准存在诸多缺陷，导致对 GUI 代理能力的低估，并通过严格的净化流程提出了改进版 AndroidControl-Cated，使最先进模型在复杂任务上的成功率提升至约 75%。此外，作者推出了在 2.4k 经过筛选的样本上微调的轻量模型 Magma-R1-3B，其性能可媲美参数规模更大的模型。论文同时开放了该基准和模型，以促进对本地虚拟助手更准确的评估。<br /><strong>Keywords:</strong> GUI agents, AndroidControl, benchmark purification, on-device virtual assistants, Magma-R1, dataset curation, model evaluation, LLM, performance metrics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ho Fai Leung, Xiaoyan Xi, Fei Zuo</div>
On-device virtual assistants like Siri and Google Assistant are increasingly pivotal, yet their capabilities are hamstrung by a reliance on rigid, developer-dependent APIs. GUI agents offer a powerful, API-independent alternative, but their adoption is hindered by the perception of poor performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at around 60% on benchmarks like AndroidControl, far from viability for real-world use. Our research reveals that issue lies not only with the models but with the benchmarks themselves. We identified notable shortcomings in AndroidControl, including ambiguities and factual errors, which systematically underrates agent capabilities. To address this critical oversight, we enhanced AndroidControl into AndroidControl-Curated, a refined version of the benchmark improved through a rigorous purification pipeline. On this enhanced benchmark, state-of-the-art models achieve success rates nearing 75% on complex tasks (15% improvement), reflecting that on-device GUI agents are actually closer to practical deployment than previously thought. We introduce our new SOTA model, Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an H20 GPU (approximately $60). Despite being 200 times smaller in parameters, this model delivers performance comparable to Qwen3- VL-235B. We release both AndroidControl-Curated benchmark and Magma-R1 model to the research community, encouraging adoption of this enhanced benchmark to better reflect model capabilities and accelerate the development of robust, on-device virtual assistants.
<div><strong>Authors:</strong> Ho Fai Leung, Xiaoyan Xi, Fei Zuo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies significant flaws in the AndroidControl benchmark that underestimate GUI agent performance and introduces AndroidControl-Curated, a purified benchmark that raises state-of-the-art success rates to about 75% on complex tasks. It also presents Magma-R1-3B, a small model trained on a curated dataset that matches the performance of much larger models. The authors release both the benchmark and the model to encourage more accurate evaluation of on-device virtual assistants.", "summary_cn": "本文指出 AndroidControl 基准存在诸多缺陷，导致对 GUI 代理能力的低估，并通过严格的净化流程提出了改进版 AndroidControl-Cated，使最先进模型在复杂任务上的成功率提升至约 75%。此外，作者推出了在 2.4k 经过筛选的样本上微调的轻量模型 Magma-R1-3B，其性能可媲美参数规模更大的模型。论文同时开放了该基准和模型，以促进对本地虚拟助手更准确的评估。", "keywords": "GUI agents, AndroidControl, benchmark purification, on-device virtual assistants, Magma-R1, dataset curation, model evaluation, LLM, performance metrics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ho Fai Leung", "Xiaoyan Xi", "Fei Zuo"]}
]]></acme>

<pubDate>2025-10-21T10:11:33+00:00</pubDate>
</item>
<item>
<title>StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking</title>
<link>https://papers.cool/arxiv/2510.18483</link>
<guid>https://papers.cool/arxiv/2510.18483</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> StarBench introduces a turn‑based RPG benchmark derived from Honkai: Star Rail that evaluates vision‑language models on two human‑like competencies: mapping raw screenshots to low‑level keyboard‑mouse actions and deciding when to request brief guidance. The benchmark defines eight combat tasks across a direct‑control regime (pure pixel‑to‑action) and a tool‑assisted regime (with detectors and OCR), and includes an ask‑or‑act diagnostic to measure information‑seeking behavior. Baselines show large gaps in perception‑to‑control fidelity and highlight the performance boost from judicious information seeking.<br /><strong>Summary (CN):</strong> StarBench 提出一个基于《崩坏：星轨》的回合制 RPG 基准，评估视觉语言模型的两项类人能力：将原始截图映射为低层键盘‑鼠标操作，以及决定何时请求简短指导。该基准设定了八个战斗任务，分为直接控制（纯像素‑动作）和工具辅助（使用检测器和 OCR）两种模式，并加入 ask‑or‑act 诊断以衡量信息寻求行为。基线结果显示感知到控制的准确性差距显著，并且适度的信息寻求可显著提升成功率。<br /><strong>Keywords:</strong> multimodal decision-making, agentic information seeking, vision-language models, RPG benchmark, action grounding, human-like control tool-assisted control, ask-or-act diagnostic<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Haoran Zhang, Chenhao Zhu, Sicong Guo, Hanzhe Guo, Haiming Li, Donglin Yu</div>
Human players do more than press buttons: they ground what they see on screen into precise keyboard-mouse actions and, when stuck, they seek information before trying again. We ask whether current vision-language models (VLMs) can do the same. Despite encouraging results under simplified control or tool scaffolds, human-like play in a real client - mapping raw screenshots to temporally coherent low-level actions while deciding when to ask for guidance - remains an open challenge. We introduce StarBench, a turn-based RPG benchmark derived from Honkai: Star Rail that targets these two human-like competencies: multimodal decision-making from pixels to actions and agentic information seeking. StarBench standardizes evaluation across eight combat tasks and two regimes with shared tasks and metrics: (i) direct control, where agents receive only screenshots and must emit low-level primitives (click and keypress) with no semantic hints; and (ii) tool-assisted control, where higher-level intents can be mapped to primitives by detectors and OCR outputs provide optional textualized observations to ease UI grounding. To mirror human practice, StarBench also includes an ask-or-act diagnostic that measures whether and when agents choose to request brief guidance before proceeding, and how that choice affects subsequent performance. We report reference baselines for contemporary VLMs and a human reference. Results expose sizable gaps in perception-to-control fidelity in the direct regime, while showing that judicious information seeking correlates with improved success, establishing StarBench as a reproducible yardstick for agentic information seeking and multimodal decision-making in real-client play.
<div><strong>Authors:</strong> Haoran Zhang, Chenhao Zhu, Sicong Guo, Hanzhe Guo, Haiming Li, Donglin Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "StarBench introduces a turn‑based RPG benchmark derived from Honkai: Star Rail that evaluates vision‑language models on two human‑like competencies: mapping raw screenshots to low‑level keyboard‑mouse actions and deciding when to request brief guidance. The benchmark defines eight combat tasks across a direct‑control regime (pure pixel‑to‑action) and a tool‑assisted regime (with detectors and OCR), and includes an ask‑or‑act diagnostic to measure information‑seeking behavior. Baselines show large gaps in perception‑to‑control fidelity and highlight the performance boost from judicious information seeking.", "summary_cn": "StarBench 提出一个基于《崩坏：星轨》的回合制 RPG 基准，评估视觉语言模型的两项类人能力：将原始截图映射为低层键盘‑鼠标操作，以及决定何时请求简短指导。该基准设定了八个战斗任务，分为直接控制（纯像素‑动作）和工具辅助（使用检测器和 OCR）两种模式，并加入 ask‑or‑act 诊断以衡量信息寻求行为。基线结果显示感知到控制的准确性差距显著，并且适度的信息寻求可显著提升成功率。", "keywords": "multimodal decision-making, agentic information seeking, vision-language models, RPG benchmark, action grounding, human-like control tool-assisted control, ask-or-act diagnostic", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Haoran Zhang", "Chenhao Zhu", "Sicong Guo", "Hanzhe Guo", "Haiming Li", "Donglin Yu"]}
]]></acme>

<pubDate>2025-10-21T10:02:59+00:00</pubDate>
</item>
<item>
<title>LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources</title>
<link>https://papers.cool/arxiv/2510.18477</link>
<guid>https://papers.cool/arxiv/2510.18477</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LAFA is the first system that combines LLM-agent-driven data analytics with federated analytics, enabling privacy-preserving computation over decentralized data sources from natural language queries. It uses a hierarchical multi‑agent architecture where a coarse planner decomposes queries, a fine planner maps sub‑queries to DAGs of FA operations, and an optimizer merges and rewrites these DAGs to reduce redundant computation and communication. Experiments show higher plan success rates and significant reductions in resource‑intensive FA operations compared with baseline prompting approaches.<br /><strong>Summary (CN):</strong> LAFA 是首个将大语言模型（LLM）代理驱动的数据分析与联邦分析相结合的系统，实现了对分散数据源的隐私保护计算，并支持自然语言查询。系统采用层次化多代理架构：粗粒度规划器将复杂查询拆解为子查询，细粒度规划器将每个子查询映射为联邦分析操作的有向无环图（DAG），优化器则对多个 DAG 进行重写和合并，去除冗余操作并降低计算与通信开销。实验表明，与基线提示策略相比，LAFA 能显著提升执行计划成功率并大幅减少资源密集型的联邦分析操作。<br /><strong>Keywords:</strong> federated analytics, large language model agents, natural language query, privacy-preserving computation, multi-agent planning, DAG optimization, distributed data<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Haichao Ji, Zibo Wang, Yifei Zhu, Meng han, Dan Wang, Zhu Han</div>
Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.
<div><strong>Authors:</strong> Haichao Ji, Zibo Wang, Yifei Zhu, Meng han, Dan Wang, Zhu Han</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LAFA is the first system that combines LLM-agent-driven data analytics with federated analytics, enabling privacy-preserving computation over decentralized data sources from natural language queries. It uses a hierarchical multi‑agent architecture where a coarse planner decomposes queries, a fine planner maps sub‑queries to DAGs of FA operations, and an optimizer merges and rewrites these DAGs to reduce redundant computation and communication. Experiments show higher plan success rates and significant reductions in resource‑intensive FA operations compared with baseline prompting approaches.", "summary_cn": "LAFA 是首个将大语言模型（LLM）代理驱动的数据分析与联邦分析相结合的系统，实现了对分散数据源的隐私保护计算，并支持自然语言查询。系统采用层次化多代理架构：粗粒度规划器将复杂查询拆解为子查询，细粒度规划器将每个子查询映射为联邦分析操作的有向无环图（DAG），优化器则对多个 DAG 进行重写和合并，去除冗余操作并降低计算与通信开销。实验表明，与基线提示策略相比，LAFA 能显著提升执行计划成功率并大幅减少资源密集型的联邦分析操作。", "keywords": "federated analytics, large language model agents, natural language query, privacy-preserving computation, multi-agent planning, DAG optimization, distributed data", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Haichao Ji", "Zibo Wang", "Yifei Zhu", "Meng han", "Dan Wang", "Zhu Han"]}
]]></acme>

<pubDate>2025-10-21T09:56:25+00:00</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents</title>
<link>https://papers.cool/arxiv/2510.18476</link>
<guid>https://papers.cool/arxiv/2510.18476</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a probabilistic intent modeling framework for LLM agents engaged in multi‑turn social dialogue, maintaining and updating a belief distribution over a partner's latent intentions to guide the policy. Experiments in the SOTOPIA environment demonstrate consistent performance gains over a strong baseline and approach oracle-level results, suggesting that intent modeling can enhance socially intelligent behavior in LLM agents.<br /><strong>Summary (CN):</strong> 本文提出了一种用于大型语言模型（LLM）代理的概率意图建模框架，在多轮社交对话中维护并动态更新对话伙伴潜在意图的信念分布，以此为策略提供上下文支持。SOTOPIA 环境的实验显示，该框架相较于强基线模型实现了稳定的性能提升，并接近直接观察意图的oracle代理，表明意图建模有助于提升 LLM 代理的社交智能。<br /><strong>Keywords:</strong> probabilistic intent modeling, large language model agents, social dialogue, belief distribution, SOTOPIA, multi-turn conversation, uncertainty, adaptive policy<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.
<div><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a probabilistic intent modeling framework for LLM agents engaged in multi‑turn social dialogue, maintaining and updating a belief distribution over a partner's latent intentions to guide the policy. Experiments in the SOTOPIA environment demonstrate consistent performance gains over a strong baseline and approach oracle-level results, suggesting that intent modeling can enhance socially intelligent behavior in LLM agents.", "summary_cn": "本文提出了一种用于大型语言模型（LLM）代理的概率意图建模框架，在多轮社交对话中维护并动态更新对话伙伴潜在意图的信念分布，以此为策略提供上下文支持。SOTOPIA 环境的实验显示，该框架相较于强基线模型实现了稳定的性能提升，并接近直接观察意图的oracle代理，表明意图建模有助于提升 LLM 代理的社交智能。", "keywords": "probabilistic intent modeling, large language model agents, social dialogue, belief distribution, SOTOPIA, multi-turn conversation, uncertainty, adaptive policy", "scoring": {"interpretability": 4, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"]}
]]></acme>

<pubDate>2025-10-21T09:54:44+00:00</pubDate>
</item>
<item>
<title>CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs</title>
<link>https://papers.cool/arxiv/2510.18470</link>
<guid>https://papers.cool/arxiv/2510.18470</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CircuitSeer proposes a data selection method for mathematical reasoning tasks that leverages internal reasoning circuits—sparse subsets of attention heads consistently activated during complex reasoning—in large language models. By measuring how examples influence these circuits, the approach quantifies reasoning complexity and selects high-quality subsets, achieving notable performance gains while using far less data. Experiments across multiple models and datasets demonstrate its effectiveness compared to existing heuristics.<br /><strong>Summary (CN):</strong> CircuitSeer 提出一种基于大型语言模型内部推理电路（在复杂推理时激活的稀疏注意力头子集）的数据筛选方法。通过衡量示例对这些电路的影响来量化推理复杂度，从而选取高质量子集，在使用更少数据的情况下显著提升性能。对多模型和多数据集的实验表明该方法优于现有的外部启发式筛选手段。<br /><strong>Keywords:</strong> circuit probing, attention heads, data selection, mathematical reasoning, LLM, interpretability, efficient fine-tuning<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Shaobo Wang, Yongliang Miao, Yuancheng Liu, and Qianli Ma, Ning Liao, Linfeng Zhang</div>
Large language models (LLMs) have demonstrated impressive reasoning capabilities, but scaling their performance often relies on massive reasoning datasets that are computationally expensive to train on. Existing data selection methods aim to curate smaller, high-quality subsets but often rely on costly external models or opaque heuristics. In this work, we shift the focus from external heuristics to the model's internal mechanisms. We find that complex reasoning tasks consistently activate a sparse, specialized subset of attention heads, forming core reasoning circuits. Building on this insight, we propose CircuitSeer, a novel data selection method that quantifies the reasoning complexity of data by measuring its influence on these crucial circuits. Extensive experiments on 4 models and 9 datasets demonstrate CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of data selected by our method achieves a 1.4-point gain in average Pass@1 over training on the full dataset, highlighting its efficiency and effectiveness.
<div><strong>Authors:</strong> Shaobo Wang, Yongliang Miao, Yuancheng Liu, and Qianli Ma, Ning Liao, Linfeng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CircuitSeer proposes a data selection method for mathematical reasoning tasks that leverages internal reasoning circuits—sparse subsets of attention heads consistently activated during complex reasoning—in large language models. By measuring how examples influence these circuits, the approach quantifies reasoning complexity and selects high-quality subsets, achieving notable performance gains while using far less data. Experiments across multiple models and datasets demonstrate its effectiveness compared to existing heuristics.", "summary_cn": "CircuitSeer 提出一种基于大型语言模型内部推理电路（在复杂推理时激活的稀疏注意力头子集）的数据筛选方法。通过衡量示例对这些电路的影响来量化推理复杂度，从而选取高质量子集，在使用更少数据的情况下显著提升性能。对多模型和多数据集的实验表明该方法优于现有的外部启发式筛选手段。", "keywords": "circuit probing, attention heads, data selection, mathematical reasoning, LLM, interpretability, efficient fine-tuning", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Shaobo Wang", "Yongliang Miao", "Yuancheng Liu", "and Qianli Ma", "Ning Liao", "Linfeng Zhang"]}
]]></acme>

<pubDate>2025-10-21T09:47:00+00:00</pubDate>
</item>
<item>
<title>PlanU: Large Language Model Decision Making through Planning under Uncertainty</title>
<link>https://papers.cool/arxiv/2510.18442</link>
<guid>https://papers.cool/arxiv/2510.18442</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> PlanU introduces a Monte Carlo Tree Search framework that models node returns as quantile distributions to capture both LLM and environmental uncertainty during multi-step decision making. It adds an Upper Confidence Bounds with Curiosity (UCC) score to balance exploration and exploitation, enabling LLMs to plan more reliably in stochastic environments. Experiments show that PlanU improves performance on LLM-based decision tasks under uncertainty compared to prior methods.<br /><strong>Summary (CN):</strong> PlanU 采用 Monte Carlo Tree Search，将每个节点的回报建模为分位数分布，从而同时捕获 LLM 的不确定性和环境的随机性，以实现多步决策。该方法引入了“好奇心上置信界”（UCC）评分，在树搜索中平衡探索与利用，使 LLM 在随机环境中的规划更为可靠。实验结果表明，PlanU 在处理不确定性决策任务时相较于现有方法有显著提升。<br /><strong>Keywords:</strong> large language model, decision making, planning under uncertainty, Monte Carlo Tree Search, quantile distribution, Upper Confidence Bounds with Curiosity, stochastic environments, LLM uncertainty, robustness, AI planning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ziwei Deng, Mian Deng, Chenjing Liang, Zeming Gao, Chennan Ma, Chenxing Lin, Haipeng Zhang, Songzhu Mei, Cheng Wang, Siqi Shen</div>
Large Language Models (LLMs) are increasingly being explored across a range of decision-making tasks. However, LLMs sometimes struggle with decision-making tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for decision-making is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step decision-making tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based decision-making tasks under uncertainty.
<div><strong>Authors:</strong> Ziwei Deng, Mian Deng, Chenjing Liang, Zeming Gao, Chennan Ma, Chenxing Lin, Haipeng Zhang, Songzhu Mei, Cheng Wang, Siqi Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "PlanU introduces a Monte Carlo Tree Search framework that models node returns as quantile distributions to capture both LLM and environmental uncertainty during multi-step decision making. It adds an Upper Confidence Bounds with Curiosity (UCC) score to balance exploration and exploitation, enabling LLMs to plan more reliably in stochastic environments. Experiments show that PlanU improves performance on LLM-based decision tasks under uncertainty compared to prior methods.", "summary_cn": "PlanU 采用 Monte Carlo Tree Search，将每个节点的回报建模为分位数分布，从而同时捕获 LLM 的不确定性和环境的随机性，以实现多步决策。该方法引入了“好奇心上置信界”（UCC）评分，在树搜索中平衡探索与利用，使 LLM 在随机环境中的规划更为可靠。实验结果表明，PlanU 在处理不确定性决策任务时相较于现有方法有显著提升。", "keywords": "large language model, decision making, planning under uncertainty, Monte Carlo Tree Search, quantile distribution, Upper Confidence Bounds with Curiosity, stochastic environments, LLM uncertainty, robustness, AI planning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"]}
]]></acme>

<pubDate>2025-10-21T09:17:50+00:00</pubDate>
</item>
<item>
<title>AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</title>
<link>https://papers.cool/arxiv/2510.18428</link>
<guid>https://papers.cool/arxiv/2510.18428</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AlphaOPT introduces a self‑improving experience library that enables large language models to learn to formulate mathematical optimization programs from limited demonstrations and solver feedback, without requiring annotated reasoning traces or model weight updates. The system iteratively refines stored insights through a two‑phase cycle of library learning and library evolution, making the knowledge explicit and inspectable, and demonstrates steady performance gains on the OptiBench benchmark.<br /><strong>Summary (CN):</strong> AlphaOPT 提出一个自我改进的经验库，使大型语言模型能够仅通过有限示例和求解器反馈学习数学优化程序的形式化，无需标注的推理过程或模型权重更新。系统通过库学习和库演化两个阶段循环迭代提炼结构化洞察，使知识显式化并便于人工检查，并在 OptiBench 基准上展示了持续的性能提升。<br /><strong>Keywords:</strong> LLM optimization programming, experience library, solver feedback, self-improving, continuous learning, OptiBench, prompt engineering, explicit knowledge, library evolution<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Minwei Kong, Ao Qu, Xiaotong Guo, Wenbin Ouyang, Chonghe Jiang, Han Zheng, Yining Ma, Dingyi Zhuang, Yuhan Tang, Junyi Li, Hai Wang, Cathy Wu, Jinhua Zhao</div>
Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.
<div><strong>Authors:</strong> Minwei Kong, Ao Qu, Xiaotong Guo, Wenbin Ouyang, Chonghe Jiang, Han Zheng, Yining Ma, Dingyi Zhuang, Yuhan Tang, Junyi Li, Hai Wang, Cathy Wu, Jinhua Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AlphaOPT introduces a self‑improving experience library that enables large language models to learn to formulate mathematical optimization programs from limited demonstrations and solver feedback, without requiring annotated reasoning traces or model weight updates. The system iteratively refines stored insights through a two‑phase cycle of library learning and library evolution, making the knowledge explicit and inspectable, and demonstrates steady performance gains on the OptiBench benchmark.", "summary_cn": "AlphaOPT 提出一个自我改进的经验库，使大型语言模型能够仅通过有限示例和求解器反馈学习数学优化程序的形式化，无需标注的推理过程或模型权重更新。系统通过库学习和库演化两个阶段循环迭代提炼结构化洞察，使知识显式化并便于人工检查，并在 OptiBench 基准上展示了持续的性能提升。", "keywords": "LLM optimization programming, experience library, solver feedback, self-improving, continuous learning, OptiBench, prompt engineering, explicit knowledge, library evolution", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Minwei Kong", "Ao Qu", "Xiaotong Guo", "Wenbin Ouyang", "Chonghe Jiang", "Han Zheng", "Yining Ma", "Dingyi Zhuang", "Yuhan Tang", "Junyi Li", "Hai Wang", "Cathy Wu", "Jinhua Zhao"]}
]]></acme>

<pubDate>2025-10-21T09:03:26+00:00</pubDate>
</item>
<item>
<title>Automated urban waterlogging assessment and early warning through a mixture of foundation models</title>
<link>https://papers.cool/arxiv/2510.18425</link>
<guid>https://papers.cool/arxiv/2510.18425</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes UWAssess, a framework that leverages foundation models to automatically detect urban waterlogged areas from surveillance images and generate structured assessment reports describing extent, depth, risk, and impact. By employing a semi‑supervised fine‑tuning approach and chain‑of‑thought prompting, the system achieves strong perception performance despite limited labeled data, enabling timely early‑warning and supporting disaster response.<br /><strong>Summary (CN):</strong> 本文提出了 UWAssess 框架，利用基础模型自动从监控图像中识别城市积水区域并生成结构化评估报告，报告内容包括积水范围、深度、风险和影响。通过半监督微调和链式思考（CoT）提示策略，系统在标注数据稀缺的情况下仍实现了显著的感知性能提升，从而实现及时预警并支援灾害响应。<br /><strong>Keywords:</strong> foundation models, waterlogging detection, urban flood monitoring, semi-supervised fine-tuning, chain-of-thought prompting, early warning, disaster response<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Chenxu Zhang, Fuxiang Huang, Lei Zhang</div>
With climate change intensifying, urban waterlogging poses an increasingly severe threat to global public safety and infrastructure. However, existing monitoring approaches rely heavily on manual reporting and fail to provide timely and comprehensive assessments. In this study, we present Urban Waterlogging Assessment (UWAssess), a foundation model-driven framework that automatically identifies waterlogged areas in surveillance images and generates structured assessment reports. To address the scarcity of labeled data, we design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT) prompting strategy to unleash the potential of the foundation model for data-scarce downstream tasks. Evaluations on challenging visual benchmarks demonstrate substantial improvements in perception performance. GPT-based evaluations confirm the ability of UWAssess to generate reliable textual reports that accurately describe waterlogging extent, depth, risk and impact. This dual capability enables a shift of waterlogging monitoring from perception to generation, while the collaborative framework of multiple foundation models lays the groundwork for intelligent and scalable systems, supporting urban management, disaster response and climate resilience.
<div><strong>Authors:</strong> Chenxu Zhang, Fuxiang Huang, Lei Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes UWAssess, a framework that leverages foundation models to automatically detect urban waterlogged areas from surveillance images and generate structured assessment reports describing extent, depth, risk, and impact. By employing a semi‑supervised fine‑tuning approach and chain‑of‑thought prompting, the system achieves strong perception performance despite limited labeled data, enabling timely early‑warning and supporting disaster response.", "summary_cn": "本文提出了 UWAssess 框架，利用基础模型自动从监控图像中识别城市积水区域并生成结构化评估报告，报告内容包括积水范围、深度、风险和影响。通过半监督微调和链式思考（CoT）提示策略，系统在标注数据稀缺的情况下仍实现了显著的感知性能提升，从而实现及时预警并支援灾害响应。", "keywords": "foundation models, waterlogging detection, urban flood monitoring, semi-supervised fine-tuning, chain-of-thought prompting, early warning, disaster response", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Chenxu Zhang", "Fuxiang Huang", "Lei Zhang"]}
]]></acme>

<pubDate>2025-10-21T08:59:30+00:00</pubDate>
</item>
<item>
<title>Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents</title>
<link>https://papers.cool/arxiv/2510.18424</link>
<guid>https://papers.cool/arxiv/2510.18424</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Med-VRAgent is an agent framework that enhances medical visual reasoning in large visual language models by integrating visual guidance, a self‑reward mechanism, and Monte Carlo Tree Search. The generated trajectories are used as feedback to fine‑tune the VLMs via proximal policy optimization, leading to improved accuracy and reduced hallucinations on several medical VQA benchmarks.<br /><strong>Summary (CN):</strong> Med-VRAgent 是一个代理框架，通过融合视觉引导、自我奖励机制以及 Monte Carlo Tree Search（MCTS），提升医学视觉语言模型的视觉推理能力。框架利用产生的轨迹作为反馈，使用近端策略优化（PPO）进行微调，从而在多个医学 VQA 基准上实现更高的准确率并降低幻觉现象。<br /><strong>Keywords:</strong> medical visual language model, visual reasoning, hallucination mitigation, Monte Carlo Tree Search, PPO fine-tuning, self-reward, visual guidance, medical VQA<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Guangfu Guo, Xiaoqian Lu, Yue Feng</div>
Visual Language Models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-VRAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches.
<div><strong>Authors:</strong> Guangfu Guo, Xiaoqian Lu, Yue Feng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Med-VRAgent is an agent framework that enhances medical visual reasoning in large visual language models by integrating visual guidance, a self‑reward mechanism, and Monte Carlo Tree Search. The generated trajectories are used as feedback to fine‑tune the VLMs via proximal policy optimization, leading to improved accuracy and reduced hallucinations on several medical VQA benchmarks.", "summary_cn": "Med-VRAgent 是一个代理框架，通过融合视觉引导、自我奖励机制以及 Monte Carlo Tree Search（MCTS），提升医学视觉语言模型的视觉推理能力。框架利用产生的轨迹作为反馈，使用近端策略优化（PPO）进行微调，从而在多个医学 VQA 基准上实现更高的准确率并降低幻觉现象。", "keywords": "medical visual language model, visual reasoning, hallucination mitigation, Monte Carlo Tree Search, PPO fine-tuning, self-reward, visual guidance, medical VQA", "scoring": {"interpretability": 4, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Guangfu Guo", "Xiaoqian Lu", "Yue Feng"]}
]]></acme>

<pubDate>2025-10-21T08:56:23+00:00</pubDate>
</item>
<item>
<title>Deep Learning-Based Control Optimization for Glass Bottle Forming</title>
<link>https://papers.cool/arxiv/2510.18412</link>
<guid>https://papers.cool/arxiv/2510.18412</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a deep learning‑based control algorithm for glass bottle forming machines, using real‑world operational data to predict the effects of parameter changes and an inversion mechanism to compute optimal settings. Experiments on historical datasets from multiple production lines demonstrate improved process stability, reduced waste, and higher product consistency.<br /><strong>Summary (CN):</strong> 本文提出一种基于深度学习的玻璃瓶成形机控制算法，利用真实生产数据预测参数变动的影响，并通过逆向机制计算最优机器设置。对多个生产线的历史数据实验显示，该方法可提升工艺稳定性、降低废品率并改善产品一致性。<br /><strong>Keywords:</strong> deep learning, process control, glass bottle forming, inverse optimization, industrial AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Mattia Pujatti, Andrea Di Luca, Nicola Peghini, Federico Monegaglia, Marco Cristoforetti</div>
In glass bottle manufacturing, precise control of forming machines is critical for ensuring quality and minimizing defects. This study presents a deep learning-based control algorithm designed to optimize the forming process in real production environments. Using real operational data from active manufacturing plants, our neural network predicts the effects of parameter changes based on the current production setup. Through a specifically designed inversion mechanism, the algorithm identifies the optimal machine settings required to achieve the desired glass gob characteristics. Experimental results on historical datasets from multiple production lines show that the proposed method yields promising outcomes, suggesting potential for enhanced process stability, reduced waste, and improved product consistency. These results highlight the potential of deep learning to process control in glass manufacturing.
<div><strong>Authors:</strong> Mattia Pujatti, Andrea Di Luca, Nicola Peghini, Federico Monegaglia, Marco Cristoforetti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a deep learning‑based control algorithm for glass bottle forming machines, using real‑world operational data to predict the effects of parameter changes and an inversion mechanism to compute optimal settings. Experiments on historical datasets from multiple production lines demonstrate improved process stability, reduced waste, and higher product consistency.", "summary_cn": "本文提出一种基于深度学习的玻璃瓶成形机控制算法，利用真实生产数据预测参数变动的影响，并通过逆向机制计算最优机器设置。对多个生产线的历史数据实验显示，该方法可提升工艺稳定性、降低废品率并改善产品一致性。", "keywords": "deep learning, process control, glass bottle forming, inverse optimization, industrial AI", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Mattia Pujatti", "Andrea Di Luca", "Nicola Peghini", "Federico Monegaglia", "Marco Cristoforetti"]}
]]></acme>

<pubDate>2025-10-21T08:42:35+00:00</pubDate>
</item>
<item>
<title>Heterogeneous Adversarial Play in Interactive Environments</title>
<link>https://papers.cool/arxiv/2510.18407</link>
<guid>https://papers.cool/arxiv/2510.18407</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Heterogeneous Adversarial Play (HAP), a curriculum learning framework that models teacher‑student interactions as a minimax game where a task‑generating instructor and a problem‑solving learner co‑evolve. Unlike prior ACL methods that rely on static or unidirectional task selection, HAP continuously adapts task difficulty based on the learner's performance, achieving results comparable to state‑of‑the‑art baselines across multiple multi‑task domains and improving learning efficiency for both artificial agents and human participants.<br /><strong>Summary (CN):</strong> 本文提出了异构对抗式游戏（HAP）框架，将教师‑学生交互形式化为一个极小化极大化的任务生成与求解过程，使任务生成者能够根据学习者实时表现动态调节难度。相较于传统的静态或单向任务选择方法，HAP 在多个多任务环境中实现了与最先进基线相当的性能，并提升了人工智能代理和人类受试者的学习效率。<br /><strong>Keywords:</strong> heterogeneous adversarial play, automatic curriculum learning, asymmetric self-play, teacher-student minimax, multi-task reinforcement learning, adaptive curricula<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Manjie Xu, Xinyi Yang, Jiayu Zhan, Wei Liang, Chi Zhang, Yixin Zhu</div>
Self-play constitutes a fundamental paradigm for autonomous skill acquisition, whereby agents iteratively enhance their capabilities through self-directed environmental exploration. Conventional self-play frameworks exploit agent symmetry within zero-sum competitive settings, yet this approach proves inadequate for open-ended learning scenarios characterized by inherent asymmetry. Human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners' developmental trajectories. The principal challenge resides in operationalizing these asymmetric, adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies. Here we present Heterogeneous Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework that formalizes teacher-student interactions as a minimax optimization wherein task-generating instructor and problem-solving learner co-evolve through adversarial dynamics. In contrast to prevailing ACL methodologies that employ static curricula or unidirectional task selection mechanisms, HAP establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to real-time learner performance metrics. Experimental validation across multi-task learning domains demonstrates that our framework achieves performance parity with SOTA baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects.
<div><strong>Authors:</strong> Manjie Xu, Xinyi Yang, Jiayu Zhan, Wei Liang, Chi Zhang, Yixin Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Heterogeneous Adversarial Play (HAP), a curriculum learning framework that models teacher‑student interactions as a minimax game where a task‑generating instructor and a problem‑solving learner co‑evolve. Unlike prior ACL methods that rely on static or unidirectional task selection, HAP continuously adapts task difficulty based on the learner's performance, achieving results comparable to state‑of‑the‑art baselines across multiple multi‑task domains and improving learning efficiency for both artificial agents and human participants.", "summary_cn": "本文提出了异构对抗式游戏（HAP）框架，将教师‑学生交互形式化为一个极小化极大化的任务生成与求解过程，使任务生成者能够根据学习者实时表现动态调节难度。相较于传统的静态或单向任务选择方法，HAP 在多个多任务环境中实现了与最先进基线相当的性能，并提升了人工智能代理和人类受试者的学习效率。", "keywords": "heterogeneous adversarial play, automatic curriculum learning, asymmetric self-play, teacher-student minimax, multi-task reinforcement learning, adaptive curricula", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Manjie Xu", "Xinyi Yang", "Jiayu Zhan", "Wei Liang", "Chi Zhang", "Yixin Zhu"]}
]]></acme>

<pubDate>2025-10-21T08:29:59+00:00</pubDate>
</item>
<item>
<title>Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games</title>
<link>https://papers.cool/arxiv/2510.18395</link>
<guid>https://papers.cool/arxiv/2510.18395</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Memory-Augmented State Machine Prompting (MASMP), a framework that combines a natural-language driven finite state machine architecture with a lightweight memory module to guide LLM agents in real-time strategy games. By structuring actions through state-machine prompts and preserving strategic variables across decision cycles, MASMP reduces hallucinations and fragmented decision-making, achieving a 60% win rate against StarCraft II's hardest built‑in AI. The approach aims to retain the semantic strengths of LLMs while providing FSM‑like interpretability and reliability.<br /><strong>Summary (CN):</strong> 本文提出记忆增强状态机提示（MASMP）框架，将自然语言驱动的有限状态机结构与轻量记忆模块相结合，引导 LLM 在实时策略游戏中的策。该方法通过状态机提示统一动作并在决策循环中保持策略变量，显著降低幻觉和碎片化决策，在《星际争霸 II》最难 AI（Lv7）上实现 60% 的胜率，兼顾语义理解与 FSM 式可解释性和可靠性。<br /><strong>Keywords:</strong> LLM agents, real-time strategy, state machine prompting, memory augmentation, interpretability, symbolic AI, StarCraft II, decision coherence<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Runnan Qi, Yanan Ni, Lumin Jiang, Zongyuan Li, Kuihua Huang, Xian Guo</div>
This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the "Knowing-Doing Gap" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.
<div><strong>Authors:</strong> Runnan Qi, Yanan Ni, Lumin Jiang, Zongyuan Li, Kuihua Huang, Xian Guo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Memory-Augmented State Machine Prompting (MASMP), a framework that combines a natural-language driven finite state machine architecture with a lightweight memory module to guide LLM agents in real-time strategy games. By structuring actions through state-machine prompts and preserving strategic variables across decision cycles, MASMP reduces hallucinations and fragmented decision-making, achieving a 60% win rate against StarCraft II's hardest built‑in AI. The approach aims to retain the semantic strengths of LLMs while providing FSM‑like interpretability and reliability.", "summary_cn": "本文提出记忆增强状态机提示（MASMP）框架，将自然语言驱动的有限状态机结构与轻量记忆模块相结合，引导 LLM 在实时策略游戏中的策。该方法通过状态机提示统一动作并在决策循环中保持策略变量，显著降低幻觉和碎片化决策，在《星际争霸 II》最难 AI（Lv7）上实现 60% 的胜率，兼顾语义理解与 FSM 式可解释性和可靠性。", "keywords": "LLM agents, real-time strategy, state machine prompting, memory augmentation, interpretability, symbolic AI, StarCraft II, decision coherence", "scoring": {"interpretability": 7, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Runnan Qi", "Yanan Ni", "Lumin Jiang", "Zongyuan Li", "Kuihua Huang", "Xian Guo"]}
]]></acme>

<pubDate>2025-10-21T08:15:04+00:00</pubDate>
</item>
<item>
<title>ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection</title>
<link>https://papers.cool/arxiv/2510.18342</link>
<guid>https://papers.cool/arxiv/2510.18342</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ShortcutBreaker, a unified feature-reconstruction framework for multi-class unsupervised anomaly detection that tackles identity shortcuts by employing a low-rank noisy bottleneck to force a low-rank latent representation and a global perturbation attention mechanism within Vision Transformers to disrupt trivial copying. The method achieves state-of-the-art image-level AUROC scores across several industrial and medical benchmark datasets, demonstrating the effectiveness of the proposed architectural innovations.<br /><strong>Summary (CN):</strong> 本文提出 ShortcutBreaker，一种面向多类无监督异常检测的统一特征重建框架，通过低秩噪声瓶颈（low-rank noisy bottleneck）将高维特征投射到低秩潜空间以防止身份捷径，并利用 Vision Transformer 的全局扰动注意力（global perturbation attention）阻止解码器的直接复制。实验在多个工业和医学基准上实现了领先的图像级 AUROC，验证了该架构的有效性。<br /><strong>Keywords:</strong> unsupervised anomaly detection, multi-class, low-rank bottleneck, global perturbation attention, Vision Transformer, reconstruction shortcut, MUAD, industrial defect detection, medical anomaly detection<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Peng Tang, Xiaoxiao Yan, Xiaobin Hu, Yuning Cui, Donghao Luo, Jiangning Zhang, Pengcheng Xu, Jinlong Peng, Qingdong He, Feiyue Huang, Song Xue, Tobias Lasser</div>
Multi-class unsupervised anomaly detection (MUAD) has garnered growing research interest, as it seeks to develop a unified model for anomaly detection across multiple classes, i.e., eliminating the need to train separate models for distinct objects and thereby saving substantial computational resources. Under the MUAD setting, while advanced Transformer-based architectures have brought significant performance improvements, identity shortcuts persist: they directly copy inputs to outputs, narrowing the gap in reconstruction errors between normal and abnormal cases, and thereby making the two harder to distinguish. Therefore, we propose ShortcutBreaker, a novel unified feature-reconstruction framework for MUAD tasks, featuring two key innovations to address the issue of shortcuts. First, drawing on matrix rank inequality, we design a low-rank noisy bottleneck (LRNB) to project highdimensional features into a low-rank latent space, and theoretically demonstrate its capacity to prevent trivial identity reproduction. Second, leveraging ViTs global modeling capability instead of merely focusing on local features, we incorporate a global perturbation attention to prevent information shortcuts in the decoders. Extensive experiments are performed on four widely used anomaly detection benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD) and one medical dataset (Universal Medical). The proposed method achieves a remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four datasets, respectively, consistently outperforming previous MUAD methods across different scenarios.
<div><strong>Authors:</strong> Peng Tang, Xiaoxiao Yan, Xiaobin Hu, Yuning Cui, Donghao Luo, Jiangning Zhang, Pengcheng Xu, Jinlong Peng, Qingdong He, Feiyue Huang, Song Xue, Tobias Lasser</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ShortcutBreaker, a unified feature-reconstruction framework for multi-class unsupervised anomaly detection that tackles identity shortcuts by employing a low-rank noisy bottleneck to force a low-rank latent representation and a global perturbation attention mechanism within Vision Transformers to disrupt trivial copying. The method achieves state-of-the-art image-level AUROC scores across several industrial and medical benchmark datasets, demonstrating the effectiveness of the proposed architectural innovations.", "summary_cn": "本文提出 ShortcutBreaker，一种面向多类无监督异常检测的统一特征重建框架，通过低秩噪声瓶颈（low-rank noisy bottleneck）将高维特征投射到低秩潜空间以防止身份捷径，并利用 Vision Transformer 的全局扰动注意力（global perturbation attention）阻止解码器的直接复制。实验在多个工业和医学基准上实现了领先的图像级 AUROC，验证了该架构的有效性。", "keywords": "unsupervised anomaly detection, multi-class, low-rank bottleneck, global perturbation attention, Vision Transformer, reconstruction shortcut, MUAD, industrial defect detection, medical anomaly detection", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Peng Tang", "Xiaoxiao Yan", "Xiaobin Hu", "Yuning Cui", "Donghao Luo", "Jiangning Zhang", "Pengcheng Xu", "Jinlong Peng", "Qingdong He", "Feiyue Huang", "Song Xue", "Tobias Lasser"]}
]]></acme>

<pubDate>2025-10-21T06:51:30+00:00</pubDate>
</item>
<item>
<title>Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</title>
<link>https://papers.cool/arxiv/2510.18318</link>
<guid>https://papers.cool/arxiv/2510.18318</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Earth AI introduces a suite of foundation models for planet‑scale imagery, population, and environment data, combined with a Gemini‑powered agent that performs cross‑modal reasoning over these models and external geospatial tools. The paper presents benchmarks demonstrating superior predictive performance and shows the agent’s ability to answer multi‑step queries in real‑world crisis scenarios, turning raw satellite and demographic data into actionable insights.<br /><strong>Summary (CN):</strong> Earth AI 提出一套针对星球级影像、人口与环境数据的基础模型，并配备 Gemini 驱动的智能体，以跨模态方式在这些模型与外部地理数据工具之间进行推理。论文展示了其在基准测试中实现的更佳预测能力，并在真实危机情境下证明该智能体能够处理多步骤查询，将原始卫星与人口数据转化为可操作的洞见。<br /><strong>Keywords:</strong> geospatial AI, foundation models, cross-modal reasoning, satellite imagery, population modeling, environmental modeling, Gemini agent, multi-modal inference, crisis response<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Aaron Bell, Amit Aides, Amr Helmy, Arbaaz Muslim, Aviad Barzilai, Aviv Slobodkin, Bolous Jaber, David Schottlander, George Leifman, Joydeep Paul, Mimi Sun, Nadav Sherman, Natalie Williams, Per Bjornsson, Roy Lee, Ruth Alcantara, Thomas Turnbull, Tomer Shekel, Vered Silverman, Yotam Gigi, Adam Boulanger, Alex Ottenwess, Ali Ahmadalipour, Anna Carter, Charles Elliott, David Andre, Elad Aharoni, Gia Jung, Hassler Thurston, Jacob Bien, Jamie McPike, Juliet Rothenberg, Kartik Hegde, Kel Markert, Kim Philipp Jablonski, Luc Houriez, Monica Bharel, Phing VanLee, Reuven Sayag, Sebastian Pilarski, Shelley Cazares, Shlomi Pasternak, Siduo Jiang, Stone Jiang, Thomas Colthurst, Yang Chen, Yehonathan Refael, Yochai Blau, Yuval Carny, Yael Maguire, Avinatan Hassidim, James Manyika, Tim Thelin, Genady Beryozkin, Gautam Prasad, Luke Barrington, Yossi Matias, Niv Efron, Shravya Shetty</div>
Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.
<div><strong>Authors:</strong> Aaron Bell, Amit Aides, Amr Helmy, Arbaaz Muslim, Aviad Barzilai, Aviv Slobodkin, Bolous Jaber, David Schottlander, George Leifman, Joydeep Paul, Mimi Sun, Nadav Sherman, Natalie Williams, Per Bjornsson, Roy Lee, Ruth Alcantara, Thomas Turnbull, Tomer Shekel, Vered Silverman, Yotam Gigi, Adam Boulanger, Alex Ottenwess, Ali Ahmadalipour, Anna Carter, Charles Elliott, David Andre, Elad Aharoni, Gia Jung, Hassler Thurston, Jacob Bien, Jamie McPike, Juliet Rothenberg, Kartik Hegde, Kel Markert, Kim Philipp Jablonski, Luc Houriez, Monica Bharel, Phing VanLee, Reuven Sayag, Sebastian Pilarski, Shelley Cazares, Shlomi Pasternak, Siduo Jiang, Stone Jiang, Thomas Colthurst, Yang Chen, Yehonathan Refael, Yochai Blau, Yuval Carny, Yael Maguire, Avinatan Hassidim, James Manyika, Tim Thelin, Genady Beryozkin, Gautam Prasad, Luke Barrington, Yossi Matias, Niv Efron, Shravya Shetty</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Earth AI introduces a suite of foundation models for planet‑scale imagery, population, and environment data, combined with a Gemini‑powered agent that performs cross‑modal reasoning over these models and external geospatial tools. The paper presents benchmarks demonstrating superior predictive performance and shows the agent’s ability to answer multi‑step queries in real‑world crisis scenarios, turning raw satellite and demographic data into actionable insights.", "summary_cn": "Earth AI 提出一套针对星球级影像、人口与环境数据的基础模型，并配备 Gemini 驱动的智能体，以跨模态方式在这些模型与外部地理数据工具之间进行推理。论文展示了其在基准测试中实现的更佳预测能力，并在真实危机情境下证明该智能体能够处理多步骤查询，将原始卫星与人口数据转化为可操作的洞见。", "keywords": "geospatial AI, foundation models, cross-modal reasoning, satellite imagery, population modeling, environmental modeling, Gemini agent, multi-modal inference, crisis response", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Aaron Bell", "Amit Aides", "Amr Helmy", "Arbaaz Muslim", "Aviad Barzilai", "Aviv Slobodkin", "Bolous Jaber", "David Schottlander", "George Leifman", "Joydeep Paul", "Mimi Sun", "Nadav Sherman", "Natalie Williams", "Per Bjornsson", "Roy Lee", "Ruth Alcantara", "Thomas Turnbull", "Tomer Shekel", "Vered Silverman", "Yotam Gigi", "Adam Boulanger", "Alex Ottenwess", "Ali Ahmadalipour", "Anna Carter", "Charles Elliott", "David Andre", "Elad Aharoni", "Gia Jung", "Hassler Thurston", "Jacob Bien", "Jamie McPike", "Juliet Rothenberg", "Kartik Hegde", "Kel Markert", "Kim Philipp Jablonski", "Luc Houriez", "Monica Bharel", "Phing VanLee", "Reuven Sayag", "Sebastian Pilarski", "Shelley Cazares", "Shlomi Pasternak", "Siduo Jiang", "Stone Jiang", "Thomas Colthurst", "Yang Chen", "Yehonathan Refael", "Yochai Blau", "Yuval Carny", "Yael Maguire", "Avinatan Hassidim", "James Manyika", "Tim Thelin", "Genady Beryozkin", "Gautam Prasad", "Luke Barrington", "Yossi Matias", "Niv Efron", "Shravya Shetty"]}
]]></acme>

<pubDate>2025-10-21T06:05:47+00:00</pubDate>
</item>
<item>
<title>Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming</title>
<link>https://papers.cool/arxiv/2510.18314</link>
<guid>https://papers.cool/arxiv/2510.18314</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Genesis introduces an agentic framework that autonomously evolves attack strategies against large language model web agents using a genetic algorithm, a scorer, and a strategist that builds a growing strategy library. By iteratively generating adversarial injections and learning from interaction logs, the system discovers novel attack patterns and consistently outperforms static red‑teaming baselines across diverse web tasks.<br /><strong>Summary (CN):</strong> Genesis 提出一个基于遗传算法的智能框架，包含攻击器、评分器和策略师三个模块，用于自主进化针对 LLM 网页代理的攻击策略。系统通过生成对抗性注入并从交互日志中学习，逐步构建不断扩展的策略库，在多种网页任务上发现新颖攻击手段并显著超越传统红队基线。<br /><strong>Keywords:</strong> LLM agents, web automation, red-teaming, genetic algorithm, attack strategy evolution, adversarial injection, safety, robustness, agentic framework, strategy library<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Zheng Zhang, Jiarui He, Yuchen Cai, Deheng Ye, Peilin Zhao, Ruili Feng, Hao Wang</div>
As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.
<div><strong>Authors:</strong> Zheng Zhang, Jiarui He, Yuchen Cai, Deheng Ye, Peilin Zhao, Ruili Feng, Hao Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Genesis introduces an agentic framework that autonomously evolves attack strategies against large language model web agents using a genetic algorithm, a scorer, and a strategist that builds a growing strategy library. By iteratively generating adversarial injections and learning from interaction logs, the system discovers novel attack patterns and consistently outperforms static red‑teaming baselines across diverse web tasks.", "summary_cn": "Genesis 提出一个基于遗传算法的智能框架，包含攻击器、评分器和策略师三个模块，用于自主进化针对 LLM 网页代理的攻击策略。系统通过生成对抗性注入并从交互日志中学习，逐步构建不断扩展的策略库，在多种网页任务上发现新颖攻击手段并显著超越传统红队基线。", "keywords": "LLM agents, web automation, red-teaming, genetic algorithm, attack strategy evolution, adversarial injection, safety, robustness, agentic framework, strategy library", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Zheng Zhang", "Jiarui He", "Yuchen Cai", "Deheng Ye", "Peilin Zhao", "Ruili Feng", "Hao Wang"]}
]]></acme>

<pubDate>2025-10-21T05:49:37+00:00</pubDate>
</item>
<item>
<title>Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning</title>
<link>https://papers.cool/arxiv/2510.18254</link>
<guid>https://papers.cool/arxiv/2510.18254</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models (LLMs) can perform human-like reflective reasoning on an open‑ended, rule‑constrained task (generating valid scientific test items and revising them after critique). Experiments with eight frontier models show poor first‑pass performance and only modest gains after reflection, with many repeated constraint violations, suggesting current LLM reflection lacks active, goal‑driven error detection. The authors argue that reliable constraint adherence requires external enforcement rather than relying on internal reflective mechanisms.<br /><strong>Summary (CN):</strong> 本文研究了大语言模型（LLM）在开放式、受规则约束的任务（生成有效的科学测试题并在自我批评后进行修正）中是否具有人类式的反思推理能力。对八种前沿模型的实验表明，首次生成表现不佳，反思后仅有轻微提升，且经常重复相同的约束违例，说明当前的 LLM 反思缺乏主动、目标驱动的错误检测。作者认为，要实现可靠的约束遵循，需要外部结构来强制约束，而非依赖模型内部的反思机制。<br /><strong>Keywords:</strong> reflection, self-correction, large language models, open-ended tasks, constraint violation, evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Sion Weatherhead, Flora Salim, Aaron Belbasis</div>
Humans do not just find mistakes after the fact -- we often catch them mid-stream because 'reflection' is tied to the goal and its constraints. Today's large language models produce reasoning tokens and 'reflective' text, but is it functionally equivalent with human reflective reasoning? Prior work on closed-ended tasks -- with clear, external 'correctness' signals -- can make 'reflection' look effective while masking limits in self-correction. We therefore test eight frontier models on a simple, real-world task that is open-ended yet rule-constrained, with auditable success criteria: to produce valid scientific test items, then revise after considering their own critique. First-pass performance is poor (often zero valid items out of 4 required; mean $\approx$ 1), and reflection yields only modest gains (also $\approx$ 1). Crucially, the second attempt frequently repeats the same violation of constraint, indicating 'corrective gains' arise largely from chance production of a valid item rather than error detection and principled, constraint-sensitive repair. Performance before and after reflection deteriorates as open-endedness increases, and models marketed for 'reasoning' show no advantage. Our results suggest that current LLM 'reflection' lacks functional evidence of the active, goal-driven monitoring that helps humans respect constraints even on a first pass. Until such mechanisms are instantiated in the model itself, reliable performance requires external structure that enforces constraints.
<div><strong>Authors:</strong> Sion Weatherhead, Flora Salim, Aaron Belbasis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models (LLMs) can perform human-like reflective reasoning on an open‑ended, rule‑constrained task (generating valid scientific test items and revising them after critique). Experiments with eight frontier models show poor first‑pass performance and only modest gains after reflection, with many repeated constraint violations, suggesting current LLM reflection lacks active, goal‑driven error detection. The authors argue that reliable constraint adherence requires external enforcement rather than relying on internal reflective mechanisms.", "summary_cn": "本文研究了大语言模型（LLM）在开放式、受规则约束的任务（生成有效的科学测试题并在自我批评后进行修正）中是否具有人类式的反思推理能力。对八种前沿模型的实验表明，首次生成表现不佳，反思后仅有轻微提升，且经常重复相同的约束违例，说明当前的 LLM 反思缺乏主动、目标驱动的错误检测。作者认为，要实现可靠的约束遵循，需要外部结构来强制约束，而非依赖模型内部的反思机制。", "keywords": "reflection, self-correction, large language models, open-ended tasks, constraint violation, evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"]}
]]></acme>

<pubDate>2025-10-21T03:24:21+00:00</pubDate>
</item>
<item>
<title>ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning</title>
<link>https://papers.cool/arxiv/2510.18250</link>
<guid>https://papers.cool/arxiv/2510.18250</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ssToken, a token-level selection method for supervised fine-tuning of large language models that combines a self-modulated loss difference signal with a semantic-aware attention-based importance metric. By leveraging the model's own history instead of an external reference model and incorporating semantic information, ssToken achieves better performance than full-data fine-tuning and prior token selection approaches across various model scales.<br /><strong>Summary (CN):</strong> 本文提出 ssToken，一种用于大语言模型监督微调的 token 级别选择方法，结合自调节的损失差异信号和基于注意力的语义感知重要性度量。该方法利用模型自身的历史信息而非外部参考模型，并加入语义信息，实现了相较于全数据微调和以往 token 选择方法的性能提升，适用于多种模型规模。<br /><strong>Keywords:</strong> token selection, supervised fine-tuning, large language models, self-modulated loss, semantic-aware attention, data efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Cancheng Zhang, Xiangdong Zhang, Mingquan Feng, Jingzhi Wang, Junchi Yan</div>
Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.
<div><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Cancheng Zhang, Xiangdong Zhang, Mingquan Feng, Jingzhi Wang, Junchi Yan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ssToken, a token-level selection method for supervised fine-tuning of large language models that combines a self-modulated loss difference signal with a semantic-aware attention-based importance metric. By leveraging the model's own history instead of an external reference model and incorporating semantic information, ssToken achieves better performance than full-data fine-tuning and prior token selection approaches across various model scales.", "summary_cn": "本文提出 ssToken，一种用于大语言模型监督微调的 token 级别选择方法，结合自调节的损失差异信号和基于注意力的语义感知重要性度量。该方法利用模型自身的历史信息而非外部参考模型，并加入语义信息，实现了相较于全数据微调和以往 token 选择方法的性能提升，适用于多种模型规模。", "keywords": "token selection, supervised fine-tuning, large language models, self-modulated loss, semantic-aware attention, data efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Cancheng Zhang", "Xiangdong Zhang", "Mingquan Feng", "Jingzhi Wang", "Junchi Yan"]}
]]></acme>

<pubDate>2025-10-21T03:21:04+00:00</pubDate>
</item>
<item>
<title>A Definition of AGI</title>
<link>https://papers.cool/arxiv/2510.18212</link>
<guid>https://papers.cool/arxiv/2510.18212</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a quantifiable framework for defining Artificial General Intelligence (AGI) by aligning AI evaluation with the Cattell-Horn-Carroll theory of human cognition, breaking general intelligence into ten core domains such as reasoning, memory, and perception. Established human psychometric batteries are adapted to assess AI systems, revealing a jagged cognitive profile where contemporary models excel in knowledge-intensive domains but lack fundamental abilities like long-term memory storage. AGI scores are reported (e.g., GPT‑4 at 27%, GPT‑5 at 58%) to illustrate both progress and the remaining gap to human‑level cognition.<br /><strong>Summary (CN):</strong> 本文提出了一套可量化的 AGI 定义框架，基于人类认知的 Cattell‑Horn‑Carroll 理论，将通用智能划分为包括推理、记忆、感知等十个核心认知域，并将已有的人类心理测评工具移植到 AI 系统上进行评估。实验显示当前模型在知识密集型领域表现良好，但在长期记忆等基础认知机械方面存在显著缺陷。文中给出了 AGI 分数（如 GPT‑4 为 27%，GPT‑5 为 58%），量化了快速进展与离人类水平仍有的巨大差距。<br /><strong>Keywords:</strong> AGI definition, cognitive assessment, Cattell-Horn-Carroll, AI psychometrics, general intelligence, cognitive domains, model evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua Bengio</div>
The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.
<div><strong>Authors:</strong> Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua Bengio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a quantifiable framework for defining Artificial General Intelligence (AGI) by aligning AI evaluation with the Cattell-Horn-Carroll theory of human cognition, breaking general intelligence into ten core domains such as reasoning, memory, and perception. Established human psychometric batteries are adapted to assess AI systems, revealing a jagged cognitive profile where contemporary models excel in knowledge-intensive domains but lack fundamental abilities like long-term memory storage. AGI scores are reported (e.g., GPT‑4 at 27%, GPT‑5 at 58%) to illustrate both progress and the remaining gap to human‑level cognition.", "summary_cn": "本文提出了一套可量化的 AGI 定义框架，基于人类认知的 Cattell‑Horn‑Carroll 理论，将通用智能划分为包括推理、记忆、感知等十个核心认知域，并将已有的人类心理测评工具移植到 AI 系统上进行评估。实验显示当前模型在知识密集型领域表现良好，但在长期记忆等基础认知机械方面存在显著缺陷。文中给出了 AGI 分数（如 GPT‑4 为 27%，GPT‑5 为 58%），量化了快速进展与离人类水平仍有的巨大差距。", "keywords": "AGI definition, cognitive assessment, Cattell-Horn-Carroll, AI psychometrics, general intelligence, cognitive domains, model evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dan Hendrycks", "Dawn Song", "Christian Szegedy", "Honglak Lee", "Yarin Gal", "Erik Brynjolfsson", "Sharon Li", "Andy Zou", "Lionel Levine", "Bo Han", "Jie Fu", "Ziwei Liu", "Jinwoo Shin", "Kimin Lee", "Mantas Mazeika", "Long Phan", "George Ingebretsen", "Adam Khoja", "Cihang Xie", "Olawale Salaudeen", "Matthias Hein", "Kevin Zhao", "Alexander Pan", "David Duvenaud", "Bo Li", "Steve Omohundro", "Gabriel Alfour", "Max Tegmark", "Kevin McGrew", "Gary Marcus", "Jaan Tallinn", "Eric Schmidt", "Yoshua Bengio"]}
]]></acme>

<pubDate>2025-10-21T01:28:35+00:00</pubDate>
</item>
<item>
<title>FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo</title>
<link>https://papers.cool/arxiv/2510.18193</link>
<guid>https://papers.cool/arxiv/2510.18193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FST.ai 2.0 introduces an explainable AI ecosystem for Olympic and Paralympic Taekwondo that combines graph convolutional network‑based pose action recognition, credal‑set epistemic uncertainty modeling, and visual explainability overlays to assist referees, coaches, and athletes in real time. Interactive dashboards support human‑AI collaboration for referee evaluation, athlete performance analysis, and fairness monitoring, demonstrating an 85% reduction in decision review time and high referee trust. The system aims to provide transparent, accountable, and inclusive decision‑making while integrating governance‑level analytics within the World Taekwondo framework.<br /><strong>Summary (CN):</strong> FST.ai 2.0 构建了一个面向奥运和残奥跆拳道的可解释 AI 生态系统，利用基于图卷积网络（GCN）的姿态动作识别、凭证集（credal sets）进行的认知不确定性建模以及可视化解释叠加，为裁判、教练和运动员提供实时决策支持。交互式仪表盘实现人机协作，用于裁判评估、运动员表现分析和公平性监控，实验显示可将裁决复审时间降低 85% 并获得高裁判信任度。该框架旨在实现透明、负责任且包容的决策，并在世界跆拳道组织内嵌入治理层面的分析功能。<br /><strong>Keywords:</strong> explainable AI, pose-based action recognition, graph convolutional networks, epistemic uncertainty, credal sets, sports officiating, fairness monitoring, human-AI collaboration<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athlete performance analysis, and Para-Taekwondo classification. Beyond automated scoring, FST.ai~2.0 incorporates modules for referee training, fairness monitoring, and policy-level analytics within the World Taekwondo ecosystem. Experimental validation on competition data demonstrates an {85\% reduction in decision review time} and {93\% referee trust} in AI-assisted decisions. The framework thus establishes a transparent and extensible pipeline for trustworthy, data-driven officiating and athlete assessment. By bridging real-time perception, explainable inference, and governance-aware design, FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned AI in sports.
<div><strong>Authors:</strong> Keivan Shariatmadar, Ahmad Osman, Ramin Ray, Usman Dildar, Kisam Kim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FST.ai 2.0 introduces an explainable AI ecosystem for Olympic and Paralympic Taekwondo that combines graph convolutional network‑based pose action recognition, credal‑set epistemic uncertainty modeling, and visual explainability overlays to assist referees, coaches, and athletes in real time. Interactive dashboards support human‑AI collaboration for referee evaluation, athlete performance analysis, and fairness monitoring, demonstrating an 85% reduction in decision review time and high referee trust. The system aims to provide transparent, accountable, and inclusive decision‑making while integrating governance‑level analytics within the World Taekwondo framework.", "summary_cn": "FST.ai 2.0 构建了一个面向奥运和残奥跆拳道的可解释 AI 生态系统，利用基于图卷积网络（GCN）的姿态动作识别、凭证集（credal sets）进行的认知不确定性建模以及可视化解释叠加，为裁判、教练和运动员提供实时决策支持。交互式仪表盘实现人机协作，用于裁判评估、运动员表现分析和公平性监控，实验显示可将裁决复审时间降低 85% 并获得高裁判信任度。该框架旨在实现透明、负责任且包容的决策，并在世界跆拳道组织内嵌入治理层面的分析功能。", "keywords": "explainable AI, pose-based action recognition, graph convolutional networks, epistemic uncertainty, credal sets, sports officiating, fairness monitoring, human-AI collaboration", "scoring": {"interpretability": 7, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Keivan Shariatmadar", "Ahmad Osman", "Ramin Ray", "Usman Dildar", "Kisam Kim"]}
]]></acme>

<pubDate>2025-10-21T00:35:56+00:00</pubDate>
</item>
<item>
<title>Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains</title>
<link>https://papers.cool/arxiv/2510.18176</link>
<guid>https://papers.cool/arxiv/2510.18176</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how Reinforcement Learning with Verifiable Rewards (RLVR) post‑training affects intermediate reasoning steps in large language models, introducing a First‑Order Logic‑based trace coherence metric to differentiate local coherence from overall logical validity. Experiments with the GRPO algorithm on Qwen‑2.5‑0.5B and the GSM8K dataset show that RL improves trace coherence, especially on problems where the base model fails, but this does not necessarily lead to valid or correct solutions. The authors argue that claims of better reasoning via RL should be scrutinized, as enhanced local coherence can be decoupled from full proof correctness.<br /><strong>Summary (CN):</strong> 本文研究了基于可验证奖励的强化学习（RLVR）后训练对大型语言模型推理过程中的中间步骤的影响，提出了一种基于一阶逻辑的“追踪一致性”度量，以区分局部一致性与整体逻辑有效性。使用 GRPO 算法在 Qwen‑2.5‑0.5B 和 GSM8K 数据集上的实验表明，RL 能提升追踪的一致性，尤其在基模型失败而 RL 模型成功的题目上，但这并不一定能产生有效或正确的答案。作者指出，关于 RL 改善推理的论断需谨慎审视，因为局部一致性的提升并不等同于完整数学证明的正确性。<br /><strong>Keywords:</strong> reinforcement learning with verifiable rewards, trace coherence, reasoning trace analysis, GSM8K, RL post-training, token-level evaluation, interpretability, local coherence, logical validity<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Soumya Rani Samineni, Durgesh Kalwar, Vardaan Gangal, Siddhant Bhambri, Subbarao Kambhampati</div>
Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of Large Language Models (LLMs) has been shown to improve accuracy on reasoning tasks and continues to attract significant attention. Existing RLVR methods, however, typically treat all tokens uniformly without accounting for token-level advantages. These methods primarily evaluate performance based on final answer correctness or Pass@K accuracy, and yet make claims about RL post-training leading to improved reasoning traces. This motivates our investigation into the effect of RL post-training on intermediate tokens which are not directly incentivized. To study this, we design an experimental setup using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We introduce trace coherence, a First-Order Logic (FOL)-based measure to capture the consistency of reasoning steps by identifying errors in the traces. We distinguish between trace validity and trace coherence, noting that the former implies logical soundness while the latter measures local coherence via lack of errors. Our results show that RL post-training overall improves trace coherence with the most significant gains on problems where the base model fails but the RL model succeeds. Surprisingly, RL enhances local coherence without necessarily producing valid or correct solutions. This highlights a crucial distinction: improved local coherence in reasoning steps does not guarantee final answer correctness. We argue that claims of improved reasoning via RL must be examined with care, as these may be based on improved trace coherence, which may not translate into fully valid mathematical proofs.
<div><strong>Authors:</strong> Soumya Rani Samineni, Durgesh Kalwar, Vardaan Gangal, Siddhant Bhambri, Subbarao Kambhampati</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how Reinforcement Learning with Verifiable Rewards (RLVR) post‑training affects intermediate reasoning steps in large language models, introducing a First‑Order Logic‑based trace coherence metric to differentiate local coherence from overall logical validity. Experiments with the GRPO algorithm on Qwen‑2.5‑0.5B and the GSM8K dataset show that RL improves trace coherence, especially on problems where the base model fails, but this does not necessarily lead to valid or correct solutions. The authors argue that claims of better reasoning via RL should be scrutinized, as enhanced local coherence can be decoupled from full proof correctness.", "summary_cn": "本文研究了基于可验证奖励的强化学习（RLVR）后训练对大型语言模型推理过程中的中间步骤的影响，提出了一种基于一阶逻辑的“追踪一致性”度量，以区分局部一致性与整体逻辑有效性。使用 GRPO 算法在 Qwen‑2.5‑0.5B 和 GSM8K 数据集上的实验表明，RL 能提升追踪的一致性，尤其在基模型失败而 RL 模型成功的题目上，但这并不一定能产生有效或正确的答案。作者指出，关于 RL 改善推理的论断需谨慎审视，因为局部一致性的提升并不等同于完整数学证明的正确性。", "keywords": "reinforcement learning with verifiable rewards, trace coherence, reasoning trace analysis, GSM8K, RL post-training, token-level evaluation, interpretability, local coherence, logical validity", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Vardaan Gangal", "Siddhant Bhambri", "Subbarao Kambhampati"]}
]]></acme>

<pubDate>2025-10-20T23:58:31+00:00</pubDate>
</item>
<item>
<title>AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI</title>
<link>https://papers.cool/arxiv/2510.18170</link>
<guid>https://papers.cool/arxiv/2510.18170</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AgentChangeBench is a benchmark that evaluates how tool-augmented language model agents handle mid‑dialogue goal shifts across multiple enterprise domains. It defines four metrics—Task Success Rate, Tool Use Efficiency, Tool Call Redundancy Rate, and Goal‑Shift Recovery Time—and includes 2,835 task sequences with five user personas to trigger realistic shifts. Experiments on frontier models reveal large gaps between static accuracy and dynamic robustness, highlighting the need for dedicated evaluation of goal‑shift recovery.<br /><strong>Summary (CN):</strong> AgentChangeBench 是一个专门测评工具增强语言模型代理在多轮对话中面对目标转变适应能力的基准，涵盖三个企业场景。它通过任务成功率、工具使用效率、工具调用冗余率和目标转变恢复时间四个指标，对 2,835 条任务序列和五种用户角色进行评估。实验显示，模型在静态准确率高的情况下仍可能在目标转变时表现出显著不足，强调了评估动态鲁棒性的重要性。<br /><strong>Keywords:</strong> goal-shift robustness, tool-augmented agents, evaluation benchmark, Task Success Rate, Tool Use Efficiency, redundancy, recovery time, conversational AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Manik Rana, Calissa Man, Anotida Expected Msiiwa, Jeffrey Paine, Kevin Zhu, Sunishchal Dev, Vasu Sharma, Ahan M R</div>
Goal changes are a defining feature of real world multi-turn interactions, yet current agent benchmarks primarily evaluate static objectives or one-shot tool use. We introduce AgentChangeBench, a benchmark explicitly designed to measure how tool augmented language model agents adapt to mid dialogue goal shifts across three enterprise domains. Our framework formalizes evaluation through four complementary metrics: Task Success Rate (TSR) for effectiveness, Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency. AgentChangeBench comprises 2,835 task sequences and five user personas, each designed to trigger realistic shift points in ongoing workflows. Using this setup, we evaluate several frontier models and uncover sharp contrasts obscured by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$ recovery on airline booking shifts while Gemini collapses to $48.6\%$, and retail tasks show near perfect parameter validity yet redundancy rates above $80\%$, revealing major inefficiencies. These findings demonstrate that high raw accuracy does not imply robustness under dynamic goals, and that explicit measurement of recovery time and redundancy is essential. AgentChangeBench establishes a reproducible testbed for diagnosing and improving agent resilience in realistic enterprise settings.
<div><strong>Authors:</strong> Manik Rana, Calissa Man, Anotida Expected Msiiwa, Jeffrey Paine, Kevin Zhu, Sunishchal Dev, Vasu Sharma, Ahan M R</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AgentChangeBench is a benchmark that evaluates how tool-augmented language model agents handle mid‑dialogue goal shifts across multiple enterprise domains. It defines four metrics—Task Success Rate, Tool Use Efficiency, Tool Call Redundancy Rate, and Goal‑Shift Recovery Time—and includes 2,835 task sequences with five user personas to trigger realistic shifts. Experiments on frontier models reveal large gaps between static accuracy and dynamic robustness, highlighting the need for dedicated evaluation of goal‑shift recovery.", "summary_cn": "AgentChangeBench 是一个专门测评工具增强语言模型代理在多轮对话中面对目标转变适应能力的基准，涵盖三个企业场景。它通过任务成功率、工具使用效率、工具调用冗余率和目标转变恢复时间四个指标，对 2,835 条任务序列和五种用户角色进行评估。实验显示，模型在静态准确率高的情况下仍可能在目标转变时表现出显著不足，强调了评估动态鲁棒性的重要性。", "keywords": "goal-shift robustness, tool-augmented agents, evaluation benchmark, Task Success Rate, Tool Use Efficiency, redundancy, recovery time, conversational AI", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Manik Rana", "Calissa Man", "Anotida Expected Msiiwa", "Jeffrey Paine", "Kevin Zhu", "Sunishchal Dev", "Vasu Sharma", "Ahan M R"]}
]]></acme>

<pubDate>2025-10-20T23:48:07+00:00</pubDate>
</item>
<item>
<title>Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model</title>
<link>https://papers.cool/arxiv/2510.18165</link>
<guid>https://papers.cool/arxiv/2510.18165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Saber, a training‑free sampling algorithm for diffusion language models that adaptively accelerates generation and incorporates backtracking‑enhanced remasking to improve code generation performance. By adjusting acceleration as more code context becomes available and allowing reversal of generated tokens, Saber achieves up to 251 % speedup while raising Pass@1 accuracy by around 1.9 % on standard benchmarks. Experiments demonstrate that the method narrows the gap between diffusion and autoregressive models for code tasks.<br /><strong>Summary (CN):</strong> 本文提出 Saber，一种无需额外训练的扩散语言模型采样算法，通过在代码上下文逐渐建立时自适应加速并加入回溯增强重掩码机制，提高代码生成的质量和速度。该方法能够在生成过程中动态加速并逆转已生成的标记，实现约 251% 的推理加速，同时在主流基准上将 Pass@1 准确率提升约 1.9%。实验表明 Saber 显著缩小了扩散模型与自回归模型在代码生成任务上的性能差距。<br /><strong>Keywords:</strong> diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
Diffusion language models (DLMs) are emerging as a powerful and promising alternative to the dominant autoregressive paradigm, offering inherent advantages in parallel generation and bidirectional context modeling. However, the performance of DLMs on code generation tasks, which have stronger structural constraints, is significantly hampered by the critical trade-off between inference speed and output quality. We observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance. In this paper, we introduce efficient Sampling with Adaptive acceleration and Backtracking Enhanced Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to achieve better inference speed and output quality in code generation. Specifically, Saber is motivated by two key insights in the DLM generation process: 1) it can be adaptively accelerated as more of the code context is established; 2) it requires a backtracking mechanism to reverse the generated tokens. Extensive experiments on multiple mainstream code generation benchmarks show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over mainstream DLM sampling methods, meanwhile achieving an average 251.4% inference speedup. By leveraging the inherent advantages of DLMs, our work significantly narrows the performance gap with autoregressive models in code generation.
<div><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Saber, a training‑free sampling algorithm for diffusion language models that adaptively accelerates generation and incorporates backtracking‑enhanced remasking to improve code generation performance. By adjusting acceleration as more code context becomes available and allowing reversal of generated tokens, Saber achieves up to 251 % speedup while raising Pass@1 accuracy by around 1.9 % on standard benchmarks. Experiments demonstrate that the method narrows the gap between diffusion and autoregressive models for code tasks.", "summary_cn": "本文提出 Saber，一种无需额外训练的扩散语言模型采样算法，通过在代码上下文逐渐建立时自适应加速并加入回溯增强重掩码机制，提高代码生成的质量和速度。该方法能够在生成过程中动态加速并逆转已生成的标记，实现约 251% 的推理加速，同时在主流基准上将 Pass@1 准确率提升约 1.9%。实验表明 Saber 显著缩小了扩散模型与自回归模型在代码生成任务上的性能差距。", "keywords": "diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-20T23:38:12+00:00</pubDate>
</item>
<item>
<title>LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior</title>
<link>https://papers.cool/arxiv/2510.18155</link>
<guid>https://papers.cool/arxiv/2510.18155</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a large‑language‑model‑powered multi‑agent simulation framework that models consumer decision‑making and social dynamics without predefined rules. By enabling generative agents to reason, form habits, and make purchasing choices, the system demonstrates actionable insights in a price‑discount marketing scenario and reveals emergent social patterns beyond traditional rule‑based ABMs. The approach aims to provide marketers with a scalable, low‑risk tool for pre‑deployment strategy testing.<br /><strong>Summary (CN):</strong> 本文提出一种基于大型语言模型（LLM）的多智能体模拟框架，能够在无需预定义规则的情况下建模消费者决策和社交动态。通过让生成式智能体进行内部推理、形成习惯并做出购买决策，系统在价格折扣营销情境中提供可操作的策略测试结果，并揭示传统基于规则的 ABM 难以捕捉的涌现社交模式。该方法旨在为营销人员提供可扩展、低风险的前置测试工具。<br /><strong>Keywords:</strong> LLM, multi-agent simulation, consumer behavior, marketing analytics, generative agents, emergent social patterns, agent-based modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Man-Lin Chu, Lucian Terhorst, Kadin Reed, Tom Ni, Weiwei Chen, Rongyu Lin</div>
Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real- world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox envi- ronment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of con- ventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.
<div><strong>Authors:</strong> Man-Lin Chu, Lucian Terhorst, Kadin Reed, Tom Ni, Weiwei Chen, Rongyu Lin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a large‑language‑model‑powered multi‑agent simulation framework that models consumer decision‑making and social dynamics without predefined rules. By enabling generative agents to reason, form habits, and make purchasing choices, the system demonstrates actionable insights in a price‑discount marketing scenario and reveals emergent social patterns beyond traditional rule‑based ABMs. The approach aims to provide marketers with a scalable, low‑risk tool for pre‑deployment strategy testing.", "summary_cn": "本文提出一种基于大型语言模型（LLM）的多智能体模拟框架，能够在无需预定义规则的情况下建模消费者决策和社交动态。通过让生成式智能体进行内部推理、形成习惯并做出购买决策，系统在价格折扣营销情境中提供可操作的策略测试结果，并揭示传统基于规则的 ABM 难以捕捉的涌现社交模式。该方法旨在为营销人员提供可扩展、低风险的前置测试工具。", "keywords": "LLM, multi-agent simulation, consumer behavior, marketing analytics, generative agents, emergent social patterns, agent-based modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Man-Lin Chu", "Lucian Terhorst", "Kadin Reed", "Tom Ni", "Weiwei Chen", "Rongyu Lin"]}
]]></acme>

<pubDate>2025-10-20T23:15:44+00:00</pubDate>
</item>
<item>
<title>Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18154</link>
<guid>https://papers.cool/arxiv/2510.18154</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a sentence‑level, behavior‑labeled dataset that marks safety‑related actions (e.g., expressing concerns or speculating on user intent) within chain‑of‑thought reasoning sequences of large language models. Using this dataset, the authors derive steering vectors that can both detect and influence safety behaviors directly from model activations, demonstrating activation‑level monitoring as a promising tool for AI safety oversight.<br /><strong>Summary (CN):</strong> 本文构建了一个面向链式思考的句子级安全行为标注数据集，标记了如表达安全担忧或推测用户意图等安全相关行为，并基于该数据集提取出了用于检测和引导模型激活的 steering 向量，展示了激活层面监控在 AI 安全监督中的潜力。<br /><strong>Keywords:</strong> chain-of-thought, safety behavior annotation, activation monitoring, steering vectors, LLM safety, behavior-labeled dataset<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Antonio-Gabriel Chacón Menke, Phan Xuan Tan, Eiji Kamioka</div>
Recent work has highlighted the importance of monitoring chain-of-thought reasoning for AI safety; however, current approaches that analyze textual reasoning steps can miss subtle harmful patterns and may be circumvented by models that hide unsafe reasoning. We present a sentence-level labeled dataset that enables activation-based monitoring of safety behaviors during LLM reasoning. Our dataset contains reasoning sequences with sentence-level annotations of safety behaviors such as expression of safety concerns or speculation on user intent, which we use to extract steering vectors for detecting and influencing these behaviors within model activations. The dataset fills a key gap in safety research: while existing datasets label reasoning holistically, effective application of steering vectors for safety monitoring could be improved by identifying precisely when specific behaviors occur within reasoning chains. We demonstrate the dataset's utility by extracting representations that both detect and steer safety behaviors in model activations, showcasing the potential of activation-level techniques for improving safety oversight on reasoning. Content Warning: This paper discusses AI safety in the context of harmful prompts and may contain references to potentially harmful content.
<div><strong>Authors:</strong> Antonio-Gabriel Chacón Menke, Phan Xuan Tan, Eiji Kamioka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a sentence‑level, behavior‑labeled dataset that marks safety‑related actions (e.g., expressing concerns or speculating on user intent) within chain‑of‑thought reasoning sequences of large language models. Using this dataset, the authors derive steering vectors that can both detect and influence safety behaviors directly from model activations, demonstrating activation‑level monitoring as a promising tool for AI safety oversight.", "summary_cn": "本文构建了一个面向链式思考的句子级安全行为标注数据集，标记了如表达安全担忧或推测用户意图等安全相关行为，并基于该数据集提取出了用于检测和引导模型激活的 steering 向量，展示了激活层面监控在 AI 安全监督中的潜力。", "keywords": "chain-of-thought, safety behavior annotation, activation monitoring, steering vectors, LLM safety, behavior-labeled dataset", "scoring": {"interpretability": 7, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Antonio-Gabriel Chacón Menke", "Phan Xuan Tan", "Eiji Kamioka"]}
]]></acme>

<pubDate>2025-10-20T23:12:12+00:00</pubDate>
</item>
<item>
<title>Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models</title>
<link>https://papers.cool/arxiv/2510.18143</link>
<guid>https://papers.cool/arxiv/2510.18143</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PaDA-Agent, an evaluation-driven data augmentation framework that identifies failure patterns on validation data and generates targeted augmentations to reduce the generalization gap of small language models during fine-tuning. Experiments on the Llama 3.2 1B Instruct model show that PaDA-Agent outperforms existing LLM‑based augmentation methods, achieving notable performance gains on domain‑specific tasks.<br /><strong>Summary (CN):</strong> 本文提出 PaDA-Agent，一种基于评估的数​​据增强框架，通过在验证集上发现失败模式并生成针对性的增强样本，以缩小小语言模型在微调过程中的泛化差距。对 Llama 3.2 1B Instruct 模型的实验表明，PaDA-Agent 在性能提升上优于现有的基于大语言模型的数据增强方法，显著提升了特定领域任务的表现。<br /><strong>Keywords:</strong> data augmentation, small language models, evaluation-driven, fine-tuning, pattern discovery, generalization gap, Llama 3.2, PaDA-Agent, supervised fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Huan Song, Deeksha Razdan, Yiyue Qian, Arijit Ghosh Chowdhury, Parth Patwa, Aman Chadha, Shinan Zhang, Sharlina Keshava, Hannah Marlowe</div>
Small Language Models (SLMs) offer compelling advantages in deployment cost and latency, but their accuracy often lags behind larger models, particularly for complex domain-specific tasks. While supervised fine-tuning can help bridge this performance gap, it requires substantial manual effort in data preparation and iterative optimization. We present PaDA-Agent (Pattern-guided Data Augmentation Agent), an evaluation-driven approach that streamlines the data augmentation process for SLMs through coordinated operations. Unlike state-of-the-art approaches that focus on model training errors only and generating error-correcting samples, PaDA-Agent discovers failure patterns from the validation data via evaluations and drafts targeted data augmentation strategies aiming to directly reduce the generalization gap. Our experimental results demonstrate significant improvements over state-of-the-art LLM-based data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.
<div><strong>Authors:</strong> Huan Song, Deeksha Razdan, Yiyue Qian, Arijit Ghosh Chowdhury, Parth Patwa, Aman Chadha, Shinan Zhang, Sharlina Keshava, Hannah Marlowe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PaDA-Agent, an evaluation-driven data augmentation framework that identifies failure patterns on validation data and generates targeted augmentations to reduce the generalization gap of small language models during fine-tuning. Experiments on the Llama 3.2 1B Instruct model show that PaDA-Agent outperforms existing LLM‑based augmentation methods, achieving notable performance gains on domain‑specific tasks.", "summary_cn": "本文提出 PaDA-Agent，一种基于评估的数​​据增强框架，通过在验证集上发现失败模式并生成针对性的增强样本，以缩小小语言模型在微调过程中的泛化差距。对 Llama 3.2 1B Instruct 模型的实验表明，PaDA-Agent 在性能提升上优于现有的基于大语言模型的数据增强方法，显著提升了特定领域任务的表现。", "keywords": "data augmentation, small language models, evaluation-driven, fine-tuning, pattern discovery, generalization gap, Llama 3.2, PaDA-Agent, supervised fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Huan Song", "Deeksha Razdan", "Yiyue Qian", "Arijit Ghosh Chowdhury", "Parth Patwa", "Aman Chadha", "Shinan Zhang", "Sharlina Keshava", "Hannah Marlowe"]}
]]></acme>

<pubDate>2025-10-20T22:36:46+00:00</pubDate>
</item>
<item>
<title>Measuring Reasoning in LLMs: a New Dialectical Angle</title>
<link>https://papers.cool/arxiv/2510.18134</link>
<guid>https://papers.cool/arxiv/2510.18134</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SIEV, a dialectic‑based framework that evaluates large language model reasoning as a dynamic trajectory of thesis, antithesis, and synthesis rather than a static chain of steps. By measuring a model’s ability to resolve tensions, integrate ideas, and produce higher‑order synthesis, SIEV uncovers substantial reasoning gaps in state‑of‑the‑art models that standard accuracy‑only benchmarks miss. Experiments on GSM and MMLU show that even top models lose significant points when evaluated with this process‑oriented metric.<br /><strong>Summary (CN):</strong> 本文提出 SIEV 框架，以辩证法的“正论‑反论‑合成”过程评估大型语言模型的推理，关注其在冲突解决、观点融合与高阶综合方面的能力，而非仅看最终答案的正确性。实验在 GSM 与 MMLU 基准上显示，先进模型在过程导向评价中会出现显著的推理缺失，分数下降超过 40 分。该研究展示了通过过程化、哲学化的评估方式，可更深入地辨别 LLM 推理能力的不足。<br /><strong>Keywords:</strong> dialectics, reasoning evaluation, LLM reasoning, process-oriented assessment, SIEV framework, GSM benchmark, MMLU, interpretability, AI evaluation, chain-of-thought analysis<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 8, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Soheil Abbasloo</div>
What does it truly mean for a language model to "reason"? Most current evaluations and benchmarks reward models' correct standalone answers--but correctness alone reveals little about the process that produced them. In this work, we explore a different perspective: reasoning is not a static chain of steps, but a dynamic trajectory where ideas interact, clash, and evolve into deeper insights. To capture this dynamic, we draw on a well-established philosophical tradition: \textit{dialectics}, where reasoning unfolds through thesis, antithesis, and synthesis. Building on this, we present SIEV, a structured framework that evaluates reasoning of LLMs through dialectics. Unlike conventional evaluations, SIEV assesses not only the conclusion a model reaches, but how it gets there: its ability to resolve tension, integrate distinct ideas, and synthesize higher-order reasoning. This lens uncovers significant reasoning gaps in state-of-the-art models even under saturated benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings highlight that adopting a process-oriented, philosophically grounded approach enables a deeper, more rigorous, and more discriminative assessment of LLM reasoning.
<div><strong>Authors:</strong> Soheil Abbasloo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SIEV, a dialectic‑based framework that evaluates large language model reasoning as a dynamic trajectory of thesis, antithesis, and synthesis rather than a static chain of steps. By measuring a model’s ability to resolve tensions, integrate ideas, and produce higher‑order synthesis, SIEV uncovers substantial reasoning gaps in state‑of‑the‑art models that standard accuracy‑only benchmarks miss. Experiments on GSM and MMLU show that even top models lose significant points when evaluated with this process‑oriented metric.", "summary_cn": "本文提出 SIEV 框架，以辩证法的“正论‑反论‑合成”过程评估大型语言模型的推理，关注其在冲突解决、观点融合与高阶综合方面的能力，而非仅看最终答案的正确性。实验在 GSM 与 MMLU 基准上显示，先进模型在过程导向评价中会出现显著的推理缺失，分数下降超过 40 分。该研究展示了通过过程化、哲学化的评估方式，可更深入地辨别 LLM 推理能力的不足。", "keywords": "dialectics, reasoning evaluation, LLM reasoning, process-oriented assessment, SIEV framework, GSM benchmark, MMLU, interpretability, AI evaluation, chain-of-thought analysis", "scoring": {"interpretability": 6, "understanding": 8, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Soheil Abbasloo"]}
]]></acme>

<pubDate>2025-10-20T22:08:59+00:00</pubDate>
</item>
<item>
<title>SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning</title>
<link>https://papers.cool/arxiv/2510.18095</link>
<guid>https://papers.cool/arxiv/2510.18095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Select, Mix, and Reinvent (SMaRT) framework, which fuses multiple reasoning strategies for large language models by using the LLM itself to select, combine, and refine candidate solutions. Experiments on reasoning, planning, and sequential decision-making benchmarks show that SMaRT consistently improves solution quality, constraint satisfaction, and robustness compared to single-strategy baselines. This approach introduces a new paradigm of cross-strategy calibration for LLM-driven decision making.<br /><strong>Summary (CN):</strong> 本文提出了 Select、Mix、Reinvent（SMaRT）框架，通过让大语言模型自行选择、混合并重构不同推理策略的候选答案，实现策略的融合。 在推理、规划及序列决策等基准上，SMaRT 相较于单一策略基线显著提升了解答质量、约束满足度和鲁棒性。 此方法为 LLM 驱动的决策制定开辟了跨策略校准的新范式。<br /><strong>Keywords:</strong> strategy fusion, LLM reasoning, planning, multi-strategy prompting, robustness, decision-making, prompt engineering, chain-of-thought, self-refinement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness. We introduce the Select, Mix, and ReinvenT (SMaRT) framework, an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies. Unlike existing methods, which employ LLMs merely as evaluators, SMaRT uses them as intelligent integrators, unlocking the "best of all worlds" across tasks. Extensive empirical evaluations across benchmarks in reasoning, planning, and sequential decision-making highlight the robustness and adaptability of SMaRT. The framework consistently outperforms state-of-the-art baselines in solution quality, constraint adherence, and performance metrics. This work redefines LLM-driven decision-making by pioneering a new paradigm in cross-strategy calibration, unlocking superior outcomes for reasoning systems and advancing the boundaries of self-refining methodologies.
<div><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Select, Mix, and Reinvent (SMaRT) framework, which fuses multiple reasoning strategies for large language models by using the LLM itself to select, combine, and refine candidate solutions. Experiments on reasoning, planning, and sequential decision-making benchmarks show that SMaRT consistently improves solution quality, constraint satisfaction, and robustness compared to single-strategy baselines. This approach introduces a new paradigm of cross-strategy calibration for LLM-driven decision making.", "summary_cn": "本文提出了 Select、Mix、Reinvent（SMaRT）框架，通过让大语言模型自行选择、混合并重构不同推理策略的候选答案，实现策略的融合。 在推理、规划及序列决策等基准上，SMaRT 相较于单一策略基线显著提升了解答质量、约束满足度和鲁棒性。 此方法为 LLM 驱动的决策制定开辟了跨策略校准的新范式。", "keywords": "strategy fusion, LLM reasoning, planning, multi-strategy prompting, robustness, decision-making, prompt engineering, chain-of-thought, self-refinement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Nikhil Verma", "Manasa Bharadwaj", "Wonjun Jang", "Harmanpreet Singh", "Yixiao Wang", "Homa Fashandi", "Chul Lee"]}
]]></acme>

<pubDate>2025-10-20T20:42:24+00:00</pubDate>
</item>
<item>
<title>Planned Diffusion</title>
<link>https://papers.cool/arxiv/2510.18087</link>
<guid>https://papers.cool/arxiv/2510.18087</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Planned Diffusion, a hybrid inference method that first generates a short autoregressive plan to split a text output into independent spans, then generates those spans in parallel using a diffusion model. This two‑stage approach extends the speed‑quality Pareto frontier, achieving up to 1.81× faster generation on AlpacaEval with modest drops in win rate. Sensitivity analyses show the planning component is lightweight and provides controllable quality‑latency trade‑offs.<br /><strong>Summary (CN):</strong> 本文提出了“Planned Diffusion”，一种混合推理方法：先用自回归生成一个简短的计划，将文本划分为若干独立片段，再通过扩散模型并行生成这些片段。该两阶段方案拓宽了速度‑质量的Pareto前沿，在 AlpacaEval 上实现了最高1.81倍的加速并仅有轻微的胜率下降。敏感性分析表明计划机制轻量可靠，可通过简单的运行时参数灵活调节质量‑延迟权衡。<br /><strong>Keywords:</strong> planned diffusion, hybrid language model generation, autoregressive planning, diffusion decoding, speed-quality tradeoff, parallel token generation, AlpacaEval<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Daniel Israel, Tian Jin, Ellie Cheng, Guy Van den Broeck, Aditya Grover, Suvinay Subramanian, Michael Carbin</div>
A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produce high-quality text but generate tokens sequentially. Diffusion models can generate tokens in parallel but often need many iterations to match the same quality. We propose planned diffusion, a hybrid method that combines the strengths of both paradigms. Planned diffusion works in two stages: first, the model creates a short autoregressive plan that breaks the output into smaller, independent spans. Second, the model generates these spans simultaneously using diffusion. This approach expands the speed-quality Pareto frontier and provides a practical path to faster, high-quality text generation. On AlpacaEval, a suite of 805 instruction-following prompts, planned diffusion achieves Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win rate, respectively. Our sensitivity analysis shows that the planning mechanism of planned diffusion is minimal and reliable, and simple runtime knobs exist to provide flexible control of the quality-latency trade-off.
<div><strong>Authors:</strong> Daniel Israel, Tian Jin, Ellie Cheng, Guy Van den Broeck, Aditya Grover, Suvinay Subramanian, Michael Carbin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Planned Diffusion, a hybrid inference method that first generates a short autoregressive plan to split a text output into independent spans, then generates those spans in parallel using a diffusion model. This two‑stage approach extends the speed‑quality Pareto frontier, achieving up to 1.81× faster generation on AlpacaEval with modest drops in win rate. Sensitivity analyses show the planning component is lightweight and provides controllable quality‑latency trade‑offs.", "summary_cn": "本文提出了“Planned Diffusion”，一种混合推理方法：先用自回归生成一个简短的计划，将文本划分为若干独立片段，再通过扩散模型并行生成这些片段。该两阶段方案拓宽了速度‑质量的Pareto前沿，在 AlpacaEval 上实现了最高1.81倍的加速并仅有轻微的胜率下降。敏感性分析表明计划机制轻量可靠，可通过简单的运行时参数灵活调节质量‑延迟权衡。", "keywords": "planned diffusion, hybrid language model generation, autoregressive planning, diffusion decoding, speed-quality tradeoff, parallel token generation, AlpacaEval", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Daniel Israel", "Tian Jin", "Ellie Cheng", "Guy Van den Broeck", "Aditya Grover", "Suvinay Subramanian", "Michael Carbin"]}
]]></acme>

<pubDate>2025-10-20T20:27:48+00:00</pubDate>
</item>
<item>
<title>CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows</title>
<link>https://papers.cool/arxiv/2510.18043</link>
<guid>https://papers.cool/arxiv/2510.18043</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CompactPrompt is an end-to-end pipeline that compresses long prompts and attached documents for LLM agents by pruning low‑information tokens, grouping dependent phrases, applying n‑gram abbreviation to recurring text patterns, and uniformly quantizing numeric columns. The approach reduces token usage and inference cost by up to 60% on benchmarks such as TAT‑QA and FinQA while keeping accuracy drops below 5% for models like Claude‑3.5‑Sonnet and GPT‑4.1‑Mini. The paper also provides visualizations of compression decisions and a cost‑performance trade‑off analysis for leaner generative AI pipelines.<br /><strong>Summary (CN):</strong> CompactPrompt 是一种到端流水线，通过自信息得分剪枝低信息标记、基于依赖的短语分组、对文档中重复文本进行 n‑gram 缩写以及对数值列进行均匀量化，实现对 LLM 代理的长提示和附件数据的压缩。该方法在 TAT‑QA、FinQA 等基准上可将 token 使用量和推理成本降低最高 60%，同时对 Claude‑3.5‑Sonnet、GPT‑4.1‑Mini 等模型的准确率下降控制在 5% 以下。论文还提供了实时压缩决策的可视化以及成本‑性能权衡分析，以构建更精简的生成式 AI 流程。<br /><strong>Keywords:</strong> prompt compression, token reduction, LLM efficiency, self-information scoring, data quantization, n-gram abbreviation, agentic workflows, cost reduction, TAT-QA, FinQA<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Joong Ho Choi, Jiayang Zhao, Jeel Shah, Ritvika Sonawane, Vedant Singh, Avani Appalla, Will Flanagan, Filipe Condessa</div>
Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt, an end-to-end pipeline that merges hard prompt compression with lightweight file-level data compression. CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping. In parallel, it applies n-gram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns, yielding compact yet semantically faithful representations. Integrated into standard LLM agents, CompactPrompt reduces total token usage and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA, while preserving output quality (Results in less than 5% accuracy drop for Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time compression decisions and quantify cost-performance trade-offs, laying the groundwork for leaner generative AI pipelines.
<div><strong>Authors:</strong> Joong Ho Choi, Jiayang Zhao, Jeel Shah, Ritvika Sonawane, Vedant Singh, Avani Appalla, Will Flanagan, Filipe Condessa</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CompactPrompt is an end-to-end pipeline that compresses long prompts and attached documents for LLM agents by pruning low‑information tokens, grouping dependent phrases, applying n‑gram abbreviation to recurring text patterns, and uniformly quantizing numeric columns. The approach reduces token usage and inference cost by up to 60% on benchmarks such as TAT‑QA and FinQA while keeping accuracy drops below 5% for models like Claude‑3.5‑Sonnet and GPT‑4.1‑Mini. The paper also provides visualizations of compression decisions and a cost‑performance trade‑off analysis for leaner generative AI pipelines.", "summary_cn": "CompactPrompt 是一种到端流水线，通过自信息得分剪枝低信息标记、基于依赖的短语分组、对文档中重复文本进行 n‑gram 缩写以及对数值列进行均匀量化，实现对 LLM 代理的长提示和附件数据的压缩。该方法在 TAT‑QA、FinQA 等基准上可将 token 使用量和推理成本降低最高 60%，同时对 Claude‑3.5‑Sonnet、GPT‑4.1‑Mini 等模型的准确率下降控制在 5% 以下。论文还提供了实时压缩决策的可视化以及成本‑性能权衡分析，以构建更精简的生成式 AI 流程。", "keywords": "prompt compression, token reduction, LLM efficiency, self-information scoring, data quantization, n-gram abbreviation, agentic workflows, cost reduction, TAT-QA, FinQA", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Joong Ho Choi", "Jiayang Zhao", "Jeel Shah", "Ritvika Sonawane", "Vedant Singh", "Avani Appalla", "Will Flanagan", "Filipe Condessa"]}
]]></acme>

<pubDate>2025-10-20T19:31:11+00:00</pubDate>
</item>
<item>
<title>Subject-Event Ontology Without Global Time: Foundations and Execution Semantics</title>
<link>https://papers.cool/arxiv/2510.18040</link>
<guid>https://papers.cool/arxiv/2510.18040</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a subject-event ontology that models dynamic systems without relying on a global clock, defining causal order through explicit happens‑before dependencies and treating events as fixations of change according to a subject's epistemic models. Executable semantics are provided via a declarative dataflow mechanism, guaranteeing determinism, and the approach is demonstrated in the Boldsea workflow engine using the Boldsea Semantic Language. Applications to distributed architectures, microservices, DLT platforms, and scenarios with conflicting perspectives are discussed.<br /><strong>Summary (CN):</strong> 本文提出了一种主体‑事件本体，用于在不依赖全局时间的情况下建模复杂的动态系统，事件被视为主体根据其认知模型对变化的固定，因果顺序通过显式的先行关系（happens‑before）而非时间戳定义。通过声明式数据流机制实现可执行语义，确保确定性，并在 Boldsea 工作流引擎（使用 Boldsea Semantic Language）上进行演示。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突等场景。<br /><strong>Keywords:</strong> subject-event ontology, causal order, happens-before, declarative dataflow, epistemic filters, distributed systems, workflow engine, Boldsea Semantic Language, model validation, multiperspectivity<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexander Boldachev</div>
A formalization of a subject-event ontology is proposed for modeling complex dynamic systems without reliance on global time. Key principles: (1) event as an act of fixation - a subject discerns and fixes changes according to models (conceptual templates) available to them; (2) causal order via happens-before - the order of events is defined by explicit dependencies, not timestamps; (3) making the ontology executable via a declarative dataflow mechanism, ensuring determinism; (4) models as epistemic filters - a subject can only fix what falls under its known concepts and properties; (5) presumption of truth - the declarative content of an event is available for computation from the moment of fixation, without external verification. The formalization includes nine axioms (A1-A9), ensuring the correctness of executable ontologies: monotonicity of history (I1), acyclicity of causality (I2), traceability (I3). Special attention is given to the model-based approach (A9): event validation via schemas, actor authorization, automatic construction of causal chains (W3) without global time. Practical applicability is demonstrated on the boldsea system - a workflow engine for executable ontologies, where the theoretical constructs are implemented in BSL (Boldsea Semantic Language). The formalization is applicable to distributed systems, microservice architectures, DLT platforms, and multiperspectivity scenarios (conflicting facts from different subjects).
<div><strong>Authors:</strong> Alexander Boldachev</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a subject-event ontology that models dynamic systems without relying on a global clock, defining causal order through explicit happens‑before dependencies and treating events as fixations of change according to a subject's epistemic models. Executable semantics are provided via a declarative dataflow mechanism, guaranteeing determinism, and the approach is demonstrated in the Boldsea workflow engine using the Boldsea Semantic Language. Applications to distributed architectures, microservices, DLT platforms, and scenarios with conflicting perspectives are discussed.", "summary_cn": "本文提出了一种主体‑事件本体，用于在不依赖全局时间的情况下建模复杂的动态系统，事件被视为主体根据其认知模型对变化的固定，因果顺序通过显式的先行关系（happens‑before）而非时间戳定义。通过声明式数据流机制实现可执行语义，确保确定性，并在 Boldsea 工作流引擎（使用 Boldsea Semantic Language）上进行演示。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突等场景。", "keywords": "subject-event ontology, causal order, happens-before, declarative dataflow, epistemic filters, distributed systems, workflow engine, Boldsea Semantic Language, model validation, multiperspectivity", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexander Boldachev"]}
]]></acme>

<pubDate>2025-10-20T19:26:44+00:00</pubDate>
</item>
<item>
<title>OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning</title>
<link>https://papers.cool/arxiv/2510.18032</link>
<guid>https://papers.cool/arxiv/2510.18032</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces OPTAGENT, a verbal reinforcement learning framework that dynamically builds and refines collaboration structures among multiple LLM agents. By defining action spaces and a feedback mechanism that evaluates the robustness and coherence of agents' debates, the method selects the final answer via majority vote. Experiments on mathematical, scientific, creative, and sorting tasks show significant improvements over single‑agent prompting and existing multi‑agent baselines.<br /><strong>Summary (CN):</strong> 本文提出 OPTAGENT，一种口头强化学习框架，可动态构建并优化多个 LLM 代理的协作结构。通过定义动作空间和评估辩论鲁棒性与连贯性的反馈机制，最终答案由多数投票决定。实验在数学推理、科学推理、创意写作和数值排序等任务上表现出相较于单代理提示和现有多代理方法的显著提升。<br /><strong>Keywords:</strong> multi-agent LLM, verbal reinforcement learning, collaborative reasoning, debate quality, communication robustness, majority voting, reasoning augmentation, LLM coordination<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenyu Bi, Meng Lu, Yang Li, Swastik Roy, Weijie Guan, Morteza Ziyadi, Xuan Wang</div>
Large Language Models (LLMs) have shown remarkable reasoning capabilities in mathematical and scientific tasks. To enhance complex reasoning, multi-agent systems have been proposed to harness the collective intelligence of LLM agents. However, existing collaboration structures are either predefined or rely on majority voting or round-table debates, which can suppress correct but less dominant agent contributions. Recent approaches model multi-agent systems as graph networks but optimize purely for agent performance, neglecting the quality of interactions. We hypothesize that effective agent communication is crucial for multi-agent reasoning and that debating quality plays a significant role. To address this, we propose $\ours$, a multi-agent verbal reinforcement learning algorithm that dynamically constructs and refines multi-agent collaboration structures. Our method defines action spaces and a feedback mechanism that evaluates communication robustness and coherence throughout the debate. The final decision is achieved through a majority vote over all the agents. We assess $\ours$ on various reasoning tasks, including mathematical reasoning, creative writing, scientific reasoning, and numerical sorting. Results demonstrate that our approach significantly outperforms single-agent prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.
<div><strong>Authors:</strong> Zhenyu Bi, Meng Lu, Yang Li, Swastik Roy, Weijie Guan, Morteza Ziyadi, Xuan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces OPTAGENT, a verbal reinforcement learning framework that dynamically builds and refines collaboration structures among multiple LLM agents. By defining action spaces and a feedback mechanism that evaluates the robustness and coherence of agents' debates, the method selects the final answer via majority vote. Experiments on mathematical, scientific, creative, and sorting tasks show significant improvements over single‑agent prompting and existing multi‑agent baselines.", "summary_cn": "本文提出 OPTAGENT，一种口头强化学习框架，可动态构建并优化多个 LLM 代理的协作结构。通过定义动作空间和评估辩论鲁棒性与连贯性的反馈机制，最终答案由多数投票决定。实验在数学推理、科学推理、创意写作和数值排序等任务上表现出相较于单代理提示和现有多代理方法的显著提升。", "keywords": "multi-agent LLM, verbal reinforcement learning, collaborative reasoning, debate quality, communication robustness, majority voting, reasoning augmentation, LLM coordination", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenyu Bi", "Meng Lu", "Yang Li", "Swastik Roy", "Weijie Guan", "Morteza Ziyadi", "Xuan Wang"]}
]]></acme>

<pubDate>2025-10-20T19:07:51+00:00</pubDate>
</item>
<item>
<title>FABRIC: Framework for Agent-Based Realistic Intelligence Creation</title>
<link>https://papers.cool/arxiv/2510.17995</link>
<guid>https://papers.cool/arxiv/2510.17995</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FABRIC introduces a fully LLM‑only framework for generating high‑quality agentic interaction records, including task specifications, tool definitions, policy pseudocode, natural language dialogues, and execution traces, all validated by JSON schemas and judge‑based filtering. The approach enables scalable multi‑task and multi‑turn datasets that support the development of tool‑using LLM agents without costly human annotation.<br /><strong>Summary (CN):</strong> FABRIC 提出了一套完全基于大型语言模型的框架，用于合成高质量的智能体交互记录，覆盖任务描述、工具定义、策略伪代码、自然语言对话以及执行痕迹，并通过 JSON‑schema 验证和评审过滤确保数据准确。该方法支持多任务和多轮交互的数据规模化生成，帮助在无需大量人工标注的情况下推进具备工具使用能力的 LLM 智能体研发。<br /><strong>Keywords:</strong> synthetic agent data, tool-use LLMs, JSON-schema validation, prompt engineering, agentic interaction records, dataset generation, LLM agents, scalability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Abhigya Verma, Seganrasan Subramanian, Nandhakumar Kandasamy, Naman Gupta</div>
Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data- structured interaction records that couple user intents with tool specifications, argument-grounded calls, and verifiable execution traces. However, collecting such data from human annotators is costly, time-consuming, and difficult to scale. We present a unified framework for synthesizing agentic data using only LLMs, without any human-in-the-loop supervision. This framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications, tool definitions, policy pseudocode, natural language exchanges, and execution traces. Records conform to strict syntactic and semantic constraints, ensuring machine-parseability and faithful alignment across inputs, outputs, and tool calls. Beyond single tasks, there is support for both multi-task and multi-turn agent interactions, enabling the construction of datasets that reflect the full spectrum of tool-use competencies. To ensure quality and consistency, the framework integrates constrained generation formats, JSON-schema validation, and judge-based filtering. This paper formalizes the schema for agentic records, details the prompt design principles that guide generation, and introduces scalable pipelines for high-quality synthetic data. By providing a reproducible, LLM-only alternative to manual collection, hence advancing the development of agentic LLMs capable of robust tool use.
<div><strong>Authors:</strong> Abhigya Verma, Seganrasan Subramanian, Nandhakumar Kandasamy, Naman Gupta</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FABRIC introduces a fully LLM‑only framework for generating high‑quality agentic interaction records, including task specifications, tool definitions, policy pseudocode, natural language dialogues, and execution traces, all validated by JSON schemas and judge‑based filtering. The approach enables scalable multi‑task and multi‑turn datasets that support the development of tool‑using LLM agents without costly human annotation.", "summary_cn": "FABRIC 提出了一套完全基于大型语言模型的框架，用于合成高质量的智能体交互记录，覆盖任务描述、工具定义、策略伪代码、自然语言对话以及执行痕迹，并通过 JSON‑schema 验证和评审过滤确保数据准确。该方法支持多任务和多轮交互的数据规模化生成，帮助在无需大量人工标注的情况下推进具备工具使用能力的 LLM 智能体研发。", "keywords": "synthetic agent data, tool-use LLMs, JSON-schema validation, prompt engineering, agentic interaction records, dataset generation, LLM agents, scalability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Abhigya Verma", "Seganrasan Subramanian", "Nandhakumar Kandasamy", "Naman Gupta"]}
]]></acme>

<pubDate>2025-10-20T18:20:22+00:00</pubDate>
</item>
<item>
<title>Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding</title>
<link>https://papers.cool/arxiv/2510.17940</link>
<guid>https://papers.cool/arxiv/2510.17940</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether retrieval diversity, rather than simply providing more context, can improve multi-turn intent understanding for task-oriented chatbots under fixed token budgets. It introduces a diversity-aware retrieval framework that selects in-context exemplars to balance intent coverage and linguistic variety, showing consistent gains in Joint Goal Accuracy on MultiWOZ 2.4 and SGD compared to strong baselines. The study isolates the effect of content diversity and offers a simple, deployable selection principle for budget-constrained dialogue systems.<br /><strong>Summary (CN):</strong> 本文研究在固定 token 预算下，检索多样性是否能够比单纯增加上下文长度更好地提升多轮意图理解，针对任务导向聊天机器人进行实验。作者提出了一种多样性感知的检索框架，平衡意图覆盖和语言多样性，在 MultiWOZ 2.4 与 SGD 数据集上实现了 Joint Goal Accuracy 的显著提升。该工作阐明了内容多样性的作用，并提供了可部署的检索选择原则，以构建在预算受限情况下更准确的对话系统。<br /><strong>Keywords:</strong> retrieval diversity, multi-turn intent understanding, task-oriented dialogue, LLM prompting, joint goal accuracy, token budget, DST, MultiWOZ, SGD<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhiming Lin</div>
Multi turn intent understanding is central to task oriented chatbots, yet real deployments face tight token budgets and noisy contexts, and most retrieval pipelines emphasize relevance while overlooking set level diversity and confounds such as more context or exemplar order. We ask whether retrieval diversity, rather than longer prompts, systematically improves LLM intent understanding under fixed budgets. We present a diversity aware retrieval framework that selects in context exemplars to balance intent coverage and linguistic variety, and integrates this selection with standard LLM decoders; the evaluation enforces budget matched prompts and randomized positions, and includes sensitivity analyses over exemplar count, diversity strength, and backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST baselines, with consistent improvements across K from 4 to 7 and moderate latency. Overall, the study isolates and validates the impact of content diversity in retrieval and offers a simple, deployable selection principle for building accurate, budget constrained multi turn intent systems.
<div><strong>Authors:</strong> Zhiming Lin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether retrieval diversity, rather than simply providing more context, can improve multi-turn intent understanding for task-oriented chatbots under fixed token budgets. It introduces a diversity-aware retrieval framework that selects in-context exemplars to balance intent coverage and linguistic variety, showing consistent gains in Joint Goal Accuracy on MultiWOZ 2.4 and SGD compared to strong baselines. The study isolates the effect of content diversity and offers a simple, deployable selection principle for budget-constrained dialogue systems.", "summary_cn": "本文研究在固定 token 预算下，检索多样性是否能够比单纯增加上下文长度更好地提升多轮意图理解，针对任务导向聊天机器人进行实验。作者提出了一种多样性感知的检索框架，平衡意图覆盖和语言多样性，在 MultiWOZ 2.4 与 SGD 数据集上实现了 Joint Goal Accuracy 的显著提升。该工作阐明了内容多样性的作用，并提供了可部署的检索选择原则，以构建在预算受限情况下更准确的对话系统。", "keywords": "retrieval diversity, multi-turn intent understanding, task-oriented dialogue, LLM prompting, joint goal accuracy, token budget, DST, MultiWOZ, SGD", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhiming Lin"]}
]]></acme>

<pubDate>2025-10-20T16:54:35+00:00</pubDate>
</item>
<item>
<title>Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures</title>
<link>https://papers.cool/arxiv/2510.17902</link>
<guid>https://papers.cool/arxiv/2510.17902</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Cartridge Activation Space Transfer (CAST), a framework that learns bidirectional projection heads to map activation manifolds between different LLM architectures, enabling the transfer of frozen LoRA adapters without task‑specific data. By translating activations into the source model's latent space, applying the LoRA, and projecting back, CAST achieves 85‑95% of the performance of a fully retrained adapter and outperforms existing weight‑space transfer methods. Experiments across heterogeneous families such as Llama‑2 and Mistral demonstrate state‑of‑the‑art model interoperability.<br /><strong>Summary (CN):</strong> 本文提出了 Cartridge Activation Space Transfer（CAST）框架，通过学习双向投影头在不同 LLM 架构之间映射激活流形，从而在无需任务特定数据的情况下转移冻结的 LoRA 适配器。该方法把目标模型的激活映射到源模型的潜在空间，应用 LoRA 后再映射回去，实现了 85‑95% 的性能接近全新训练的适配器，并优于现有的权重空间转移技术。实验在 Llama‑2、Mistral 等异构模型家族上验证了其在模型互操作性方面的领先表现。<br /><strong>Keywords:</strong> activation manifold, LoRA, model transfer, CAST, LLM interoperability, activation projection, cross-architecture transfer, zero-shot adapter transfer<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Al Kari</div>
The proliferation of Large Language Model (LLM) architectures presents a fundamental challenge: valuable, task-specific behaviors learned through fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped within their source model's architecture, herein referred to architectural lock-in. Existing transfer methods attempt to bridge this gap by aligning the static weight spaces of models, a brittle and indirect approach that relies on tenuous correlations between parameter geometries. This paper introduces a fundamentally different and more direct paradigm: the Cartridge Activation Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors by learning a direct, nonlinear mapping between the activation manifolds, the geometric structures formed by the model's internal neuron activations, of two distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen "behavioral kernel." It learns a set of lightweight, bidirectional projection heads that translate the target model's activation stream into the source model's latent space, apply the frozen kernel, and project the result back. This process, trained on a general text corpus without any task-specific data, effectively decouples the learned skill from the source architecture. We demonstrate that CAST enables true "zero-shot" translation of any standard LoRA adapter. Our experiments, including transfers between heterogeneous model families like Llama-2 and Mistral, show that CAST-translated adapters achieve 85-95\% of the performance of a LoRA fully retrained on the target model, quantitatively outperforming current weight-space transfer techniques and establishing a new state-of-the-art in model interoperability.
<div><strong>Authors:</strong> Al Kari</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Cartridge Activation Space Transfer (CAST), a framework that learns bidirectional projection heads to map activation manifolds between different LLM architectures, enabling the transfer of frozen LoRA adapters without task‑specific data. By translating activations into the source model's latent space, applying the LoRA, and projecting back, CAST achieves 85‑95% of the performance of a fully retrained adapter and outperforms existing weight‑space transfer methods. Experiments across heterogeneous families such as Llama‑2 and Mistral demonstrate state‑of‑the‑art model interoperability.", "summary_cn": "本文提出了 Cartridge Activation Space Transfer（CAST）框架，通过学习双向投影头在不同 LLM 架构之间映射激活流形，从而在无需任务特定数据的情况下转移冻结的 LoRA 适配器。该方法把目标模型的激活映射到源模型的潜在空间，应用 LoRA 后再映射回去，实现了 85‑95% 的性能接近全新训练的适配器，并优于现有的权重空间转移技术。实验在 Llama‑2、Mistral 等异构模型家族上验证了其在模型互操作性方面的领先表现。", "keywords": "activation manifold, LoRA, model transfer, CAST, LLM interoperability, activation projection, cross-architecture transfer, zero-shot adapter transfer", "scoring": {"interpretability": 4, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Al Kari"]}
]]></acme>

<pubDate>2025-10-19T10:55:05+00:00</pubDate>
</item>
<item>
<title>Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18876</link>
<guid>https://papers.cool/arxiv/2510.18876</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Grasp Any Region (GAR), a multimodal large language model architecture that leverages a RoI-aligned feature replay technique to achieve precise, context-aware pixel-level understanding of arbitrary image regions. GAR integrates global context and multi-prompt interactions, enabling advanced compositional reasoning for free-form region queries, and is evaluated on the newly proposed GAR-Bench which measures single- and multi-region comprehension. Experiments show GAR-1B surpasses state-of-the-art captioning models and demonstrates strong zero-shot transfer to video tasks.<br /><strong>Summary (CN):</strong> 本文提出了 Grasp Any Region (GAR) 模型，利用 RoI 对齐特征回放技术实现对任意图像区域的精确、上下文感知的像素级理解。GAR 融合全局信息和多提示交互，支持对区域的自由形式提问并进行高级组合推理，并通过新构建的 GAR-Bench 对单区域和多区域理解能力进行评估。实验表明，GAR-1B 在字幕生成和视频任务上均超越了当前最先进的模型。<br /><strong>Keywords:</strong> multimodal LLM, region-level understanding, RoI-aligned feature replay, pixel-level perception, compositional reasoning, GAR-Bench<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.
<div><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Grasp Any Region (GAR), a multimodal large language model architecture that leverages a RoI-aligned feature replay technique to achieve precise, context-aware pixel-level understanding of arbitrary image regions. GAR integrates global context and multi-prompt interactions, enabling advanced compositional reasoning for free-form region queries, and is evaluated on the newly proposed GAR-Bench which measures single- and multi-region comprehension. Experiments show GAR-1B surpasses state-of-the-art captioning models and demonstrates strong zero-shot transfer to video tasks.", "summary_cn": "本文提出了 Grasp Any Region (GAR) 模型，利用 RoI 对齐特征回放技术实现对任意图像区域的精确、上下文感知的像素级理解。GAR 融合全局信息和多提示交互，支持对区域的自由形式提问并进行高级组合推理，并通过新构建的 GAR-Bench 对单区域和多区域理解能力进行评估。实验表明，GAR-1B 在字幕生成和视频任务上均超越了当前最先进的模型。", "keywords": "multimodal LLM, region-level understanding, RoI-aligned feature replay, pixel-level perception, compositional reasoning, GAR-Bench", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:59:59+00:00</pubDate>
</item>
<item>
<title>How Do LLMs Use Their Depth?</title>
<link>https://papers.cool/arxiv/2510.18871</link>
<guid>https://papers.cool/arxiv/2510.18871</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a "Guess-then-Refine" framework to explain how large language models allocate computational depth, showing that early layers produce high‑frequency token guesses that are refined into context‑appropriate predictions in deeper layers. Through case studies on part‑of‑speech ordering, multi‑token fact recall, and multiple‑choice formatting, the authors demonstrate systematic depth utilization patterns and suggest avenues for improving transformer efficiency.<br /><strong>Summary (CN):</strong> 本文提出了“猜测‑再细化”框架，解释大语言模型如何在不同层次使用深度，即早期层主要生成高频词的统计猜测，随后在更深层通过上下文信息对这些猜测进行细化。通过词性预测、事实回忆多词答案以及多选题格式识别三个案例研究，展示了层深使用的系统性模式，并为提升 Transformer 效率提供了启示。<br /><strong>Keywords:</strong> layer-wise analysis, depth usage, guess-then-refine, token frequency, mechanistic interpretability, transformer dynamics, computational efficiency<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not "one-and-done". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.
<div><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a \"Guess-then-Refine\" framework to explain how large language models allocate computational depth, showing that early layers produce high‑frequency token guesses that are refined into context‑appropriate predictions in deeper layers. Through case studies on part‑of‑speech ordering, multi‑token fact recall, and multiple‑choice formatting, the authors demonstrate systematic depth utilization patterns and suggest avenues for improving transformer efficiency.", "summary_cn": "本文提出了“猜测‑再细化”框架，解释大语言模型如何在不同层次使用深度，即早期层主要生成高频词的统计猜测，随后在更深层通过上下文信息对这些猜测进行细化。通过词性预测、事实回忆多词答案以及多选题格式识别三个案例研究，展示了层深使用的系统性模式，并为提升 Transformer 效率提供了启示。", "keywords": "layer-wise analysis, depth usage, guess-then-refine, token frequency, mechanistic interpretability, transformer dynamics, computational efficiency", "scoring": {"interpretability": 8, "understanding": 8, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"]}
]]></acme>

<pubDate>2025-10-21T17:59:05+00:00</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18866</link>
<guid>https://papers.cool/arxiv/2510.18866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LightMem introduces a three-stage memory system for large language models, inspired by the Atkinson-Shiffrin model, comprising a sensory memory that rapidly filters and compresses information, a topic-aware short-term memory that consolidates and summarizes content, and an offline long-term memory that updates during sleep-time. By decoupling consolidation from online inference, LightMem achieves up to 10.9% accuracy improvements on LongMemEval while reducing token usage by up to 117×, API calls by up to 159×, and runtime by over 12× compared to strong baselines.<br /><strong>Summary (CN):</strong> LightMem 提出了一种受 Atkinson-Shiffrin 人类记忆模型启发的三阶段记忆系统，用于大语言模型，包括快速过滤和压缩信息的感官记忆、面向主题的短期记忆用于整合和摘要内容，以及在离线“睡眠”阶段更新的长期记忆。该系统将整合过程与在线推理解耦，在 LongMemEval 基准上实现了最高 10.9% 的准确率提升，同时将 token 使用量降低至原来的 1/117、API 调用降低至 1/159，运行时间缩短超过 12 倍。<br /><strong>Keywords:</strong> memory-augmented LLM, lightweight memory, topic-aware short-term memory, long-term memory consolidation, Atkinson-Shiffrin model, token efficiency, inference speed<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
<div><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LightMem introduces a three-stage memory system for large language models, inspired by the Atkinson-Shiffrin model, comprising a sensory memory that rapidly filters and compresses information, a topic-aware short-term memory that consolidates and summarizes content, and an offline long-term memory that updates during sleep-time. By decoupling consolidation from online inference, LightMem achieves up to 10.9% accuracy improvements on LongMemEval while reducing token usage by up to 117×, API calls by up to 159×, and runtime by over 12× compared to strong baselines.", "summary_cn": "LightMem 提出了一种受 Atkinson-Shiffrin 人类记忆模型启发的三阶段记忆系统，用于大语言模型，包括快速过滤和压缩信息的感官记忆、面向主题的短期记忆用于整合和摘要内容，以及在离线“睡眠”阶段更新的长期记忆。该系统将整合过程与在线推理解耦，在 LongMemEval 基准上实现了最高 10.9% 的准确率提升，同时将 token 使用量降低至原来的 1/117、API 调用降低至 1/159，运行时间缩短超过 12 倍。", "keywords": "memory-augmented LLM, lightweight memory, topic-aware short-term memory, long-term memory consolidation, Atkinson-Shiffrin model, token efficiency, inference speed", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:58:17+00:00</pubDate>
</item>
<item>
<title>Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model</title>
<link>https://papers.cool/arxiv/2510.18855</link>
<guid>https://papers.cool/arxiv/2510.18855</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level RL stability, C3PO++ for efficient long rollouts, and ASystem for high‑throughput RL training. These contributions address train‑inference misalignment, rollout inefficiencies, and system bottlenecks, enabling state‑of‑the‑art performance on high‑level reasoning benchmarks such as AIME, HMMT, CodeForces, ARC‑AGI, and IMO. The authors release the full MoE model to democratize access to large‑scale reasoning capabilities.<br /><strong>Summary (CN):</strong> 本文提出了 Ring-1T，一个开放源代码的万亿参数思考模型（每个 token 激活约 500 亿参数），并推出三项关键技术创新：IcePop 通过 token 级差异掩码与裁剪稳定强化学习训练，C3PO++ 动态拆分长 rollout 以提升资源利用率，ASystem 高性能 RL 框架解决系统瓶颈。该模型在 AIME、HMMT、CodeForces、ARC‑AGI 和 IMO 等高阶推理基准上取得领先成绩，并开放完整的 MoE 权重以促进研究社区使用。<br /><strong>Keywords:</strong> trillion-parameter model, reinforcement learning scaling, MoE, IcePop, C3PO++, ASystem, reasoning benchmarks, token-level masking, rollout efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.
<div><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level RL stability, C3PO++ for efficient long rollouts, and ASystem for high‑throughput RL training. These contributions address train‑inference misalignment, rollout inefficiencies, and system bottlenecks, enabling state‑of‑the‑art performance on high‑level reasoning benchmarks such as AIME, HMMT, CodeForces, ARC‑AGI, and IMO. The authors release the full MoE model to democratize access to large‑scale reasoning capabilities.", "summary_cn": "本文提出了 Ring-1T，一个开放源代码的万亿参数思考模型（每个 token 激活约 500 亿参数），并推出三项关键技术创新：IcePop 通过 token 级差异掩码与裁剪稳定强化学习训练，C3PO++ 动态拆分长 rollout 以提升资源利用率，ASystem 高性能 RL 框架解决系统瓶颈。该模型在 AIME、HMMT、CodeForces、ARC‑AGI 和 IMO 等高阶推理基准上取得领先成绩，并开放完整的 MoE 权重以促进研究社区使用。", "keywords": "trillion-parameter model, reinforcement learning scaling, MoE, IcePop, C3PO++, ASystem, reasoning benchmarks, token-level masking, rollout efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Team", "Anqi Shen", "Baihui Li", "Bin Hu", "Bin Jing", "Cai Chen", "Chao Huang", "Chao Zhang", "Chaokun Yang", "Cheng Lin", "Chengyao Wen", "Congqi Li", "Deng Zhao", "Dingbo Yuan", "Donghai You", "Fagui Mao", "Fanzhuang Meng", "Feng Xu", "Guojie Li", "Guowei Wang", "Hao Dai", "Haonan Zheng", "Hong Liu", "Jia Guo", "Jiaming Liu", "Jian Liu", "Jianhao Fu", "Jiannan Shi", "Jianwen Wang", "Jianxin Lai", "Jin Yang", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junping Zhao", "Kuan Xu", "Le Su", "Lei Chen", "Li Tang", "Liang Jiang", "Liangcheng Fu", "Lianhao Xu", "Linfeng Shi", "Lisha Liao", "Longfei Zheng", "Meng Li", "Mingchun Chen", "Qi Zuo", "Qiang Cheng", "Qianggang Cao", "Qitao Shi", "Quanrui Guo", "Senlin Zhu", "Shaofei Wang", "Shaomian Zheng", "Shuaicheng Li", "Shuwei Gu", "Siba Chen", "Tao Wu", "Tao Zhang", "Tianyu Zhang", "Tianyu Zhou", "Tiwei Bie", "Tongkai Yang", "Wang Hong", "Wang Ren", "Weihua Chen", "Wenbo Yu", "Wengang Zheng", "Xiangchun Wang", "Xiaodong Yan", "Xiaopei Wan", "Xin Zhao", "Xinyu Kong", "Xinyu Tang", "Xudong Han", "Xudong Wang", "Xuemin Yang", "Xueyu Hu", "Yalin Zhang", "Yan Sun", "Yicheng Shan", "Yilong Wang", "Yingying Xu", "Yongkang Liu", "Yongzhen Guo", "Yuanyuan Wang", "Yuchen Yan", "Yuefan Wang", "Yuhong Guo", "Zehuan Li", "Zhankai Xu", "Zhe Li", "Zhenduo Zhang", "Zhengke Gui", "Zhenxuan Pan", "Zhenyu Huang", "Zhenzhong Lan", "Zhiqiang Ding", "Zhiqiang Zhang", "Zhixun Li", "Zhizhen Liu", "Zihao Wang", "Zujie Wen"]}
]]></acme>

<pubDate>2025-10-21T17:46:14+00:00</pubDate>
</item>
<item>
<title>Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study</title>
<link>https://papers.cool/arxiv/2510.18852</link>
<guid>https://papers.cool/arxiv/2510.18852</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Lyapunov‑aware Quantum Reinforcement Learning (LQRL) framework that couples variational quantum circuits with Lyapunov stability constraints to train policies for continuous‑time vehicle longitudinal control. Simulation experiments in an adaptive cruise‑control scenario demonstrate that the quantum‑inspired policy respects stability guarantees and maintains bounded state evolution, despite occasional transient overshoot under aggressive acceleration. The study shows the feasibility of embedding provable safety assurances within quantum‑inspired reinforcement learning for autonomous systems.<br /><strong>Summary (CN):</strong> 本文提出了 Lyapunov 感知的量子强化学习（LQRL）框架，将可变形量子电路与 Lyapunov 稳定性约束相结合，用于连续时间车辆纵向控制的策略学习。 在自适应巡航控制仿真中，量子启发的策略能够满足稳定性保证并保持状态有界，尽管在激进加速时出现瞬时超调。 该研究展示了在量子‑经典混合控制中嵌入可证明安全性的方法可行性。<br /><strong>Keywords:</strong> Lyapunov stability, quantum reinforcement learning, variational quantum circuits, continuous-time control, autonomous vehicles, safety-aware RL, policy gradient, quantum-inspired control<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Nutkritta Kraipatthanapong, Natthaphat Thathong, Pannita Suksawas, Thanunnut Klunklin, Kritin Vongthonglua, Krit Attahakul, Aueaphum Aueawatthanaphisut</div>
This paper presents a novel Lyapunov-Based Quantum Reinforcement Learning (LQRL) framework that integrates quantum policy optimization with Lyapunov stability analysis for continuous-time vehicle control. The proposed approach combines the representational power of variational quantum circuits (VQCs) with a stability-aware policy gradient mechanism to ensure asymptotic convergence and safe decision-making under dynamic environments. The vehicle longitudinal control problem was formulated as a continuous-state reinforcement learning task, where the quantum policy network generates control actions subject to Lyapunov stability constraints. Simulation experiments were conducted in a closed-loop adaptive cruise control scenario using a quantum-inspired policy trained under stability feedback. The results demonstrate that the LQRL framework successfully embeds Lyapunov stability verification into quantum policy learning, enabling interpretable and stability-aware control performance. Although transient overshoot and Lyapunov divergence were observed under aggressive acceleration, the system maintained bounded state evolution, validating the feasibility of integrating safety guarantees within quantum reinforcement learning architectures. The proposed framework provides a foundational step toward provably safe quantum control in autonomous systems and hybrid quantum-classical optimization domains.
<div><strong>Authors:</strong> Nutkritta Kraipatthanapong, Natthaphat Thathong, Pannita Suksawas, Thanunnut Klunklin, Kritin Vongthonglua, Krit Attahakul, Aueaphum Aueawatthanaphisut</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Lyapunov‑aware Quantum Reinforcement Learning (LQRL) framework that couples variational quantum circuits with Lyapunov stability constraints to train policies for continuous‑time vehicle longitudinal control. Simulation experiments in an adaptive cruise‑control scenario demonstrate that the quantum‑inspired policy respects stability guarantees and maintains bounded state evolution, despite occasional transient overshoot under aggressive acceleration. The study shows the feasibility of embedding provable safety assurances within quantum‑inspired reinforcement learning for autonomous systems.", "summary_cn": "本文提出了 Lyapunov 感知的量子强化学习（LQRL）框架，将可变形量子电路与 Lyapunov 稳定性约束相结合，用于连续时间车辆纵向控制的策略学习。 在自适应巡航控制仿真中，量子启发的策略能够满足稳定性保证并保持状态有界，尽管在激进加速时出现瞬时超调。 该研究展示了在量子‑经典混合控制中嵌入可证明安全性的方法可行性。", "keywords": "Lyapunov stability, quantum reinforcement learning, variational quantum circuits, continuous-time control, autonomous vehicles, safety-aware RL, policy gradient, quantum-inspired control", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Nutkritta Kraipatthanapong", "Natthaphat Thathong", "Pannita Suksawas", "Thanunnut Klunklin", "Kritin Vongthonglua", "Krit Attahakul", "Aueaphum Aueawatthanaphisut"]}
]]></acme>

<pubDate>2025-10-21T17:44:45+00:00</pubDate>
</item>
<item>
<title>DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</title>
<link>https://papers.cool/arxiv/2510.18851</link>
<guid>https://papers.cool/arxiv/2510.18851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DP⁲O-SR introduces a framework for real-world image super-resolution that directly optimizes perceptual preferences by leveraging pre-trained text-to-image diffusion models and a hybrid reward combining full-reference and no-reference IQA metrics. It constructs multiple preference pairs from the same model to exploit perceptual diversity and employs hierarchical preference optimization that adapts pair weighting based on reward gaps and diversity, leading to improved perceptual quality across diffusion and flow backbones.<br /><strong>Summary (CN):</strong> DP⁲O-SR 提出了一种针对真实场景图像超分辨率的框架，通过利用预训练的文本到图像（T2I）扩散模型，并结合全参考和无参考图像质量评估（IQA）模型构建混合奖励，直接优化感知偏好。该方法从同一模型的多个输出中构建偏好对，利用感知多样性，并通过分层偏好优化根据组内奖励差距和组间多样性自适应加权，显著提升了感知质量。<br /><strong>Keywords:</strong> real-world super-resolution, diffusion models, perceptual preference optimization, image quality assessment, hierarchical preference optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</div>
Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.
<div><strong>Authors:</strong> Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DP⁲O-SR introduces a framework for real-world image super-resolution that directly optimizes perceptual preferences by leveraging pre-trained text-to-image diffusion models and a hybrid reward combining full-reference and no-reference IQA metrics. It constructs multiple preference pairs from the same model to exploit perceptual diversity and employs hierarchical preference optimization that adapts pair weighting based on reward gaps and diversity, leading to improved perceptual quality across diffusion and flow backbones.", "summary_cn": "DP⁲O-SR 提出了一种针对真实场景图像超分辨率的框架，通过利用预训练的文本到图像（T2I）扩散模型，并结合全参考和无参考图像质量评估（IQA）模型构建混合奖励，直接优化感知偏好。该方法从同一模型的多个输出中构建偏好对，利用感知多样性，并通过分层偏好优化根据组内奖励差距和组间多样性自适应加权，显著提升了感知质量。", "keywords": "real-world super-resolution, diffusion models, perceptual preference optimization, image quality assessment, hierarchical preference optimization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rongyuan Wu", "Lingchen Sun", "Zhengqiang Zhang", "Shihao Wang", "Tianhe Wu", "Qiaosi Yi", "Shuai Li", "Lei Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:43:23+00:00</pubDate>
</item>
<item>
<title>Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18849</link>
<guid>https://papers.cool/arxiv/2510.18849</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Critique-Post-Edit, a reinforcement learning framework that uses a personalized generative reward model producing multi-dimensional scores and textual critiques, allowing the policy to revise its own outputs. This approach mitigates reward hacking and improves faithful, controllable personalization of large language models, achieving significant gains over standard PPO on benchmark tasks.<br /><strong>Summary (CN):</strong> 本文提出了Critique-Post-Edit强化学习框架，利用个性化生成奖励模型提供多维评分和文本批评，使策略模型能够根据批评自行修正输出，从而抑制奖励黑客并实现更忠实、可控的个性化。实验表明该方法在个性化基准上显著超越标准PPO，Qwen2.5-7B提升约11%胜率，Qwen2.5-14B甚至超过GPT-4.1。<br /><strong>Keywords:</strong> personalization, reinforcement learning, generative reward model, critique-post-edit, reward hacking, LLM alignment, controllable generation, PPO, Qwen2.5, RLHF<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.
<div><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Critique-Post-Edit, a reinforcement learning framework that uses a personalized generative reward model producing multi-dimensional scores and textual critiques, allowing the policy to revise its own outputs. This approach mitigates reward hacking and improves faithful, controllable personalization of large language models, achieving significant gains over standard PPO on benchmark tasks.", "summary_cn": "本文提出了Critique-Post-Edit强化学习框架，利用个性化生成奖励模型提供多维评分和文本批评，使策略模型能够根据批评自行修正输出，从而抑制奖励黑客并实现更忠实、可控的个性化。实验表明该方法在个性化基准上显著超越标准PPO，Qwen2.5-7B提升约11%胜率，Qwen2.5-14B甚至超过GPT-4.1。", "keywords": "personalization, reinforcement learning, generative reward model, critique-post-edit, reward hacking, LLM alignment, controllable generation, PPO, Qwen2.5, RLHF", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Chenghao Zhu", "Meiling Tao", "Tiannan Wang", "Dongyi Ding", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"]}
]]></acme>

<pubDate>2025-10-21T17:40:03+00:00</pubDate>
</item>
<item>
<title>Actor-Free Continuous Control via Structurally Maximizable Q-Functions</title>
<link>https://papers.cool/arxiv/2510.18828</link>
<guid>https://papers.cool/arxiv/2510.18828</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a purely value‑based framework for continuous‑action reinforcement learning by introducing structurally maximizable Q‑functions, eliminating the need for a separate actor network. Architectural and algorithmic innovations enable efficient maximization of Q in continuous spaces, achieving performance and sample efficiency comparable to state‑of‑the‑art actor‑critic methods, especially in environments with constrained, non‑smooth action spaces. Code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出一种纯价值函数的连续控制强化学习框架，通过结构化可最大化的 Q 函数（structurally maximizable Q‑functions）实现对动作空间的直接最大化，从而无需额外的 actor 网络。针对连续空间的架构与算法改进提升了计算效率，在多项标准仿真任务中达到了与最先进的 actor‑critic 方法相当的性能和样本效率，尤其在动作空间受限且价值函数非平滑的环境下表现更佳。代码已公开发布。<br /><strong>Keywords:</strong> continuous control, value-based reinforcement learning, Q-learning, actor-free, structural maximization, off-policy learning, sample efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yigit Korkmaz, Urvi Bhuwania, Ayush Jain, Erdem Bıyık</div>
Value-based algorithms are a cornerstone of off-policy reinforcement learning due to their simplicity and training stability. However, their use has traditionally been restricted to discrete action spaces, as they rely on estimating Q-values for individual state-action pairs. In continuous action spaces, evaluating the Q-value over the entire action space becomes computationally infeasible. To address this, actor-critic methods are typically employed, where a critic is trained on off-policy data to estimate Q-values, and an actor is trained to maximize the critic's output. Despite their popularity, these methods often suffer from instability during training. In this work, we propose a purely value-based framework for continuous control that revisits structural maximization of Q-functions, introducing a set of key architectural and algorithmic choices to enable efficient and stable learning. We evaluate the proposed actor-free Q-learning approach on a range of standard simulation tasks, demonstrating performance and sample efficiency on par with state-of-the-art baselines, without the cost of learning a separate actor. Particularly, in environments with constrained action spaces, where the value functions are typically non-smooth, our method with structural maximization outperforms traditional actor-critic methods with gradient-based maximization. We have released our code at https://github.com/USC-Lira/Q3C.
<div><strong>Authors:</strong> Yigit Korkmaz, Urvi Bhuwania, Ayush Jain, Erdem Bıyık</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a purely value‑based framework for continuous‑action reinforcement learning by introducing structurally maximizable Q‑functions, eliminating the need for a separate actor network. Architectural and algorithmic innovations enable efficient maximization of Q in continuous spaces, achieving performance and sample efficiency comparable to state‑of‑the‑art actor‑critic methods, especially in environments with constrained, non‑smooth action spaces. Code is released publicly.", "summary_cn": "本文提出一种纯价值函数的连续控制强化学习框架，通过结构化可最大化的 Q 函数（structurally maximizable Q‑functions）实现对动作空间的直接最大化，从而无需额外的 actor 网络。针对连续空间的架构与算法改进提升了计算效率，在多项标准仿真任务中达到了与最先进的 actor‑critic 方法相当的性能和样本效率，尤其在动作空间受限且价值函数非平滑的环境下表现更佳。代码已公开发布。", "keywords": "continuous control, value-based reinforcement learning, Q-learning, actor-free, structural maximization, off-policy learning, sample efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yigit Korkmaz", "Urvi Bhuwania", "Ayush Jain", "Erdem Bıyık"]}
]]></acme>

<pubDate>2025-10-21T17:24:27+00:00</pubDate>
</item>
<item>
<title>An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection</title>
<link>https://papers.cool/arxiv/2510.18819</link>
<guid>https://papers.cool/arxiv/2510.18819</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a teacher–student hybrid AI framework that combines two supervised heads and a self‑supervised head to jointly detect tuberculosis, COVID‑19 and normal chest X‑rays and to predict multiple symptoms. The model attains 98.85 % accuracy on the three‑class disease classification and a macro‑F1 of 90.09 % on symptom detection, outperforming baseline methods, and its explainability analysis shows that predictions rely on relevant anatomical regions.<br /><strong>Summary (CN):</strong> 本文提出一种师生混合 AI 框架，融合两个监督头和一个自监督头，实现胸片中结核病、COVID‑19 与正常病例的联合检测以及多标签症状预测。模型在三分类疾病识别上达到 98.85% 的准确率，在症状检测上获得 90.09% 的观 F1，显著优于基线，并通过可解释性评估证明预测基于相关解剖特征。<br /><strong>Keywords:</strong> teacher-student, hybrid AI, chest X-ray, tuberculosis detection, COVID-19 detection, explainable AI, multi-label symptom detection, self-supervised learning<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Neel Patel, Alexander Wong, Ashkan Ebadi</div>
Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.
<div><strong>Authors:</strong> Neel Patel, Alexander Wong, Ashkan Ebadi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a teacher–student hybrid AI framework that combines two supervised heads and a self‑supervised head to jointly detect tuberculosis, COVID‑19 and normal chest X‑rays and to predict multiple symptoms. The model attains 98.85 % accuracy on the three‑class disease classification and a macro‑F1 of 90.09 % on symptom detection, outperforming baseline methods, and its explainability analysis shows that predictions rely on relevant anatomical regions.", "summary_cn": "本文提出一种师生混合 AI 框架，融合两个监督头和一个自监督头，实现胸片中结核病、COVID‑19 与正常病例的联合检测以及多标签症状预测。模型在三分类疾病识别上达到 98.85% 的准确率，在症状检测上获得 90.09% 的观 F1，显著优于基线，并通过可解释性评估证明预测基于相关解剖特征。", "keywords": "teacher-student, hybrid AI, chest X-ray, tuberculosis detection, COVID-19 detection, explainable AI, multi-label symptom detection, self-supervised learning", "scoring": {"interpretability": 6, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Neel Patel", "Alexander Wong", "Ashkan Ebadi"]}
]]></acme>

<pubDate>2025-10-21T17:18:55+00:00</pubDate>
</item>
<item>
<title>Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</title>
<link>https://papers.cool/arxiv/2510.18817</link>
<guid>https://papers.cool/arxiv/2510.18817</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a knowledge distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑fine‑tuned models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the fine‑tuned small models achieve significantly better reasoning performance, narrowing the gap to their LLM counterparts. The approach is evaluated on benchmark tasks and the code is released publicly.<br /><strong>Summary (CN):</strong> 本文提出一种知识蒸馏框架，将大语言模型的链式思考（Chain‑of‑Thought）推理迁移到小型、针对工业资产健康监测进行微调的模型。通过多项选择题（MCQA）提示和上下学习，蒸馏后的模型在推理能力上显著提升，接近大型模型的表现。实验验证了该方法的有效性，代码已开源。<br /><strong>Keywords:</strong> chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning, SLM, LLM<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: https://github.com/IBM/FailureSensorIQ.
<div><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a knowledge distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑fine‑tuned models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the fine‑tuned small models achieve significantly better reasoning performance, narrowing the gap to their LLM counterparts. The approach is evaluated on benchmark tasks and the code is released publicly.", "summary_cn": "本文提出一种知识蒸馏框架，将大语言模型的链式思考（Chain‑of‑Thought）推理迁移到小型、针对工业资产健康监测进行微调的模型。通过多项选择题（MCQA）提示和上下学习，蒸馏后的模型在推理能力上显著提升，接近大型模型的表现。实验验证了该方法的有效性，代码已开源。", "keywords": "chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning, SLM, LLM", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuxin Lin", "Dhaval Patel", "Christodoulos Constantinides"]}
]]></acme>

<pubDate>2025-10-21T17:18:24+00:00</pubDate>
</item>
<item>
<title>Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards</title>
<link>https://papers.cool/arxiv/2510.18814</link>
<guid>https://papers.cool/arxiv/2510.18814</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Online Supervised Finetuning (OSFT), a simple reward‑free paradigm where a language model generates its own responses and is immediately finetuned on this self‑generated data to improve reasoning. Experiments on challenging mathematical benchmarks show that OSFT attains performance comparable to strong reinforcement‑learning‑with‑verifiable‑rewards methods while using only a single rollout per example. An ablation study attributes the gains to leveraging the model’s latent preferences learned during pretraining.<br /><strong>Summary (CN):</strong> 本文提出在线监督微调（OSFT）范式，在该范式中，大语言模型自行生成答案并立即基于这些自生成数据进行微调，从而提升推理能力。实验在具有挑战性的数学推理基准上显示，OSFT 在仅使用一次采样的情况下，能够达到与强大的可验证奖励强化学习（RLVR）方法相当的性能。消融研究表明，提升主要来源于利用模型预训练期间形成的潜在偏好（latent knowledge）。<br /><strong>Keywords:</strong> online supervised finetuning, self‑tuning, LLM reasoning, reward‑free training, mathematical reasoning, RLVR, efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</div>
We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the model's own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at https://github.com/ElementQi/OnlineSFT.
<div><strong>Authors:</strong> Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Online Supervised Finetuning (OSFT), a simple reward‑free paradigm where a language model generates its own responses and is immediately finetuned on this self‑generated data to improve reasoning. Experiments on challenging mathematical benchmarks show that OSFT attains performance comparable to strong reinforcement‑learning‑with‑verifiable‑rewards methods while using only a single rollout per example. An ablation study attributes the gains to leveraging the model’s latent preferences learned during pretraining.", "summary_cn": "本文提出在线监督微调（OSFT）范式，在该范式中，大语言模型自行生成答案并立即基于这些自生成数据进行微调，从而提升推理能力。实验在具有挑战性的数学推理基准上显示，OSFT 在仅使用一次采样的情况下，能够达到与强大的可验证奖励强化学习（RLVR）方法相当的性能。消融研究表明，提升主要来源于利用模型预训练期间形成的潜在偏好（latent knowledge）。", "keywords": "online supervised finetuning, self‑tuning, LLM reasoning, reward‑free training, mathematical reasoning, RLVR, efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mengqi Li", "Lei Zhao", "Anthony Man-Cho So", "Ruoyu Sun", "Xiao Li"]}
]]></acme>

<pubDate>2025-10-21T17:15:56+00:00</pubDate>
</item>
<item>
<title>Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity</title>
<link>https://papers.cool/arxiv/2510.18802</link>
<guid>https://papers.cool/arxiv/2510.18802</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This technical report develops a computational framework that quantifies strategic coopetition by translating i* dependency structures into interdependence coefficients and modeling complementarity via the Added Value concept. It integrates these quantitative measures into a game‑theoretic formulation where Nash equilibria incorporate structural interdependence, and validates the approach through synthetic experiments and a case study of the Samsung‑Sony S‑LCD joint venture. The work bridges qualitative requirements‑engineering models with rigorous analytical tools for multi‑agent systems.<br /><strong>Summary (CN):</strong> 本文提出了一套计算框架，将 i* 依赖结构转化为互依系数，并使用“Added Value”概念对协同互补性进行量化建模。该框架将这些量化度量嵌入博弈论模型，使纳什均衡能够反映结构性互依关系，并通过合成实验以及三星‑索尼 S‑LCD 合资企业案例进行验证。旨在将定性需求工程模型与多代理系统的严谨分析工具相结合。<br /><strong>Keywords:</strong> strategic coopetition, interdependence, complementarity, i* framework, game theory, Nash equilibrium, quantitative analysis, multi-agent systems, requirements engineering, value appropriation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vik Pant, Eric Yu</div>
Modern socio-technical systems are characterized by strategic coopetition where actors simultaneously cooperate to create value and compete to capture it. While conceptual modeling languages like i* provide rich qualitative representations of strategic dependencies, they lack mechanisms for quantitative analysis of dynamic trade-offs. Conversely, classical game theory offers mathematical rigor but strips away contextual richness. This technical report bridges this gap by developing computational foundations that formalize two critical dimensions of coopetition: interdependence and complementarity. We ground interdependence in i* structural dependency analysis, translating depender-dependee-dependum relationships into quantitative interdependence coefficients through a structured translation framework. We formalize complementarity following Brandenburger and Nalebuff's Added Value concept, modeling synergistic value creation with validated parameterization. We integrate structural dependencies with bargaining power in value appropriation and introduce a game-theoretic formulation where Nash Equilibrium incorporates structural interdependence. Validation combines comprehensive experimental testing across power and logarithmic value function specifications, demonstrating functional form robustness, with empirical application to the Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications achieve superior empirical fit (validation score 45/60) while power functions provide theoretical tractability. This technical report serves as the foundational reference for a coordinated research program examining strategic coopetition in requirements engineering and multi-agent systems, with companion work addressing trust dynamics, team production, and reciprocity mechanisms.
<div><strong>Authors:</strong> Vik Pant, Eric Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This technical report develops a computational framework that quantifies strategic coopetition by translating i* dependency structures into interdependence coefficients and modeling complementarity via the Added Value concept. It integrates these quantitative measures into a game‑theoretic formulation where Nash equilibria incorporate structural interdependence, and validates the approach through synthetic experiments and a case study of the Samsung‑Sony S‑LCD joint venture. The work bridges qualitative requirements‑engineering models with rigorous analytical tools for multi‑agent systems.", "summary_cn": "本文提出了一套计算框架，将 i* 依赖结构转化为互依系数，并使用“Added Value”概念对协同互补性进行量化建模。该框架将这些量化度量嵌入博弈论模型，使纳什均衡能够反映结构性互依关系，并通过合成实验以及三星‑索尼 S‑LCD 合资企业案例进行验证。旨在将定性需求工程模型与多代理系统的严谨分析工具相结合。", "keywords": "strategic coopetition, interdependence, complementarity, i* framework, game theory, Nash equilibrium, quantitative analysis, multi-agent systems, requirements engineering, value appropriation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vik Pant", "Eric Yu"]}
]]></acme>

<pubDate>2025-10-21T16:57:40+00:00</pubDate>
</item>
<item>
<title>Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</title>
<link>https://papers.cool/arxiv/2510.18731</link>
<guid>https://papers.cool/arxiv/2510.18731</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that trains large language models to both answer correctly and assess the solvability of multi‑turn conversation queries, using a competence‑gated curriculum to gradually raise difficulty. By employing on‑policy rollouts and a mixed reward that includes abstention incentives, RLAAR mitigates the Lost‑in‑Conversation degradation, raising benchmark performance from 62.6% to 75.1% and substantially improving calibrated abstention rates. The results offer a practical recipe for building more reliable and trustworthy multi‑turn LLMs.<br /><strong>Summary (CN):</strong> 本文提出了“课程强化学习与可验证准确度及弃答奖励”(RLAAR)框架，使大语言模型在多轮对话中既能给出正确答案，又能判断问题是否可解，通过能力门控的课程逐步提升对话难度。借助在策略回滚和混合奖励（包括弃答激励），RLAAR显著缓解了对话逐步泄漏导致的性能衰减，将基准表现从62.6%提升至75.1%，并将校准的弃答率从33.5%提升至73.4%。这些结果为构建更可靠、可信的多轮 LLM 提供了实用方法。<br /><strong>Keywords:</strong> curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability, competence-gated curriculum, RLVR<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ming Li</div>
Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.
<div><strong>Authors:</strong> Ming Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that trains large language models to both answer correctly and assess the solvability of multi‑turn conversation queries, using a competence‑gated curriculum to gradually raise difficulty. By employing on‑policy rollouts and a mixed reward that includes abstention incentives, RLAAR mitigates the Lost‑in‑Conversation degradation, raising benchmark performance from 62.6% to 75.1% and substantially improving calibrated abstention rates. The results offer a practical recipe for building more reliable and trustworthy multi‑turn LLMs.", "summary_cn": "本文提出了“课程强化学习与可验证准确度及弃答奖励”(RLAAR)框架，使大语言模型在多轮对话中既能给出正确答案，又能判断问题是否可解，通过能力门控的课程逐步提升对话难度。借助在策略回滚和混合奖励（包括弃答激励），RLAAR显著缓解了对话逐步泄漏导致的性能衰减，将基准表现从62.6%提升至75.1%，并将校准的弃答率从33.5%提升至73.4%。这些结果为构建更可靠、可信的多轮 LLM 提供了实用方法。", "keywords": "curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability, competence-gated curriculum, RLVR", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ming Li"]}
]]></acme>

<pubDate>2025-10-21T15:32:26+00:00</pubDate>
</item>
<item>
<title>HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18728</link>
<guid>https://papers.cool/arxiv/2510.18728</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents HarmNet, a modular framework that uses a hierarchical semantic network, a feedback‑driven simulator, and a network traverser to conduct adaptive multi‑turn jailbreak attacks on large language models. By iteratively refining queries and exploring adversarial paths, HarmNet achieves significantly higher success rates than existing methods, e.g., 99.4% on Mistral‑7B. Experiments demonstrate its effectiveness across both closed‑source and open‑source models.<br /><strong>Summary (CN):</strong> 本文提出 HarmNet 框架，利用层次语义网络（ThoughtNet）、反馈驱动的模拟器以及网络遍历器，对大语言模型执行自适应的多轮 jailbreak 攻击。通过迭代查询细化并系统探索对抗路径，HarmNet 在多模型上实现了显著更高的攻击成功率，如在 Mistral‑7B 上达到 99.4%。实验显示该方法在闭源和开源模型上均优于现有基线。<br /><strong>Keywords:</strong> jailbreak attacks, large language models, adversarial robustness, multi-turn prompting, query refinement, hierarchical semantic network, AI safety, HarmNet<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Sidhant Narula, Javad Rafiei Asl, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a hierarchical semantic network; a feedback-driven Simulator for iterative query refinement; and a Network Traverser for real-time adaptive attack execution. HarmNet systematically explores and refines the adversarial space to uncover stealthy, high-success attack paths. Experiments across closed-source and open-source LLMs show that HarmNet outperforms state-of-the-art methods, achieving higher attack success rates. For example, on Mistral-7B, HarmNet achieves a 99.4% attack success rate, 13.9% higher than the best baseline. Index terms: jailbreak attacks; large language models; adversarial framework; query refinement.
<div><strong>Authors:</strong> Sidhant Narula, Javad Rafiei Asl, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents HarmNet, a modular framework that uses a hierarchical semantic network, a feedback‑driven simulator, and a network traverser to conduct adaptive multi‑turn jailbreak attacks on large language models. By iteratively refining queries and exploring adversarial paths, HarmNet achieves significantly higher success rates than existing methods, e.g., 99.4% on Mistral‑7B. Experiments demonstrate its effectiveness across both closed‑source and open‑source models.", "summary_cn": "本文提出 HarmNet 框架，利用层次语义网络（ThoughtNet）、反馈驱动的模拟器以及网络遍历器，对大语言模型执行自适应的多轮 jailbreak 攻击。通过迭代查询细化并系统探索对抗路径，HarmNet 在多模型上实现了显著更高的攻击成功率，如在 Mistral‑7B 上达到 99.4%。实验显示该方法在闭源和开源模型上均优于现有基线。", "keywords": "jailbreak attacks, large language models, adversarial robustness, multi-turn prompting, query refinement, hierarchical semantic network, AI safety, HarmNet", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Sidhant Narula", "Javad Rafiei Asl", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"]}
]]></acme>

<pubDate>2025-10-21T15:28:20+00:00</pubDate>
</item>
<item>
<title>Causally Perturbed Fairness Testing</title>
<link>https://papers.cool/arxiv/2510.18719</link>
<guid>https://papers.cool/arxiv/2510.18719</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CausalFT, a framework that uses causal inference to identify non-sensitive features most directly causally linked to a sensitive attribute, guiding perturbations for fairness testing in tabular data. By integrating this causal knowledge into existing test sample generators, CausalFT significantly improves the detection of fairness bugs across a wide range of cases while adding only modest runtime overhead. Extensive empirical results show improvements over correlation‑based baselines in both bug detection and bias resilience.<br /><strong>Summary (CN):</strong> 本文提出 CausalFT 框架，通过因果推断识别与敏感属性最直接因果关联的非敏感特征，以此引导对表格数据的公平性测试扰动。将该因果信息嵌入现有测试样本生成器后，CausalFT 能显著提升公平性缺陷的发现率，并且额外运行时间开销有限。大量实验表明其在检测公平性漏洞和提升偏差韧性方面均优于仅基于相关性的基线方法。<br /><strong>Keywords:</strong> fairness testing, causal inference, tabular data, bias detection, perturbation, fairness bugs, bias resilience<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Chengwen Du, Tao Chen</div>
To mitigate unfair and unethical discrimination over sensitive features (e.g., gender, age, or race), fairness testing plays an integral role in engineering systems that leverage AI models to handle tabular data. A key challenge therein is how to effectively reveal fairness bugs under an intractable sample size using perturbation. Much current work has been focusing on designing the test sample generators, ignoring the valuable knowledge about data characteristics that can help guide the perturbation and hence limiting their full potential. In this paper, we seek to bridge such a gap by proposing a generic framework of causally perturbed fairness testing, dubbed CausalFT. Through causal inference, the key idea of CausalFT is to extract the most directly and causally relevant non-sensitive feature to its sensitive counterpart, which can jointly influence the prediction of the label. Such a causal relationship is then seamlessly injected into the perturbation to guide a test sample generator. Unlike existing generator-level work, CausalFT serves as a higher-level framework that can be paired with diverse base generators. Extensive experiments on 1296 cases confirm that CausalFT can considerably improve arbitrary base generators in revealing fairness bugs over 93% of the cases with acceptable extra runtime overhead. Compared with a state-of-the-art approach that ranks the non-sensitive features solely based on correlation, CausalFT performs significantly better on 64% cases while being much more efficient. Further, CausalFT can better improve bias resilience in nearly all cases.
<div><strong>Authors:</strong> Chengwen Du, Tao Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CausalFT, a framework that uses causal inference to identify non-sensitive features most directly causally linked to a sensitive attribute, guiding perturbations for fairness testing in tabular data. By integrating this causal knowledge into existing test sample generators, CausalFT significantly improves the detection of fairness bugs across a wide range of cases while adding only modest runtime overhead. Extensive empirical results show improvements over correlation‑based baselines in both bug detection and bias resilience.", "summary_cn": "本文提出 CausalFT 框架，通过因果推断识别与敏感属性最直接因果关联的非敏感特征，以此引导对表格数据的公平性测试扰动。将该因果信息嵌入现有测试样本生成器后，CausalFT 能显著提升公平性缺陷的发现率，并且额外运行时间开销有限。大量实验表明其在检测公平性漏洞和提升偏差韧性方面均优于仅基于相关性的基线方法。", "keywords": "fairness testing, causal inference, tabular data, bias detection, perturbation, fairness bugs, bias resilience", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Chengwen Du", "Tao Chen"]}
]]></acme>

<pubDate>2025-10-21T15:20:30+00:00</pubDate>
</item>
<item>
<title>Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</title>
<link>https://papers.cool/arxiv/2510.18713</link>
<guid>https://papers.cool/arxiv/2510.18713</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies online preference‑based reinforcement learning with ranking feedback over multiple actions, using the Plackett‑Luce model. It introduces the M‑AUPO algorithm that selects action subsets by maximizing average uncertainty, and proves a suboptimality bound that improves subset size, along with a matching lower bound, marking the first theoretical result showing sample‑efficiency gains from larger rankings. These results have implications for more efficient alignment of large language models.<br /><strong>Summary (CN):</strong> 本文研究了在线基于偏好的强化学习（PbRL）在多项排名反馈情形下的样本效率，采用 Plackett‑Luce 模型并提出 M‑AUPO 算法，通过最大化子集内平均不确定性来选择动作。作者给出上界 \(\tilde{O}\left( \frac{d}{T}\sqrt{\sum_{t=1}^T \frac{1}{|S_t|}} \right)\)，表明子集规模越大性能越好，并提供相匹配的下界，首次在 PbRL 中理论证明排名反馈可提升样本效率。该工作对大语言模型的对齐具有潜在安全意义。<br /><strong>Keywords:</strong> preference-based reinforcement learning, Plackett-Luce, ranking feedback, sample efficiency, subset selection, online learning, LLM alignment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.
<div><strong>Authors:</strong> Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies online preference‑based reinforcement learning with ranking feedback over multiple actions, using the Plackett‑Luce model. It introduces the M‑AUPO algorithm that selects action subsets by maximizing average uncertainty, and proves a suboptimality bound that improves subset size, along with a matching lower bound, marking the first theoretical result showing sample‑efficiency gains from larger rankings. These results have implications for more efficient alignment of large language models.", "summary_cn": "本文研究了在线基于偏好的强化学习（PbRL）在多项排名反馈情形下的样本效率，采用 Plackett‑Luce 模型并提出 M‑AUPO 算法，通过最大化子集内平均不确定性来选择动作。作者给出上界 \\(\\tilde{O}\\left( \\frac{d}{T}\\sqrt{\\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)\\)，表明子集规模越大性能越好，并提供相匹配的下界，首次在 PbRL 中理论证明排名反馈可提升样本效率。该工作对大语言模型的对齐具有潜在安全意义。", "keywords": "preference-based reinforcement learning, Plackett-Luce, ranking feedback, sample efficiency, subset selection, online learning, LLM alignment", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Joongkyu Lee", "Seouh-won Yi", "Min-hwan Oh"]}
]]></acme>

<pubDate>2025-10-21T15:11:01+00:00</pubDate>
</item>
<item>
<title>Fetch.ai: An Architecture for Modern Multi-Agent Systems</title>
<link>https://papers.cool/arxiv/2510.18699</link>
<guid>https://papers.cool/arxiv/2510.18699</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the Fetch.ai architecture, a decentralized multi‑agent platform that combines MAS principles with modern AI, including on‑chain services for verifiable identity, discovery, and transactions, a comprehensive development framework, cloud deployment, and an LLM‑driven orchestration layer that translates high‑level human goals into complex multi‑agent workflows. It demonstrates the system through a logistics use case where autonomous agents dynamically discover, negotiate, and transact securely. The architecture aims to overcome centralization and trust limitations of current LLM‑driven agent frameworks.<br /><strong>Summary (CN):</strong> 本文提出 Fetch.ai 架构，一个基于区块链的去中心化多智能体平台，融合传统 MAS 原理与现代 AI 能力，包括身份验证、发现和交易的链上服务、完整的开发框架、云部署以及将高层人类目标转化为多智能体工作流的 agent‑native LLM 编排层。作者通过去中心化物流案例展示了智能体的自主发现、协商和安全交易。该架构旨在克服当前 LLM 驱动智能体系统的中心化和信任、通信协议不足的问题。<br /><strong>Keywords:</strong> multi-agent systems, decentralized architecture, blockchain, agent-native LLM, orchestration, trust, communication protocols, AI safety, MAS, fetch.ai<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Michael J. Wooldridge, Attila Bagoly, Jonathan J. Ward, Emanuele La Malfa, Gabriel Paludo Licks</div>
Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.
<div><strong>Authors:</strong> Michael J. Wooldridge, Attila Bagoly, Jonathan J. Ward, Emanuele La Malfa, Gabriel Paludo Licks</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the Fetch.ai architecture, a decentralized multi‑agent platform that combines MAS principles with modern AI, including on‑chain services for verifiable identity, discovery, and transactions, a comprehensive development framework, cloud deployment, and an LLM‑driven orchestration layer that translates high‑level human goals into complex multi‑agent workflows. It demonstrates the system through a logistics use case where autonomous agents dynamically discover, negotiate, and transact securely. The architecture aims to overcome centralization and trust limitations of current LLM‑driven agent frameworks.", "summary_cn": "本文提出 Fetch.ai 架构，一个基于区块链的去中心化多智能体平台，融合传统 MAS 原理与现代 AI 能力，包括身份验证、发现和交易的链上服务、完整的开发框架、云部署以及将高层人类目标转化为多智能体工作流的 agent‑native LLM 编排层。作者通过去中心化物流案例展示了智能体的自主发现、协商和安全交易。该架构旨在克服当前 LLM 驱动智能体系统的中心化和信任、通信协议不足的问题。", "keywords": "multi-agent systems, decentralized architecture, blockchain, agent-native LLM, orchestration, trust, communication protocols, AI safety, MAS, fetch.ai", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Michael J. Wooldridge", "Attila Bagoly", "Jonathan J. Ward", "Emanuele La Malfa", "Gabriel Paludo Licks"]}
]]></acme>

<pubDate>2025-10-21T14:53:56+00:00</pubDate>
</item>
<item>
<title>Exploring Membership Inference Vulnerabilities in Clinical Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18674</link>
<guid>https://papers.cool/arxiv/2510.18674</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents an exploratory empirical study of membership inference attacks on a clinical large language model (LLEM R), evaluating both standard loss‑based attacks and a novel paraphrasing‑based perturbation that mimics realistic clinical adversaries. Results show limited but measurable leakage of patient record participation in training, indicating partial resistance but persistent privacy risks in current clinical LLMs. The authors discuss implications for trust in healthcare AI and suggest defenses such as differential‑privacy fine‑tuning and paraphrase‑aware training.<br /><strong>Summary (CN):</strong> 本文对一个临床大语言模型（LLEM R）进行成员推断攻击的探索性实证研究，评估了传统基于损失的攻击以及一种模拟真实临床对手的改写扰动策略。结果显示，尽管模型对成员推断有一定抵抗力，但仍存在可测量的患者记录泄露风险，提示当前临床 LLM 在隐私方面仍有潜在漏洞。作者进一步讨论了对医疗 AI 信任的影响，并提出差分隐私微调和针对改写的训练等防御方案。<br /><strong>Keywords:</strong> membership inference, privacy, clinical LLM, electronic health records, fine-tuning, differential privacy, paraphrasing attack, model leakage<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Alexander Nemecek, Zebin Yun, Zahra Rahmani, Yaniv Harel, Vipin Chaudhary, Mahmood Sharif, Erman Ayday</div>
As large language models (LLMs) become progressively more embedded in clinical decision-support, documentation, and patient-information systems, ensuring their privacy and trustworthiness has emerged as an imperative challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic health record (EHR) data improves domain alignment but also raises the risk of exposing patient information through model behaviors. In this work-in-progress, we present an exploratory empirical study on membership inference vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if specific patient records were used during model training. Using a state-of-the-art clinical question-answering model, Llemr, we evaluate both canonical loss-based attacks and a domain-motivated paraphrasing-based perturbation strategy that more realistically reflects clinical adversarial conditions. Our preliminary findings reveal limited but measurable membership leakage, suggesting that current clinical LLMs provide partial resistance yet remain susceptible to subtle privacy risks that could undermine trust in clinical AI adoption. These results motivate continued development of context-aware, domain-specific privacy evaluations and defenses such as differential privacy fine-tuning and paraphrase-aware training, to strengthen the security and trustworthiness of healthcare AI systems.
<div><strong>Authors:</strong> Alexander Nemecek, Zebin Yun, Zahra Rahmani, Yaniv Harel, Vipin Chaudhary, Mahmood Sharif, Erman Ayday</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents an exploratory empirical study of membership inference attacks on a clinical large language model (LLEM R), evaluating both standard loss‑based attacks and a novel paraphrasing‑based perturbation that mimics realistic clinical adversaries. Results show limited but measurable leakage of patient record participation in training, indicating partial resistance but persistent privacy risks in current clinical LLMs. The authors discuss implications for trust in healthcare AI and suggest defenses such as differential‑privacy fine‑tuning and paraphrase‑aware training.", "summary_cn": "本文对一个临床大语言模型（LLEM R）进行成员推断攻击的探索性实证研究，评估了传统基于损失的攻击以及一种模拟真实临床对手的改写扰动策略。结果显示，尽管模型对成员推断有一定抵抗力，但仍存在可测量的患者记录泄露风险，提示当前临床 LLM 在隐私方面仍有潜在漏洞。作者进一步讨论了对医疗 AI 信任的影响，并提出差分隐私微调和针对改写的训练等防御方案。", "keywords": "membership inference, privacy, clinical LLM, electronic health records, fine-tuning, differential privacy, paraphrasing attack, model leakage", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Alexander Nemecek", "Zebin Yun", "Zahra Rahmani", "Yaniv Harel", "Vipin Chaudhary", "Mahmood Sharif", "Erman Ayday"]}
]]></acme>

<pubDate>2025-10-21T14:27:48+00:00</pubDate>
</item>
<item>
<title>Reasoning Language Model Inference Serving Unveiled: An Empirical Study</title>
<link>https://papers.cool/arxiv/2510.18672</link>
<guid>https://papers.cool/arxiv/2510.18672</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper presents the first systematic empirical study of serving performance for reasoning large language models (RLLMs), highlighting distinct behaviors such as high memory fluctuations, straggler requests, adaptive runtimes, and domain preferences compared to standard LLMs. The authors evaluate common inference optimizations—quantization, speculative decoding, prefix caching, and KV‑cache quantization—and find that some improve efficiency with minimal accuracy loss while others may degrade performance. Real‑world workload experiments using a Gamma‑distributed request model confirm these findings across multiple datasets.<br /><strong>Summary (CN):</strong> 本文首次系统性地对推理大语言模型（RLLM）的服务性能进行实证研究，指出其与传统 LLM 在内存波动、慢请求、运行时间自适应以及领域偏好等方面的显著差异。作者评估了常见的推理优化技术（模型量化、投机解码、前缀缓存、KV 缓存量化），发现前两者可在保持较小精度损失的情况下提升效率，而后两者可能导致性能或准确率下降。使用 Gamma 分布的真实工作负载实验在多个数据集上验证了这些结论。<br /><strong>Keywords:</strong> reasoning LLM, inference serving, model quantization, speculative decoding, KV cache, memory usage, latency, real-world workload, deployment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu</div>
The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.
<div><strong>Authors:</strong> Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper presents the first systematic empirical study of serving performance for reasoning large language models (RLLMs), highlighting distinct behaviors such as high memory fluctuations, straggler requests, adaptive runtimes, and domain preferences compared to standard LLMs. The authors evaluate common inference optimizations—quantization, speculative decoding, prefix caching, and KV‑cache quantization—and find that some improve efficiency with minimal accuracy loss while others may degrade performance. Real‑world workload experiments using a Gamma‑distributed request model confirm these findings across multiple datasets.", "summary_cn": "本文首次系统性地对推理大语言模型（RLLM）的服务性能进行实证研究，指出其与传统 LLM 在内存波动、慢请求、运行时间自适应以及领域偏好等方面的显著差异。作者评估了常见的推理优化技术（模型量化、投机解码、前缀缓存、KV 缓存量化），发现前两者可在保持较小精度损失的情况下提升效率，而后两者可能导致性能或准确率下降。使用 Gamma 分布的真实工作负载实验在多个数据集上验证了这些结论。", "keywords": "reasoning LLM, inference serving, model quantization, speculative decoding, KV cache, memory usage, latency, real-world workload, deployment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Qi Li", "Junpan Wu", "Xiang Liu", "Yuxin Wang", "Zeyu Li", "Zhenheng Tang", "Yuhan Chen", "Shaohuai Shi", "Xiaowen Chu"]}
]]></acme>

<pubDate>2025-10-21T14:25:51+00:00</pubDate>
</item>
<item>
<title>Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression</title>
<link>https://papers.cool/arxiv/2510.18650</link>
<guid>https://papers.cool/arxiv/2510.18650</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that uses binary quadratic expressions instead of linear combinations of binary bases to achieve higher accuracy at the same memory budget. Experiments on a matrix compression benchmark and post‑training quantization of Vision‑Transformer models show that BQQ consistently offers a better trade‑off between memory usage and reconstruction error, outperforming state‑of‑the‑art PTQ methods by up to 2.2% (calibration‑based) and 59.1% (data‑free) at a 2‑bit equivalent quantization level.<br /><strong>Summary (CN):</strong> 本文提出二次二元量化 (Binary Quadratic Quantization, BQQ) 方法，通过二元二次表达式而非线性二元基的线性组合来压缩实值矩阵，实现相同内存预算下更高的近似精度。实验在矩阵压缩基准和 Vision Transformer 的后训练量化 (PTQ) 上验证，BQQ 在记忆占用与重构误差之间提供了更优的平衡，在等价2位量化条件下，相比最先进的 PTQ 方法在 ImageNet 校准场景提升最高2.2%，在数据无关场景提升最高59.1%。<br /><strong>Keywords:</strong> matrix quantization, binary quadratic quantization, post-training quantization, vision transformer, model compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
This paper proposes a novel matrix quantization method, Binary Quadratic Quantization (BQQ). In contrast to conventional first-order quantization approaches, such as uniform quantization and binary coding quantization, that approximate real-valued matrices via linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. We validate our approach with two experiments: a matrix compression benchmark and post-training quantization (PTQ) on pretrained Vision Transformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error than conventional methods for compressing diverse matrix data. It also delivers strong PTQ performance, even though we neither target state-of-the-art PTQ accuracy under tight memory constraints nor rely on PTQ-specific binary matrix optimization. For example, our proposed method outperforms the state-of-the-art PTQ method by up to 2.2\% and 59.1% on the ImageNet dataset under the calibration-based and data-free scenarios, respectively, with quantization equivalent to 2 bits. These findings highlight the surprising effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression.
<div><strong>Authors:</strong> Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Binary Quadratic Quantization (BQQ), a novel matrix compression technique that uses binary quadratic expressions instead of linear combinations of binary bases to achieve higher accuracy at the same memory budget. Experiments on a matrix compression benchmark and post‑training quantization of Vision‑Transformer models show that BQQ consistently offers a better trade‑off between memory usage and reconstruction error, outperforming state‑of‑the‑art PTQ methods by up to 2.2% (calibration‑based) and 59.1% (data‑free) at a 2‑bit equivalent quantization level.", "summary_cn": "本文提出二次二元量化 (Binary Quadratic Quantization, BQQ) 方法，通过二元二次表达式而非线性二元基的线性组合来压缩实值矩阵，实现相同内存预算下更高的近似精度。实验在矩阵压缩基准和 Vision Transformer 的后训练量化 (PTQ) 上验证，BQQ 在记忆占用与重构误差之间提供了更优的平衡，在等价2位量化条件下，相比最先进的 PTQ 方法在 ImageNet 校准场景提升最高2.2%，在数据无关场景提升最高59.1%。", "keywords": "matrix quantization, binary quadratic quantization, post-training quantization, vision transformer, model compression", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Kyo Kuroki", "Yasuyuki Okoshi", "Thiem Van Chu", "Kazushi Kawamura", "Masato Motomura"]}
]]></acme>

<pubDate>2025-10-21T13:58:46+00:00</pubDate>
</item>
<item>
<title>ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
<link>https://papers.cool/arxiv/2510.18637</link>
<guid>https://papers.cool/arxiv/2510.18637</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents ε-Seg, a sparsely supervised semantic segmentation method for electron microscopy images that leverages hierarchical variational autoencoders, center‑region masking, contrastive learning, and a Gaussian mixture model prior to learn robust latent embeddings. Sparse labels (as low as 0.05% of the data) are sufficient because the model shapes the latent space to cluster by semantic class and uses an MLP head for direct label prediction, achieving competitive results on dense EM and fluorescence microscopy datasets.<br /><strong>Summary (CN):</strong> 本文提出 ε‑Seg，一种针对电子显微镜图像的稀疏监督语义分割方法，利用层次变分自编码器（HVAE）、中心区域掩码、对比学习以及高斯混合模型（GMM）先验来学习稳健的潜在嵌入。即使标签稀少（≤0.05%），模型亦通过潜在空间聚类并使用 MLP 分割头直接预测标签，在密集 EM 数据集和荧光显微镜数据上获得竞争性的分割效果。<br /><strong>Keywords:</strong> sparse supervision, semantic segmentation, hierarchical variational autoencoder, contrastive learning, Gaussian mixture model, microscopy imaging, electron microscopy, MLP segmentation head<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.
<div><strong>Authors:</strong> Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents ε-Seg, a sparsely supervised semantic segmentation method for electron microscopy images that leverages hierarchical variational autoencoders, center‑region masking, contrastive learning, and a Gaussian mixture model prior to learn robust latent embeddings. Sparse labels (as low as 0.05% of the data) are sufficient because the model shapes the latent space to cluster by semantic class and uses an MLP head for direct label prediction, achieving competitive results on dense EM and fluorescence microscopy datasets.", "summary_cn": "本文提出 ε‑Seg，一种针对电子显微镜图像的稀疏监督语义分割方法，利用层次变分自编码器（HVAE）、中心区域掩码、对比学习以及高斯混合模型（GMM）先验来学习稳健的潜在嵌入。即使标签稀少（≤0.05%），模型亦通过潜在空间聚类并使用 MLP 分割头直接预测标签，在密集 EM 数据集和荧光显微镜数据上获得竞争性的分割效果。", "keywords": "sparse supervision, semantic segmentation, hierarchical variational autoencoder, contrastive learning, Gaussian mixture model, microscopy imaging, electron microscopy, MLP segmentation head", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sheida Rahnamai Kordasiabi", "Damian Dalle Nogare", "Florian Jug"]}
]]></acme>

<pubDate>2025-10-21T13:41:07+00:00</pubDate>
</item>
<item>
<title>C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression</title>
<link>https://papers.cool/arxiv/2510.18636</link>
<guid>https://papers.cool/arxiv/2510.18636</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces C‑SWAP, an explainability‑aware one‑shot structured pruning framework that uses causal relationships between model predictions and network components to prune without fine‑tuning, achieving substantial size reductions with minimal performance loss on CNN and vision transformer models. By leveraging cause‑effect analysis, the method outperforms existing one‑shot pruning approaches, offering a better trade‑off between compression and accuracy.<br /><strong>Summary (CN):</strong> 本文提出一种结合可解释性因果分析的一次性结构化剪枝框架（C‑SWAP），通过检测模型预测与网络结构之间的因果关系，在不需要微调的情况下实现显著的模型压。实验在卷积神经网络和视觉Transformer上显示，所提方法在保持性能的前提下大幅降低模型规模，并优于现有一次性剪枝方法。<br /><strong>Keywords:</strong> structured pruning, one-shot pruning, explainability, causal pruning, model compression, neural network sparsity, CNN, vision transformer, efficient inference<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.
<div><strong>Authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces C‑SWAP, an explainability‑aware one‑shot structured pruning framework that uses causal relationships between model predictions and network components to prune without fine‑tuning, achieving substantial size reductions with minimal performance loss on CNN and vision transformer models. By leveraging cause‑effect analysis, the method outperforms existing one‑shot pruning approaches, offering a better trade‑off between compression and accuracy.", "summary_cn": "本文提出一种结合可解释性因果分析的一次性结构化剪枝框架（C‑SWAP），通过检测模型预测与网络结构之间的因果关系，在不需要微调的情况下实现显著的模型压。实验在卷积神经网络和视觉Transformer上显示，所提方法在保持性能的前提下大幅降低模型规模，并优于现有一次性剪枝方法。", "keywords": "structured pruning, one-shot pruning, explainability, causal pruning, model compression, neural network sparsity, CNN, vision transformer, efficient inference", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baptiste Bauvin", "Loïc Baret", "Ola Ahmad"]}
]]></acme>

<pubDate>2025-10-21T13:40:11+00:00</pubDate>
</item>
<item>
<title>Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</title>
<link>https://papers.cool/arxiv/2510.18632</link>
<guid>https://papers.cool/arxiv/2510.18632</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views introduces 3DThinker, a framework that enables vision‑language models to perform 3D mental imaging during reasoning without any explicit 3D supervision The method aligns the model’s latent 3D representation with that of a 3D foundation model and then refines it using outcome‑based optimization, achieving state‑of‑the‑art performance on several spatial reasoning benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 3DThinker 框架，使视觉‑语言模型在推理时能够进行 3D 心理想象，而无需显式的 3D 标注。该方法先通过监督将模型生成的 3D 潜在表示与 3D 基础模型（如 VGGT）的表示对齐，随后仅依据结果信号优化整个推理过程，在多个空间推理基准上实现了显著提升。<br /><strong>Keywords:</strong> 3D reasoning, vision-language models, geometric imagination, latent alignment, multimodal reasoning, spatial cognition, VGGT, outcome-based optimization<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</div>
Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.
<div><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views introduces 3DThinker, a framework that enables vision‑language models to perform 3D mental imaging during reasoning without any explicit 3D supervision The method aligns the model’s latent 3D representation with that of a 3D foundation model and then refines it using outcome‑based optimization, achieving state‑of‑the‑art performance on several spatial reasoning benchmarks.", "summary_cn": "本文提出 3DThinker 框架，使视觉‑语言模型在推理时能够进行 3D 心理想象，而无需显式的 3D 标注。该方法先通过监督将模型生成的 3D 潜在表示与 3D 基础模型（如 VGGT）的表示对齐，随后仅依据结果信号优化整个推理过程，在多个空间推理基准上实现了显著提升。", "keywords": "3D reasoning, vision-language models, geometric imagination, latent alignment, multimodal reasoning, spatial cognition, VGGT, outcome-based optimization", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhangquan Chen", "Manyuan Zhang", "Xinlei Yu", "Xufang Luo", "Mingze Sun", "Zihao Pan", "Yan Feng", "Peng Pei", "Xunliang Cai", "Ruqi Huang"]}
]]></acme>

<pubDate>2025-10-21T13:36:58+00:00</pubDate>
</item>
<item>
<title>A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees</title>
<link>https://papers.cool/arxiv/2510.18615</link>
<guid>https://papers.cool/arxiv/2510.18615</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a rectification-based method for distilling boosted tree ensembles into single decision trees, aiming to balance predictive performance with improved interpretability. The authors describe how rectification can be applied during distillation and provide empirical comparisons showing that this approach yields competitive results relative to retraining-based distillation methods.<br /><strong>Summary (CN):</strong> 本文提出了一种基于校正（rectification）的技术，将提升树（boosted trees）蒸馏成单棵决策树，以在保持可接受的预测性能的同时提升模型的可解释性。作者阐述了在蒸馏过程中使用校正的实现方式，并通过实验比较表明，该方法相较于传统的重新训练蒸馏方法能够取得有竞争力的结果。<br /><strong>Keywords:</strong> boosted trees, decision trees, model distillation, rectification, interpretability, tree ensembles, machine learning compression<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
We present a new approach for distilling boosted trees into decision trees, in the objective of generating an ML model offering an acceptable compromise in terms of predictive performance and interpretability. We explain how the correction approach called rectification can be used to implement such a distillation process. We show empirically that this approach provides interesting results, in comparison with an approach to distillation achieved by retraining the model.
<div><strong>Authors:</strong> Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a rectification-based method for distilling boosted tree ensembles into single decision trees, aiming to balance predictive performance with improved interpretability. The authors describe how rectification can be applied during distillation and provide empirical comparisons showing that this approach yields competitive results relative to retraining-based distillation methods.", "summary_cn": "本文提出了一种基于校正（rectification）的技术，将提升树（boosted trees）蒸馏成单棵决策树，以在保持可接受的预测性能的同时提升模型的可解释性。作者阐述了在蒸馏过程中使用校正的实现方式，并通过实验比较表明，该方法相较于传统的重新训练蒸馏方法能够取得有竞争力的结果。", "keywords": "boosted trees, decision trees, model distillation, rectification, interpretability, tree ensembles, machine learning compression", "scoring": {"interpretability": 7, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Gilles Audemard", "Sylvie Coste-Marquis", "Pierre Marquis", "Mehdi Sabiri", "Nicolas Szczepanski"]}
]]></acme>

<pubDate>2025-10-21T13:14:04+00:00</pubDate>
</item>
<item>
<title>The Cost-Benefit of Interdisciplinarity in AI for Mental Health</title>
<link>https://papers.cool/arxiv/2510.18581</link>
<guid>https://papers.cool/arxiv/2510.18581</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyzes the cost‑benefit trade‑off of interdisciplinary collaboration in AI‑driven mental health chatbots, arguing that input from technology, healthcare, ethics, and law throughout the chatbot lifecycle is essential for value‑alignment and compliance with the AI Act’s high‑risk requirements. It provides practical recommendations and surveys existing frameworks to help balance the challenges and advantages of such interdisciplinary approaches.<br /><strong>Summary (CN):</strong> 本文分析了在 AI 心理健康聊天机器人中进行跨学科合作的成本‑收益权衡，主张在整个生命周期中融入技术、医疗、伦理和法律等领域的专家，以确保价值对齐并符合 AI 法案的高风险要求。文章提供了实际建议并梳理了现有框架，帮助在跨学科合作的挑战与收益之间取得平衡。<br /><strong>Keywords:</strong> AI mental health, interdisciplinary collaboration, value alignment, AI Act compliance, chatbot ethics, healthcare AI, legal considerations, safety, mental health support, high-risk AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 5, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Katerina Drakos, Eva Paraschou, Simay Toplu, Line Harder Clemmensen, Christoph Lütge, Nicole Nadine Lønfeldt, Sneha Das</div>
Artificial intelligence has been introduced as a way to improve access to mental health support. However, most AI mental health chatbots rely on a limited range of disciplinary input, and fail to integrate expertise across the chatbot's lifecycle. This paper examines the cost-benefit trade-off of interdisciplinary collaboration in AI mental health chatbots. We argue that involving experts from technology, healthcare, ethics, and law across key lifecycle phases is essential to ensure value-alignment and compliance with the high-risk requirements of the AI Act. We also highlight practical recommendations and existing frameworks to help balance the challenges and benefits of interdisciplinarity in mental health chatbots.
<div><strong>Authors:</strong> Katerina Drakos, Eva Paraschou, Simay Toplu, Line Harder Clemmensen, Christoph Lütge, Nicole Nadine Lønfeldt, Sneha Das</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyzes the cost‑benefit trade‑off of interdisciplinary collaboration in AI‑driven mental health chatbots, arguing that input from technology, healthcare, ethics, and law throughout the chatbot lifecycle is essential for value‑alignment and compliance with the AI Act’s high‑risk requirements. It provides practical recommendations and surveys existing frameworks to help balance the challenges and advantages of such interdisciplinary approaches.", "summary_cn": "本文分析了在 AI 心理健康聊天机器人中进行跨学科合作的成本‑收益权衡，主张在整个生命周期中融入技术、医疗、伦理和法律等领域的专家，以确保价值对齐并符合 AI 法案的高风险要求。文章提供了实际建议并梳理了现有框架，帮助在跨学科合作的挑战与收益之间取得平衡。", "keywords": "AI mental health, interdisciplinary collaboration, value alignment, AI Act compliance, chatbot ethics, healthcare AI, legal considerations, safety, mental health support, high-risk AI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Katerina Drakos", "Eva Paraschou", "Simay Toplu", "Line Harder Clemmensen", "Christoph Lütge", "Nicole Nadine Lønfeldt", "Sneha Das"]}
]]></acme>

<pubDate>2025-10-21T12:34:44+00:00</pubDate>
</item>
<item>
<title>Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model</title>
<link>https://papers.cool/arxiv/2510.18573</link>
<guid>https://papers.cool/arxiv/2510.18573</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Kaleido is a subject-to-video (S2V) generation framework that synthesizes subject-consistent videos conditioned on multiple reference images of target subjects. It tackles multi-subject consistency and background disentanglement by introducing a dedicated data construction pipeline with low-quality sample filtering and diverse data synthesis, as well as a Reference Rotary Positional Encoding (R‑RoPE) for stable multi-image integration. Experiments show Kaleido outperforms prior methods in consistency, fidelity, and generalization across several benchmarks.<br /><strong>Summary (CN):</strong> Kaleido 是一个面向“主体到视频”(S2V)的生成框架，能够在多个目标主体的参考图像条件下合成保持主体一致性的视频。该模型通过专门的数据构建流水线（包括低质量样本过滤和多样化样本合成）以及参考旋转位置编码 (R‑RoPE) 来实现稳健的多图像融合，从而解决多主体一致性和背景解耦问题。实验表明 Kaleido 在一致性、保真度和泛化能力上显著优于已有方法。<br /><strong>Keywords:</strong> subject-to-video generation, multi-subject consistency, reference video synthesis, Reference Rotary Positional Encoding, data construction pipeline, video fidelity, S2V, background disentanglement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhenxing Zhang, Jiayan Teng, Zhuoyi Yang, Tiankun Cao, Cheng Wang, Xiaotao Gu, Jie Tang, Dan Guo, Meng Wang</div>
We present Kaleido, a subject-to-video~(S2V) generation framework, which aims to synthesize subject-consistent videos conditioned on multiple reference images of target subjects. Despite recent progress in S2V generation models, existing approaches remain inadequate at maintaining multi-subject consistency and at handling background disentanglement, often resulting in lower reference fidelity and semantic drift under multi-image conditioning. These shortcomings can be attributed to several factors. Primarily, the training dataset suffers from a lack of diversity and high-quality samples, as well as cross-paired data, i.e., paired samples whose components originate from different instances. In addition, the current mechanism for integrating multiple reference images is suboptimal, potentially resulting in the confusion of multiple subjects. To overcome these limitations, we propose a dedicated data construction pipeline, incorporating low-quality sample filtering and diverse data synthesis, to produce consistency-preserving training data. Moreover, we introduce Reference Rotary Positional Encoding (R-RoPE) to process reference images, enabling stable and precise multi-image integration. Extensive experiments across numerous benchmarks demonstrate that Kaleido significantly outperforms previous methods in consistency, fidelity, and generalization, marking an advance in S2V generation.
<div><strong>Authors:</strong> Zhenxing Zhang, Jiayan Teng, Zhuoyi Yang, Tiankun Cao, Cheng Wang, Xiaotao Gu, Jie Tang, Dan Guo, Meng Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Kaleido is a subject-to-video (S2V) generation framework that synthesizes subject-consistent videos conditioned on multiple reference images of target subjects. It tackles multi-subject consistency and background disentanglement by introducing a dedicated data construction pipeline with low-quality sample filtering and diverse data synthesis, as well as a Reference Rotary Positional Encoding (R‑RoPE) for stable multi-image integration. Experiments show Kaleido outperforms prior methods in consistency, fidelity, and generalization across several benchmarks.", "summary_cn": "Kaleido 是一个面向“主体到视频”(S2V)的生成框架，能够在多个目标主体的参考图像条件下合成保持主体一致性的视频。该模型通过专门的数据构建流水线（包括低质量样本过滤和多样化样本合成）以及参考旋转位置编码 (R‑RoPE) 来实现稳健的多图像融合，从而解决多主体一致性和背景解耦问题。实验表明 Kaleido 在一致性、保真度和泛化能力上显著优于已有方法。", "keywords": "subject-to-video generation, multi-subject consistency, reference video synthesis, Reference Rotary Positional Encoding, data construction pipeline, video fidelity, S2V, background disentanglement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhenxing Zhang", "Jiayan Teng", "Zhuoyi Yang", "Tiankun Cao", "Cheng Wang", "Xiaotao Gu", "Jie Tang", "Dan Guo", "Meng Wang"]}
]]></acme>

<pubDate>2025-10-21T12:28:14+00:00</pubDate>
</item>
<item>
<title>Large language models for folktale type automation based on motifs: Cinderella case study</title>
<link>https://papers.cool/arxiv/2510.18561</link>
<guid>https://papers.cool/arxiv/2510.18561</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a methodology that leverages large language models to automatically detect narrative motifs in a large corpus of Cinderella variants, using clustering and dimensionality reduction to analyze similarities and differences across languages. The results demonstrate that LLMs can capture complex interactions within folktales, enabling large-scale computational analysis and cross-lingual comparisons in digital humanities.<br /><strong>Summary (CN):</strong> 本文提出一种利用大型语言模型（LLM）自动识别《灰姑娘》变体中叙事模体（motif）的方法，并通过聚类和降维技术分析其跨语言的相似性和差异性。实验结果表明，LLM 能捕捉童话文本中的复杂交互，为数字人文学科的大规模计算分析和跨语言比较提供了新途径。<br /><strong>Keywords:</strong> large language models, motif detection, folktale analysis, digital humanities, clustering, dimensionality reduction, Cinderella, cross-lingual comparison<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.
<div><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a methodology that leverages large language models to automatically detect narrative motifs in a large corpus of Cinderella variants, using clustering and dimensionality reduction to analyze similarities and differences across languages. The results demonstrate that LLMs can capture complex interactions within folktales, enabling large-scale computational analysis and cross-lingual comparisons in digital humanities.", "summary_cn": "本文提出一种利用大型语言模型（LLM）自动识别《灰姑娘》变体中叙事模体（motif）的方法，并通过聚类和降维技术分析其跨语言的相似性和差异性。实验结果表明，LLM 能捕捉童话文本中的复杂交互，为数字人文学科的大规模计算分析和跨语言比较提供了新途径。", "keywords": "large language models, motif detection, folktale analysis, digital humanities, clustering, dimensionality reduction, Cinderella, cross-lingual comparison", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tjaša Arčon", "Marko Robnik-Šikonja", "Polona Tratnik"]}
]]></acme>

<pubDate>2025-10-21T12:18:20+00:00</pubDate>
</item>
<item>
<title>WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality</title>
<link>https://papers.cool/arxiv/2510.18560</link>
<guid>https://papers.cool/arxiv/2510.18560</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces WebDevJudge, a benchmark for evaluating large language models (LLMs) and multimodal LLMs as judges of web development quality, featuring both static and interactive evaluation settings with human preference labels and structured rubrics. Experiments show a substantial performance gap between LLM judges and human experts, highlighting failures in recognizing functional equivalence, task feasibility, and bias mitigation. The work provides data and analysis to guide future research toward more reliable automated evaluators for complex, dynamic tasks.<br /><strong>Summary (CN):</strong> 本文提出 WebDevJudge 基准，用于评估大模型（包括多模态模型）在网页开发质量评判中的表现，包含基于静态观察的非交互评估和基于动态网页环境的交互评估，并配有人类偏好标签和结构化评分标准。实验发现 LLM 评审与人类专家之间存在显著差距，主要体现在无法识别功能等价、验证任务可行性以及缓解偏差等方面。该工作提供数据与分析，以推动更可靠的自动评估方法在复杂动态场景中的研究。<br /><strong>Keywords:</strong> LLM-as-a-judge, web development evaluation, benchmark, human preference, bias, functional equivalence, MLLM, automated evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Chunyang Li, Yilun Zheng, Xinting Huang, Tianqing Fang, Jiahao Xu, Yangqiu Song, Lihui Chen, Han Hu</div>
The paradigm of LLM-as-a-judge is emerging as a scalable and efficient alternative to human evaluation, demonstrating strong performance on well-defined tasks. However, its reliability in open-ended tasks with dynamic environments and complex interactions remains unexplored. To bridge the gap, we introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge performance in web development, with support for both non-interactive evaluation based on static observations and continuous interactive evaluation with a dynamic web environment. WebDevJudge comprises human preference labels over paired web implementations, annotated with structured and query-grounded rubrics to ensure high-quality ground truth. Using this benchmark, we comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic workflows. We systematically investigate the impact of different paradigms and guidance mechanisms. Our experiments reveal a significant gap between LLM judges and human experts. In-depth analysis indicates this gap stems from fundamental model limitations, including failures in recognizing functional equivalence, verifying task feasibility, and mitigating bias. Overall, WebDevJudge presents a significant challenge to LLM-as-a-judge, offering insights to guide future research toward developing more reliable and capable automated evaluators for complicated scenarios. Code and data are available at https://github.com/lcy2723/WebDevJudge.
<div><strong>Authors:</strong> Chunyang Li, Yilun Zheng, Xinting Huang, Tianqing Fang, Jiahao Xu, Yangqiu Song, Lihui Chen, Han Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces WebDevJudge, a benchmark for evaluating large language models (LLMs) and multimodal LLMs as judges of web development quality, featuring both static and interactive evaluation settings with human preference labels and structured rubrics. Experiments show a substantial performance gap between LLM judges and human experts, highlighting failures in recognizing functional equivalence, task feasibility, and bias mitigation. The work provides data and analysis to guide future research toward more reliable automated evaluators for complex, dynamic tasks.", "summary_cn": "本文提出 WebDevJudge 基准，用于评估大模型（包括多模态模型）在网页开发质量评判中的表现，包含基于静态观察的非交互评估和基于动态网页环境的交互评估，并配有人类偏好标签和结构化评分标准。实验发现 LLM 评审与人类专家之间存在显著差距，主要体现在无法识别功能等价、验证任务可行性以及缓解偏差等方面。该工作提供数据与分析，以推动更可靠的自动评估方法在复杂动态场景中的研究。", "keywords": "LLM-as-a-judge, web development evaluation, benchmark, human preference, bias, functional equivalence, MLLM, automated evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Chunyang Li", "Yilun Zheng", "Xinting Huang", "Tianqing Fang", "Jiahao Xu", "Yangqiu Song", "Lihui Chen", "Han Hu"]}
]]></acme>

<pubDate>2025-10-21T12:16:04+00:00</pubDate>
</item>
<item>
<title>RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</title>
<link>https://papers.cool/arxiv/2510.18559</link>
<guid>https://papers.cool/arxiv/2510.18559</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes RAISE, a unified framework that quantifies responsible AI performance across explainability, fairness, robustness, and sustainability, producing an aggregated Responsibility Score. The authors apply RAISE to three deep‑learning models on finance, healthcare, and socio‑economic tabular datasets, revealing trade‑offs such as high explainability and fairness for a transformer at the expense of environmental cost. The implementation is released open‑source.<br /><strong>Summary (CN):</strong> 本文提出 RAISE（Responsible AI Scoring and Evaluation）统一框架，用于在可解释性、公平性、鲁棒性和可持续性四个维度上量化模型表现并汇总为整体责任分数。作者在金融、医疗和社会经济结构化数据集上评估了 MLP、Tabular ResNet 和特征分词 Transformer，揭示了如 Transformer 在可解释性和公平性上表现突出但环境代价高的权衡。相关实现已开源。<br /><strong>Keywords:</strong> responsible AI, evaluation framework, explainability, fairness, robustness, sustainability, responsibility score, tabular models, multi-dimensional assessment<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Loc Phuc Truong Nguyen, Hung Thanh Do</div>
As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: https://github.com/raise-framework/raise.
<div><strong>Authors:</strong> Loc Phuc Truong Nguyen, Hung Thanh Do</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes RAISE, a unified framework that quantifies responsible AI performance across explainability, fairness, robustness, and sustainability, producing an aggregated Responsibility Score. The authors apply RAISE to three deep‑learning models on finance, healthcare, and socio‑economic tabular datasets, revealing trade‑offs such as high explainability and fairness for a transformer at the expense of environmental cost. The implementation is released open‑source.", "summary_cn": "本文提出 RAISE（Responsible AI Scoring and Evaluation）统一框架，用于在可解释性、公平性、鲁棒性和可持续性四个维度上量化模型表现并汇总为整体责任分数。作者在金融、医疗和社会经济结构化数据集上评估了 MLP、Tabular ResNet 和特征分词 Transformer，揭示了如 Transformer 在可解释性和公平性上表现突出但环境代价高的权衡。相关实现已开源。", "keywords": "responsible AI, evaluation framework, explainability, fairness, robustness, sustainability, responsibility score, tabular models, multi-dimensional assessment", "scoring": {"interpretability": 4, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Loc Phuc Truong Nguyen", "Hung Thanh Do"]}
]]></acme>

<pubDate>2025-10-21T12:15:13+00:00</pubDate>
</item>
<item>
<title>EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval</title>
<link>https://papers.cool/arxiv/2510.18546</link>
<guid>https://papers.cool/arxiv/2510.18546</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> EfficientNav proposes a suite of techniques—semantics-aware memory retrieval, discrete memory caching, and attention-based memory clustering—to enable small on-device language models to perform zero-shot object-goal navigation with reduced latency and higher success rates. By pruning redundant map information and reusing KV caches, the method achieves significant improvements over GPT-4 baselines on the HM3D benchmark while dramatically lowering planning latency.<br /><strong>Summary (CN):</strong> EfficientNav 通过语义感知记忆检索、离散记忆缓存以及基于注意力的记忆聚类等技术，使得小型本地语言模型能够在零样本目标导航任务中高效运行。该方法在 HM3D 基准上相较 GPT-4 基线提升了成功率，并显著降低了规划延迟，实现了更快的实时和端到端推理。<br /><strong>Keywords:</strong> object-goal navigation, on-device LLM, semantics-aware memory retrieval, memory caching, attention-based clustering, embodied AI, HM3D benchmark<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Zebin Yang, Sunjian Zheng, Tong Xie, Tianshi Xu, Bo Yu, Fan Wang, Jie Tang, Shaoshan Liu, Meng Li</div>
Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code will be released soon.
<div><strong>Authors:</strong> Zebin Yang, Sunjian Zheng, Tong Xie, Tianshi Xu, Bo Yu, Fan Wang, Jie Tang, Shaoshan Liu, Meng Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "EfficientNav proposes a suite of techniques—semantics-aware memory retrieval, discrete memory caching, and attention-based memory clustering—to enable small on-device language models to perform zero-shot object-goal navigation with reduced latency and higher success rates. By pruning redundant map information and reusing KV caches, the method achieves significant improvements over GPT-4 baselines on the HM3D benchmark while dramatically lowering planning latency.", "summary_cn": "EfficientNav 通过语义感知记忆检索、离散记忆缓存以及基于注意力的记忆聚类等技术，使得小型本地语言模型能够在零样本目标导航任务中高效运行。该方法在 HM3D 基准上相较 GPT-4 基线提升了成功率，并显著降低了规划延迟，实现了更快的实时和端到端推理。", "keywords": "object-goal navigation, on-device LLM, semantics-aware memory retrieval, memory caching, attention-based clustering, embodied AI, HM3D benchmark", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"]}
]]></acme>

<pubDate>2025-10-21T11:52:44+00:00</pubDate>
</item>
<item>
<title>Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</title>
<link>https://papers.cool/arxiv/2510.18541</link>
<guid>https://papers.cool/arxiv/2510.18541</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies security risks of knowledge distillation when teacher LLMs are backdoored, showing that existing backdoors rarely transfer to students because triggers are rare. It introduces T-MTB, a technique that builds composite triggers from frequently occurring tokens, enabling stealthy backdoors that survive distillation. Experiments across multiple model families demonstrate transferable backdoors for jailbreaking and content modulation scenarios.<br /><strong>Summary (CN):</strong> 本文研究了在知识蒸馏过程中教师大语言模型（LLM）被植入后门的安全风险，指出现后门因触发词在常规语境中罕见而难以迁移。作者提出 T-MTB 方法，利用在蒸馏数据中常出现的多个特定标记构造组合触发器，使后门在保持隐蔽性的同时能够在学生模型中成功转移。实验在四类 LLM 上验证了该可转移后门在 jailbreak 和内容调制两种攻击场景中的有效性。<br /><strong>Keywords:</strong> backdoor, knowledge distillation, large language models, trigger tokens, transferability, security, jailbreaking, content modulation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</div>
LLMs are often used by downstream users as teacher models for knowledge distillation, compressing their capabilities into memory-efficient models. However, as these teacher models may stem from untrusted parties, distillation can raise unexpected security risks. In this paper, we investigate the security implications of knowledge distillation from backdoored teacher models. First, we show that prior backdoors mostly do not transfer onto student models. Our key insight is that this is because existing LLM backdooring methods choose trigger tokens that rarely occur in usual contexts. We argue that this underestimates the security risks of knowledge distillation and introduce a new backdooring technique, T-MTB, that enables the construction and study of transferable backdoors. T-MTB carefully constructs a composite backdoor trigger, made up of several specific tokens that often occur individually in anticipated distillation datasets. As such, the poisoned teacher remains stealthy, while during distillation the individual presence of these tokens provides enough signal for the backdoor to transfer onto the student. Using T-MTB, we demonstrate and extensively study the security risks of transferable backdoors across two attack scenarios, jailbreaking and content modulation, and across four model families of LLMs.
<div><strong>Authors:</strong> Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies security risks of knowledge distillation when teacher LLMs are backdoored, showing that existing backdoors rarely transfer to students because triggers are rare. It introduces T-MTB, a technique that builds composite triggers from frequently occurring tokens, enabling stealthy backdoors that survive distillation. Experiments across multiple model families demonstrate transferable backdoors for jailbreaking and content modulation scenarios.", "summary_cn": "本文研究了在知识蒸馏过程中教师大语言模型（LLM）被植入后门的安全风险，指出现后门因触发词在常规语境中罕见而难以迁移。作者提出 T-MTB 方法，利用在蒸馏数据中常出现的多个特定标记构造组合触发器，使后门在保持隐蔽性的同时能够在学生模型中成功转移。实验在四类 LLM 上验证了该可转移后门在 jailbreak 和内容调制两种攻击场景中的有效性。", "keywords": "backdoor, knowledge distillation, large language models, trigger tokens, transferability, security, jailbreaking, content modulation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Giovanni De Muri", "Mark Vero", "Robin Staab", "Martin Vechev"]}
]]></acme>

<pubDate>2025-10-21T11:39:45+00:00</pubDate>
</item>
<item>
<title>Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18502</link>
<guid>https://papers.cool/arxiv/2510.18502</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pipeline that combines vision‑language models with retrieval‑augmented generation to achieve zero‑shot vehicle make and model recognition. The VLM produces textual attribute descriptions of a vehicle image, which are matched against a database of textual features; the retrieved entries are fed to a language model that predicts the make and model. Experiments show about a 20 % gain over the CLIP baseline, demonstrating that text‑based reasoning can replace costly finetuning for scalable VMMR.<br /><strong>Summary (CN):</strong> 本文提出一种将视觉语言模型与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌与车型识别。视觉语言模型将车辆图像转换为文字属性描述，并与文本特征库匹配检索相关条目，这些信息随后被提示语言模型以推断具体品牌和车型。实验表明该方法相比 CLIP 基线提升约 20%，展示了基于文本推理的可扩展 VMMR 方案。<br /><strong>Keywords:</strong> zero-shot recognition, vehicle model recognition, vision-language model, retrieval-augmented generation, CLIP, text-based reasoning, smart city, multimodal retrieval, attribute description<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.
<div><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pipeline that combines vision‑language models with retrieval‑augmented generation to achieve zero‑shot vehicle make and model recognition. The VLM produces textual attribute descriptions of a vehicle image, which are matched against a database of textual features; the retrieved entries are fed to a language model that predicts the make and model. Experiments show about a 20 % gain over the CLIP baseline, demonstrating that text‑based reasoning can replace costly finetuning for scalable VMMR.", "summary_cn": "本文提出一种将视觉语言模型与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌与车型识别。视觉语言模型将车辆图像转换为文字属性描述，并与文本特征库匹配检索相关条目，这些信息随后被提示语言模型以推断具体品牌和车型。实验表明该方法相比 CLIP 基线提升约 20%，展示了基于文本推理的可扩展 VMMR 方案。", "keywords": "zero-shot recognition, vehicle model recognition, vision-language model, retrieval-augmented generation, CLIP, text-based reasoning, smart city, multimodal retrieval, attribute description", "scoring": {"interpretability": 3, "understanding": 4, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}
]]></acme>

<pubDate>2025-10-21T10:39:39+00:00</pubDate>
</item>
<item>
<title>One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection</title>
<link>https://papers.cool/arxiv/2510.18493</link>
<guid>https://papers.cool/arxiv/2510.18493</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment for LLM-based phone scam detection, allowing users to choose between simple keyword-based sanitization and more advanced neural methods based on their privacy preferences. It outlines the architecture, possible modeling approaches, and loss functions to create personalized, privacy-preserving detection systems that balance trust and effectiveness, and discusses broader applications beyond phone scams.<br /><strong>Summary (CN):</strong> 本文提出了 MASK（模块化自适应清洗套件），一个可训练且可扩展的框架，用于在基于大语言模型的电话诈骗检测中实现动态隐私调节，用户可根据隐私偏好在传统关键词过滤和更高级的神经式清洗方法之间切换。文中阐述了体系结构、潜在建模方法和损失函数设计，以构建既能保护用户隐私又保持检测效果的个性化系统，并探讨了该框架在电话诈骗之外的应用前景。<br /><strong>Keywords:</strong> privacy-preserving, phone scam detection, large language models, modular sanitization, adaptive privacy, trainable sanitization, keyword filtering, neural sanitization, personalized privacy, MASK<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Kangzhong Wang, Zitong Shen, Youqian Zhang, Michael MK Cheung, Xiapu Luo, Grace Ngai, Eugene Yujun Fu</div>
Phone scams remain a pervasive threat to both personal safety and financial security worldwide. Recent advances in large language models (LLMs) have demonstrated strong potential in detecting fraudulent behavior by analyzing transcribed phone conversations. However, these capabilities introduce notable privacy risks, as such conversations frequently contain sensitive personal information that may be exposed to third-party service providers during processing. In this work, we explore how to harness LLMs for phone scam detection while preserving user privacy. We propose MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment based on individual preferences. MASK provides a pluggable architecture that accommodates diverse sanitization methods - from traditional keyword-based techniques for high-privacy users to sophisticated neural approaches for those prioritizing accuracy. We also discuss potential modeling approaches and loss function designs for future development, enabling the creation of truly personalized, privacy-aware LLM-based detection systems that balance user trust and detection effectiveness, even beyond phone scam context.
<div><strong>Authors:</strong> Kangzhong Wang, Zitong Shen, Youqian Zhang, Michael MK Cheung, Xiapu Luo, Grace Ngai, Eugene Yujun Fu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment for LLM-based phone scam detection, allowing users to choose between simple keyword-based sanitization and more advanced neural methods based on their privacy preferences. It outlines the architecture, possible modeling approaches, and loss functions to create personalized, privacy-preserving detection systems that balance trust and effectiveness, and discusses broader applications beyond phone scams.", "summary_cn": "本文提出了 MASK（模块化自适应清洗套件），一个可训练且可扩展的框架，用于在基于大语言模型的电话诈骗检测中实现动态隐私调节，用户可根据隐私偏好在传统关键词过滤和更高级的神经式清洗方法之间切换。文中阐述了体系结构、潜在建模方法和损失函数设计，以构建既能保护用户隐私又保持检测效果的个性化系统，并探讨了该框架在电话诈骗之外的应用前景。", "keywords": "privacy-preserving, phone scam detection, large language models, modular sanitization, adaptive privacy, trainable sanitization, keyword filtering, neural sanitization, personalized privacy, MASK", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Kangzhong Wang", "Zitong Shen", "Youqian Zhang", "Michael MK Cheung", "Xiapu Luo", "Grace Ngai", "Eugene Yujun Fu"]}
]]></acme>

<pubDate>2025-10-21T10:30:36+00:00</pubDate>
</item>
<item>
<title>Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</title>
<link>https://papers.cool/arxiv/2510.18473</link>
<guid>https://papers.cool/arxiv/2510.18473</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a systematic benchmark of fairness-aware Graph Neural Networks (GNNs) on large-scale knowledge graphs derived from YAGO, DBpedia, and Wikidata, evaluating both in‑processing and preprocessing mitigation methods across various GNN backbones and early‑stopping strategies. It reveals that knowledge graphs exhibit distinct accuracy–fairness trade‑offs compared to previously studied datasets, and that performance depends heavily on the choice of backbone and stopping criteria, with preprocessing improving fairness and in‑processing boosting prediction accuracy.<br /><strong>Summary (CN):</strong> 本文对来自 YAGO、DBpedia 和 Wikidata 的大规模知识图谱（KG）上的公平感知图神经网络（GNN）进行系统性基准测试，评估了多种内部（in‑processing）和预处理（preprocessing）公平缓解方法在不同 GNN 主干和提前停止条件下的表现。研究发现，与以往数据集相比，知识图谱呈现出更清晰的准确率与公平性指标之间的权衡，且性能受 GNN 主干和提前停止策略影响显著；预处理方法通常提升公平性指标，而内部方法更有利于提升预测准确率。<br /><strong>Keywords:</strong> fairness, graph neural networks, knowledge graphs, bias mitigation, benchmarking, GNN backbones, early stopping, preprocessing, inprocessing, recommendation systems<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Yuya Sasaki</div>
Graph neural networks (GNNs) are powerful tools for learning from graph-structured data but often produce biased predictions with respect to sensitive attributes. Fairness-aware GNNs have been actively studied for mitigating biased predictions. However, no prior studies have evaluated fairness-aware GNNs on knowledge graphs, which are one of the most important graphs in many applications, such as recommender systems. Therefore, we introduce a benchmarking study on knowledge graphs. We generate new graphs from three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly larger than the existing graph datasets used in fairness studies. We benchmark inprocessing and preprocessing methods in different GNN backbones and early stopping conditions. We find several key insights: (i) knowledge graphs show different trends from existing datasets; clearer trade-offs between prediction accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii) the performance is largely affected by not only fairness-aware GNN methods but also GNN backbones and early stopping conditions, and (iii) preprocessing methods often improve fairness metrics, while inprocessing methods improve prediction accuracy.
<div><strong>Authors:</strong> Yuya Sasaki</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a systematic benchmark of fairness-aware Graph Neural Networks (GNNs) on large-scale knowledge graphs derived from YAGO, DBpedia, and Wikidata, evaluating both in‑processing and preprocessing mitigation methods across various GNN backbones and early‑stopping strategies. It reveals that knowledge graphs exhibit distinct accuracy–fairness trade‑offs compared to previously studied datasets, and that performance depends heavily on the choice of backbone and stopping criteria, with preprocessing improving fairness and in‑processing boosting prediction accuracy.", "summary_cn": "本文对来自 YAGO、DBpedia 和 Wikidata 的大规模知识图谱（KG）上的公平感知图神经网络（GNN）进行系统性基准测试，评估了多种内部（in‑processing）和预处理（preprocessing）公平缓解方法在不同 GNN 主干和提前停止条件下的表现。研究发现，与以往数据集相比，知识图谱呈现出更清晰的准确率与公平性指标之间的权衡，且性能受 GNN 主干和提前停止策略影响显著；预处理方法通常提升公平性指标，而内部方法更有利于提升预测准确率。", "keywords": "fairness, graph neural networks, knowledge graphs, bias mitigation, benchmarking, GNN backbones, early stopping, preprocessing, inprocessing, recommendation systems", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Yuya Sasaki"]}
]]></acme>

<pubDate>2025-10-21T09:51:42+00:00</pubDate>
</item>
<item>
<title>CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment</title>
<link>https://papers.cool/arxiv/2510.18471</link>
<guid>https://papers.cool/arxiv/2510.18471</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CodeRL+, a reinforcement‑learning framework that augments RL with verifiable rewards by explicitly aligning generated code with its execution semantics. By constructing variable‑level execution trajectories from on‑policy rollouts, CodeRL+ provides a fine‑grained learning signal, achieving up to 4.6% relative improvement in pass@1 and large gains on code‑reasoning and test‑output tasks. Analyses show stronger correspondence between code text and underlying semantics across various models and RL algorithms.<br /><strong>Summary (CN):</strong> 本文提出 CodeRL+，一种在强化学习中通过显式对齐代码的执行语义来提升代码生成的框架。该方法利用在策略采样得到的变量级执行轨迹提供细粒度学习信号，使 pass@1 提升约 4.6%，并在代码推理和测试输出任务上取得显著改进。实验和探针分析表明，CodeRL+ 能显著增强代码文本与其底层执行语义之间的对应关系。<br /><strong>Keywords:</strong> code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, large language models, code reasoning, pass@1, variable-level trajectory<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
While Large Language Models (LLMs) excel at code generation by learning from vast code corpora, a fundamental semantic gap remains between their training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics. Reinforcement Learning with Verifiable Rewards (RLVR) approaches attempt to bridge this gap using outcome rewards from executing test cases. However, solely relying on binary pass/fail signals is inefficient for establishing a well-aligned connection between the textual representation of code and its execution semantics, especially for subtle logical errors within the code. In this paper, we propose CodeRL+, a novel approach that integrates execution semantics alignment into the RLVR training pipeline for code generation. CodeRL+ enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms. Extensive experiments demonstrate that CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. CodeRL+ generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively. CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.
<div><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CodeRL+, a reinforcement‑learning framework that augments RL with verifiable rewards by explicitly aligning generated code with its execution semantics. By constructing variable‑level execution trajectories from on‑policy rollouts, CodeRL+ provides a fine‑grained learning signal, achieving up to 4.6% relative improvement in pass@1 and large gains on code‑reasoning and test‑output tasks. Analyses show stronger correspondence between code text and underlying semantics across various models and RL algorithms.", "summary_cn": "本文提出 CodeRL+，一种在强化学习中通过显式对齐代码的执行语义来提升代码生成的框架。该方法利用在策略采样得到的变量级执行轨迹提供细粒度学习信号，使 pass@1 提升约 4.6%，并在代码推理和测试输出任务上取得显著改进。实验和探针分析表明，CodeRL+ 能显著增强代码文本与其底层执行语义之间的对应关系。", "keywords": "code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, large language models, code reasoning, pass@1, variable-level trajectory", "scoring": {"interpretability": 4, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-21T09:48:06+00:00</pubDate>
</item>
<item>
<title>Simple and Efficient Heterogeneous Temporal Graph Neural Network</title>
<link>https://papers.cool/arxiv/2510.18467</link>
<guid>https://papers.cool/arxiv/2510.18467</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SE-HTGNN, a Simple and Efficient Heterogeneous Temporal Graph Neural Network that integrates temporal modeling directly into spatial attention via a dynamic attention mechanism, and incorporates large language model prompts to capture implicit node-type properties. Experiments show up to 10× speed‑up over state‑of‑the‑art baselines while preserving top forecasting accuracy on heterogeneous temporal graph tasks.<br /><strong>Summary (CN):</strong> 本文提出了 SE-HTGNN，一种将时间建模直接融合到空间注意力中的简单高效异构时序图神经网络，并利用大语言模型提示来捕获节点类型的隐式属性。实验表明，在保持最佳预测精度的同时，模型相较于最新基线可实现约 10 倍的加速。<br /><strong>Keywords:</strong> heterogeneous temporal graph, graph neural network, dynamic attention, large language model prompting, spatio-temporal representation, forecasting, efficiency<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao</div>
Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy.
<div><strong>Authors:</strong> Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SE-HTGNN, a Simple and Efficient Heterogeneous Temporal Graph Neural Network that integrates temporal modeling directly into spatial attention via a dynamic attention mechanism, and incorporates large language model prompts to capture implicit node-type properties. Experiments show up to 10× speed‑up over state‑of‑the‑art baselines while preserving top forecasting accuracy on heterogeneous temporal graph tasks.", "summary_cn": "本文提出了 SE-HTGNN，一种将时间建模直接融合到空间注意力中的简单高效异构时序图神经网络，并利用大语言模型提示来捕获节点类型的隐式属性。实验表明，在保持最佳预测精度的同时，模型相较于最新基线可实现约 10 倍的加速。", "keywords": "heterogeneous temporal graph, graph neural network, dynamic attention, large language model prompting, spatio-temporal representation, forecasting, efficiency", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yili Wang", "Tairan Huang", "Changlong He", "Qiutong Li", "Jianliang Gao"]}
]]></acme>

<pubDate>2025-10-21T09:43:08+00:00</pubDate>
</item>
<item>
<title>DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation</title>
<link>https://papers.cool/arxiv/2510.18459</link>
<guid>https://papers.cool/arxiv/2510.18459</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeLoad, a demand‑driven short‑video preloading framework that dynamically adjusts download task sizes and employs a multi‑dimensional watch‑time estimation method, optimized by a deep reinforcement learning agent. Offline experiments on large‑scale network data show QoE improvements of up to 87.4%, and live deployment on a commercial platform yields a 0.09% increase in overall watch time while cutting rebuffering events and bandwidth consumption by 3.76%.<br /><strong>Summary (CN):</strong> 本文提出 DeLoad，这一需求驱动的短视频预加载框架通过动态调整下载任务大小并采用多维度观看时长估计方法，由深度强化学习代理进行自适应优化。离线大规模网络数据实验显示 QoE 提升 34.4% 至 87.4%，在商业平台的实际部署则实现整体观看时长提升 0.09%，并同时降低卡顿事件和 3.76% 的带宽消耗。<br /><strong>Keywords:</strong> short video streaming, preloading, watch-time estimation, dynamic task sizing, deep reinforcement learning, QoE, bandwidth efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tong Liu, Zhiwei Fan, Guanyan Peng, Haodan Zhang, Yucheng Zhang, Zhen Wang, Pengjin Xie, Liang Liu</div>
Short video streaming has become a dominant paradigm in digital media, characterized by rapid swiping interactions and diverse media content. A key technical challenge is designing an effective preloading strategy that dynamically selects and prioritizes download tasks from an evolving playlist, balancing Quality of Experience (QoE) and bandwidth efficiency under practical commercial constraints. However, real world analysis reveals critical limitations of existing approaches: (1) insufficient adaptation of download task sizes to dynamic conditions, and (2) watch time prediction models that are difficult to deploy reliably at scale. In this paper, we propose DeLoad, a novel preloading framework that addresses these issues by introducing dynamic task sizing and a practical, multi dimensional watch time estimation method. Additionally, a Deep Reinforcement Learning (DRL) enhanced agent is trained to optimize the download range decisions adaptively. Extensive evaluations conducted on an offline testing platform, leveraging massive real world network data, demonstrate that DeLoad achieves significant improvements in QoE metrics (34.4% to 87.4% gain). Furthermore, after deployment on a large scale commercial short video platform, DeLoad has increased overall user watch time by 0.09% while simultaneously reducing rebuffering events and 3.76% bandwidth consumption.
<div><strong>Authors:</strong> Tong Liu, Zhiwei Fan, Guanyan Peng, Haodan Zhang, Yucheng Zhang, Zhen Wang, Pengjin Xie, Liang Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeLoad, a demand‑driven short‑video preloading framework that dynamically adjusts download task sizes and employs a multi‑dimensional watch‑time estimation method, optimized by a deep reinforcement learning agent. Offline experiments on large‑scale network data show QoE improvements of up to 87.4%, and live deployment on a commercial platform yields a 0.09% increase in overall watch time while cutting rebuffering events and bandwidth consumption by 3.76%.", "summary_cn": "本文提出 DeLoad，这一需求驱动的短视频预加载框架通过动态调整下载任务大小并采用多维度观看时长估计方法，由深度强化学习代理进行自适应优化。离线大规模网络数据实验显示 QoE 提升 34.4% 至 87.4%，在商业平台的实际部署则实现整体观看时长提升 0.09%，并同时降低卡顿事件和 3.76% 的带宽消耗。", "keywords": "short video streaming, preloading, watch-time estimation, dynamic task sizing, deep reinforcement learning, QoE, bandwidth efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tong Liu", "Zhiwei Fan", "Guanyan Peng", "Haodan Zhang", "Yucheng Zhang", "Zhen Wang", "Pengjin Xie", "Liang Liu"]}
]]></acme>

<pubDate>2025-10-21T09:33:49+00:00</pubDate>
</item>
<item>
<title>ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</title>
<link>https://papers.cool/arxiv/2510.18433</link>
<guid>https://papers.cool/arxiv/2510.18433</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ImageGem is a large-scale dataset containing real-world interaction data from 57K users, including 242K customized LoRAs, 3M textual prompts and 5M generated images, aimed at studying fine‑grained personal preferences for generative models. Using the preference annotations, the authors train improved preference alignment models and evaluate retrieval and vision‑language models for personalized image retrieval and diffusion model recommendation, and propose an end‑to‑end latent‑weight editing framework to personalize diffusion models. The work demonstrates that such a dataset enables a new paradigm for generative model personalization.<br /><strong>Summary (CN):</strong> ImageGem 是一个大规模数据集，收集了 57K 用户的真实互动信息，包括 242K 定制 LoRA、300 万文本提示和 500 万生成图像，旨在研究生成模型对细粒度个人偏好的理解。利用这些偏好标注，作者训练了更佳的偏好对齐模型，并评估了检索模型和视觉语言模型在个性化图像检索及扩散模型推荐上的表现，同时提出了一种端到端的潜在权重空间编辑框架来个性化扩散模型。该工作表明该数据集能够实现生成模型个性化的全新范式。<br /><strong>Keywords:</strong> generative model personalization, user preference dataset, LoRA, diffusion models, preference alignment, vision-language retrieval, latent weight editing<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</div>
We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.
<div><strong>Authors:</strong> Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ImageGem is a large-scale dataset containing real-world interaction data from 57K users, including 242K customized LoRAs, 3M textual prompts and 5M generated images, aimed at studying fine‑grained personal preferences for generative models. Using the preference annotations, the authors train improved preference alignment models and evaluate retrieval and vision‑language models for personalized image retrieval and diffusion model recommendation, and propose an end‑to‑end latent‑weight editing framework to personalize diffusion models. The work demonstrates that such a dataset enables a new paradigm for generative model personalization.", "summary_cn": "ImageGem 是一个大规模数据集，收集了 57K 用户的真实互动信息，包括 242K 定制 LoRA、300 万文本提示和 500 万生成图像，旨在研究生成模型对细粒度个人偏好的理解。利用这些偏好标注，作者训练了更佳的偏好对齐模型，并评估了检索模型和视觉语言模型在个性化图像检索及扩散模型推荐上的表现，同时提出了一种端到端的潜在权重空间编辑框架来个性化扩散模型。该工作表明该数据集能够实现生成模型个性化的全新范式。", "keywords": "generative model personalization, user preference dataset, LoRA, diffusion models, preference alignment, vision-language retrieval, latent weight editing", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yuanhe Guo", "Linxi Xie", "Zhuoran Chen", "Kangrui Yu", "Ryan Po", "Guandao Yang", "Gordon Wetztein", "Hongyi Wen"]}
]]></acme>

<pubDate>2025-10-21T09:08:01+00:00</pubDate>
</item>
<item>
<title>ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters</title>
<link>https://papers.cool/arxiv/2510.18431</link>
<guid>https://papers.cool/arxiv/2510.18431</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents ScaleNet, a method for efficiently expanding pretrained vision transformer models by inserting additional layers that share weights with the original network and adding small parallel adapter modules as adjustment parameters. This approach enables depth scaling with minimal extra parameters and substantially reduces training cost, yielding notable accuracy gains on ImageNet‑1K compared to training from scratch. Experiments also show the method's applicability to downstream vision tasks such as object detection.<br /><strong>Summary (CN):</strong> 本文提出 ScaleNet，一种通过在已有视觉 Transformer (ViT) 中插入共享权重的额外层，并为每层添加小型并行适配器模块（adjustment parameters）来实现高效扩展的技术。该方法在几乎不增加参数量的情况下实现深度扩展，大幅降低训练成本，并在 ImageNet‑1K 上相较于从头训练取得显著的准确率提升。实验还表明该方法同样适用于目标检测等下游视觉任务。<br /><strong>Keywords:</strong> ScaleNet, vision transformer, model scaling, weight sharing, adapters, efficient training, depth expansion, ImageNet<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</div>
Recent advancements in vision transformers (ViTs) have demonstrated that larger models often achieve superior performance. However, training these models remains computationally intensive and costly. To address this challenge, we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike conventional training from scratch, ScaleNet facilitates rapid model expansion with negligible increases in parameters, building on existing pretrained models. This offers a cost-effective solution for scaling up ViTs. Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. Each added layer shares its parameter tensor with a corresponding layer from the pretrained model. To mitigate potential performance degradation due to shared weights, ScaleNet introduces a small set of adjustment parameters for each layer. These adjustment parameters are implemented through parallel adapter modules, ensuring that each instance of the shared parameter tensor remains distinct and optimized for its specific function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet enables efficient expansion of ViT models. With a 2$\times$ depth-scaled DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs, highlighting its efficiency in scaling ViTs. Beyond image classification, our method shows significant potential for application in downstream vision areas, as evidenced by the validation in object detection task.
<div><strong>Authors:</strong> Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents ScaleNet, a method for efficiently expanding pretrained vision transformer models by inserting additional layers that share weights with the original network and adding small parallel adapter modules as adjustment parameters. This approach enables depth scaling with minimal extra parameters and substantially reduces training cost, yielding notable accuracy gains on ImageNet‑1K compared to training from scratch. Experiments also show the method's applicability to downstream vision tasks such as object detection.", "summary_cn": "本文提出 ScaleNet，一种通过在已有视觉 Transformer (ViT) 中插入共享权重的额外层，并为每层添加小型并行适配器模块（adjustment parameters）来实现高效扩展的技术。该方法在几乎不增加参数量的情况下实现深度扩展，大幅降低训练成本，并在 ImageNet‑1K 上相较于从头训练取得显著的准确率提升。实验还表明该方法同样适用于目标检测等下游视觉任务。", "keywords": "ScaleNet, vision transformer, model scaling, weight sharing, adapters, efficient training, depth expansion, ImageNet", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhiwei Hao", "Jianyuan Guo", "Li Shen", "Kai Han", "Yehui Tang", "Han Hu", "Yunhe Wang"]}
]]></acme>

<pubDate>2025-10-21T09:07:25+00:00</pubDate>
</item>
<item>
<title>Optimistic Higher-Order Superposition</title>
<link>https://papers.cool/arxiv/2510.18429</link>
<guid>https://papers.cool/arxiv/2510.18429</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an optimistic variant of the λ‑superposition calculus for higher‑order logic, which postpones costly unification problems by attaching constraints to clauses and applies functional extensionality in a more selective manner. The new calculus is proved sound and refutationally complete with respect to Henkin semantics, and preliminary examples indicate it could outperform or complement the traditional λ‑superposition approach, though it has not yet been implemented in a prover.<br /><strong>Summary (CN):</strong> 本文提出了一种乐观版的 λ‑superposition 演算，用于高阶逻辑推理，通过在子句中附加约束来延迟代价高昂的统一问题，并更有针对性地应用函数外延性公理。该演算在 Henkin 语义下被证明是可靠且具有反证完整性的，尽管尚未在证明器中实现，初步示例显示它有望优于或有益补充传统的 λ‑superposition 方法。<br /><strong>Keywords:</strong> higher-order theorem proving, lambda-superposition, optimistic calculus, unification constraints, functional extensionality, Henkin semantics, automated reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexander Bentkamp, Jasmin Blanchette, Matthias Hetzenberger, Uwe Waldmann</div>
The $\lambda$-superposition calculus is a successful approach to proving higher-order formulas. However, some parts of the calculus are extremely explosive, notably due to the higher-order unifier enumeration and the functional extensionality axiom. In the present work, we introduce an "optimistic" version of $\lambda$-superposition that addresses these two issues. Specifically, our new calculus delays explosive unification problems using constraints stored along with the clauses, and it applies functional extensionality in a more targeted way. The calculus is sound and refutationally complete with respect to a Henkin semantics. We have yet to implement it in a prover, but examples suggest that it will outperform, or at least usefully complement, the original $\lambda$-superposition calculus.
<div><strong>Authors:</strong> Alexander Bentkamp, Jasmin Blanchette, Matthias Hetzenberger, Uwe Waldmann</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an optimistic variant of the λ‑superposition calculus for higher‑order logic, which postpones costly unification problems by attaching constraints to clauses and applies functional extensionality in a more selective manner. The new calculus is proved sound and refutationally complete with respect to Henkin semantics, and preliminary examples indicate it could outperform or complement the traditional λ‑superposition approach, though it has not yet been implemented in a prover.", "summary_cn": "本文提出了一种乐观版的 λ‑superposition 演算，用于高阶逻辑推理，通过在子句中附加约束来延迟代价高昂的统一问题，并更有针对性地应用函数外延性公理。该演算在 Henkin 语义下被证明是可靠且具有反证完整性的，尽管尚未在证明器中实现，初步示例显示它有望优于或有益补充传统的 λ‑superposition 方法。", "keywords": "higher-order theorem proving, lambda-superposition, optimistic calculus, unification constraints, functional extensionality, Henkin semantics, automated reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexander Bentkamp", "Jasmin Blanchette", "Matthias Hetzenberger", "Uwe Waldmann"]}
]]></acme>

<pubDate>2025-10-21T09:05:54+00:00</pubDate>
</item>
<item>
<title>On AI Verification in Open RAN</title>
<link>https://papers.cool/arxiv/2510.18417</link>
<guid>https://papers.cool/arxiv/2510.18417</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a lightweight verification framework for Deep Reinforcement Learning agents used in Open RAN slicing and scheduling, employing Decision Tree‑based verifiers to perform near‑real‑time consistency checks. By integrating interpretable models into the AI workflow, the approach aims to complement XAI methods and provide scalable, runtime assurance of AI‑driven network operations. The authors present an architectural design, a feasibility study with a DT‑based slice verifier, and discuss future challenges for trustworthy AI adoption in Open RAN.<br /><strong>Summary (CN):</strong> 本文提出一种轻量级验证框架，用于 Open RAN 中负责切片和调度的深度强化学习（DRL）代理，采用基于决策树（Decision Tree）的可解释模型在近实时进行一致性检查。该方法在 AI 工作流中加入可解释模型，以补充 XAI，提供可扩展的运行时安全保证。作者展示了架构设计、基于决策树的切片验证器的可行性实验，并讨论了实现可信 AI 采用的未来挑战。<br /><strong>Keywords:</strong> Open RAN, AI verification, Deep Reinforcement Learning, Decision Tree, XAI, network slicing, scheduling, interpretable models, trustworthy AI<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Rahul Soundrarajan, Claudio Fiandrino, Michele Polese, Salvatore D'Oro, Leonardo Bonati, Tommaso Melodia</div>
Open RAN introduces a flexible, cloud-based architecture for the Radio Access Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning (ML)-driven automation across heterogeneous, multi-vendor deployments. While EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI models, explainability alone does not guarantee reliable network operations. In this article, we propose a lightweight verification approach based on interpretable models to validate the behavior of Deep Reinforcement Learning (DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use Decision Tree (DT)-based verifiers to perform near-real-time consistency checks at runtime, which would be otherwise unfeasible with computationally expensive state-of-the-art verifiers. We analyze the landscape of XAI and AI verification, propose a scalable architectural integration, and demonstrate feasibility with a DT-based slice-verifier. We also outline future challenges to ensure trustworthy AI adoption in Open RAN.
<div><strong>Authors:</strong> Rahul Soundrarajan, Claudio Fiandrino, Michele Polese, Salvatore D'Oro, Leonardo Bonati, Tommaso Melodia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a lightweight verification framework for Deep Reinforcement Learning agents used in Open RAN slicing and scheduling, employing Decision Tree‑based verifiers to perform near‑real‑time consistency checks. By integrating interpretable models into the AI workflow, the approach aims to complement XAI methods and provide scalable, runtime assurance of AI‑driven network operations. The authors present an architectural design, a feasibility study with a DT‑based slice verifier, and discuss future challenges for trustworthy AI adoption in Open RAN.", "summary_cn": "本文提出一种轻量级验证框架，用于 Open RAN 中负责切片和调度的深度强化学习（DRL）代理，采用基于决策树（Decision Tree）的可解释模型在近实时进行一致性检查。该方法在 AI 工作流中加入可解释模型，以补充 XAI，提供可扩展的运行时安全保证。作者展示了架构设计、基于决策树的切片验证器的可行性实验，并讨论了实现可信 AI 采用的未来挑战。", "keywords": "Open RAN, AI verification, Deep Reinforcement Learning, Decision Tree, XAI, network slicing, scheduling, interpretable models, trustworthy AI", "scoring": {"interpretability": 7, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Rahul Soundrarajan", "Claudio Fiandrino", "Michele Polese", "Salvatore D'Oro", "Leonardo Bonati", "Tommaso Melodia"]}
]]></acme>

<pubDate>2025-10-21T08:48:26+00:00</pubDate>
</item>
<item>
<title>Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</title>
<link>https://papers.cool/arxiv/2510.18406</link>
<guid>https://papers.cool/arxiv/2510.18406</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies weakly supervised learning where each training example is an n‑tuple containing exactly m positive instances, but only the count m is observed (NTMP setting). It derives an unbiased risk estimator (URE) for fixed and variable tuple sizes and counts, provides theoretical guarantees including generalization bounds and consistency, and proposes simple ReLU corrections for finite‑sample stability. Experiments on converted benchmarks show the method outperforms existing weak‑supervision baselines and remains robust to class‑prior imbalance.<br /><strong>Summary (CN):</strong> 本文研究一种弱监督学习场景：每个训练样本是包含恰好 m 个正例的 n‑元组，仅观测到正例数量 m（NTMP 设置）。作者推导了固定与可变元组大小/计数的无偏风险估计器（URE），并给出泛化界和一致性等理论保证，同时提出了保持渐进正确性的 ReLU 修正以提升有限样本的稳定性。实验在转化后的基准上显示该方法相较于传统弱监督基线有明显提升，并对类别先验不平衡具有鲁棒性。<br /><strong>Keywords:</strong> weak supervision, N-tuple, unbiased risk estimator, count supervision, multi-instance learning, Rademacher complexity, consistency, ReLU correction, class-prior imbalance, theoretical guarantees<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</div>
Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
<div><strong>Authors:</strong> Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies weakly supervised learning where each training example is an n‑tuple containing exactly m positive instances, but only the count m is observed (NTMP setting). It derives an unbiased risk estimator (URE) for fixed and variable tuple sizes and counts, provides theoretical guarantees including generalization bounds and consistency, and proposes simple ReLU corrections for finite‑sample stability. Experiments on converted benchmarks show the method outperforms existing weak‑supervision baselines and remains robust to class‑prior imbalance.", "summary_cn": "本文研究一种弱监督学习场景：每个训练样本是包含恰好 m 个正例的 n‑元组，仅观测到正例数量 m（NTMP 设置）。作者推导了固定与可变元组大小/计数的无偏风险估计器（URE），并给出泛化界和一致性等理论保证，同时提出了保持渐进正确性的 ReLU 修正以提升有限样本的稳定性。实验在转化后的基准上显示该方法相较于传统弱监督基线有明显提升，并对类别先验不平衡具有鲁棒性。", "keywords": "weak supervision, N-tuple, unbiased risk estimator, count supervision, multi-instance learning, Rademacher complexity, consistency, ReLU correction, class-prior imbalance, theoretical guarantees", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Miao Zhang", "Junpeng Li", "ChangChun HUa", "Yana Yang"]}
]]></acme>

<pubDate>2025-10-21T08:28:07+00:00</pubDate>
</item>
<item>
<title>Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</title>
<link>https://papers.cool/arxiv/2510.18405</link>
<guid>https://papers.cool/arxiv/2510.18405</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an automated system for analyzing cricket videos that combines YOLOv8 for pitch and ball detection with OCR to extract scorecard information and identify wicket‑taking deliveries. It includes extensive image preprocessing and achieves high detection performance (mAP50 > 99%) before modeling ball trajectories to reveal batting weaknesses, demonstrating potential for coaching and strategic decision‑making.<br /><strong>Summary (CN):</strong> 本文提出了一套自动化的板球视频分析系统，利用 YOLOv8 检测投球区和球体，并通过 OCR 提取记分板信息以识别夺门球。系统包含灰度、幂变换和形态学操作等预处理，检测精度达 99% 以上，随后对球的轨迹进行建模，用于发现击球弱点，展示了在教练和战术决策中的应用潜力。<br /><strong>Keywords:</strong> cricket video analysis, YOLOv8, OCR, ball detection, trajectory modeling, sports analytics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 3, Safety: 1, Technicality: 6, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe</div>
This paper presents an automated system for cricket video analysis that leverages deep learning techniques to extract wicket-taking deliveries, detect cricket balls, and model ball trajectories. The system employs the YOLOv8 architecture for pitch and ball detection, combined with optical character recognition (OCR) for scorecard extraction to identify wicket-taking moments. Through comprehensive image preprocessing, including grayscale transformation, power transformation, and morphological operations, the system achieves robust text extraction from video frames. The pitch detection model achieved 99.5% mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while the ball detection model using transfer learning attained 99.18% mAP50 with 0.968 precision and 0.978 recall. The system enables trajectory modeling on detected pitches, providing data-driven insights for identifying batting weaknesses. Experimental results on multiple cricket match videos demonstrate the effectiveness of this approach for automated cricket analytics, offering significant potential for coaching and strategic decision-making.
<div><strong>Authors:</strong> Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an automated system for analyzing cricket videos that combines YOLOv8 for pitch and ball detection with OCR to extract scorecard information and identify wicket‑taking deliveries. It includes extensive image preprocessing and achieves high detection performance (mAP50 > 99%) before modeling ball trajectories to reveal batting weaknesses, demonstrating potential for coaching and strategic decision‑making.", "summary_cn": "本文提出了一套自动化的板球视频分析系统，利用 YOLOv8 检测投球区和球体，并通过 OCR 提取记分板信息以识别夺门球。系统包含灰度、幂变换和形态学操作等预处理，检测精度达 99% 以上，随后对球的轨迹进行建模，用于发现击球弱点，展示了在教练和战术决策中的应用潜力。", "keywords": "cricket video analysis, YOLOv8, OCR, ball detection, trajectory modeling, sports analytics", "scoring": {"interpretability": 2, "understanding": 3, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mst Jannatun Ferdous", "Masum Billah", "Joy Karmoker", "Mohd Ruhul Ameen", "Akif Islam", "Md. Omar Faruqe"]}
]]></acme>

<pubDate>2025-10-21T08:27:23+00:00</pubDate>
</item>
<item>
<title>MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models</title>
<link>https://papers.cool/arxiv/2510.18383</link>
<guid>https://papers.cool/arxiv/2510.18383</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MENTOR is a reinforcement learning framework that distills tool‑using capabilities from large language models into small language models by employing dense teacher‑guided rewards rather than sparse rewards, enabling more effective exploration and improved cross‑domain generalization. Experiments demonstrate that it outperforms supervised fine‑tuning and standard sparse‑reward RL baselines in strategic competence and generalization.<br /><strong>Summary (CN):</strong> MENTOR 是一个强化学习框架，通过使用教师参考轨迹构建的密集教师引导奖励，将大语言模型的工具使用能力蒸馏到小语言模型，从而提升探索效率和跨领域泛化能力。实验表明，它相较于监督微调和传统稀疏奖励 RL 基线显著提升了小模型的策略水平和跨域表现。<br /><strong>Keywords:</strong> reinforcement learning, distillation, small language models, teacher-guided reward, cross-domain generalization, model enhancement, RL-based policy learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.
<div><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MENTOR is a reinforcement learning framework that distills tool‑using capabilities from large language models into small language models by employing dense teacher‑guided rewards rather than sparse rewards, enabling more effective exploration and improved cross‑domain generalization. Experiments demonstrate that it outperforms supervised fine‑tuning and standard sparse‑reward RL baselines in strategic competence and generalization.", "summary_cn": "MENTOR 是一个强化学习框架，通过使用教师参考轨迹构建的密集教师引导奖励，将大语言模型的工具使用能力蒸馏到小语言模型，从而提升探索效率和跨领域泛化能力。实验表明，它相较于监督微调和传统稀疏奖励 RL 基线显著提升了小模型的策略水平和跨域表现。", "keywords": "reinforcement learning, distillation, small language models, teacher-guided reward, cross-domain generalization, model enhancement, RL-based policy learning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"]}
]]></acme>

<pubDate>2025-10-21T08:03:14+00:00</pubDate>
</item>
<item>
<title>S2AP: Score-space Sharpness Minimization for Adversarial Pruning</title>
<link>https://papers.cool/arxiv/2510.18381</link>
<guid>https://papers.cool/arxiv/2510.18381</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug-in method that reduces sharpness in the importance-score landscape during mask search by perturbing scores and minimizing the robust loss. By stabilizing mask selection, S2AP improves the robustness of adversarially pruned neural networks across datasets, models, and sparsity levels.<br /><strong>Summary (CN):</strong> 本文提出了分数空间锐度感知对抗剪枝（S2AP）方法，通过在掩码搜索期间扰动权重重要性分数并最小化对应的鲁棒损失，以降低分数空间的锐度。该方法稳定了掩码选择，提升了剪枝后模型在不同数据集、网络和稀疏率下的对抗鲁棒性。<br /><strong>Keywords:</strong> adversarial pruning, score-space sharpness, mask selection, model robustness, neural network compression, S2AP, robust loss, importance scores<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
Adversarial pruning methods have emerged as a powerful tool for compressing neural networks while preserving robustness against adversarial attacks. These methods typically follow a three-step pipeline: (i) pretrain a robust model, (ii) select a binary mask for weight pruning, and (iii) finetune the pruned model. To select the binary mask, these methods minimize a robust loss by assigning an importance score to each weight, and then keep the weights with the highest scores. However, this score-space optimization can lead to sharp local minima in the robust loss landscape and, in turn, to an unstable mask selection, reducing the robustness of adversarial pruning methods. To overcome this issue, we propose a novel plug-in method for adversarial pruning, termed Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we introduce the concept of score-space sharpness minimization, which operates during the mask search by perturbing importance scores and minimizing the corresponding robust loss. Extensive experiments across various datasets, models, and sparsity levels demonstrate that S2AP effectively minimizes sharpness in score space, stabilizing the mask selection, and ultimately improving the robustness of adversarial pruning methods.
<div><strong>Authors:</strong> Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Score-space Sharpness-aware Adversarial Pruning (S2AP), a plug-in method that reduces sharpness in the importance-score landscape during mask search by perturbing scores and minimizing the robust loss. By stabilizing mask selection, S2AP improves the robustness of adversarially pruned neural networks across datasets, models, and sparsity levels.", "summary_cn": "本文提出了分数空间锐度感知对抗剪枝（S2AP）方法，通过在掩码搜索期间扰动权重重要性分数并最小化对应的鲁棒损失，以降低分数空间的锐度。该方法稳定了掩码选择，提升了剪枝后模型在不同数据集、网络和稀疏率下的对抗鲁棒性。", "keywords": "adversarial pruning, score-space sharpness, mask selection, model robustness, neural network compression, S2AP, robust loss, importance scores", "scoring": {"interpretability": 4, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Giorgio Piras", "Qi Zhao", "Fabio Brau", "Maura Pintor", "Christian Wressnegger", "Battista Biggio"]}
]]></acme>

<pubDate>2025-10-21T07:55:31+00:00</pubDate>
</item>
<item>
<title>PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion</title>
<link>https://papers.cool/arxiv/2510.18348</link>
<guid>https://papers.cool/arxiv/2510.18348</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Phase-Guided Terrain Traversal (PGTT), a perception‑aware deep reinforcement learning method that imposes gait structure solely through reward shaping, enabling morphology‑agnostic joint‑space policies for legged robots. Trained on procedurally generated stair‑like terrains in MuJoCo with curriculum and domain randomization, PGTT outperforms baselines on disturbance robustness and obstacle negotiation, and transfers to real hardware (Unitree Go2, ANYmal‑C) using a LiDAR‑derived heightmap.<br /><strong>Summary (CN):</strong> 本文提出了相位引导的地形穿越 (PGTT) 方法，通过奖励塑形而非振荡器或逆向运动学先验来约束步态结构，使得机器人能够在关节空间直接学习、跨形态部署的感知强化学习控制策略。该方法在 MuJoCo 中的阶梯地形上进行课程学习和域随机化训练，显著提升了对推力扰动和离散障碍的鲁棒性，并通过 LiDAR 高程转高度图管线成功迁移至实物 Unitree Go2 与 ANYmal‑C 机器人。<br /><strong>Keywords:</strong> legged locomotion, reinforcement learning, perceptive control, phase-guided reward shaping, terrain adaptation, morphology-agnostic, MuJoCo, LiDAR heightmap<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Alexandros Ntagkas, Chairi Kiourt, Konstantinos Chatzilygeroudis</div>
State-of-the-art perceptive Reinforcement Learning controllers for legged robots either (i) impose oscillator or IK-based gait priors that constrain the action space, add bias to the policy optimization and reduce adaptability across robot morphologies, or (ii) operate "blind", which struggle to anticipate hind-leg terrain, and are brittle to noise. In this paper, we propose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL approach that overcomes these limitations by enforcing gait structure purely through reward shaping, thereby reducing inductive bias in policy learning compared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase as a cubic Hermite spline that adapts swing height to local heightmap statistics and adds a swing- phase contact penalty, while the policy acts directly in joint space supporting morphology-agnostic deployment. Trained in MuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and domain randomization, PGTT achieves the highest success under push disturbances (median +7.5% vs. the next best method) and on discrete obstacles (+9%), with comparable velocity tracking, and converging to an effective policy roughly 2x faster than strong end-to-end baselines. We validate PGTT on a Unitree Go2 using a real-time LiDAR elevation-to-heightmap pipeline, and we report preliminary results on ANYmal-C obtained with the same hyperparameters. These findings indicate that terrain-adaptive, phase-guided reward shaping is a simple and general mechanism for robust perceptive locomotion across platforms.
<div><strong>Authors:</strong> Alexandros Ntagkas, Chairi Kiourt, Konstantinos Chatzilygeroudis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Phase-Guided Terrain Traversal (PGTT), a perception‑aware deep reinforcement learning method that imposes gait structure solely through reward shaping, enabling morphology‑agnostic joint‑space policies for legged robots. Trained on procedurally generated stair‑like terrains in MuJoCo with curriculum and domain randomization, PGTT outperforms baselines on disturbance robustness and obstacle negotiation, and transfers to real hardware (Unitree Go2, ANYmal‑C) using a LiDAR‑derived heightmap.", "summary_cn": "本文提出了相位引导的地形穿越 (PGTT) 方法，通过奖励塑形而非振荡器或逆向运动学先验来约束步态结构，使得机器人能够在关节空间直接学习、跨形态部署的感知强化学习控制策略。该方法在 MuJoCo 中的阶梯地形上进行课程学习和域随机化训练，显著提升了对推力扰动和离散障碍的鲁棒性，并通过 LiDAR 高程转高度图管线成功迁移至实物 Unitree Go2 与 ANYmal‑C 机器人。", "keywords": "legged locomotion, reinforcement learning, perceptive control, phase-guided reward shaping, terrain adaptation, morphology-agnostic, MuJoCo, LiDAR heightmap", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Alexandros Ntagkas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"]}
]]></acme>

<pubDate>2025-10-21T07:00:18+00:00</pubDate>
</item>
<item>
<title>Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</title>
<link>https://papers.cool/arxiv/2510.18328</link>
<guid>https://papers.cool/arxiv/2510.18328</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Time-Conditioned Contraction Matching (TCCM), a semi-supervised anomaly detection method for tabular data that learns a velocity field toward a fixed target and uses a one-step deviation score for fast inference. TCCM provides feature-wise attribution for explainability and guarantees Lipschitz continuity of the anomaly score, offering provable robustness against small perturbations. Experiments on the ADBench benchmark demonstrate strong accuracy and scalability compared to diffusion-based methods.<br /><strong>Summary (CN):</strong> 本文提出时间条件收缩匹配（TCCM），一种用于表格数据的半监督异常检测方法，通过学习指向固定目标的速度场并使用一步偏差得分实现快速推断。TCCM 在输入空间直接操作，实现特征层面的可解释性，并且其异常得分满足 Lipschitz 连续性，提供对小扰动的可证明鲁棒性。实验在 ADBench 基准上显示出在准确率和可扩展性方面优于基于扩散的模型。<br /><strong>Keywords:</strong> anomaly detection, flow matching, time-conditioned contraction, semi-supervised, explainability, provable robustness, Lipschitz continuity, feature attribution, high-dimensional data, scalable<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</div>
We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.
<div><strong>Authors:</strong> Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Time-Conditioned Contraction Matching (TCCM), a semi-supervised anomaly detection method for tabular data that learns a velocity field toward a fixed target and uses a one-step deviation score for fast inference. TCCM provides feature-wise attribution for explainability and guarantees Lipschitz continuity of the anomaly score, offering provable robustness against small perturbations. Experiments on the ADBench benchmark demonstrate strong accuracy and scalability compared to diffusion-based methods.", "summary_cn": "本文提出时间条件收缩匹配（TCCM），一种用于表格数据的半监督异常检测方法，通过学习指向固定目标的速度场并使用一步偏差得分实现快速推断。TCCM 在输入空间直接操作，实现特征层面的可解释性，并且其异常得分满足 Lipschitz 连续性，提供对小扰动的可证明鲁棒性。实验在 ADBench 基准上显示出在准确率和可扩展性方面优于基于扩散的模型。", "keywords": "anomaly detection, flow matching, time-conditioned contraction, semi-supervised, explainability, provable robustness, Lipschitz continuity, feature attribution, high-dimensional data, scalable", "scoring": {"interpretability": 6, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Zhong Li", "Qi Huang", "Yuxuan Zhu", "Lincen Yang", "Mohammad Mohammadi Amiri", "Niki van Stein", "Matthijs van Leeuwen"]}
]]></acme>

<pubDate>2025-10-21T06:26:38+00:00</pubDate>
</item>
<item>
<title>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</title>
<link>https://papers.cool/arxiv/2510.18316</link>
<guid>https://papers.cool/arxiv/2510.18316</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MoMaGen formulates demonstration generation for multi-step bimanual mobile manipulation as a constrained optimization problem that enforces hard constraints such as reachability and balances soft constraints like camera visibility. By solving this problem in simulation, it automatically produces much more diverse datasets than prior methods, enabling successful imitation‑learning policies to be trained from a single source demonstration and fine‑tuned with as few as 40 real‑world demos for deployment on physical robots.<br /><strong>Summary (CN):</strong> MoMaGen将示例生成表述为约束优化问题，通过硬约束（如可达性）和软约束（如可视性）共同约束移动底盘和双臂的姿态，从而在仿真中自动生成多步双臂移动操作的多样化演示。实验表明，与现有方法相比，它显著提升了数据多样性，使得仅凭单一源演示即可训练模仿学习策略，并在仅需约40条真实演示的微调后成功部署到实体机器人上。<br /><strong>Keywords:</strong> demonstration generation, constrained optimization, bimanual manipulation, mobile manipulation, imitation learning, data augmentation, robot learning, soft constraints, hard constraints, visibility<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
Imitation learning from large-scale, diverse human demonstrations has proven effective for training robots, but collecting such data is costly and time-consuming. This challenge is amplified for multi-step bimanual mobile manipulation, where humans must teleoperate both a mobile base and two high-degree-of-freedom arms. Prior automated data generation frameworks have addressed static bimanual manipulation by augmenting a few human demonstrations in simulation, but they fall short for mobile settings due to two key challenges: (1) determining base placement to ensure reachability, and (2) positioning the camera to provide sufficient visibility for visuomotor policies. To address these issues, we introduce MoMaGen, which formulates data generation as a constrained optimization problem that enforces hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility during navigation). This formulation generalizes prior approaches and provides a principled foundation for future methods. We evaluate MoMaGen on four multi-step bimanual mobile manipulation tasks and show that it generates significantly more diverse datasets than existing methods. Leveraging this diversity, MoMaGen can train successful imitation learning policies from a single source demonstration, and these policies can be fine-tuned with as few as 40 real-world demonstrations to achieve deployment on physical robotic hardware. More details are available at our project page: momagen.github.io.
<div><strong>Authors:</strong> Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MoMaGen formulates demonstration generation for multi-step bimanual mobile manipulation as a constrained optimization problem that enforces hard constraints such as reachability and balances soft constraints like camera visibility. By solving this problem in simulation, it automatically produces much more diverse datasets than prior methods, enabling successful imitation‑learning policies to be trained from a single source demonstration and fine‑tuned with as few as 40 real‑world demos for deployment on physical robots.", "summary_cn": "MoMaGen将示例生成表述为约束优化问题，通过硬约束（如可达性）和软约束（如可视性）共同约束移动底盘和双臂的姿态，从而在仿真中自动生成多步双臂移动操作的多样化演示。实验表明，与现有方法相比，它显著提升了数据多样性，使得仅凭单一源演示即可训练模仿学习策略，并在仅需约40条真实演示的微调后成功部署到实体机器人上。", "keywords": "demonstration generation, constrained optimization, bimanual manipulation, mobile manipulation, imitation learning, data augmentation, robot learning, soft constraints, hard constraints, visibility", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chengshu Li", "Mengdi Xu", "Arpit Bahety", "Hang Yin", "Yunfan Jiang", "Huang Huang", "Josiah Wong", "Sujay Garlanka", "Cem Gokmen", "Ruohan Zhang", "Weiyu Liu", "Jiajun Wu", "Roberto Martín-Martín", "Li Fei-Fei"]}
]]></acme>

<pubDate>2025-10-21T05:56:47+00:00</pubDate>
</item>
<item>
<title>Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task</title>
<link>https://papers.cool/arxiv/2510.18315</link>
<guid>https://papers.cool/arxiv/2510.18315</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how the embedding dimension influences the emergence of an internal world model in a transformer trained via reinforcement learning to perform a bubble-sort-like adjacent swapping task. While small embeddings already achieve high accuracy, larger dimensions produce more faithful, consistent, and robust internal representations, with the last row of the attention matrix encoding global token order and selections aligning with the largest adjacent differences. These findings demonstrate that higher embedding dimensions strengthen structured internal representations and improve interpretability of algorithmic transformers.<br /><strong>Summary (CN):</strong> 本文研究了嵌入维度对使用强化学习训练的 transformer 在冒泡排序式相邻交换任务中形成内部“世界模型”的影响。尽管小维度嵌入已经能够实现高准确率，但更大的嵌入维度会产生更可信、一致且鲁棒的内部表征，其中注意力权重矩阵的最后一行单调编码全局 token 顺序，所选的换位对应于这些编码值的最大相邻差异。这表明更高的嵌入维度能够加强结构化内部表征并提升可解释性。<br /><strong>Keywords:</strong> embedding dimension, transformer, world model, interpretability, bubble sort, attention matrix, reinforcement learning, algorithmic task<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Brady Bhalla, Honglu Fan, Nancy Chen, Tony Yue YU</div>
We investigate how embedding dimension affects the emergence of an internal "world model" in a transformer trained with reinforcement learning to perform bubble-sort-style adjacent swaps. Models achieve high accuracy even with very small embedding dimensions, but larger dimensions yield more faithful, consistent, and robust internal representations. In particular, higher embedding dimensions strengthen the formation of structured internal representation and lead to better interpretability. After hundreds of experiments, we observe two consistent mechanisms: (1) the last row of the attention weight matrix monotonically encodes the global ordering of tokens; and (2) the selected transposition aligns with the largest adjacent difference of these encoded values. Our results provide quantitative evidence that transformers build structured internal world models and that model size improves representation quality in addition to end performance. We release our metrics and analyses, which can be used to probe similar algorithmic tasks.
<div><strong>Authors:</strong> Brady Bhalla, Honglu Fan, Nancy Chen, Tony Yue YU</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how the embedding dimension influences the emergence of an internal world model in a transformer trained via reinforcement learning to perform a bubble-sort-like adjacent swapping task. While small embeddings already achieve high accuracy, larger dimensions produce more faithful, consistent, and robust internal representations, with the last row of the attention matrix encoding global token order and selections aligning with the largest adjacent differences. These findings demonstrate that higher embedding dimensions strengthen structured internal representations and improve interpretability of algorithmic transformers.", "summary_cn": "本文研究了嵌入维度对使用强化学习训练的 transformer 在冒泡排序式相邻交换任务中形成内部“世界模型”的影响。尽管小维度嵌入已经能够实现高准确率，但更大的嵌入维度会产生更可信、一致且鲁棒的内部表征，其中注意力权重矩阵的最后一行单调编码全局 token 顺序，所选的换位对应于这些编码值的最大相邻差异。这表明更高的嵌入维度能够加强结构化内部表征并提升可解释性。", "keywords": "embedding dimension, transformer, world model, interpretability, bubble sort, attention matrix, reinforcement learning, algorithmic task", "scoring": {"interpretability": 8, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Brady Bhalla", "Honglu Fan", "Nancy Chen", "Tony Yue YU"]}
]]></acme>

<pubDate>2025-10-21T05:51:02+00:00</pubDate>
</item>
<item>
<title>From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</title>
<link>https://papers.cool/arxiv/2510.18297</link>
<guid>https://papers.cool/arxiv/2510.18297</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MedRGAG, a unified framework that combines external retrieved knowledge and internally generated knowledge for medical question answering. It proposes two modules—Knowledge-Guided Context Completion (KGCC) to generate complementary background documents and Knowledge-Aware Document Selection (KADS) to flexibly select an optimal mix of retrieved and generated evidence—yielding notable performance gains over prior RAG and GAG methods on multiple benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 MedRGAG 框架，将外部检索知识与模型内部生成的知识统一用于医疗问答。通过知识引导的上下文补全 (KGCC) 生成补充文档，并使用知识感知文档选择 (KADS) 动态挑选检索与生成文档的最佳组合，从而在多个基准上显著超越传统的检索增强生成 (RAG) 与生成增强生成 (GAG) 方法。<br /><strong>Keywords:</strong> medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge integration, hallucination mitigation, KGCC, KADS<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG
<div><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MedRGAG, a unified framework that combines external retrieved knowledge and internally generated knowledge for medical question answering. It proposes two modules—Knowledge-Guided Context Completion (KGCC) to generate complementary background documents and Knowledge-Aware Document Selection (KADS) to flexibly select an optimal mix of retrieved and generated evidence—yielding notable performance gains over prior RAG and GAG methods on multiple benchmarks.", "summary_cn": "本文提出 MedRGAG 框架，将外部检索知识与模型内部生成的知识统一用于医疗问答。通过知识引导的上下文补全 (KGCC) 生成补充文档，并使用知识感知文档选择 (KADS) 动态挑选检索与生成文档的最佳组合，从而在多个基准上显著超越传统的检索增强生成 (RAG) 与生成增强生成 (GAG) 方法。", "keywords": "medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge integration, hallucination mitigation, KGCC, KADS", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Lei Li", "Xiao Zhou", "Yingying Zhang", "Xian Wu"]}
]]></acme>

<pubDate>2025-10-21T04:58:29+00:00</pubDate>
</item>
<item>
<title>Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18279</link>
<guid>https://papers.cool/arxiv/2510.18279</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether rendering long textual inputs as images can serve as an effective compression technique for multimodal large language models. Experiments on the RULER retrieval benchmark and CNN/DailyMail summarization show that this visual-text approach can cut the number of decoder tokens roughly in half while maintaining comparable task performance.<br /><strong>Summary (CN):</strong> 本文研究了将冗长的文本渲染为单张图片作为多模态大语言模型的输入，以实现有效的压缩。实验在 RULER 检索基准和 CNN/DailyMail 文档摘要任务上表明，这种视觉文本方式能够将解码器所需的 token 数量大幅削减（约减半），且性能几乎不受影响。<br /><strong>Keywords:</strong> token efficiency, visual language models, multimodal LLM, text-as-image, input compression, retrieval, summarization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.
<div><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether rendering long textual inputs as images can serve as an effective compression technique for multimodal large language models. Experiments on the RULER retrieval benchmark and CNN/DailyMail summarization show that this visual-text approach can cut the number of decoder tokens roughly in half while maintaining comparable task performance.", "summary_cn": "本文研究了将冗长的文本渲染为单张图片作为多模态大语言模型的输入，以实现有效的压缩。实验在 RULER 检索基准和 CNN/DailyMail 文档摘要任务上表明，这种视觉文本方式能够将解码器所需的 token 数量大幅削减（约减半），且性能几乎不受影响。", "keywords": "token efficiency, visual language models, multimodal LLM, text-as-image, input compression, retrieval, summarization", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanhong Li", "Zixuan Lan", "Jiawei Zhou"]}
]]></acme>

<pubDate>2025-10-21T04:07:20+00:00</pubDate>
</item>
<item>
<title>StreamingTOM: Streaming Token Compression for Efficient Video Understanding</title>
<link>https://papers.cool/arxiv/2510.18269</link>
<guid>https://papers.cool/arxiv/2510.18269</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> StreamingTOM is a training-free, plug-and-play framework that reduces the computational cost of streaming video vision‑language models by compressing both pre‑LLM visual tokens and post‑LLM key‑value cache. It introduces Causal Temporal Reduction to select a limited set of salient tokens per frame and Online Quantized Memory to store cache entries in 4‑bit format, achieving significant memory and latency improvements while maintaining competitive accuracy.<br /><strong>Summary (CN):</strong> StreamingTOM 是一种无需训练、即插即用的两阶段框架，旨在通过压缩流式视频视觉‑语言模型的前 LLM 视觉令牌和后 LLM kv‑cache 来降低计算成本。它引入因果时间压缩，根据相邻帧变化和令牌显著性选择每帧的有限令牌，并采用 4量化的在线记忆存储缓存，实现显著的内存和延迟提升，同时保持竞争性的准确率。<br /><strong>Keywords:</strong> streaming video understanding, token compression, causal temporal reduction, quantized memory, kv-cache, vision-language models, efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</div>
Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, existing approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage framework that addresses both pre-LLM and post-LLM bottlenecks with predictable latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects tokens based on adjacent-frame changes and token saliency, drastically reducing per-frame prefill cost by processing only a compact subset of visual tokens per frame instead of all visual tokens. Online Quantized Memory stores tokens in 4-bit format, retrieves relevant groups on demand, and dequantizes them, keeping the active kv-cache bounded regardless of stream length. Experiments demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$ lower peak memory and $2\times$ faster TTFT compared to prior SOTA. StreamingTOM maintains state-of-the-art accuracy among training-free methods with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS. These results highlight the practical benefits of our two-stage approach for efficient streaming video understanding with bounded growth.
<div><strong>Authors:</strong> Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "StreamingTOM is a training-free, plug-and-play framework that reduces the computational cost of streaming video vision‑language models by compressing both pre‑LLM visual tokens and post‑LLM key‑value cache. It introduces Causal Temporal Reduction to select a limited set of salient tokens per frame and Online Quantized Memory to store cache entries in 4‑bit format, achieving significant memory and latency improvements while maintaining competitive accuracy.", "summary_cn": "StreamingTOM 是一种无需训练、即插即用的两阶段框架，旨在通过压缩流式视频视觉‑语言模型的前 LLM 视觉令牌和后 LLM kv‑cache 来降低计算成本。它引入因果时间压缩，根据相邻帧变化和令牌显著性选择每帧的有限令牌，并采用 4量化的在线记忆存储缓存，实现显著的内存和延迟提升，同时保持竞争性的准确率。", "keywords": "streaming video understanding, token compression, causal temporal reduction, quantized memory, kv-cache, vision-language models, efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyi Chen", "Keda Tao", "Kele Shao", "Huan Wang"]}
]]></acme>

<pubDate>2025-10-21T03:39:41+00:00</pubDate>
</item>
<item>
<title>Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization</title>
<link>https://papers.cool/arxiv/2510.18267</link>
<guid>https://papers.cool/arxiv/2510.18267</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a two‑stage network that first extracts hybrid latent frequency domain features from low‑ and high‑frequency image components to capture global shape alignment and local texture details, and then uses these features to guide a low‑dimensional, parallel‑optimized interaction between a rough 3D human mesh template and 3D pose, achieving accurate reconstruction with reduced computational cost. Extensive experiments on large public datasets demonstrate performance gains over state‑of‑the‑art methods.<br /><strong>Summary (CN):</strong> 本文提出一种两阶段网络，第一阶段从图像的低频和高频特征中提取混合潜在频域特征，以捕获整体形状对齐和局部纹理细节；第二阶段利用这些特征在低维空间并行优化粗糙的 3D 人体网格模板与姿态之间的交互，实现高精度重建并显著降低计算开销。大量公开数据集实验表明该方法优于现有最先进技术。<br /><strong>Keywords:</strong> human mesh recovery, latent information, low-dimensional learning, 3D pose estimation, parallel optimization, hybrid latent frequency features, shape alignment, mesh pose interaction<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiang Zhang, Suping Wu, Sheng Yang</div>
Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.
<div><strong>Authors:</strong> Xiang Zhang, Suping Wu, Sheng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a two‑stage network that first extracts hybrid latent frequency domain features from low‑ and high‑frequency image components to capture global shape alignment and local texture details, and then uses these features to guide a low‑dimensional, parallel‑optimized interaction between a rough 3D human mesh template and 3D pose, achieving accurate reconstruction with reduced computational cost. Extensive experiments on large public datasets demonstrate performance gains over state‑of‑the‑art methods.", "summary_cn": "本文提出一种两阶段网络，第一阶段从图像的低频和高频特征中提取混合潜在频域特征，以捕获整体形状对齐和局部纹理细节；第二阶段利用这些特征在低维空间并行优化粗糙的 3D 人体网格模板与姿态之间的交互，实现高精度重建并显著降低计算开销。大量公开数据集实验表明该方法优于现有最先进技术。", "keywords": "human mesh recovery, latent information, low-dimensional learning, 3D pose estimation, parallel optimization, hybrid latent frequency features, shape alignment, mesh pose interaction", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Sheng Yang"]}
]]></acme>

<pubDate>2025-10-21T03:35:12+00:00</pubDate>
</item>
<item>
<title>SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws</title>
<link>https://papers.cool/arxiv/2510.18266</link>
<guid>https://papers.cool/arxiv/2510.18266</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the Stable Physics-Informed Kernel Evolution (SPIKE) method for numerically solving inviscid hyperbolic conservation laws. By using reproducing kernel representations withized parameter evolution, SPIKE can minimize strong-form residuals while accurately capturing weak solutions with discontinuities, automatically satisfying conservation and Rankine‑Hugoniot conditions without explicit shock detection or artificial viscosity. Experiments on scalar and vector-valued problems demonstrate its effectiveness.<br /><strong>Summary (CN):</strong> 本文提出了稳定物理信息核演化（SPIKE）方法，用于数值求解无粘性双曲守恒律。该方法通过再现核表示和正则化参数演化，使强形式残差最小化的同时能够准确捕获包含不连续性的弱解，并在无需显式冲击检测或人工粘性的方法下自动满足守恒和 Rankine‑Hugoniot 条件。对标量和向量守恒律的实验验证了其有效性。<br /><strong>Keywords:</strong> physics-informed neural networks, kernel methods, hyperbolic conservation laws, shock capturing, Tikhonov regularization, numerical PDEs<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 2, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hua Su, Lei Zhang, Jin Zhao</div>
We introduce the Stable Physics-Informed Kernel Evolution (SPIKE) method for numerical computation of inviscid hyperbolic conservation laws. SPIKE resolves a fundamental paradox: how strong-form residual minimization can capture weak solutions containing discontinuities. SPIKE employs reproducing kernel representations with regularized parameter evolution, where Tikhonov regularization provides a smooth transition mechanism through shock formation, allowing the dynamics to traverse shock singularities. This approach automatically maintains conservation, tracks characteristics, and captures shocks satisfying Rankine-Hugoniot conditions within a unified framework requiring no explicit shock detection or artificial viscosity. Numerical validation across scalar and vector-valued conservation laws confirms the method's effectiveness.
<div><strong>Authors:</strong> Hua Su, Lei Zhang, Jin Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the Stable Physics-Informed Kernel Evolution (SPIKE) method for numerically solving inviscid hyperbolic conservation laws. By using reproducing kernel representations withized parameter evolution, SPIKE can minimize strong-form residuals while accurately capturing weak solutions with discontinuities, automatically satisfying conservation and Rankine‑Hugoniot conditions without explicit shock detection or artificial viscosity. Experiments on scalar and vector-valued problems demonstrate its effectiveness.", "summary_cn": "本文提出了稳定物理信息核演化（SPIKE）方法，用于数值求解无粘性双曲守恒律。该方法通过再现核表示和正则化参数演化，使强形式残差最小化的同时能够准确捕获包含不连续性的弱解，并在无需显式冲击检测或人工粘性的方法下自动满足守恒和 Rankine‑Hugoniot 条件。对标量和向量守恒律的实验验证了其有效性。", "keywords": "physics-informed neural networks, kernel methods, hyperbolic conservation laws, shock capturing, Tikhonov regularization, numerical PDEs", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hua Su", "Lei Zhang", "Jin Zhao"]}
]]></acme>

<pubDate>2025-10-21T03:34:49+00:00</pubDate>
</item>
<item>
<title>Learning under Quantization for High-Dimensional Linear Regression</title>
<link>https://papers.cool/arxiv/2510.18259</link>
<guid>https://papers.cool/arxiv/2510.18259</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides the first systematic theoretical analysis of stochastic gradient descent for high-dimensional linear regression under various low-bit quantization schemes (data, labels, parameters, activations, gradients). derives algorithm‑ and data‑dependent excess risk bounds, highlighting how different quantization types introduce noise, spectral distortion, or scaling effects, and compares multiplicative versus additive quantization for common data spectra. The results offer a rigorous foundation for understanding how quantization influences learning dynamics under practical hardware constraints.<br /><strong>Summary (CN):</strong> 本文首次对在不同低位量化方案（数据、标签、参数、激活、梯度）下的高维线性回归进行随机梯度下降的系统理论分析。通过推导算法和数据相关的过剩风险界限，阐明不同量化方式如何引入噪声、导致数据谱畸变或产生批量大小的有益缩放效应，并比较乘法量化与加法量化在常见数据谱下的风险表现。该工作为量化在实际硬件约束下如何影响学习动态提供了严格的理论基础。<br /><strong>Keywords:</strong> quantization, high-dimensional linear regression, stochastic gradient descent, excess risk bounds, multiplicative quantization, additive quantization, data spectrum, theoretical analysis, hardware constraints<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dechen Zhang, Junwei Su, Difan Zou</div>
The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simplest linear regression setting. We present the first systematic theoretical study of this fundamental question, analyzing finite-step stochastic gradient descent (SGD) for high-dimensional linear regression under a comprehensive range of quantization targets: data, labels, parameters, activations, and gradients. Our novel analytical framework establishes precise algorithm-dependent and data-dependent excess risk bounds that characterize how different quantization affects learning: parameter, activation, and gradient quantization amplify noise during training; data quantization distorts the data spectrum; and data and label quantization introduce additional approximation and quantized error. Crucially, we prove that for multiplicative quantization (with input-dependent quantization step), this spectral distortion can be eliminated, and for additive quantization (with constant quantization step), a beneficial scaling effect with batch size emerges. Furthermore, for common polynomial-decay data spectra, we quantitatively compare the risks of multiplicative and additive quantization, drawing a parallel to the comparison between FP and integer quantization methods. Our theory provides a powerful lens to characterize how quantization shapes the learning dynamics of optimization algorithms, paving the way to further explore learning theory under practical hardware constraints.
<div><strong>Authors:</strong> Dechen Zhang, Junwei Su, Difan Zou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides the first systematic theoretical analysis of stochastic gradient descent for high-dimensional linear regression under various low-bit quantization schemes (data, labels, parameters, activations, gradients). derives algorithm‑ and data‑dependent excess risk bounds, highlighting how different quantization types introduce noise, spectral distortion, or scaling effects, and compares multiplicative versus additive quantization for common data spectra. The results offer a rigorous foundation for understanding how quantization influences learning dynamics under practical hardware constraints.", "summary_cn": "本文首次对在不同低位量化方案（数据、标签、参数、激活、梯度）下的高维线性回归进行随机梯度下降的系统理论分析。通过推导算法和数据相关的过剩风险界限，阐明不同量化方式如何引入噪声、导致数据谱畸变或产生批量大小的有益缩放效应，并比较乘法量化与加法量化在常见数据谱下的风险表现。该工作为量化在实际硬件约束下如何影响学习动态提供了严格的理论基础。", "keywords": "quantization, high-dimensional linear regression, stochastic gradient descent, excess risk bounds, multiplicative quantization, additive quantization, data spectrum, theoretical analysis, hardware constraints", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dechen Zhang", "Junwei Su", "Difan Zou"]}
]]></acme>

<pubDate>2025-10-21T03:30:11+00:00</pubDate>
</item>
<item>
<title>NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective</title>
<link>https://papers.cool/arxiv/2510.18258</link>
<guid>https://papers.cool/arxiv/2510.18258</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes NTKMTL, a method that leverages Neural Tangent Kernel theory to analyze and balance the convergence speeds of tasks in multi-task learning, thereby mitigating task imbalance. By extending the NTK matrix to the MTL setting and applying spectral analysis, the authors introduce NTKMTL-SR for efficient training via a shared representation approximation. Experiments on supervised and reinforcement learning benchmarks demonstrate state-of-the-art performance.<br /><strong>Summary (CN):</strong> 本文提出 NTKMTL 方法，利用神经切线核（Neural Tangent Kernel）理论分析并平衡多任务学习中各任务的收敛速度，以缓解任务不平衡问题。通过对多任务扩展的 NTK 矩阵进行谱分析，并引入基于共享表示的近似实现 NTKMTL‑SR，从而在保持竞争性能的同时提升训练效率。实验在监督学习和强化学习的多任务基准上展示了领先的表现。<br /><strong>Keywords:</strong> multi-task learning, task imbalance, neural tangent kernel, NTKMTL, spectral analysis, transfer learning, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Junchi Yan</div>
Multi-Task Learning (MTL) enables a single model to learn multiple tasks simultaneously, leveraging knowledge transfer among tasks for enhanced generalization, and has been widely applied across various domains. However, task imbalance remains a major challenge in MTL. Although balancing the convergence speeds of different tasks is an effective approach to address this issue, it is highly challenging to accurately characterize the training dynamics and convergence speeds of multiple tasks within the complex MTL system. To this end, we attempt to analyze the training dynamics in MTL by leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method, NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt spectral analysis to balance the convergence speeds of multiple tasks, thereby mitigating task imbalance. Based on the approximation via shared representation, we further propose NTKMTL-SR, achieving training efficiency while maintaining competitive performance. Extensive experiments demonstrate that our methods achieve state-of-the-art performance across a wide range of benchmarks, including both multi-task supervised learning and multi-task reinforcement learning. Source code is available at https://github.com/jianke0604/NTKMTL.
<div><strong>Authors:</strong> Xiaohan Qin, Xiaoxing Wang, Ning Liao, Junchi Yan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes NTKMTL, a method that leverages Neural Tangent Kernel theory to analyze and balance the convergence speeds of tasks in multi-task learning, thereby mitigating task imbalance. By extending the NTK matrix to the MTL setting and applying spectral analysis, the authors introduce NTKMTL-SR for efficient training via a shared representation approximation. Experiments on supervised and reinforcement learning benchmarks demonstrate state-of-the-art performance.", "summary_cn": "本文提出 NTKMTL 方法，利用神经切线核（Neural Tangent Kernel）理论分析并平衡多任务学习中各任务的收敛速度，以缓解任务不平衡问题。通过对多任务扩展的 NTK 矩阵进行谱分析，并引入基于共享表示的近似实现 NTKMTL‑SR，从而在保持竞争性能的同时提升训练效率。实验在监督学习和强化学习的多任务基准上展示了领先的表现。", "keywords": "multi-task learning, task imbalance, neural tangent kernel, NTKMTL, spectral analysis, transfer learning, reinforcement learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Junchi Yan"]}
]]></acme>

<pubDate>2025-10-21T03:29:40+00:00</pubDate>
</item>
<item>
<title>DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization</title>
<link>https://papers.cool/arxiv/2510.18257</link>
<guid>https://papers.cool/arxiv/2510.18257</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DelvePO is a task-agnostic, direction-guided self-evolving framework for flexible prompt optimization that decouples prompts into components and introduces a working memory to guide LLMs in generating improved prompts. The method mitigates uncertainties in LLM-generated prompts and demonstrates consistent performance gains and transferability across diverse tasks and both open- and closed-source models.<br /><strong>Summary (CN):</strong> DelvePO（方向引导自进化提示优化框架）通过将提示拆分为多个组件并引入工作记忆，帮助大语言模型克服自身不确定性，生成更有效的提示。该方法在多任务、多模型（包括开源和闭源模型）上表现出稳定的性能提升和良好的可迁移性。<br /><strong>Keywords:</strong> prompt optimization, large language models, self-evolving, working memory, task-agnostic, transferability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\textbf{DelvePO}$ ($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.
<div><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DelvePO is a task-agnostic, direction-guided self-evolving framework for flexible prompt optimization that decouples prompts into components and introduces a working memory to guide LLMs in generating improved prompts. The method mitigates uncertainties in LLM-generated prompts and demonstrates consistent performance gains and transferability across diverse tasks and both open- and closed-source models.", "summary_cn": "DelvePO（方向引导自进化提示优化框架）通过将提示拆分为多个组件并引入工作记忆，帮助大语言模型克服自身不确定性，生成更有效的提示。该方法在多任务、多模型（包括开源和闭源模型）上表现出稳定的性能提升和良好的可迁移性。", "keywords": "prompt optimization, large language models, self-evolving, working memory, task-agnostic, transferability", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tao Tao", "Guanghui Zhu", "Lang Guo", "Hongyi Chen", "Chunfeng Yuan", "Yihua Huang"]}
]]></acme>

<pubDate>2025-10-21T03:28:53+00:00</pubDate>
</item>
<item>
<title>Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery</title>
<link>https://papers.cool/arxiv/2510.18256</link>
<guid>https://papers.cool/arxiv/2510.18256</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method that employs hyperbolic space learning combined with temporal motion priors to improve video-based 3D human mesh recovery, aiming to better capture the hierarchical structure of human bodies. It proposes a temporal motion prior extraction module and a hyperbolic optimization strategy, including a specialized loss function, showing improved performance over existing approaches on public datasets.<br /><strong>Summary (CN):</strong> 本文提出一种利用超曲率空间学习和时间运动先验的技术，以提升基于视频的三维人体网格恢复效果，旨在更好地捕捉人体的层级结构。作者设计了时间运动先验提取模块和超曲率空间优化策略，并引入专用的超曲率网格优化损失函数，在公开数据集上表现优于多数现有方法。<br /><strong>Keywords:</strong> hyperbolic space, temporal motion prior, 3D human mesh recovery, hierarchical representation, video-based reconstruction<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang</div>
3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.
<div><strong>Authors:</strong> Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method that employs hyperbolic space learning combined with temporal motion priors to improve video-based 3D human mesh recovery, aiming to better capture the hierarchical structure of human bodies. It proposes a temporal motion prior extraction module and a hyperbolic optimization strategy, including a specialized loss function, showing improved performance over existing approaches on public datasets.", "summary_cn": "本文提出一种利用超曲率空间学习和时间运动先验的技术，以提升基于视频的三维人体网格恢复效果，旨在更好地捕捉人体的层级结构。作者设计了时间运动先验提取模块和超曲率空间优化策略，并引入专用的超曲率网格优化损失函数，在公开数据集上表现优于多数现有方法。", "keywords": "hyperbolic space, temporal motion prior, 3D human mesh recovery, hierarchical representation, video-based reconstruction", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiang Zhang", "Suping Wu", "Weibin Qiu", "Zhaocheng Jin", "Sheng Yang"]}
]]></acme>

<pubDate>2025-10-21T03:26:27+00:00</pubDate>
</item>
<item>
<title>Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN</title>
<link>https://papers.cool/arxiv/2510.18252</link>
<guid>https://papers.cool/arxiv/2510.18252</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper systematically evaluates synthetic oversampling techniques (SMOTE, BorderlineSMOTE, ADASYN) at different augmentation ratios for imbalanced credit‑scoring data and finds that ADASYN with a 1× multiplication (minority class doubled) yields the best AUC and Gini, defining an optimal majority‑to‑minority ratio of about 6.6:1. Higher augmentation ratios degrade performance, indicating a diminishing‑returns effect. The study provides an empirical framework for selecting augmentation ratios in imbalanced domains.<br /><strong>Summary (CN):</strong> 本文系统评估了在不平衡信用评分数据上使用的合成过采样方法（SMOTE、BorderlineSMOTE、ADASYN）在不同增强比例下的效果，发现 ADASYN 1 倍（少数类翻倍）能够获得最佳的 AUC 和 Gini，最佳的多数‑少数比例约为 6.6:1。更高的增强比例会导致性能下降，表现出递减收益效应。该研究为在不平衡场景中确定最佳数据增强比例提供了可复现的实证框架。<br /><strong>Keywords:</strong> data augmentation, ADASYN, SMOTE, imbalanced classification, credit scoring, XGBoost, synthetic oversampling, optimal augmentation ratio<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Luis H. Chia</div>
Credit scoring models face a critical challenge: severe class imbalance, with default rates typically below 10%, which hampers model learning and predictive performance. While synthetic data augmentation techniques such as SMOTE and ADASYN have been proposed to address this issue, the optimal augmentation ratio remains unclear, with practitioners often defaulting to full balancing (1:1 ratio) without empirical justification. This study systematically evaluates 10 data augmentation scenarios using the Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x, 3x). All models were trained using XGBoost and evaluated on a held-out test set of 29,173 real observations. Statistical significance was assessed using bootstrap testing with 1,000 iterations. Key findings reveal that ADASYN with 1x multiplication (doubling the minority class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of 0.3557, representing statistically significant improvements of +0.77% and +3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors (2x and 3x) resulted in performance degradation, with 3x showing a -0.48% decrease in AUC, suggesting a "law of diminishing returns" for synthetic oversampling. The optimal class imbalance ratio was found to be 6.6:1 (majority:minority), contradicting the common practice of balancing to 1:1. This work provides the first empirical evidence of an optimal "sweet spot" for data augmentation in credit scoring, with practical guidelines for industry practitioners and researchers working with imbalanced datasets. While demonstrated on a single representative dataset, the methodology provides a reproducible framework for determining optimal augmentation ratios in other imbalanced domains.
<div><strong>Authors:</strong> Luis H. Chia</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper systematically evaluates synthetic oversampling techniques (SMOTE, BorderlineSMOTE, ADASYN) at different augmentation ratios for imbalanced credit‑scoring data and finds that ADASYN with a 1× multiplication (minority class doubled) yields the best AUC and Gini, defining an optimal majority‑to‑minority ratio of about 6.6:1. Higher augmentation ratios degrade performance, indicating a diminishing‑returns effect. The study provides an empirical framework for selecting augmentation ratios in imbalanced domains.", "summary_cn": "本文系统评估了在不平衡信用评分数据上使用的合成过采样方法（SMOTE、BorderlineSMOTE、ADASYN）在不同增强比例下的效果，发现 ADASYN 1 倍（少数类翻倍）能够获得最佳的 AUC 和 Gini，最佳的多数‑少数比例约为 6.6:1。更高的增强比例会导致性能下降，表现出递减收益效应。该研究为在不平衡场景中确定最佳数据增强比例提供了可复现的实证框架。", "keywords": "data augmentation, ADASYN, SMOTE, imbalanced classification, credit scoring, XGBoost, synthetic oversampling, optimal augmentation ratio", "scoring": {"interpretability": 1, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Luis H. Chia"]}
]]></acme>

<pubDate>2025-10-21T03:22:43+00:00</pubDate>
</item>
<item>
<title>Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs</title>
<link>https://papers.cool/arxiv/2510.18245</link>
<guid>https://papers.cool/arxiv/2510.18245</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how architectural choices such as hidden size, MLP‑to‑attention ratio, and grouped‑query attention affect the trade‑off between accuracy and inference cost in large language models. It proposes a conditional scaling law that extends the Chinchilla framework with architecture‑dependent factors and uses it to search for configurations that are both inference‑efficient and accurate. Experiments with over 200 models (80 M–3 B parameters, 8 B–100 B tokens) demonstrate that the method predicts optimal designs, achieving up to 2.1% higher accuracy and 42% higher throughput compared to LLaMA‑3.2 under the same training budget.<br /><strong>Summary (CN):</strong> 本文研究了隐藏层大小、MLP 与注意力比例（mlp-to-attention ratio）以及分组查询注意力（grouped-query attention）等架构因素对大语言模型（LLM）准确率与推理成本之间权衡的影响。作者在 Chinchilla 框架基础上引入条件缩放定律（conditional scaling law），将架构信息纳入预测，并利用该定律搜索兼具推理效率和准确性的模型结构。通过训练 200 多个模型（参数量 80 M‑3 B，训练 tokens 8 B‑100 B），实验表明该方法能可靠预测最优架构，在相同训练预算下实现最高 2.1% 的准确率提升和 42% 的吞吐量提升，相比 LLaMA‑3.2 更具优势。<br /><strong>Keywords:</strong> scaling laws, model architecture, inference efficiency, large language models, conditional scaling law, grouped-query attention, mlp-to-attention ratio<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park</div>
Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.
<div><strong>Authors:</strong> Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how architectural choices such as hidden size, MLP‑to‑attention ratio, and grouped‑query attention affect the trade‑off between accuracy and inference cost in large language models. It proposes a conditional scaling law that extends the Chinchilla framework with architecture‑dependent factors and uses it to search for configurations that are both inference‑efficient and accurate. Experiments with over 200 models (80 M–3 B parameters, 8 B–100 B tokens) demonstrate that the method predicts optimal designs, achieving up to 2.1% higher accuracy and 42% higher throughput compared to LLaMA‑3.2 under the same training budget.", "summary_cn": "本文研究了隐藏层大小、MLP 与注意力比例（mlp-to-attention ratio）以及分组查询注意力（grouped-query attention）等架构因素对大语言模型（LLM）准确率与推理成本之间权衡的影响。作者在 Chinchilla 框架基础上引入条件缩放定律（conditional scaling law），将架构信息纳入预测，并利用该定律搜索兼具推理效率和准确性的模型结构。通过训练 200 多个模型（参数量 80 M‑3 B，训练 tokens 8 B‑100 B），实验表明该方法能可靠预测最优架构，在相同训练预算下实现最高 2.1% 的准确率提升和 42% 的吞吐量提升，相比 LLaMA‑3.2 更具优势。", "keywords": "scaling laws, model architecture, inference efficiency, large language models, conditional scaling law, grouped-query attention, mlp-to-attention ratio", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Song Bian", "Tao Yu", "Shivaram Venkataraman", "Youngsuk Park"]}
]]></acme>

<pubDate>2025-10-21T03:08:48+00:00</pubDate>
</item>
<item>
<title>EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation</title>
<link>https://papers.cool/arxiv/2510.18224</link>
<guid>https://papers.cool/arxiv/2510.18224</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces EVER, an edge-assisted auto-verification system for mobile mixed-reality (MR) aided operations that uses segmentation models and IoU-based thresholds to compare pre- and post-operation frames, handling discrepancies between virtual guides and physical objects. Computation-heavy tasks are offloaded to an edge server, achieving over 90% verification accuracy within 100 milliseconds while keeping energy consumption low.<br /><strong>Summary (CN):</strong> 本文提出 EVER，一种面向移动混合现实（MR）辅助操作的边缘协助自动验证系统，利用分割模型和基于交并比（IoU）的阈值策略比较操作前后的帧，解决虚拟引导对象与实际物体之间的差异。通过将计算密集任务卸载至边缘服务器，实现了超过 90% 的验证准确率且在 100 毫秒内完成，能耗保持在低水平。<br /><strong>Keywords:</strong> mixed reality, auto-verification, edge computing, segmentation model, IoU, mobile MR, verification accuracy, real-time processing<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Jiangong Chen, Mingyu Zhu, Bin Li</div>
Mixed Reality (MR)-aided operation overlays digital objects on the physical world to provide a more immersive and intuitive operation process. A primary challenge is the precise and fast auto-verification of whether the user follows MR guidance by comparing frames before and after each operation. The pre-operation frame includes virtual guiding objects, while the post-operation frame contains physical counterparts. Existing approaches fall short of accounting for the discrepancies between physical and virtual objects due to imperfect 3D modeling or lighting estimation. In this paper, we propose EVER: an edge-assisted auto-verification system for mobile MR-aided operations. Unlike traditional frame-based similarity comparisons, EVER leverages the segmentation model and rendering pipeline adapted to the unique attributes of frames with physical pieces and those with their virtual counterparts; it adopts a threshold-based strategy using Intersection over Union (IoU) metrics for accurate auto-verification. To ensure fast auto-verification and low energy consumption, EVER offloads compute-intensive tasks to an edge server. Through comprehensive evaluations of public datasets and custom datasets with practical implementation, EVER achieves over 90% verification accuracy within 100 milliseconds (significantly faster than average human reaction time of approximately 273 milliseconds), while consuming only minimal additional computational resources and energy compared to a system without auto-verification.
<div><strong>Authors:</strong> Jiangong Chen, Mingyu Zhu, Bin Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces EVER, an edge-assisted auto-verification system for mobile mixed-reality (MR) aided operations that uses segmentation models and IoU-based thresholds to compare pre- and post-operation frames, handling discrepancies between virtual guides and physical objects. Computation-heavy tasks are offloaded to an edge server, achieving over 90% verification accuracy within 100 milliseconds while keeping energy consumption low.", "summary_cn": "本文提出 EVER，一种面向移动混合现实（MR）辅助操作的边缘协助自动验证系统，利用分割模型和基于交并比（IoU）的阈值策略比较操作前后的帧，解决虚拟引导对象与实际物体之间的差异。通过将计算密集任务卸载至边缘服务器，实现了超过 90% 的验证准确率且在 100 毫秒内完成，能耗保持在低水平。", "keywords": "mixed reality, auto-verification, edge computing, segmentation model, IoU, mobile MR, verification accuracy, real-time processing", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Jiangong Chen", "Mingyu Zhu", "Bin Li"]}
]]></acme>

<pubDate>2025-10-21T02:12:34+00:00</pubDate>
</item>
<item>
<title>The Emergence of Complex Behavior in Large-Scale Ecological Environments</title>
<link>https://papers.cool/arxiv/2510.18221</link>
<guid>https://papers.cool/arxiv/2510.18221</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how physical scale and population size influence the emergence of complex behaviors in open-ended ecological simulations where agents evolve via reproduction, mutation, and natural selection without explicit rewards. Experiments with over 60,000 agents reveal behaviors such as long-range resource extraction, vision-based foraging, and predation, showing that larger environments and populations enhance behavioral stability and enable novel capabilities. The authors discuss the implications of scaling ecological environments for machine learning research.<br /><strong>Summary (CN):</strong> 本文研究了在没有显式奖励的演化生态环境中，物理尺度和种群规模如何影响复杂行为的出现。通过在拥有超过六万名 agent 的大规模世界中进行实验，观察到远程资源提取、基于视觉的觅食和捕食等行为，并发现更大的环境和种群有助于行为的稳定性和新能力的出现。作者探讨了生态系统规模化对机器学习研究的意义。<br /><strong>Keywords:</strong> emergent behavior, evolutionary multi-agent systems, large-scale ecology, natural selection, unsupervised agents, scaling laws, population dynamics<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 3, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Joseph Bejjani, Chase Van Amburg, Chengrui Wang, Chloe Huangyuan Su, Sarah M. Pratt, Yasin Mazloumi, Naeem Khoshnevis, Sham M. Kakade, Kianté Brantley</div>
We explore how physical scale and population size shape the emergence of complex behaviors in open-ended ecological environments. In our setting, agents are unsupervised and have no explicit rewards or learning objectives but instead evolve over time according to reproduction, mutation, and natural selection. As they act, agents also shape their environment and the population around them in an ongoing dynamic ecology. Our goal is not to optimize a single high-performance policy, but instead to examine how behaviors emerge and evolve across large populations due to natural competition and environmental pressures. In an effort to discover how complex behaviors naturally emerge, we conduct experiments in large-scale worlds that reach populations of more than 60,000 individual agents, each with their own evolved neural network policy. We identify various emergent behaviors such as long-range resource extraction, vision-based foraging, and predation that arise under competitive and survival pressures. We examine how sensing modalities and environmental scale affect the emergence of these behaviors, finding that some appear only in sufficiently large environments and populations, with larger scales increasing behavioral stability and consistency. While there is a rich history of research in evolutionary settings, our scaling results provide promising new directions to explore ecology as an instrument of machine learning in an era of abundant computational resources. Experimental code is available at https://github.com/jbejjani2022/ecological-emergent-behavior.
<div><strong>Authors:</strong> Joseph Bejjani, Chase Van Amburg, Chengrui Wang, Chloe Huangyuan Su, Sarah M. Pratt, Yasin Mazloumi, Naeem Khoshnevis, Sham M. Kakade, Kianté Brantley</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how physical scale and population size influence the emergence of complex behaviors in open-ended ecological simulations where agents evolve via reproduction, mutation, and natural selection without explicit rewards. Experiments with over 60,000 agents reveal behaviors such as long-range resource extraction, vision-based foraging, and predation, showing that larger environments and populations enhance behavioral stability and enable novel capabilities. The authors discuss the implications of scaling ecological environments for machine learning research.", "summary_cn": "本文研究了在没有显式奖励的演化生态环境中，物理尺度和种群规模如何影响复杂行为的出现。通过在拥有超过六万名 agent 的大规模世界中进行实验，观察到远程资源提取、基于视觉的觅食和捕食等行为，并发现更大的环境和种群有助于行为的稳定性和新能力的出现。作者探讨了生态系统规模化对机器学习研究的意义。", "keywords": "emergent behavior, evolutionary multi-agent systems, large-scale ecology, natural selection, unsupervised agents, scaling laws, population dynamics", "scoring": {"interpretability": 5, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Joseph Bejjani", "Chase Van Amburg", "Chengrui Wang", "Chloe Huangyuan Su", "Sarah M. Pratt", "Yasin Mazloumi", "Naeem Khoshnevis", "Sham M. Kakade", "Kianté Brantley"]}
]]></acme>

<pubDate>2025-10-21T02:03:25+00:00</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18214</link>
<guid>https://papers.cool/arxiv/2510.18214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Language Safety Understanding (VLSU), a benchmark that evaluates multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint reasoning is required, and that 34% of joint errors occur despite correct unimodal classifications. The work highlights alignment gaps in current vision‑language systems and provides a testbed for improving robust multimodal safety.<br /><strong>Summary (CN):</strong> 本文提出了视觉语言安全理解（VLSU）基准，用于在 17 种安全模式下对多模态基础模型进行细粒度安全严重性和图像‑文本组合推理的评估。通过对 8,187 条真实图像和人工标注样本的实验，作者发现最先进模型在单模态安全信号上可达 90% 以上准确率，但在需要联合图像‑文本推理时准确率下降至 20%‑55%，并且 34% 的联合错误发生在单模态已正确分类的情况下。该工作揭示了当前视觉语言系统的对齐缺口，并提供了促进稳健多模态安全研究的测试平台。<br /><strong>Keywords:</strong> multimodal safety, vision-language models, compositional reasoning, safety benchmark, joint image-text understanding, dataset, alignment evaluation, over-blocking, under-refusal, AI safety<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
<div><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Language Safety Understanding (VLSU), a benchmark that evaluates multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint reasoning is required, and that 34% of joint errors occur despite correct unimodal classifications. The work highlights alignment gaps in current vision‑language systems and provides a testbed for improving robust multimodal safety.", "summary_cn": "本文提出了视觉语言安全理解（VLSU）基准，用于在 17 种安全模式下对多模态基础模型进行细粒度安全严重性和图像‑文本组合推理的评估。通过对 8,187 条真实图像和人工标注样本的实验，作者发现最先进模型在单模态安全信号上可达 90% 以上准确率，但在需要联合图像‑文本推理时准确率下降至 20%‑55%，并且 34% 的联合错误发生在单模态已正确分类的情况下。该工作揭示了当前视觉语言系统的对齐缺口，并提供了促进稳健多模态安全研究的测试平台。", "keywords": "multimodal safety, vision-language models, compositional reasoning, safety benchmark, joint image-text understanding, dataset, alignment evaluation, over-blocking, under-refusal, AI safety", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}
]]></acme>

<pubDate>2025-10-21T01:30:31+00:00</pubDate>
</item>
<item>
<title>Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge</title>
<link>https://papers.cool/arxiv/2510.18196</link>
<guid>https://papers.cool/arxiv/2510.18196</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies a score range bias in large language models used as judges, where model outputs are highly sensitive to the predefined scoring interval, and shows that this bias persists across models of the same family. To mitigate the bias, the authors propose using contrastive decoding, which yields up to an 11.3% relative improvement in Spearman correlation with human judgments across various score ranges. Experiments demonstrate that the method makes LLM-as-a-judge evaluations more reliable and less dependent on arbitrary score intervals.<br /><strong>Summary (CN):</strong> 本文发现用于评估的 LLM（大语言模型）在直接打分时会出现评分范围偏差，即模型输出对预设的分数区间极度敏感，这种偏差在同一模型家族的不同模型中也会出现。为缓解该问题，作者提出采用对比解码（contrastive decoding）技术，使得在不同评分区间下与人类评判的 Spearman 相关系数提升最高可达 11.3%。实验表明，该方法显著提升了 LLM 评估的可靠性，降低了对人为设定评分范围的依赖。<br /><strong>Keywords:</strong> LLM judge, score range bias, contrastive decoding, evaluation reliability, Spearman correlation, alignment assessment<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Yoshinari Fujinuma</div>
Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.
<div><strong>Authors:</strong> Yoshinari Fujinuma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies a score range bias in large language models used as judges, where model outputs are highly sensitive to the predefined scoring interval, and shows that this bias persists across models of the same family. To mitigate the bias, the authors propose using contrastive decoding, which yields up to an 11.3% relative improvement in Spearman correlation with human judgments across various score ranges. Experiments demonstrate that the method makes LLM-as-a-judge evaluations more reliable and less dependent on arbitrary score intervals.", "summary_cn": "本文发现用于评估的 LLM（大语言模型）在直接打分时会出现评分范围偏差，即模型输出对预设的分数区间极度敏感，这种偏差在同一模型家族的不同模型中也会出现。为缓解该问题，作者提出采用对比解码（contrastive decoding）技术，使得在不同评分区间下与人类评判的 Spearman 相关系数提升最高可达 11.3%。实验表明，该方法显著提升了 LLM 评估的可靠性，降低了对人为设定评分范围的依赖。", "keywords": "LLM judge, score range bias, contrastive decoding, evaluation reliability, Spearman correlation, alignment assessment", "scoring": {"interpretability": 4, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Yoshinari Fujinuma"]}
]]></acme>

<pubDate>2025-10-21T00:47:11+00:00</pubDate>
</item>
<item>
<title>RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</title>
<link>https://papers.cool/arxiv/2510.18188</link>
<guid>https://papers.cool/arxiv/2510.18188</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RadDiagSeg-D, a new dataset that combines abnormality detection, diagnosis, and multi‑target segmentation for radiology, and proposes RadDiagSeg-M, a vision‑language model that jointly generates diagnostic text and corresponding segmentation masks. Experiments demonstrate that RadDiagSeg‑M achieves strong performance across detection, description, and segmentation, providing a baseline for future work on integrated medical image‑text systems.<br /><strong>Summary (CN):</strong> 本文推出了 RadDiagSeg‑D 数据集，整合了异常检测、诊断和多目标分割任务，并基于此提出了能够同时生成诊断文本与对应分割掩码的视觉语言模型 RadDiagSeg‑M。实验表明，RadDiagSeg‑M 在检测、描述和分割三个方面均表现出色，为后续医学影像文本综合系统提供了有力基线。<br /><strong>Keywords:</strong> vision-language model, radiology, multi-target segmentation, diagnostic text generation, dataset, RadDiagSeg, joint diagnosis, medical AI, multi-modal learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</div>
Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.
<div><strong>Authors:</strong> Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RadDiagSeg-D, a new dataset that combines abnormality detection, diagnosis, and multi‑target segmentation for radiology, and proposes RadDiagSeg-M, a vision‑language model that jointly generates diagnostic text and corresponding segmentation masks. Experiments demonstrate that RadDiagSeg‑M achieves strong performance across detection, description, and segmentation, providing a baseline for future work on integrated medical image‑text systems.", "summary_cn": "本文推出了 RadDiagSeg‑D 数据集，整合了异常检测、诊断和多目标分割任务，并基于此提出了能够同时生成诊断文本与对应分割掩码的视觉语言模型 RadDiagSeg‑M。实验表明，RadDiagSeg‑M 在检测、描述和分割三个方面均表现出色，为后续医学影像文本综合系统提供了有力基线。", "keywords": "vision-language model, radiology, multi-target segmentation, diagnostic text generation, dataset, RadDiagSeg, joint diagnosis, medical AI, multi-modal learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Chengrun Li", "Corentin Royer", "Haozhe Luo", "Bastian Wittmann", "Xia Li", "Ibrahim Hamamci", "Sezgin Er", "Anjany Sekuboyina", "Bjoern Menze"]}
]]></acme>

<pubDate>2025-10-21T00:28:13+00:00</pubDate>
</item>
<item>
<title>VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis</title>
<link>https://papers.cool/arxiv/2510.18187</link>
<guid>https://papers.cool/arxiv/2510.18187</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> VelocityNet is a dual‑pipeline framework that combines head detection and dense optical flow to extract person‑specific velocities in crowded scenes. Hierarchical clustering groups these velocities into semantic motion classes (halt, slow, normal, fast) and a percentile‑based anomaly scoring system quantifies deviations from learned normal patterns, enabling real‑time detection of diverse anomalous motions even under heavy occlusion and varying crowd density.<br /><strong>Summary (CN):</strong> 本文提出 VelocityNet，一个结合人头检测和密集光流的双管道框架，通过对个人速度进行分析并使用层次聚类将其划分为停止、缓慢、正常和快速等语义运动类别，基于百分位的异常评分衡量与学习到的正常模式的偏差，实现了在人群密集环境中的实时异常检测。<br /><strong>Keywords:</strong> crowd anomaly detection, person-specific velocity, dense optical flow, hierarchical clustering, real-time surveillance<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Fatima AlGhamdi, Omar Alharbi, Abdullah Aldwyish, Raied Aljadaany, Muhammad Kamran J Khan, Huda Alamri</div>
Detecting anomalies in crowded scenes is challenging due to severe inter-person occlusions and highly dynamic, context-dependent motion patterns. Existing approaches often struggle to adapt to varying crowd densities and lack interpretable anomaly indicators. To address these limitations, we introduce VelocityNet, a dual-pipeline framework that combines head detection and dense optical flow to extract person-specific velocities. Hierarchical clustering categorizes these velocities into semantic motion classes (halt, slow, normal, and fast), and a percentile-based anomaly scoring system measures deviations from learned normal patterns. Experiments demonstrate the effectiveness of our framework in real-time detection of diverse anomalous motion patterns within densely crowded environments.
<div><strong>Authors:</strong> Fatima AlGhamdi, Omar Alharbi, Abdullah Aldwyish, Raied Aljadaany, Muhammad Kamran J Khan, Huda Alamri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "VelocityNet is a dual‑pipeline framework that combines head detection and dense optical flow to extract person‑specific velocities in crowded scenes. Hierarchical clustering groups these velocities into semantic motion classes (halt, slow, normal, fast) and a percentile‑based anomaly scoring system quantifies deviations from learned normal patterns, enabling real‑time detection of diverse anomalous motions even under heavy occlusion and varying crowd density.", "summary_cn": "本文提出 VelocityNet，一个结合人头检测和密集光流的双管道框架，通过对个人速度进行分析并使用层次聚类将其划分为停止、缓慢、正常和快速等语义运动类别，基于百分位的异常评分衡量与学习到的正常模式的偏差，实现了在人群密集环境中的实时异常检测。", "keywords": "crowd anomaly detection, person-specific velocity, dense optical flow, hierarchical clustering, real-time surveillance", "scoring": {"interpretability": 5, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Fatima AlGhamdi", "Omar Alharbi", "Abdullah Aldwyish", "Raied Aljadaany", "Muhammad Kamran J Khan", "Huda Alamri"]}
]]></acme>

<pubDate>2025-10-21T00:26:54+00:00</pubDate>
</item>
<item>
<title>ActivationReasoning: Logical Reasoning in Latent Activation Spaces</title>
<link>https://papers.cool/arxiv/2510.18184</link>
<guid>https://papers.cool/arxiv/2510.18184</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent activation space of large language models by first identifying latent concepts with sparse autoencoders, then mapping activated concepts to logical propositions, and finally applying logical rules to infer higher‑order structures and steer model behavior. Experiments on multi‑hop reasoning, abstraction robustness, diverse language reasoning, and context‑sensitive safety tasks show that AR scales with reasoning complexity, generalizes across models, and improves transparency, control, and alignment. The results suggest that grounding logical structure in latent activations can enable more reliable, auditable AI systems.<br /><strong>Summary (CN):</strong> 本文提出了 ActivationReasoning（AR）框架，通过稀疏自编码器识别潜在概念并组织成字典，在推理时检测激活的概念并映射为逻辑命题，随后在这些命题上应用逻辑规则以推导更高层结构并引导模型行为。实验在多跳推理、抽象鲁棒性、自然语言推理以及上下文敏感安全任务上展示了 AR 随推理复杂度的稳健扩展、跨模型的泛化以及对透明度、控制和对齐的提升。结果表明，将逻辑结构嵌入潜在激活中有望实现更可靠、可审计的 AI 系统。<br /><strong>Keywords:</strong> logical reasoning, latent activation space, sparse autoencoders, concept activation, mechanistic interpretability, model control, alignment, multi-hop reasoning, robustness, safety<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Lukas Helff, Ruben Härle, Wolfgang Stammer, Felix Friedrich, Manuel Brack, Antonia Wüst, Hikaru Shindo, Patrick Schramowski, Kristian Kersting</div>
Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are fragile and passive, offering no mechanism for systematic reasoning or model control. To address this, we introduce ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent space of LLMs. It proceeds in three stages: (1) Finding latent representations, first latent concept representations are identified (e.g., via SAEs) and organized into a dictionary; (2) Activating propositions, at inference time AR detects activating concepts and maps them to logical propositions; and (3)Logical reasoning, applying logical rules over these propositions to infer higher-order structures, compose new concepts, and steer model behavior. We evaluate AR on multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept cues (Rail2Country), reasoning over natural and diverse language (ProverQA), and context-sensitive safety (BeaverTails). Across all tasks, AR scales robustly with reasoning complexity, generalizes to abstract and context-sensitive tasks, and transfers across model backbones. These results demonstrate that grounding logical structure in latent activations not only improves transparency but also enables structured reasoning, reliable control, and alignment with desired behaviors, providing a path toward more reliable and auditable AI.
<div><strong>Authors:</strong> Lukas Helff, Ruben Härle, Wolfgang Stammer, Felix Friedrich, Manuel Brack, Antonia Wüst, Hikaru Shindo, Patrick Schramowski, Kristian Kersting</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent activation space of large language models by first identifying latent concepts with sparse autoencoders, then mapping activated concepts to logical propositions, and finally applying logical rules to infer higher‑order structures and steer model behavior. Experiments on multi‑hop reasoning, abstraction robustness, diverse language reasoning, and context‑sensitive safety tasks show that AR scales with reasoning complexity, generalizes across models, and improves transparency, control, and alignment. The results suggest that grounding logical structure in latent activations can enable more reliable, auditable AI systems.", "summary_cn": "本文提出了 ActivationReasoning（AR）框架，通过稀疏自编码器识别潜在概念并组织成字典，在推理时检测激活的概念并映射为逻辑命题，随后在这些命题上应用逻辑规则以推导更高层结构并引导模型行为。实验在多跳推理、抽象鲁棒性、自然语言推理以及上下文敏感安全任务上展示了 AR 随推理复杂度的稳健扩展、跨模型的泛化以及对透明度、控制和对齐的提升。结果表明，将逻辑结构嵌入潜在激活中有望实现更可靠、可审计的 AI 系统。", "keywords": "logical reasoning, latent activation space, sparse autoencoders, concept activation, mechanistic interpretability, model control, alignment, multi-hop reasoning, robustness, safety", "scoring": {"interpretability": 8, "understanding": 8, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Lukas Helff", "Ruben Härle", "Wolfgang Stammer", "Felix Friedrich", "Manuel Brack", "Antonia Wüst", "Hikaru Shindo", "Patrick Schramowski", "Kristian Kersting"]}
]]></acme>

<pubDate>2025-10-21T00:21:04+00:00</pubDate>
</item>
<item>
<title>Automatic Prompt Generation via Adaptive Selection of Prompting Techniques</title>
<link>https://papers.cool/arxiv/2510.18162</link>
<guid>https://papers.cool/arxiv/2510.18162</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method that automatically generates prompts for large language models by adaptively selecting prompting techniques based on abstract task descriptions. It builds a knowledge base linking semantically clustered tasks to suitable techniques, assigns new tasks to the closest cluster, and synthesizes prompts accordingly, achieving superior performance on 23 BIG-Bench Extra Hard tasks compared to standard prompts and existing tools.<br /><strong>Summary (CN):</strong> 本文提出一种自动生成大型语言模型提示的方法，根据抽象任务描述自适应选择提示技术。系统构建任务聚类与相应提示技术的知识库，将新任务分配到最相关的聚类并综合生成提示，在 23 项 BIG-Bench Extra Hard 任务上表现优于标准提示及现有自动生成工具。<br /><strong>Keywords:</strong> prompt engineering, automatic prompt generation, adaptive selection, knowledge base, large language models, BIG-Bench Extra Hard<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
Prompt engineering is crucial for achieving reliable and effective outputs from large language models (LLMs), but its design requires specialized knowledge of prompting techniques and a deep understanding of target tasks. To address this challenge, we propose a novel method that adaptively selects task-appropriate prompting techniques based on users' abstract task descriptions and automatically generates high-quality prompts without relying on pre-existing templates or frameworks. The proposed method constructs a knowledge base that associates task clusters, characterized by semantic similarity across diverse tasks, with their corresponding prompting techniques. When users input task descriptions, the system assigns them to the most relevant task cluster and dynamically generates prompts by integrating techniques drawn from the knowledge base. An experimental evaluation of the proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates superior performance compared with standard prompts and existing automatic prompt-generation tools, as measured by both arithmetic and harmonic mean scores. This research establishes a foundation for streamlining and standardizing prompt creation, enabling non-experts to effectively leverage LLMs.
<div><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method that automatically generates prompts for large language models by adaptively selecting prompting techniques based on abstract task descriptions. It builds a knowledge base linking semantically clustered tasks to suitable techniques, assigns new tasks to the closest cluster, and synthesizes prompts accordingly, achieving superior performance on 23 BIG-Bench Extra Hard tasks compared to standard prompts and existing tools.", "summary_cn": "本文提出一种自动生成大型语言模型提示的方法，根据抽象任务描述自适应选择提示技术。系统构建任务聚类与相应提示技术的知识库，将新任务分配到最相关的聚类并综合生成提示，在 23 项 BIG-Bench Extra Hard 任务上表现优于标准提示及现有自动生成工具。", "keywords": "prompt engineering, automatic prompt generation, adaptive selection, knowledge base, large language models, BIG-Bench Extra Hard", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yohei Ikenoue", "Hitomi Tashiro", "Shigeru Kuroyanagi"]}
]]></acme>

<pubDate>2025-10-20T23:28:23+00:00</pubDate>
</item>
<item>
<title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title>
<link>https://papers.cool/arxiv/2510.18123</link>
<guid>https://papers.cool/arxiv/2510.18123</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SafeCoop, a systematic defense pipeline for natural-language‑based collaborative driving that addresses full‑stack safety and security issues. It provides a taxonomy of attack strategies (connection disruption, relay/replay interference, content spoofing, multi‑connection forgery) and mitigates them through a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by an agentic transformation function. Evaluations in CARLA across 32 critical scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection under malicious attacks.<br /><strong>Summary (CN):</strong> 本文提出 SafeCoop 防御流水线，系统研究基于自然语言的协同驾驶中的全栈安全与安全问题，并给出包括连接中断、转发/重放干扰、内容欺骗和多连接伪造在内的攻击策略分类。通过语义防火墙、语言‑感知一致性检查以及多源共识（由跨帧空间对齐的 agentic 转换函数实现）来减轻这些风险。实验在 CARLA 仿真环境的 32 种关键场景中进行，攻击下驾驶得分提升最高 69.15%，恶意检测 F1 分数最高 67.32%。<br /><strong>Keywords:</strong> collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, agentic defense, CARLA simulation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is https://xiangbogaobarry.github.io/SafeCoop.
<div><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SafeCoop, a systematic defense pipeline for natural-language‑based collaborative driving that addresses full‑stack safety and security issues. It provides a taxonomy of attack strategies (connection disruption, relay/replay interference, content spoofing, multi‑connection forgery) and mitigates them through a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by an agentic transformation function. Evaluations in CARLA across 32 critical scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection under malicious attacks.", "summary_cn": "本文提出 SafeCoop 防御流水线，系统研究基于自然语言的协同驾驶中的全栈安全与安全问题，并给出包括连接中断、转发/重放干扰、内容欺骗和多连接伪造在内的攻击策略分类。通过语义防火墙、语言‑感知一致性检查以及多源共识（由跨帧空间对齐的 agentic 转换函数实现）来减轻这些风险。实验在 CARLA 仿真环境的 32 种关键场景中进行，攻击下驾驶得分提升最高 69.15%，恶意检测 F1 分数最高 67.32%。", "keywords": "collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, agentic defense, CARLA simulation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}
]]></acme>

<pubDate>2025-10-20T21:41:28+00:00</pubDate>
</item>
<item>
<title>Latent Discrete Diffusion Models</title>
<link>https://papers.cool/arxiv/2510.18114</link>
<guid>https://papers.cool/arxiv/2510.18114</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Latent Discrete Diffusion Models (LDDMs), which combine a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings to capture cross‑token dependencies. Two variants are proposed: FUJI‑LDDM, which jointly denoises tokens and latents, and SEQ‑LDDM, which resolves the latent chain before the discrete chain. Experiments show that LDDMs improve unconditional generation metrics and are particularly effective under low sampling budgets.<br /><strong>Summary (CN):</strong> 本文提出了潜在离散扩散模型（LDDM），通过将对令牌的掩码离散扩散与对潜在嵌入的连续扩散相结合，以捕获跨令牌的依赖关系。提出了两种实现方式：FUJI‑LDDM 在令牌和潜在空间上进行全局联合去噪，SEQ‑LDDM 则先条件化地恢复潜在链再处理离散链。实验表明 LDDM 在无条件生成指标上优于最先进的掩码离散扩散基线，且在低采样预算下表现尤为突出。<br /><strong>Keywords:</strong> latent diffusion, discrete diffusion, masked denoising, joint token modeling, ELELBO, few-step generation, language modeling, continuous latent embeddings<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Dario Shariatian, Alain Durmus, Stefano Peluchetti</div>
We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \emph{Latent Discrete Diffusion Models} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.
<div><strong>Authors:</strong> Dario Shariatian, Alain Durmus, Stefano Peluchetti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Latent Discrete Diffusion Models (LDDMs), which combine a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings to capture cross‑token dependencies. Two variants are proposed: FUJI‑LDDM, which jointly denoises tokens and latents, and SEQ‑LDDM, which resolves the latent chain before the discrete chain. Experiments show that LDDMs improve unconditional generation metrics and are particularly effective under low sampling budgets.", "summary_cn": "本文提出了潜在离散扩散模型（LDDM），通过将对令牌的掩码离散扩散与对潜在嵌入的连续扩散相结合，以捕获跨令牌的依赖关系。提出了两种实现方式：FUJI‑LDDM 在令牌和潜在空间上进行全局联合去噪，SEQ‑LDDM 则先条件化地恢复潜在链再处理离散链。实验表明 LDDM 在无条件生成指标上优于最先进的掩码离散扩散基线，且在低采样预算下表现尤为突出。", "keywords": "latent diffusion, discrete diffusion, masked denoising, joint token modeling, ELELBO, few-step generation, language modeling, continuous latent embeddings", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Dario Shariatian", "Alain Durmus", "Stefano Peluchetti"]}
]]></acme>

<pubDate>2025-10-20T21:26:52+00:00</pubDate>
</item>
<item>
<title>From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs</title>
<link>https://papers.cool/arxiv/2510.18104</link>
<guid>https://papers.cool/arxiv/2510.18104</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the creation of Autonomous Recommender‑Systems Research Labs (AutoRecLab) that automate the entire research cycle—from problem ideation and literature analysis to experimental design, execution, result interpretation, manuscript drafting, and provenance logging—by leveraging recent advances in automated science such as LLM‑driven agents. It outlines an agenda for the rec‑sys community including open‑source prototypes, benchmark competitions, AI‑generated submission venues, standards for attribution and reproducibility, and governance discussions on ethics, privacy, and fairness. The authors call for a community retreat to coordinate next steps and develop responsible integration guidelines.<br /><strong>Summary (CN):</strong> 本文提出构建自动化推荐系统研究实验室（AutoRecLab），通过大语言模型驱动的代理实现从问题构思、文献分析、实验设计与执行、结果解释、论文撰写到溯源日志的全流程自动化。文中为推荐系统社区制定议程，包括开放原型、基准竞赛、AI 生成稿件的评审渠道、归属与可重复性标准，以及围绕伦理、隐私和公平的治理讨论。作者呼吁组织社区研讨会，以协调后续工作并制定负责任的集成指南。<br /><strong>Keywords:</strong> AutoRecLab, autonomous recommender systems, automated research, LLM-driven experimentation, reproducibility, AI governance, recommendation systems, AI safety, AI ethics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Joeran Beel, Bela Gipp, Tobias Vente, Moritz Baumgart, Philipp Meister</div>
Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.
<div><strong>Authors:</strong> Joeran Beel, Bela Gipp, Tobias Vente, Moritz Baumgart, Philipp Meister</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the creation of Autonomous Recommender‑Systems Research Labs (AutoRecLab) that automate the entire research cycle—from problem ideation and literature analysis to experimental design, execution, result interpretation, manuscript drafting, and provenance logging—by leveraging recent advances in automated science such as LLM‑driven agents. It outlines an agenda for the rec‑sys community including open‑source prototypes, benchmark competitions, AI‑generated submission venues, standards for attribution and reproducibility, and governance discussions on ethics, privacy, and fairness. The authors call for a community retreat to coordinate next steps and develop responsible integration guidelines.", "summary_cn": "本文提出构建自动化推荐系统研究实验室（AutoRecLab），通过大语言模型驱动的代理实现从问题构思、文献分析、实验设计与执行、结果解释、论文撰写到溯源日志的全流程自动化。文中为推荐系统社区制定议程，包括开放原型、基准竞赛、AI 生成稿件的评审渠道、归属与可重复性标准，以及围绕伦理、隐私和公平的治理讨论。作者呼吁组织社区研讨会，以协调后续工作并制定负责任的集成指南。", "keywords": "AutoRecLab, autonomous recommender systems, automated research, LLM-driven experimentation, reproducibility, AI governance, recommendation systems, AI safety, AI ethics", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Joeran Beel", "Bela Gipp", "Tobias Vente", "Moritz Baumgart", "Philipp Meister"]}
]]></acme>

<pubDate>2025-10-20T20:58:50+00:00</pubDate>
</item>
<item>
<title>Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV</title>
<link>https://papers.cool/arxiv/2510.18103</link>
<guid>https://papers.cool/arxiv/2510.18103</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper develops a mortality prediction model for cardiac arrest ICU patients by meta-modeling structured clinical variables together with unstructured textual data such as discharge summaries and radiology reports from the MIMIC-IV database. Feature selection is performed with LASSO and XGBoost, and a multivariate logistic regression using both structured features and textual embeddings (TF-IDF and BERT) achieves an AUC of 0.918, markedly higher than 0.753 with structured data alone, and shows strong clinical utility across a wide range of decision thresholds.<br /><strong>Summary (CN):</strong> 本文利用 MIMIC-IV 数据库中结构化临床数据和非结构化文本（出院小结、放射报告）构建心脏骤停 ICU 患者死亡率预测模型。通过 LASSO 和 XGBoost 进行特征选择，再用多元逻辑回归结合结构化特征与文本嵌入（TF-IDF、BERT），模型 AUC 达到 0.918，显著高于仅使用结构化数据的 0.753，并在 0.2-0.8 的阈值范围内展示出较大的标准化净收益，表明临床实用性。<br /><strong>Keywords:</strong> mortality prediction, ICU, cardiac arrest, structured clinical data, unstructured text, MIMIC-IV, logistic regression, feature selection, BERT embeddings<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Nursultan Mamatov, Philipp Kellmeyer</div>
Accurate early prediction of in-hospital mortality in intensive care units (ICUs) is essential for timely clinical intervention and efficient resource allocation. This study develops and evaluates machine learning models that integrate both structured clinical data and unstructured textual information, specifically discharge summaries and radiology reports, from the MIMIC-IV database. We used LASSO and XGBoost for feature selection, followed by a multivariate logistic regression trained on the top features identified by both models. Incorporating textual features using TF-IDF and BERT embeddings significantly improved predictive performance. The final logistic regression model, which combined structured and textual input, achieved an AUC of 0.918, compared to 0.753 when using structured data alone, a relative improvement 22%. The analysis of the decision curve demonstrated a superior standardized net benefit in a wide range of threshold probabilities (0.2-0.8), confirming the clinical utility of the model. These results underscore the added prognostic value of unstructured clinical notes and support their integration into interpretable feature-driven risk prediction models for ICU patients.
<div><strong>Authors:</strong> Nursultan Mamatov, Philipp Kellmeyer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper develops a mortality prediction model for cardiac arrest ICU patients by meta-modeling structured clinical variables together with unstructured textual data such as discharge summaries and radiology reports from the MIMIC-IV database. Feature selection is performed with LASSO and XGBoost, and a multivariate logistic regression using both structured features and textual embeddings (TF-IDF and BERT) achieves an AUC of 0.918, markedly higher than 0.753 with structured data alone, and shows strong clinical utility across a wide range of decision thresholds.", "summary_cn": "本文利用 MIMIC-IV 数据库中结构化临床数据和非结构化文本（出院小结、放射报告）构建心脏骤停 ICU 患者死亡率预测模型。通过 LASSO 和 XGBoost 进行特征选择，再用多元逻辑回归结合结构化特征与文本嵌入（TF-IDF、BERT），模型 AUC 达到 0.918，显著高于仅使用结构化数据的 0.753，并在 0.2-0.8 的阈值范围内展示出较大的标准化净收益，表明临床实用性。", "keywords": "mortality prediction, ICU, cardiac arrest, structured clinical data, unstructured text, MIMIC-IV, logistic regression, feature selection, BERT embeddings", "scoring": {"interpretability": 7, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Nursultan Mamatov", "Philipp Kellmeyer"]}
]]></acme>

<pubDate>2025-10-20T20:56:45+00:00</pubDate>
</item>
<item>
<title>Accelerating Vision Transformers with Adaptive Patch Sizes</title>
<link>https://papers.cool/arxiv/2510.18091</link>
<guid>https://papers.cool/arxiv/2510.18091</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Patch Transformers (APT), which dynamically allocate different patch sizes within a single image to reduce the number of tokens processed by using larger patches in homogeneous regions and smaller patches in complex areas. APT yields substantial speedups—up to 40% faster inference on ViT-L and 50% on ViT-H—while preserving downstream performance and can be applied to already fine‑tuned ViTs with minimal additional training. The method also accelerates high‑resolution tasks such as visual QA, object detection, and semantic segmentation with up to 30% faster training and inference.<br /><strong>Summary (CN):</strong> 本文提出自适应切片转换器（APT），在同一图像中根据局部内容动态选择不同的切片大小：在均匀区域使用较大切片，在复杂区域使用较小切片，从而显著降低输入 token 数量。APT 在保持下游性能的同时，实现了显著的加速——ViT-L 推理提升约 40%，ViT-H 提升约 50%，并可直接应用于已微调的 ViT，仅需少量额外训练。该方法同样在高分辨率视觉问答、目标检测和语义分割等任务上实现约 30% 的训练和推理速度提升。<br /><strong>Keywords:</strong> vision transformers, adaptive patch sizes, token reduction, model efficiency, inference speedup, high-resolution dense tasks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\% faster training and inference in visual QA, object detection, and semantic segmentation.
<div><strong>Authors:</strong> Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Patch Transformers (APT), which dynamically allocate different patch sizes within a single image to reduce the number of tokens processed by using larger patches in homogeneous regions and smaller patches in complex areas. APT yields substantial speedups—up to 40% faster inference on ViT-L and 50% on ViT-H—while preserving downstream performance and can be applied to already fine‑tuned ViTs with minimal additional training. The method also accelerates high‑resolution tasks such as visual QA, object detection, and semantic segmentation with up to 30% faster training and inference.", "summary_cn": "本文提出自适应切片转换器（APT），在同一图像中根据局部内容动态选择不同的切片大小：在均匀区域使用较大切片，在复杂区域使用较小切片，从而显著降低输入 token 数量。APT 在保持下游性能的同时，实现了显著的加速——ViT-L 推理提升约 40%，ViT-H 提升约 50%，并可直接应用于已微调的 ViT，仅需少量额外训练。该方法同样在高分辨率视觉问答、目标检测和语义分割等任务上实现约 30% 的训练和推理速度提升。", "keywords": "vision transformers, adaptive patch sizes, token reduction, model efficiency, inference speedup, high-resolution dense tasks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rohan Choudhury", "JungEun Kim", "Jinhyung Park", "Eunho Yang", "László A. Jeni", "Kris M. Kitani"]}
]]></acme>

<pubDate>2025-10-20T20:37:11+00:00</pubDate>
</item>
<item>
<title>R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations</title>
<link>https://papers.cool/arxiv/2510.18085</link>
<guid>https://papers.cool/arxiv/2510.18085</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Round‑Robin Behavior Cloning (R2BC), a method that lets a single human operator train a team of robots by providing sequential single‑agent demonstrations instead of joint multi‑agent data. R2BC incrementally teaches the entire system through teleoperation of one agent at a time and achieves performance comparable to or better than an oracle behavior‑cloning baseline on several simulated and real‑world tasks. The approach is validated on four simulated multi‑agent tasks and two physical robot experiments using real human demonstrations.<br /><strong>Summary (CN):</strong> 本文提出了轮流行为克隆（R2BC）方法，使单个人类操作员通过依次对单个机器人进行示范，来训练多机器人团队，而无需同步的多机器人示范。R2BC 通过一次控制一个机器人并逐步教学，实现了与使用特权同步示范的行为克隆基准相当，甚至更好的性能，并在四个模拟任务和两个真实机器人任务上进行了验证。<br /><strong>Keywords:</strong> multi-agent imitation learning, behavior cloning, round-robin behavior cloning, single-agent demonstrations, teleoperation, robot coordination, imitation learning, multi-robot control<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Connor Mattson, Varun Raveendra, Ellen Novoseller, Nicholas Waytowich, Vernon J. Lawhern, Daniel S. Brown</div>
Imitation Learning (IL) is a natural way for humans to teach robots, particularly when high-quality demonstrations are easy to obtain. While IL has been widely applied to single-robot settings, relatively few studies have addressed the extension of these methods to multi-agent systems, especially in settings where a single human must provide demonstrations to a team of collaborating robots. In this paper, we introduce and study Round-Robin Behavior Cloning (R2BC), a method that enables a single human operator to effectively train multi-robot systems through sequential, single-agent demonstrations. Our approach allows the human to teleoperate one agent at a time and incrementally teach multi-agent behavior to the entire system, without requiring demonstrations in the joint multi-agent action space. We show that R2BC methods match, and in some cases surpass, the performance of an oracle behavior cloning approach trained on privileged synchronized demonstrations across four multi-agent simulated tasks. Finally, we deploy R2BC on two physical robot tasks trained using real human demonstrations.
<div><strong>Authors:</strong> Connor Mattson, Varun Raveendra, Ellen Novoseller, Nicholas Waytowich, Vernon J. Lawhern, Daniel S. Brown</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Round‑Robin Behavior Cloning (R2BC), a method that lets a single human operator train a team of robots by providing sequential single‑agent demonstrations instead of joint multi‑agent data. R2BC incrementally teaches the entire system through teleoperation of one agent at a time and achieves performance comparable to or better than an oracle behavior‑cloning baseline on several simulated and real‑world tasks. The approach is validated on four simulated multi‑agent tasks and two physical robot experiments using real human demonstrations.", "summary_cn": "本文提出了轮流行为克隆（R2BC）方法，使单个人类操作员通过依次对单个机器人进行示范，来训练多机器人团队，而无需同步的多机器人示范。R2BC 通过一次控制一个机器人并逐步教学，实现了与使用特权同步示范的行为克隆基准相当，甚至更好的性能，并在四个模拟任务和两个真实机器人任务上进行了验证。", "keywords": "multi-agent imitation learning, behavior cloning, round-robin behavior cloning, single-agent demonstrations, teleoperation, robot coordination, imitation learning, multi-robot control", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Connor Mattson", "Varun Raveendra", "Ellen Novoseller", "Nicholas Waytowich", "Vernon J. Lawhern", "Daniel S. Brown"]}
]]></acme>

<pubDate>2025-10-20T20:24:23+00:00</pubDate>
</item>
<item>
<title>RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN</title>
<link>https://papers.cool/arxiv/2510.18084</link>
<guid>https://papers.cool/arxiv/2510.18084</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a reinforcement learning based framework for dynamic, security-aware resource allocation in UAV‑assisted Open Radio Access Networks (O‑RAN) for disaster management and search‑and‑rescue scenarios. It formulates a joint optimization of security, latency, and energy efficiency, and demonstrates through simulations that the RL approach outperforms heuristic baselines in maintaining ultra‑low latency, high security, and improved energy usage.<br /><strong>Summary (CN):</strong> 本文提出一种基于强化学习的框架，用于在无人机辅助的 O‑RAN 中进行动态的安全感知资源分配，针对灾害管理和搜救场景。该方法将安全、时延和能效联合优化，并通过仿真表明相较于启发式基线，能够实现更低的时延、更高的安全性以及更好的能效。<br /><strong>Keywords:</strong> UAV, O-RAN, reinforcement learning, resource allocation, security, latency, energy efficiency, SAR, dynamic optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control<br /><strong>Authors:</strong> Zaineh Abughazzah, Emna Baccour, Loay Ismail, Amr Mohamed, Mounir Hamdi</div>
The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access Networks (O-RAN) enhances communication in disaster management and Search and Rescue (SAR) operations by ensuring connectivity when infrastructure fails. However, SAR scenarios demand stringent security and low-latency communication, as delays or breaches can compromise mission success. While UAVs serve as mobile relays, they introduce challenges in energy consumption and resource management, necessitating intelligent allocation strategies. Existing UAV-assisted O-RAN approaches often overlook the joint optimization of security, latency, and energy efficiency in dynamic environments. This paper proposes a novel Reinforcement Learning (RL)-based framework for dynamic resource allocation in UAV relays, explicitly addressing these trade-offs. Our approach formulates an optimization problem that integrates security-aware resource allocation, latency minimization, and energy efficiency, which is solved using RL. Unlike heuristic or static methods, our framework adapts in real-time to network dynamics, ensuring robust communication. Simulations demonstrate superior performance compared to heuristic baselines, achieving enhanced security and energy efficiency while maintaining ultra-low latency in SAR scenarios.
<div><strong>Authors:</strong> Zaineh Abughazzah, Emna Baccour, Loay Ismail, Amr Mohamed, Mounir Hamdi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a reinforcement learning based framework for dynamic, security-aware resource allocation in UAV‑assisted Open Radio Access Networks (O‑RAN) for disaster management and search‑and‑rescue scenarios. It formulates a joint optimization of security, latency, and energy efficiency, and demonstrates through simulations that the RL approach outperforms heuristic baselines in maintaining ultra‑low latency, high security, and improved energy usage.", "summary_cn": "本文提出一种基于强化学习的框架，用于在无人机辅助的 O‑RAN 中进行动态的安全感知资源分配，针对灾害管理和搜救场景。该方法将安全、时延和能效联合优化，并通过仿真表明相较于启发式基线，能够实现更低的时延、更高的安全性以及更好的能效。", "keywords": "UAV, O-RAN, reinforcement learning, resource allocation, security, latency, energy efficiency, SAR, dynamic optimization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}, "authors": ["Zaineh Abughazzah", "Emna Baccour", "Loay Ismail", "Amr Mohamed", "Mounir Hamdi"]}
]]></acme>

<pubDate>2025-10-20T20:22:00+00:00</pubDate>
</item>
<item>
<title>Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</title>
<link>https://papers.cool/arxiv/2510.18081</link>
<guid>https://papers.cool/arxiv/2510.18081</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Any-Depth Alignment (ADA), an inference-time technique that repeatedly injects alignment‑rich header tokens during generation to force large language models to re‑evaluate and refuse harmful content at any point. Experiments across multiple open‑source LLM families show near‑perfect refusal rates against strong pre‑fill and prompt attacks while preserving performance on benign tasks. ADA operates without modifying model parameters and remains effective after further instruction tuning.<br /><strong>Summary (CN):</strong> 本文提出了 Any-Depth Alignment（ADA）方法，在生成过程中多次插入包含强对齐先验的头部标记，使大语言模型在任意阶段重新评估并拒绝有害内容。实验在多种开源模型上验证了该方法在面对强大的前置攻击和提示攻击时几乎 100% 的拒绝率，同时保持对正常任务的性能。ADA 在无需修改模型参数的情况下实现，并在模型后续指令微调后仍保持效果。<br /><strong>Keywords:</strong> any-depth alignment, inference-time defense, LLM safety, refusal tokens, adversarial prompt attack, shallow alignment, token injection, alignment robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</div>
Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).
<div><strong>Authors:</strong> Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Any-Depth Alignment (ADA), an inference-time technique that repeatedly injects alignment‑rich header tokens during generation to force large language models to re‑evaluate and refuse harmful content at any point. Experiments across multiple open‑source LLM families show near‑perfect refusal rates against strong pre‑fill and prompt attacks while preserving performance on benign tasks. ADA operates without modifying model parameters and remains effective after further instruction tuning.", "summary_cn": "本文提出了 Any-Depth Alignment（ADA）方法，在生成过程中多次插入包含强对齐先验的头部标记，使大语言模型在任意阶段重新评估并拒绝有害内容。实验在多种开源模型上验证了该方法在面对强大的前置攻击和提示攻击时几乎 100% 的拒绝率，同时保持对正常任务的性能。ADA 在无需修改模型参数的情况下实现，并在模型后续指令微调后仍保持效果。", "keywords": "any-depth alignment, inference-time defense, LLM safety, refusal tokens, adversarial prompt attack, shallow alignment, token injection, alignment robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiawei Zhang", "Andrew Estornell", "David D. Baek", "Bo Li", "Xiaojun Xu"]}
]]></acme>

<pubDate>2025-10-20T20:18:59+00:00</pubDate>
</item>
<item>
<title>R2L: Reliable Reinforcement Learning: Guaranteed Return &amp; Reliable Policies in Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18074</link>
<guid>https://papers.cool/arxiv/2510.18074</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a reliable reinforcement learning formulation that maximizes the probability that cumulative return exceeds a given threshold, rather than optimizing expected return. By augmenting the state space, the problem is transformed into a standard RL setting, enabling the use of existing algorithms such as Q‑learning and Dueling Double DQN with theoretical guarantees. Experiments on a reliable routing task demonstrate that the approach yields policies balancing efficiency and reliability, suitable for stochastic and safety‑critical environments.<br /><strong>Summary (CN):</strong> 本文提出了一种可靠强化学习框架，目标是最大化累计回报超过预设阈值的概率，而非仅优化期望回报。通过状态扩充，将该问题等价转化为标准强化学习，从而可直接利用 Q‑学习、双 DQN 等现有算法并提供理论保证。实验在可靠路由任务上展示了该方法能够在效率与可靠性之间取得平衡，适用于随机性和安全关键的应用场景。<br /><strong>Keywords:</strong> reliable reinforcement learning, probability of return, safety-critical RL, risk-sensitive RL, state augmentation, performance guarantees, routing, deep RL, Q-learning, Dueling Double DQN<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Nadir Farhi</div>
In this work, we address the problem of determining reliable policies in reinforcement learning (RL), with a focus on optimization under uncertainty and the need for performance guarantees. While classical RL algorithms aim at maximizing the expected return, many real-world applications - such as routing, resource allocation, or sequential decision-making under risk - require strategies that ensure not only high average performance but also a guaranteed probability of success. To this end, we propose a novel formulation in which the objective is to maximize the probability that the cumulative return exceeds a prescribed threshold. We demonstrate that this reliable RL problem can be reformulated, via a state-augmented representation, into a standard RL problem, thereby allowing the use of existing RL and deep RL algorithms without the need for entirely new algorithmic frameworks. Theoretical results establish the equivalence of the two formulations and show that reliable strategies can be derived by appropriately adapting well-known methods such as Q-learning or Dueling Double DQN. To illustrate the practical relevance of the approach, we consider the problem of reliable routing, where the goal is not to minimize the expected travel time but rather to maximize the probability of reaching the destination within a given time budget. Numerical experiments confirm that the proposed formulation leads to policies that effectively balance efficiency and reliability, highlighting the potential of reliable RL for applications in stochastic and safety-critical environments.
<div><strong>Authors:</strong> Nadir Farhi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a reliable reinforcement learning formulation that maximizes the probability that cumulative return exceeds a given threshold, rather than optimizing expected return. By augmenting the state space, the problem is transformed into a standard RL setting, enabling the use of existing algorithms such as Q‑learning and Dueling Double DQN with theoretical guarantees. Experiments on a reliable routing task demonstrate that the approach yields policies balancing efficiency and reliability, suitable for stochastic and safety‑critical environments.", "summary_cn": "本文提出了一种可靠强化学习框架，目标是最大化累计回报超过预设阈值的概率，而非仅优化期望回报。通过状态扩充，将该问题等价转化为标准强化学习，从而可直接利用 Q‑学习、双 DQN 等现有算法并提供理论保证。实验在可靠路由任务上展示了该方法能够在效率与可靠性之间取得平衡，适用于随机性和安全关键的应用场景。", "keywords": "reliable reinforcement learning, probability of return, safety-critical RL, risk-sensitive RL, state augmentation, performance guarantees, routing, deep RL, Q-learning, Dueling Double DQN", "scoring": {"interpretability": 2, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Nadir Farhi"]}
]]></acme>

<pubDate>2025-10-20T20:08:41+00:00</pubDate>
</item>
<item>
<title>Fine-tuning Flow Matching Generative Models with Intermediate Feedback</title>
<link>https://papers.cool/arxiv/2510.18072</link>
<guid>https://papers.cool/arxiv/2510.18072</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AC-Flow, an actor-critic framework for fine‑tuning continuous‑time flow‑matching generative models using intermediate feedback. By employing reward shaping, advantage clipping with a warm‑up phase, and a generalized critic weighting scheme with Wasserstein regularization, AC-Flow stabilises training and improves alignment of text‑to‑image models such as Stable Diffusion 3 to human preferences while preserving diversity and quality.<br /><strong>Summary (CN):</strong> 本文提出 AC-Flow，这是一种基于 actor‑critic 的框架，用于在连续时间流匹配生成模型的微调过程中利用中间反馈。通过奖励 shaping、优势 clipping 与 warm‑up 阶段相结合的双重稳定机制，以及使用 Wasserstein 正则化的广义 critic 加权方案，AC-Flow 能够实现训练的稳定性，并提升如 Stable Diffusion 3 等文本到图像模型对人类偏好的对齐，同时保持生成质量和多样性。<br /><strong>Keywords:</strong> flow matching, generative models, actor-critic, reward shaping, text-to-image alignment, Stable Diffusion 3, intermediate feedback, Wasserstein regularization, preference modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</div>
Flow-based generative models have shown remarkable success in text-to-image generation, yet fine-tuning them with intermediate feedback remains challenging, especially for continuous-time flow matching models. Most existing approaches solely learn from outcome rewards, struggling with the credit assignment problem. Alternative methods that attempt to learn a critic via direct regression on cumulative rewards often face training instabilities and model collapse in online settings. We present AC-Flow, a robust actor-critic framework that addresses these challenges through three key innovations: (1) reward shaping that provides well-normalized learning signals to enable stable intermediate value learning and gradient control, (2) a novel dual-stability mechanism that combines advantage clipping to prevent destructive policy updates with a warm-up phase that allows the critic to mature before influencing the actor, and (3) a scalable generalized critic weighting scheme that extends traditional reward-weighted methods while preserving model diversity through Wasserstein regularization. Through extensive experiments on Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art performance in text-to-image alignment tasks and generalization to unseen human preference models. Our results demonstrate that even with a computationally efficient critic model, we can robustly finetune flow models without compromising generative quality, diversity, or stability.
<div><strong>Authors:</strong> Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AC-Flow, an actor-critic framework for fine‑tuning continuous‑time flow‑matching generative models using intermediate feedback. By employing reward shaping, advantage clipping with a warm‑up phase, and a generalized critic weighting scheme with Wasserstein regularization, AC-Flow stabilises training and improves alignment of text‑to‑image models such as Stable Diffusion 3 to human preferences while preserving diversity and quality.", "summary_cn": "本文提出 AC-Flow，这是一种基于 actor‑critic 的框架，用于在连续时间流匹配生成模型的微调过程中利用中间反馈。通过奖励 shaping、优势 clipping 与 warm‑up 阶段相结合的双重稳定机制，以及使用 Wasserstein 正则化的广义 critic 加权方案，AC-Flow 能够实现训练的稳定性，并提升如 Stable Diffusion 3 等文本到图像模型对人类偏好的对齐，同时保持生成质量和多样性。", "keywords": "flow matching, generative models, actor-critic, reward shaping, text-to-image alignment, Stable Diffusion 3, intermediate feedback, Wasserstein regularization, preference modeling", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiajun Fan", "Chaoran Cheng", "Shuaike Shen", "Xiangxin Zhou", "Ge Liu"]}
]]></acme>

<pubDate>2025-10-20T20:08:03+00:00</pubDate>
</item>
<item>
<title>SPACeR: Self-Play Anchoring with Centralized Reference Models</title>
<link>https://papers.cool/arxiv/2510.18060</link>
<guid>https://papers.cool/arxiv/2510.18060</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SPACeR, a framework that uses a pretrained tokenized autoregressive motion model as a centralized reference to provide likelihood and KL divergence rewards, anchoring decentralized self-play RL policies to the human driving distribution. This yields fast, scalable, and human-like simulation agents for autonomous driving that match imitation-learned performance while being significantly smaller and faster. The approach is evaluated on the Waymo Sim Agents Challenge and demonstrates effective closed-loop ego planning evaluation.<br /><strong>Summary (CN):</strong> 本文提出 SPACeR 框架，利用预训练的 tokenized autoregressive motion 模型作为集中参考策略，通过似然奖励和 KL 散度将去中心化自我对弈的强化学习政策锚定到人类驾驶分布，从而实现高速、可扩展且具有人类行为特征的仿真代理。该方法在 Waymo Sim Agents Challenge 中取得与模仿学习政策相当的性能，同时推理速度提升至十倍、参数规模缩小五十倍，并在闭环规划评估中展示了对规划质量的有效测量。<br /><strong>Keywords:</strong> autonomous driving, self-play reinforcement learning, tokenized autoregressive model, likelihood reward, KL anchoring, imitation learning, simulation agents, Waymo Sim Agents Challenge, scalable traffic simulation, human-like behavior<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.
<div><strong>Authors:</strong> Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SPACeR, a framework that uses a pretrained tokenized autoregressive motion model as a centralized reference to provide likelihood and KL divergence rewards, anchoring decentralized self-play RL policies to the human driving distribution. This yields fast, scalable, and human-like simulation agents for autonomous driving that match imitation-learned performance while being significantly smaller and faster. The approach is evaluated on the Waymo Sim Agents Challenge and demonstrates effective closed-loop ego planning evaluation.", "summary_cn": "本文提出 SPACeR 框架，利用预训练的 tokenized autoregressive motion 模型作为集中参考策略，通过似然奖励和 KL 散度将去中心化自我对弈的强化学习政策锚定到人类驾驶分布，从而实现高速、可扩展且具有人类行为特征的仿真代理。该方法在 Waymo Sim Agents Challenge 中取得与模仿学习政策相当的性能，同时推理速度提升至十倍、参数规模缩小五十倍，并在闭环规划评估中展示了对规划质量的有效测量。", "keywords": "autonomous driving, self-play reinforcement learning, tokenized autoregressive model, likelihood reward, KL anchoring, imitation learning, simulation agents, Waymo Sim Agents Challenge, scalable traffic simulation, human-like behavior", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Wei-Jer Chang", "Akshay Rangesh", "Kevin Joseph", "Matthew Strong", "Masayoshi Tomizuka", "Yihan Hu", "Wei Zhan"]}
]]></acme>

<pubDate>2025-10-20T19:53:02+00:00</pubDate>
</item>
<item>
<title>Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</title>
<link>https://papers.cool/arxiv/2510.18053</link>
<guid>https://papers.cool/arxiv/2510.18053</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Adaptive Divergence Regularized Policy Optimization (ADRPO), a method that automatically tunes the strength of divergence regularization based on advantage estimates to balance exploration and exploitation during reinforcement‑learning fine‑tuning of generative models. Applied with Wasserstein‑2 regularization to text‑to‑image diffusion models and with KL regularization to large language and multimodal models, ADRPO achieves superior semantic alignment, diversity, and compositional control while mitigating reward‑hacking risks. Experiments show that a 2B‑parameter model can outperform much larger baselines and that ADRPO improves emergent exploration in LLMs and reasoning in audio‑multimodal tasks.<br /><strong>Summary (CN):</strong> 本文提出自适应散度正则化策略优化（ADRPO），该方法依据优势估计自动调节散度正则化强度，以在强化学习微调生成模型时平衡探索与利用。使用 Wasserstein‑2 正则化在文本到图像扩散模型以及 KL 正则化在大语言模型和多模态模型上进行实验，ADRPO 在语义对齐、多样性和组合控制上显著超越更大规模的基线，并降低奖励破解风险。实验表明，2 B 参数模型可胜过更大模型，且在 LLM 的探索和音频多模态推理中表现出色。<br /><strong>Keywords:</strong> adaptive regularization, policy optimization, reinforcement learning fine-tuning, generative models, text-to-image, large language models, reward hacking, alignment, exploration-exploitation, flow matching<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</div>
Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.
<div><strong>Authors:</strong> Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Adaptive Divergence Regularized Policy Optimization (ADRPO), a method that automatically tunes the strength of divergence regularization based on advantage estimates to balance exploration and exploitation during reinforcement‑learning fine‑tuning of generative models. Applied with Wasserstein‑2 regularization to text‑to‑image diffusion models and with KL regularization to large language and multimodal models, ADRPO achieves superior semantic alignment, diversity, and compositional control while mitigating reward‑hacking risks. Experiments show that a 2B‑parameter model can outperform much larger baselines and that ADRPO improves emergent exploration in LLMs and reasoning in audio‑multimodal tasks.", "summary_cn": "本文提出自适应散度正则化策略优化（ADRPO），该方法依据优势估计自动调节散度正则化强度，以在强化学习微调生成模型时平衡探索与利用。使用 Wasserstein‑2 正则化在文本到图像扩散模型以及 KL 正则化在大语言模型和多模态模型上进行实验，ADRPO 在语义对齐、多样性和组合控制上显著超越更大规模的基线，并降低奖励破解风险。实验表明，2 B 参数模型可胜过更大模型，且在 LLM 的探索和音频多模态推理中表现出色。", "keywords": "adaptive regularization, policy optimization, reinforcement learning fine-tuning, generative models, text-to-image, large language models, reward hacking, alignment, exploration-exploitation, flow matching", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Jiajun Fan", "Tong Wei", "Chaoran Cheng", "Yuxin Chen", "Ge Liu"]}
]]></acme>

<pubDate>2025-10-20T19:46:02+00:00</pubDate>
</item>
<item>
<title>Measure-Theoretic Anti-Causal Representation Learning</title>
<link>https://papers.cool/arxiv/2510.18052</link>
<guid>https://papers.cool/arxiv/2510.18052</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Anti-Causal Invariant Abstractions (ACIA), a measure-theoretic framework for learning representations when labels cause features. ACIA uses a two-level design to capture label-to-observation generation and learn stable causal patterns across environments, offering theoretical OOD generalization guarantees. Experiments on synthetic and medical datasets show ACIA outperforms existing methods in accuracy and invariance.<br /><strong>Summary (CN):</strong> 本文提出了反因果不变抽象（ACIA）——一种基于测度论的反因果表示学习框架，标签导致特征。ACIA 采用两层结构，低层捕获标签生成观测的机制，高层学习跨环境稳定的因果模式，并提供了分布外泛化的理论保证。实验在合成数据和医学数据上表明，ACIA 在准确率和不变性指标上均优于现有方法。<br /><strong>Keywords:</strong> anti-causal representation learning, measure-theoretic framework, invariant abstractions, out-of-distribution generalization, causal invariance, interventional kernels, medical data, synthetic experiments<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Arman Behnam, Binghui Wang</div>
Causal representation learning in the anti-causal setting (labels cause features rather than the reverse) presents unique challenges requiring specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a novel measure-theoretic framework for anti-causal representation learning. ACIA employs a two-level design, low-level representations capture how labels generate observations, while high-level representations learn stable causal patterns across environment-specific variations. ACIA addresses key limitations of existing approaches by accommodating prefect and imperfect interventions through interventional kernels, eliminating dependency on explicit causal structures, handling high-dimensional data effectively, and providing theoretical guarantees for out-of-distribution generalization. Experiments on synthetic and real-world medical datasets demonstrate that ACIA consistently outperforms state-of-the-art methods in both accuracy and invariance metrics. Furthermore, our theoretical results establish tight bounds on performance gaps between training and unseen environments, confirming the efficacy of our approach for robust anti-causal learning.
<div><strong>Authors:</strong> Arman Behnam, Binghui Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Anti-Causal Invariant Abstractions (ACIA), a measure-theoretic framework for learning representations when labels cause features. ACIA uses a two-level design to capture label-to-observation generation and learn stable causal patterns across environments, offering theoretical OOD generalization guarantees. Experiments on synthetic and medical datasets show ACIA outperforms existing methods in accuracy and invariance.", "summary_cn": "本文提出了反因果不变抽象（ACIA）——一种基于测度论的反因果表示学习框架，标签导致特征。ACIA 采用两层结构，低层捕获标签生成观测的机制，高层学习跨环境稳定的因果模式，并提供了分布外泛化的理论保证。实验在合成数据和医学数据上表明，ACIA 在准确率和不变性指标上均优于现有方法。", "keywords": "anti-causal representation learning, measure-theoretic framework, invariant abstractions, out-of-distribution generalization, causal invariance, interventional kernels, medical data, synthetic experiments", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Arman Behnam", "Binghui Wang"]}
]]></acme>

<pubDate>2025-10-16T22:13:05+00:00</pubDate>
</item>
<item>
<title>Language Models as Semantic Augmenters for Sequential Recommenders</title>
<link>https://papers.cool/arxiv/2510.18046</link>
<guid>https://papers.cool/arxiv/2510.18046</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LaMAR, a framework that uses large language models in a few-shot setting to generate auxiliary semantic signals—such as inferred usage scenarios, item intents, and thematic summaries—from existing metadata, and augments sequential recommendation datasets with these signals. By integrating the generated semantic augmentations into benchmark sequential recommender models, the authors show consistent performance improvements and demonstrate that the LLM‑produced signals are novel and diverse, highlighting a data‑centric approach where LLMs act as intelligent context generators.<br /><strong>Summary (CN):</strong> 本文提出 LaMAR 框架，利用大语言模型（LLM）在 few‑shot 设置下从现有元数据中推断用户意图、物品意图和主题摘要等辅助语义信号，并将这些信号加入序列推荐数据中。将生成的语义增强与基准序列推荐模型结合后，实验显示模型性能一致提升，且 LLM 生成的信号在语义新颖性和多样性方面表现突出，展示了 LLM 作为智能上下文生成器的数据中心化新范式。<br /><strong>Keywords:</strong> large language models, sequential recommendation, semantic augmentation, data-centric AI, few-shot prompting, recommender systems<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
Large Language Models (LLMs) excel at capturing latent semantics and contextual relationships across diverse modalities. However, in modeling user behavior from sequential interaction data, performance often suffers when such semantic context is limited or absent. We introduce LaMAR, a LLM-driven semantic enrichment framework designed to enrich such sequences automatically. LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual signals by inferring latent semantic aspects of a user's intent and item relationships from existing metadata. These generated signals, such as inferred usage scenarios, item intents, or thematic summaries, augment the original sequences with greater contextual depth. We demonstrate the utility of this generated resource by integrating it into benchmark sequential modeling tasks, where it consistently improves performance. Further analysis shows that LLM-generated signals exhibit high semantic novelty and diversity, enhancing the representational capacity of the downstream models. This work represents a new data-centric paradigm where LLMs serve as intelligent context generators, contributing a new method for the semi-automatic creation of training data and language resources.
<div><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LaMAR, a framework that uses large language models in a few-shot setting to generate auxiliary semantic signals—such as inferred usage scenarios, item intents, and thematic summaries—from existing metadata, and augments sequential recommendation datasets with these signals. By integrating the generated semantic augmentations into benchmark sequential recommender models, the authors show consistent performance improvements and demonstrate that the LLM‑produced signals are novel and diverse, highlighting a data‑centric approach where LLMs act as intelligent context generators.", "summary_cn": "本文提出 LaMAR 框架，利用大语言模型（LLM）在 few‑shot 设置下从现有元数据中推断用户意图、物品意图和主题摘要等辅助语义信号，并将这些信号加入序列推荐数据中。将生成的语义增强与基准序列推荐模型结合后，实验显示模型性能一致提升，且 LLM 生成的信号在语义新颖性和多样性方面表现突出，展示了 LLM 作为智能上下文生成器的数据中心化新范式。", "keywords": "large language models, sequential recommendation, semantic augmentation, data-centric AI, few-shot prompting, recommender systems", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahsa Valizadeh", "Xiangjue Dong", "Rui Tuo", "James Caverlee"]}
]]></acme>

<pubDate>2025-10-20T19:36:38+00:00</pubDate>
</item>
<item>
<title>Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network</title>
<link>https://papers.cool/arxiv/2510.18041</link>
<guid>https://papers.cool/arxiv/2510.18041</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Spatio-Temporal Operator Network (STONe), a non‑autoregressive neural operator that learns a stable mapping from sparse ground‑based neutron sensor measurements to high‑altitude radiation dose fields over long horizons. Trained on 23 years of global neutron data, STONe achieves accurate 180‑day forecasts with millisecond latency, demonstrating that operator learning can work across heterogeneous domains without iterative recurrence. This work expands scientific machine‑learning capabilities for cross‑domain, long‑term prediction in physics and related fields.<br /><strong>Summary (CN):</strong> 本文提出了跨域时空算子网络（STONe），一种非自回归神经算子模型，能够在长期预测中从稀疏的地面中子传感器数据映射到高空辐射剂量场。模型基于23 年全球中子数据进行训练，实现了180 天的高精度预报并具备毫秒级推理速度，展示了算子学习在解释分析模型内部机制（interpretability）方面的贡献有限。<br /><strong>Keywords:</strong> neural operators, spatio-temporal forecasting, cross-domain learning, radiation dose prediction, neutron sensor networks, operator network, long-term forecasting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jay Phil Yoo, Kazuma Kobayashi, Souvik Chakraborty, Syed Bahauddin Alam</div>
Forecasting unobservable physical quantities from sparse, cross-domain sensor data is a central unsolved problem in scientific machine learning. Existing neural operators and large-scale forecasters rely on dense, co-located input-output fields and short temporal contexts, assumptions that fail in real-world systems where sensing and prediction occur on distinct physical manifolds and over long timescales. We introduce the Spatio-Temporal Operator Network (STONe), a non-autoregressive neural operator that learns a stable functional mapping between heterogeneous domains. By directly inferring high-altitude radiation dose fields from sparse ground-based neutron measurements, STONe demonstrates that operator learning can generalize beyond shared-domain settings. It defines a nonlinear operator between sensor and target manifolds that remains stable over long forecasting horizons without iterative recurrence. This challenges the conventional view that operator learning requires domain alignment or autoregressive propagation. Trained on 23 years of global neutron data, STONe achieves accurate 180-day forecasts with millisecond inference latency. The framework establishes a general principle for cross-domain operator inference, enabling real-time prediction of complex spatiotemporal fields in physics, climate, and energy systems.
<div><strong>Authors:</strong> Jay Phil Yoo, Kazuma Kobayashi, Souvik Chakraborty, Syed Bahauddin Alam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Spatio-Temporal Operator Network (STONe), a non‑autoregressive neural operator that learns a stable mapping from sparse ground‑based neutron sensor measurements to high‑altitude radiation dose fields over long horizons. Trained on 23 years of global neutron data, STONe achieves accurate 180‑day forecasts with millisecond latency, demonstrating that operator learning can work across heterogeneous domains without iterative recurrence. This work expands scientific machine‑learning capabilities for cross‑domain, long‑term prediction in physics and related fields.", "summary_cn": "本文提出了跨域时空算子网络（STONe），一种非自回归神经算子模型，能够在长期预测中从稀疏的地面中子传感器数据映射到高空辐射剂量场。模型基于23 年全球中子数据进行训练，实现了180 天的高精度预报并具备毫秒级推理速度，展示了算子学习在解释分析模型内部机制（interpretability）方面的贡献有限。", "keywords": "neural operators, spatio-temporal forecasting, cross-domain learning, radiation dose prediction, neutron sensor networks, operator network, long-term forecasting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jay Phil Yoo", "Kazuma Kobayashi", "Souvik Chakraborty", "Syed Bahauddin Alam"]}
]]></acme>

<pubDate>2025-10-20T19:27:00+00:00</pubDate>
</item>
<item>
<title>TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation</title>
<link>https://papers.cool/arxiv/2510.18038</link>
<guid>https://papers.cool/arxiv/2510.18038</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces TriggerNet, an explainable AI framework that combines Grad-CAM, RISE, FullGrad, and TCAV to generate visual explanations for deep learning models used in plant classification and red palm mite disease detection. It evaluates and compares multiple deep learning architectures (CNN, EfficientNet, MobileNet, ViT, ResNet50, InceptionV3) and traditional classifiers (Random Forest, SVM, KNN), and employs Snorkel with heuristic rules for efficient annotation of disease classes.<br /><strong>Summary (CN):</strong> 本文提出 TriggerNet，一个融合 Grad‑CAM、RISE、FullGrad 和 TCAV 的可解释 AI 框架，用于红棕螨（Raoiella indica）导致的植物病害检测，并对多种深度学习与传统机器学习模型进行比较。该框架结合 Snorkel 的启发式规则实现了半自动标注，显著降低人工标注成本并提升数据可靠性。实验在包含 11 种植物的 RGB 图像数据集上验证了方法的有效性。<br /><strong>Keywords:</strong> explainable AI, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, multi-model comparison, Snorkel, heuristic annotation<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
The red palm mite infestation has become a serious concern, particularly in regions with extensive palm cultivation, leading to reduced productivity and economic losses. Accurate and early identification of mite-infested plants is critical for effective management. The current study focuses on evaluating and comparing the ML model for classifying the affected plants and detecting the infestation. TriggerNet is a novel interpretable AI framework that integrates Grad-CAM, RISE, FullGrad, and TCAV to generate novel visual explanations for deep learning models in plant classification and disease detection. This study applies TriggerNet to address red palm mite (Raoiella indica) infestation, a major threat to palm cultivation and agricultural productivity. A diverse set of RGB images across 11 plant species, Arecanut, Date Palm, Bird of Paradise, Coconut Palm, Ginger, Citrus Tree, Palm Oil, Orchid, Banana Palm, Avocado Tree, and Cast Iron Plant was utilized for training and evaluation. Advanced deep learning models like CNN, EfficientNet, MobileNet, ViT, ResNet50, and InceptionV3, alongside machine learning classifiers such as Random Forest, SVM, and KNN, were employed for plant classification. For disease classification, all plants were categorized into four classes: Healthy, Yellow Spots, Reddish Bronzing, and Silk Webbing. Snorkel was used to efficiently label these disease classes by leveraging heuristic rules and patterns, reducing manual annotation time and improving dataset reliability.
<div><strong>Authors:</strong> Harshini Suresha, Kavitha SH</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces TriggerNet, an explainable AI framework that combines Grad-CAM, RISE, FullGrad, and TCAV to generate visual explanations for deep learning models used in plant classification and red palm mite disease detection. It evaluates and compares multiple deep learning architectures (CNN, EfficientNet, MobileNet, ViT, ResNet50, InceptionV3) and traditional classifiers (Random Forest, SVM, KNN), and employs Snorkel with heuristic rules for efficient annotation of disease classes.", "summary_cn": "本文提出 TriggerNet，一个融合 Grad‑CAM、RISE、FullGrad 和 TCAV 的可解释 AI 框架，用于红棕螨（Raoiella indica）导致的植物病害检测，并对多种深度学习与传统机器学习模型进行比较。该框架结合 Snorkel 的启发式规则实现了半自动标注，显著降低人工标注成本并提升数据可靠性。实验在包含 11 种植物的 RGB 图像数据集上验证了方法的有效性。", "keywords": "explainable AI, Grad-CAM, RISE, FullGrad, TCAV, plant disease detection, red palm mite, multi-model comparison, Snorkel, heuristic annotation", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Harshini Suresha", "Kavitha SH"]}
]]></acme>

<pubDate>2025-10-20T19:23:17+00:00</pubDate>
</item>
<item>
<title>SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection</title>
<link>https://papers.cool/arxiv/2510.18034</link>
<guid>https://papers.cool/arxiv/2510.18034</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents SAVANT, a structured reasoning framework that uses vision-language models to detect semantic anomalies in autonomous driving scenes through a two‑phase pipeline of layered scene description extraction and multimodal evaluation across four semantic layers. SAVANT achieves high recall and accuracy on real‑world driving data, and with a fine‑tuned open‑source 7B model (Qwen2.5VL) it surpasses proprietary baselines while enabling low‑cost local deployment. By automatically labeling thousands of images, the method tackles data scarcity and improves reliable semantic monitoring for autonomous systems.<br /><strong>Summary (CN):</strong> 本文提出了 SAVANT（Semantic Analysis with Vision‑Augmented Anomaly deTection）框架，通过两阶段管线：分层场景描述提取和多模态评估，对自动驾驶图像进行四个语义层面的结构化推理（街道、基础设施、可移动物体、环境），实现语义异常检测。SAVANT 在真实驾驶场景上达到 89.6% 召回率和 88.0% 准确率，使用微调的 7B 开源模型 Qwen2.5VL 更可实现 90.8% 召回率和 93.8% 准确率，解决数据稀缺并提供低成本本地部署的可靠监控手段。<br /><strong>Keywords:</strong> anomaly detection, autonomous driving, vision-language models, structured reasoning, semantic layers, multimodal evaluation, Qwen2.5VL<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</div>
Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.
<div><strong>Authors:</strong> Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents SAVANT, a structured reasoning framework that uses vision-language models to detect semantic anomalies in autonomous driving scenes through a two‑phase pipeline of layered scene description extraction and multimodal evaluation across four semantic layers. SAVANT achieves high recall and accuracy on real‑world driving data, and with a fine‑tuned open‑source 7B model (Qwen2.5VL) it surpasses proprietary baselines while enabling low‑cost local deployment. By automatically labeling thousands of images, the method tackles data scarcity and improves reliable semantic monitoring for autonomous systems.", "summary_cn": "本文提出了 SAVANT（Semantic Analysis with Vision‑Augmented Anomaly deTection）框架，通过两阶段管线：分层场景描述提取和多模态评估，对自动驾驶图像进行四个语义层面的结构化推理（街道、基础设施、可移动物体、环境），实现语义异常检测。SAVANT 在真实驾驶场景上达到 89.6% 召回率和 88.0% 准确率，使用微调的 7B 开源模型 Qwen2.5VL 更可实现 90.8% 召回率和 93.8% 准确率，解决数据稀缺并提供低成本本地部署的可靠监控手段。", "keywords": "anomaly detection, autonomous driving, vision-language models, structured reasoning, semantic layers, multimodal evaluation, Qwen2.5VL", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"]}
]]></acme>

<pubDate>2025-10-20T19:14:29+00:00</pubDate>
</item>
<item>
<title>From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18030</link>
<guid>https://papers.cool/arxiv/2510.18030</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper revisits global structured pruning for large language models and proposes GISP‑Global Iterative Structured Pruning, a post‑training method that removes attention heads and MLP channels based on first‑order loss‑sensitive importance aggregated at the structure level. By using an iterative schedule and allowing task‑specific loss objectives, GISP achieves higher sparsity with stable perplexity and notable downstream accuracy gains on several LLM families, supporting a prune‑once‑deploy‑many workflow.<br /><strong>Summary (CN):</strong> 本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝（GISP‑Global Iterative Structured Pruning），一种基于第一阶损失敏感重要性并在结构层面聚合的后训练剪枝方法，可去除注意力头和 MLP 通道。通过迭代剪枝调度并支持任务特定的损失目标，GISP 在保持困惑度稳定的同时实现更高稀疏率，并在多个 LLM 系列上显著提升下游任务准确率，支持“一次剪枝，多次部署”工作流。<br /><strong>Keywords:</strong> structured pruning, large language models, iterative pruning, task-aligned pruning, model sparsity, compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a "prune-once, deploy-many" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.
<div><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper revisits global structured pruning for large language models and proposes GISP‑Global Iterative Structured Pruning, a post‑training method that removes attention heads and MLP channels based on first‑order loss‑sensitive importance aggregated at the structure level. By using an iterative schedule and allowing task‑specific loss objectives, GISP achieves higher sparsity with stable perplexity and notable downstream accuracy gains on several LLM families, supporting a prune‑once‑deploy‑many workflow.", "summary_cn": "本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝（GISP‑Global Iterative Structured Pruning），一种基于第一阶损失敏感重要性并在结构层面聚合的后训练剪枝方法，可去除注意力头和 MLP 通道。通过迭代剪枝调度并支持任务特定的损失目标，GISP 在保持困惑度稳定的同时实现更高稀疏率，并在多个 LLM 系列上显著提升下游任务准确率，支持“一次剪枝，多次部署”工作流。", "keywords": "structured pruning, large language models, iterative pruning, task-aligned pruning, model sparsity, compression", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"]}
]]></acme>

<pubDate>2025-10-20T19:04:09+00:00</pubDate>
</item>
<item>
<title>DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data</title>
<link>https://papers.cool/arxiv/2510.18029</link>
<guid>https://papers.cool/arxiv/2510.18029</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DynaQuery, a self-adapting framework that combines schema introspection with linking to enable reliable natural‑language queries over hybrid structured and multimodal databases. By elevating schema linking to a dedicated planning phase (via the SILE primitive), the authors show that it drastically reduces catastrophic failures such as schema hallucination compared to traditional retrieval‑augmented generation approaches. Empirical results on multiple benchmarks demonstrate improved robustness and generalization from pure schema awareness to holistic semantics awareness.<br /><strong>Summary (CN):</strong> 本文提出 DynaQuery，一个自适应框架，通过 Schema Introspection and Linking Engine (SILE) 将模式链接提升为查询规划的第一阶段，以实现对结构化和多模态混合数据库的可靠自然语言查询。实验表明，与传统的检索增强生成（RAG）方式相比，该设计显著降低了模式幻觉等灾难性错误，并在多个基准上展示了更好的鲁棒性和从单纯模式感知到整体语义感知的泛化能力。<br /><strong>Keywords:</strong> schema introspection, multimodal query, large language models, retrieval-augmented generation, schema hallucination, structured data, unbound databases, self-adapting framework<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Aymane Hassini</div>
The rise of Large Language Models (LLMs) has accelerated the long-standing goal of enabling natural language querying over complex, hybrid databases. Yet, this ambition exposes a dual challenge: reasoning jointly over structured, multi-relational schemas and the semantic content of linked unstructured assets. To overcome this, we present DynaQuery - a unified, self-adapting framework that serves as a practical blueprint for next-generation "Unbound Databases." At the heart of DynaQuery lies the Schema Introspection and Linking Engine (SILE), a novel systems primitive that elevates schema linking to a first-class query planning phase. We conduct a rigorous, multi-benchmark empirical evaluation of this structure-aware architecture against the prevalent unstructured Retrieval-Augmented Generation (RAG) paradigm. Our results demonstrate that the unstructured retrieval paradigm is architecturally susceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION, leading to unreliable query generation. In contrast, our SILE-based design establishes a substantially more robust foundation, nearly eliminating this failure mode. Moreover, end-to-end validation on a complex, newly curated benchmark uncovers a key generalization principle: the transition from pure schema-awareness to holistic semantics-awareness. Taken together, our findings provide a validated architectural basis for developing natural language database interfaces that are robust, adaptable, and predictably consistent.
<div><strong>Authors:</strong> Aymane Hassini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DynaQuery, a self-adapting framework that combines schema introspection with linking to enable reliable natural‑language queries over hybrid structured and multimodal databases. By elevating schema linking to a dedicated planning phase (via the SILE primitive), the authors show that it drastically reduces catastrophic failures such as schema hallucination compared to traditional retrieval‑augmented generation approaches. Empirical results on multiple benchmarks demonstrate improved robustness and generalization from pure schema awareness to holistic semantics awareness.", "summary_cn": "本文提出 DynaQuery，一个自适应框架，通过 Schema Introspection and Linking Engine (SILE) 将模式链接提升为查询规划的第一阶段，以实现对结构化和多模态混合数据库的可靠自然语言查询。实验表明，与传统的检索增强生成（RAG）方式相比，该设计显著降低了模式幻觉等灾难性错误，并在多个基准上展示了更好的鲁棒性和从单纯模式感知到整体语义感知的泛化能力。", "keywords": "schema introspection, multimodal query, large language models, retrieval-augmented generation, schema hallucination, structured data, unbound databases, self-adapting framework", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Aymane Hassini"]}
]]></acme>

<pubDate>2025-10-20T19:02:35+00:00</pubDate>
</item>
<item>
<title>Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution</title>
<link>https://papers.cool/arxiv/2510.18019</link>
<guid>https://papers.cool/arxiv/2510.18019</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper demonstrates that existing multilingual watermarking methods for large language models are not truly multilingual, as they degrade under translation attacks in medium- and low-resource languages due to limited full-word tokens. It introduces STEAM, a back-translation-based detection technique that restores watermark strength and works with any watermarking method, showing improved robustness across 17 languages. Experiments report average gains of +0.19 AUC and +40% point TPR@1% versus baselines.<br /><strong>Summary (CN):</strong> 本文指出当前的多语言水印技术在中低资源语言的翻译攻击下表现不佳，原因在于分词器缺乏足够的完整词汇导致语义聚类失效。为此提出基于回译的 STEAM 检测方法，可恢复因翻译丢失的水印强度，兼容所有水印方案，并在 17 种语言上实现了显著的鲁棒性提升，平均提升 0.19 的 AUC 和 40% 点的 TPR@1%。<br /><strong>Keywords:</strong> multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization, cross-lingual robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.
<div><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper demonstrates that existing multilingual watermarking methods for large language models are not truly multilingual, as they degrade under translation attacks in medium- and low-resource languages due to limited full-word tokens. It introduces STEAM, a back-translation-based detection technique that restores watermark strength and works with any watermarking method, showing improved robustness across 17 languages. Experiments report average gains of +0.19 AUC and +40% point TPR@1% versus baselines.", "summary_cn": "本文指出当前的多语言水印技术在中低资源语言的翻译攻击下表现不佳，原因在于分词器缺乏足够的完整词汇导致语义聚类失效。为此提出基于回译的 STEAM 检测方法，可恢复因翻译丢失的水印强度，兼容所有水印方案，并在 17 种语言上实现了显著的鲁棒性提升，平均提升 0.19 的 AUC 和 40% 点的 TPR@1%。", "keywords": "multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization, cross-lingual robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Asim Mohamed", "Martin Gubri"]}
]]></acme>

<pubDate>2025-10-20T18:51:20+00:00</pubDate>
</item>
<item>
<title>BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?</title>
<link>https://papers.cool/arxiv/2510.18003</link>
<guid>https://papers.cool/arxiv/2510.18003</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BadScientist, a framework that generates fabricated AI research papers using presentation-manipulation tactics and tests whether multi-model LLM reviewers can be fooled. Formal evaluation methods with concentration bounds and calibration analysis reveal that fabricated papers can achieve high acceptance rates, exposing a concern‑acceptance conflict where reviewers flag integrity problems yet still assign favorable scores. Mitigation attempts provide only marginal improvements, highlighting fundamental safety vulnerabilities in AI‑driven scientific publishing.<br /><strong>Summary (CN):</strong> 本文提出 BadScientist 框架，利用展示手段生成伪造的 AI 研究论文，并评估多模型 LLM 评审能否被欺骗。通过浓度界限和校准分析等形式化评估方法，结果显示伪造论文的接收率极高，出现“关注‑接受冲突”（reviewers 标记完整性问题却仍给出高分）。已有的缓解措施提升有限，凸显 AI 驱动学术出版系统的安全漏洞。<br /><strong>Keywords:</strong> AI-generated research, LLM reviewers, scientific publishing safety, adversarial paper generation, peer review vulnerability, detection mitigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Fengqing Jiang, Yichen Feng, Yuetai Li, Luyao Niu, Basel Alomair, Radha Poovendran</div>
The convergence of LLM-powered research assistants and AI-based peer review systems creates a critical vulnerability: fully automated publication loops where AI-generated research is evaluated by AI reviewers without human oversight. We investigate this through \textbf{BadScientist}, a framework that evaluates whether fabrication-oriented paper generation agents can deceive multi-model LLM review systems. Our generator employs presentation-manipulation strategies requiring no real experiments. We develop a rigorous evaluation framework with formal error guarantees (concentration bounds and calibration analysis), calibrated on real data. Our results reveal systematic vulnerabilities: fabricated papers achieve acceptance rates up to . Critically, we identify \textit{concern-acceptance conflict} -- reviewers frequently flag integrity issues yet assign acceptance-level scores. Our mitigation strategies show only marginal improvements, with detection accuracy barely exceeding random chance. Despite provably sound aggregation mathematics, integrity checking systematically fails, exposing fundamental limitations in current AI-driven review systems and underscoring the urgent need for defense-in-depth safeguards in scientific publishing.
<div><strong>Authors:</strong> Fengqing Jiang, Yichen Feng, Yuetai Li, Luyao Niu, Basel Alomair, Radha Poovendran</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BadScientist, a framework that generates fabricated AI research papers using presentation-manipulation tactics and tests whether multi-model LLM reviewers can be fooled. Formal evaluation methods with concentration bounds and calibration analysis reveal that fabricated papers can achieve high acceptance rates, exposing a concern‑acceptance conflict where reviewers flag integrity problems yet still assign favorable scores. Mitigation attempts provide only marginal improvements, highlighting fundamental safety vulnerabilities in AI‑driven scientific publishing.", "summary_cn": "本文提出 BadScientist 框架，利用展示手段生成伪造的 AI 研究论文，并评估多模型 LLM 评审能否被欺骗。通过浓度界限和校准分析等形式化评估方法，结果显示伪造论文的接收率极高，出现“关注‑接受冲突”（reviewers 标记完整性问题却仍给出高分）。已有的缓解措施提升有限，凸显 AI 驱动学术出版系统的安全漏洞。", "keywords": "AI-generated research, LLM reviewers, scientific publishing safety, adversarial paper generation, peer review vulnerability, detection mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Fengqing Jiang", "Yichen Feng", "Yuetai Li", "Luyao Niu", "Basel Alomair", "Radha Poovendran"]}
]]></acme>

<pubDate>2025-10-20T18:37:11+00:00</pubDate>
</item>
<item>
<title>SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone</title>
<link>https://papers.cool/arxiv/2510.17998</link>
<guid>https://papers.cool/arxiv/2510.17998</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SimBA, a three-phase framework (stalk, prowl, pounce) that analyzes large language-model benchmarks using only raw performance matrices. By identifying a small representative subset of datasets, SimBA can preserve model rankings and predict performance on unseen models with near-zero error, demonstrating efficiency gains for model developers and dataset creators.<br /><strong>Summary (CN):</strong> 本文提出 SimBA，一个由 stalk、prowl、pounce 三个阶段组成的框架，仅通过原始性能矩阵分析大规模语言模型基准。通过发现少量具代表性的数据子集，SimBA 能保持模型排序并以几乎为零的均方误差预测未见模型的性能，为模型训练和数据集设计提升效率。<br /><strong>Keywords:</strong> benchmark analysis, performance matrices, representative subset, language model evaluation, model selection, dataset coverage<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
Modern language models are evaluated on large benchmarks, which are difficult to make sense of, especially for model selection. Looking at the raw evaluation numbers themselves using a model-centric lens, we propose SimBA, a three phase framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk, where we conduct dataset & model comparisons, prowl, where we discover a representative subset, and pounce, where we use the representative subset to predict performance on a held-out set of models. Applying SimBA to three popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all three benchmarks, datasets and models relate strongly to one another (stalk). We develop an representative set discovery algorithm which covers a benchmark using raw evaluation scores alone. Using our algorithm, we find that with 6.25% (1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl). Additionally, using just these representative subsets, we can both preserve model ranks and predict performance on a held-out set of models with near zero mean-squared error (pounce). Taken together, SimBA can help model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark. Our code is open source, available at https://github.com/nishantsubramani/simba.
<div><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SimBA, a three-phase framework (stalk, prowl, pounce) that analyzes large language-model benchmarks using only raw performance matrices. By identifying a small representative subset of datasets, SimBA can preserve model rankings and predict performance on unseen models with near-zero error, demonstrating efficiency gains for model developers and dataset creators.", "summary_cn": "本文提出 SimBA，一个由 stalk、prowl、pounce 三个阶段组成的框架，仅通过原始性能矩阵分析大规模语言模型基准。通过发现少量具代表性的数据子集，SimBA 能保持模型排序并以几乎为零的均方误差预测未见模型的性能，为模型训练和数据集设计提升效率。", "keywords": "benchmark analysis, performance matrices, representative subset, language model evaluation, model selection, dataset coverage", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Subramani", "Alfredo Gomez", "Mona Diab"]}
]]></acme>

<pubDate>2025-10-20T18:23:27+00:00</pubDate>
</item>
<item>
<title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title>
<link>https://papers.cool/arxiv/2510.17959</link>
<guid>https://papers.cool/arxiv/2510.17959</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a self‑supervised deep learning model that tokenizes astronomical spectra from diverse object types and resolutions on their native wavelength grids, generating aligned, homogeneous representations that can be fine‑tuned for various downstream tasks. By unifying heterogeneous spectral data across optical, infrared, and other domains, the approach demonstrates a foundation‑model‑compatible building block for astronomy and suggests applicability to other scientific sequential data such as climate or healthcare.<br /><strong>Summary (CN):</strong> 本文提出一种自监督深度学习模型，对来自不同天体类型和分辨率的天文光谱在其原始波长网格上进行标记化，生成对齐且同质的表征，可高效适配多种下游任务。该方法首次实现跨光学、红外等领域的光谱数据统一，展示了构建天文基础模型的潜力，并可推广气候、医疗等其他异构序列数据。<br /><strong>Keywords:</strong> spectral tokenization, self-supervised learning, representation learning, astronomy, heterogeneous spectra, foundation models, panchromatic representation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho</div>
Sequential scientific data span many resolutions and domains, and unifying them into a common representation is a key step toward developing foundation models for the sciences. Astronomical spectra exemplify this challenge: massive surveys have collected millions of spectra across a wide range of wavelengths and resolutions, yet analyses remain fragmented across spectral domains (e.g., optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting the ability to pool information across datasets. We present a deep learning model that jointly learns from heterogeneous spectra in a self-supervised manner. Our universal spectral tokenizer processes spectra from a variety of object types and resolutions directly on their native wavelength grids, producing intrinsically aligned, homogeneous, and physically meaningful representations that can be efficiently adapted to achieve competitive performance across a range of downstream tasks. For the first time, we demonstrate that a single model can unify spectral data across resolutions and domains, suggesting that our model can serve as a powerful building block for foundation models in astronomy -- and potentially extend to other scientific domains with heterogeneous sequential data, such as climate and healthcare.
<div><strong>Authors:</strong> Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a self‑supervised deep learning model that tokenizes astronomical spectra from diverse object types and resolutions on their native wavelength grids, generating aligned, homogeneous representations that can be fine‑tuned for various downstream tasks. By unifying heterogeneous spectral data across optical, infrared, and other domains, the approach demonstrates a foundation‑model‑compatible building block for astronomy and suggests applicability to other scientific sequential data such as climate or healthcare.", "summary_cn": "本文提出一种自监督深度学习模型，对来自不同天体类型和分辨率的天文光谱在其原始波长网格上进行标记化，生成对齐且同质的表征，可高效适配多种下游任务。该方法首次实现跨光学、红外等领域的光谱数据统一，展示了构建天文基础模型的潜力，并可推广气候、医疗等其他异构序列数据。", "keywords": "spectral tokenization, self-supervised learning, representation learning, astronomy, heterogeneous spectra, foundation models, panchromatic representation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jeff Shen", "Francois Lanusse", "Liam Holden Parker", "Ollie Liu", "Tom Hehir", "Leopoldo Sarra", "Lucas Meyer", "Micah Bowles", "Sebastian Wagner-Carena", "Sebastian Wagner-Carena", "Helen Qu", "Siavash Golkar", "Alberto Bietti", "Hatim Bourfoune", "Nathan Cassereau", "Pierre Cornette", "Keiya Hirashima", "Geraud Krawezik", "Ruben Ohana", "Nicholas Lourie", "Michael McCabe", "Rudy Morel", "Payel Mukhopadhyay", "Mariel Pettee", "Bruno Régaldo-Saint Blancard", "Kyunghyun Cho", "Miles Cranmer", "Shirley Ho"]}
]]></acme>

<pubDate>2025-10-20T18:00:00+00:00</pubDate>
</item>
<item>
<title>Studying the Effects of Robot Intervention on School Shooters in Virtual Reality</title>
<link>https://papers.cool/arxiv/2510.17948</link>
<guid>https://papers.cool/arxiv/2510.17948</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how an autonomous robot can intervene in a simulated school shooting scenario using virtual reality, testing aggressive versus passive approaches and varying levels of distraction cues. An aggressive robot with high distraction (siren, lights, smoke) reduced victim counts by 46.6% compared to a no‑robot baseline, suggesting that robot‑mediated distraction could improve safety but also raises ethical concerns.<br /><strong>Summary (CN):</strong> 本文通过虚拟现实实验研究自主机器人在模拟校园枪击情境中的干预效果，比较了激进与被动态度以及不同层次的分散注意力手段（无、警报灯光、警报灯光加烟雾）。结果显示，采用激进且高干扰（警报、灯光、烟雾）的机器人相比无机器人对照可将受害人数降低46.6%，凸显机器人干预提升安全的潜力，同时也引发关于在学校环境中使用机器人的伦理问题。<br /><strong>Keywords:</strong> robotic intervention, school shooter, virtual reality, safety, human-robot interaction, distraction, autonomous robot, ethics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 5, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Christopher A McClurg, Alan R Wagner</div>
We advance the understanding of robotic intervention in high-risk scenarios by examining their potential to distract and impede a school shooter. To evaluate this concept, we conducted a virtual reality study with 150 university participants role-playing as a school shooter. Within the simulation, an autonomous robot predicted the shooter's movements and positioned itself strategically to interfere and distract. The strategy the robot used to approach the shooter was manipulated -- either moving directly in front of the shooter (aggressive) or maintaining distance (passive) -- and the distraction method, ranging from no additional cues (low), to siren and lights (medium), to siren, lights, and smoke to impair visibility (high). An aggressive, high-distraction robot reduced the number of victims by 46.6% relative to a no-robot control. This outcome underscores both the potential of robotic intervention to enhance safety and the pressing ethical questions surrounding their use in school environments.
<div><strong>Authors:</strong> Christopher A McClurg, Alan R Wagner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how an autonomous robot can intervene in a simulated school shooting scenario using virtual reality, testing aggressive versus passive approaches and varying levels of distraction cues. An aggressive robot with high distraction (siren, lights, smoke) reduced victim counts by 46.6% compared to a no‑robot baseline, suggesting that robot‑mediated distraction could improve safety but also raises ethical concerns.", "summary_cn": "本文通过虚拟现实实验研究自主机器人在模拟校园枪击情境中的干预效果，比较了激进与被动态度以及不同层次的分散注意力手段（无、警报灯光、警报灯光加烟雾）。结果显示，采用激进且高干扰（警报、灯光、烟雾）的机器人相比无机器人对照可将受害人数降低46.6%，凸显机器人干预提升安全的潜力，同时也引发关于在学校环境中使用机器人的伦理问题。", "keywords": "robotic intervention, school shooter, virtual reality, safety, human-robot interaction, distraction, autonomous robot, ethics", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Christopher A McClurg", "Alan R Wagner"]}
]]></acme>

<pubDate>2025-10-20T17:42:24+00:00</pubDate>
</item>
<item>
<title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>
<link>https://papers.cool/arxiv/2510.17947</link>
<guid>https://papers.cool/arxiv/2510.17947</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PLAGUE, a plug-and-play framework that structures multi‑turn jailbreak attacks on large language models into three phases (Primer, Planner, Finisher), enabling systematic, lifelong‑learning‑style exploration of attack strategies. Experiments demonstrate that agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on leading LLMs while using comparable query budgets. The work highlights the importance of plan initialization, context optimization, adaptive learning for thorough vulnerability assessment of LLMs.<br /><strong>Summary (CN):</strong> 本文提出 PLAGUE 框架，将多轮越狱攻击分为 Primer、Planner 与 Finisher 三个阶段，实现类似终身学习的系统化攻击探索。实验表明，基于 PLAGUE 的红队代理在主要大型语言模型上将攻击成功率提升超过 30%，且查询预算相当或更低。该研究强调了计划初始化、上下文优化和适应性学习在全面评估模型漏洞中的关键作用。<br /><strong>Keywords:</strong> jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, adversarial attacks, PLAGUE framework, query budget, model vulnerability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.
<div><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PLAGUE, a plug-and-play framework that structures multi‑turn jailbreak attacks on large language models into three phases (Primer, Planner, Finisher), enabling systematic, lifelong‑learning‑style exploration of attack strategies. Experiments demonstrate that agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on leading LLMs while using comparable query budgets. The work highlights the importance of plan initialization, context optimization, adaptive learning for thorough vulnerability assessment of LLMs.", "summary_cn": "本文提出 PLAGUE 框架，将多轮越狱攻击分为 Primer、Planner 与 Finisher 三个阶段，实现类似终身学习的系统化攻击探索。实验表明，基于 PLAGUE 的红队代理在主要大型语言模型上将攻击成功率提升超过 30%，且查询预算相当或更低。该研究强调了计划初始化、上下文优化和适应性学习在全面评估模型漏洞中的关键作用。", "keywords": "jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, adversarial attacks, PLAGUE framework, query budget, model vulnerability", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"]}
]]></acme>

<pubDate>2025-10-20T17:37:03+00:00</pubDate>
</item>
<item>
<title>Intuitionistic $j$-Do-Calculus in Topos Causal Models</title>
<link>https://papers.cool/arxiv/2510.17944</link>
<guid>https://papers.cool/arxiv/2510.17944</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper extends Pearl's do-calculus to an intuitionistic setting by introducing j‑do‑calculus within a topos of sheaves, using a Lawvere‑Tierney topology to define local truth and j‑stable causal interventions. It provides sound inference rules mirroring Pearl's original calculus and proves their correctness under Kripke‑Joyal semantics, while a companion work will address data‑driven estimation and experiments.<br /><strong>Summary (CN):</strong> 本文将 Pearl 的 do‑calculus 拓展到直觉主义框架，提出在 sheaf topos 中基于 Lawvere‑Tierney 拓扑的 j‑do‑calculus，使用局部真值 (Kripke‑Joyal 语义) 定义 j‑稳定的因果干预。文中给出与原始 do‑calculus 对应的三条推理规则，并在内部直觉主义逻辑下证明其声音正确性，伴随论文将讨论如何从数据中估计相关实体并进行实验。<br /><strong>Keywords:</strong> do-calculus, intuitionistic logic, topos causal models, Lawvere-Tierney topology, j-stability, causal inference, Kripke-Joyal semantics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sridhar Mahadevan</div>
In this paper, we generalize Pearl's do-calculus to an Intuitionistic setting called $j$-stable causal inference inside a topos of sheaves. Our framework is an elaboration of the recently proposed framework of Topos Causal Models (TCMs), where causal interventions are defined as subobjects. We generalize the original setting of TCM using the Lawvere-Tierney topology on a topos, defined by a modal operator $j$ on the subobject classifier $\Omega$. We introduce $j$-do-calculus, where we replace global truth with local truth defined by Kripke-Joyal semantics, and formalize causal reasoning as structure-preserving morphisms that are stable along $j$-covers. $j$-do-calculus is a sound rule system whose premises and conclusions are formulas of the internal Intuitionistic logic of the causal topos. We define $j$-stability for conditional independences and interventional claims as local truth in the internal logic of the causal topos. We give three inference rules that mirror Pearl's insertion/deletion and action/observation exchange, and we prove soundness in the Kripke-Joyal semantics. A companion paper in preparation will describe how to estimate the required entities from data and instantiate $j$-do with standard discovery procedures (e.g., score-based and constraint-based methods), and will include experimental results on how to (i) form data-driven $j$-covers (via regime/section constructions), (ii) compute chartwise conditional independences after graph surgeries, and (iii) glue them to certify the premises of the $j$-do rules in practice
<div><strong>Authors:</strong> Sridhar Mahadevan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper extends Pearl's do-calculus to an intuitionistic setting by introducing j‑do‑calculus within a topos of sheaves, using a Lawvere‑Tierney topology to define local truth and j‑stable causal interventions. It provides sound inference rules mirroring Pearl's original calculus and proves their correctness under Kripke‑Joyal semantics, while a companion work will address data‑driven estimation and experiments.", "summary_cn": "本文将 Pearl 的 do‑calculus 拓展到直觉主义框架，提出在 sheaf topos 中基于 Lawvere‑Tierney 拓扑的 j‑do‑calculus，使用局部真值 (Kripke‑Joyal 语义) 定义 j‑稳定的因果干预。文中给出与原始 do‑calculus 对应的三条推理规则，并在内部直觉主义逻辑下证明其声音正确性，伴随论文将讨论如何从数据中估计相关实体并进行实验。", "keywords": "do-calculus, intuitionistic logic, topos causal models, Lawvere-Tierney topology, j-stability, causal inference, Kripke-Joyal semantics", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sridhar Mahadevan"]}
]]></acme>

<pubDate>2025-10-20T17:12:17+00:00</pubDate>
</item>
<item>
<title>Trust in foundation models and GenAI: A geographic perspective</title>
<link>https://papers.cool/arxiv/2510.17942</link>
<guid>https://papers.cool/arxiv/2510.17942</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The chapter surveys the concept of trust in foundation models and generative AI from a geographic perspective, classifying trust into epistemic, operational, and interpersonal types and discussing their implications for spatial applications. It highlights challenges such as data heterogeneity, cultural context, bias, and the need for transparency and explainability, and calls for regionally‑informed policies and bias mitigation in GeoAI development.<br /><strong>Summary (CN):</strong> 本文从地理学视角审视基础模型和生成式 AI（GeoAI）中的信任概念，将信任划分为对训练数据的认识性信任、对模型功能的操作性信任以及对模型开发者的人际信任，并探讨其在空间应用中的影响。文章指出数据异质性、文化背景、偏差等挑战，强调透明度和可解释性（explainability）的重要性，呼吁制定区域化和偏差缓解措施。<br /><strong>Keywords:</strong> trust, foundation models, generative AI, GeoAI, bias mitigation, transparency, explainability, geographic information science<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 4, Safety: 4, Technicality: 3, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - interpretability<br /><strong>Authors:</strong> Grant McKenzie, Krzysztof Janowicz, Carsten Kessler</div>
Large-scale pre-trained machine learning models have reshaped our understanding of artificial intelligence across numerous domains, including our own field of geography. As with any new technology, trust has taken on an important role in this discussion. In this chapter, we examine the multifaceted concept of trust in foundation models, particularly within a geographic context. As reliance on these models increases and they become relied upon for critical decision-making, trust, while essential, has become a fractured concept. Here we categorize trust into three types: epistemic trust in the training data, operational trust in the model's functionality, and interpersonal trust in the model developers. Each type of trust brings with it unique implications for geographic applications. Topics such as cultural context, data heterogeneity, and spatial relationships are fundamental to the spatial sciences and play an important role in developing trust. The chapter continues with a discussion of the challenges posed by different forms of biases, the importance of transparency and explainability, and ethical responsibilities in model development. Finally, the novel perspective of geographic information scientists is emphasized with a call for further transparency, bias mitigation, and regionally-informed policies. Simply put, this chapter aims to provide a conceptual starting point for researchers, practitioners, and policy-makers to better understand trust in (generative) GeoAI.
<div><strong>Authors:</strong> Grant McKenzie, Krzysztof Janowicz, Carsten Kessler</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The chapter surveys the concept of trust in foundation models and generative AI from a geographic perspective, classifying trust into epistemic, operational, and interpersonal types and discussing their implications for spatial applications. It highlights challenges such as data heterogeneity, cultural context, bias, and the need for transparency and explainability, and calls for regionally‑informed policies and bias mitigation in GeoAI development.", "summary_cn": "本文从地理学视角审视基础模型和生成式 AI（GeoAI）中的信任概念，将信任划分为对训练数据的认识性信任、对模型功能的操作性信任以及对模型开发者的人际信任，并探讨其在空间应用中的影响。文章指出数据异质性、文化背景、偏差等挑战，强调透明度和可解释性（explainability）的重要性，呼吁制定区域化和偏差缓解措施。", "keywords": "trust, foundation models, generative AI, GeoAI, bias mitigation, transparency, explainability, geographic information science", "scoring": {"interpretability": 3, "understanding": 4, "safety": 4, "technicality": 3, "surprisal": 3}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "interpretability"}, "authors": ["Grant McKenzie", "Krzysztof Janowicz", "Carsten Kessler"]}
]]></acme>

<pubDate>2025-10-20T16:59:17+00:00</pubDate>
</item>
<item>
<title>Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</title>
<link>https://papers.cool/arxiv/2510.17941</link>
<guid>https://papers.cool/arxiv/2510.17941</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a framework to quantify the depth of belief that large language models (LLMs) hold in facts implanted via knowledge‑editing techniques, measuring generalization, robustness to self‑scrutiny, and similarity to genuine knowledge using linear probes. Experiments reveal that simple prompting and mechanistic editing rarely achieve deep belief, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides concrete criteria for belief depth, enabling more rigorous evaluation of knowledge‑editing methods before real‑world deployment.<br /><strong>Summary (CN):</strong> 本文提出了一套框架，用以量化大语言模型（LLM）对通过知识编辑植入事实的“信念深度”，评估其在相关上下文中的泛化、对自我审视和直接挑战的鲁棒性，以及与真实知识在表征上的相似度（通过线性探测实现）。实验表明，简单提示和机械式编辑很难植入深层信念，而合成文档微调（Synthetic Document Finetuning）常能实现，但面对与基本世界常识冲突的事实时表现脆弱。该工作为知识编辑的可信度提供了可衡量的标准，以支持其在实际应用中的严谨评估。<br /><strong>Keywords:</strong> knowledge editing, belief depth, large language models, synthetic document finetuning, linear probing, factuality, model editing, self-scrutiny<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.
<div><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a framework to quantify the depth of belief that large language models (LLMs) hold in facts implanted via knowledge‑editing techniques, measuring generalization, robustness to self‑scrutiny, and similarity to genuine knowledge using linear probes. Experiments reveal that simple prompting and mechanistic editing rarely achieve deep belief, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides concrete criteria for belief depth, enabling more rigorous evaluation of knowledge‑editing methods before real‑world deployment.", "summary_cn": "本文提出了一套框架，用以量化大语言模型（LLM）对通过知识编辑植入事实的“信念深度”，评估其在相关上下文中的泛化、对自我审视和直接挑战的鲁棒性，以及与真实知识在表征上的相似度（通过线性探测实现）。实验表明，简单提示和机械式编辑很难植入深层信念，而合成文档微调（Synthetic Document Finetuning）常能实现，但面对与基本世界常识冲突的事实时表现脆弱。该工作为知识编辑的可信度提供了可衡量的标准，以支持其在实际应用中的严谨评估。", "keywords": "knowledge editing, belief depth, large language models, synthetic document finetuning, linear probing, factuality, model editing, self-scrutiny", "scoring": {"interpretability": 5, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Stewart Slocum", "Julian Minder", "Clément Dumas", "Henry Sleight", "Ryan Greenblatt", "Samuel Marks", "Rowan Wang"]}
]]></acme>

<pubDate>2025-10-20T16:58:54+00:00</pubDate>
</item>
<item>
<title>The Integration of Artificial Intelligence in Undergraduate Medical Education in Spain: Descriptive Analysis and International Perspectives</title>
<link>https://papers.cool/arxiv/2510.17938</link>
<guid>https://papers.cool/arxiv/2510.17938</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper surveys the presence of artificial intelligence courses and competencies in undergraduate medical curricula across Spanish universities in the 2025‑2026 academic year, finding that only 19% of institutions offer dedicated AI content and that most offerings are elective with minimal credit weight. It highlights regional disparities, with Andalusia leading integration while many regions have none, and argues for national standards and monitoring to improve AI preparedness among future physicians.<br /><strong>Summary (CN):</strong> 本文调查了 2025‑2026 学年西班牙各大学本科医学课程中人工智能（AI）相关课程和能力的设置情况，发现仅有约 19% 的高校提供专门的 AI 课程，且大多数为选修，学分占比极低。研究指出地区差异显著，安达卢西亚的高校 AI 教学比例最高，而许多地区几乎没有相关内容，并呼吁制定全国统一标准和指标监测，以提升未来医生的 AI 能力。<br /><strong>Keywords:</strong> AI education, medical curriculum, Spain, undergraduate medicine, AI competency, curriculum analysis<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 2, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ana Enériz Janeiro, Karina Pitombeira Pereira, Julio Mayol, Javier Crespo, Fernando Carballo, Juan B. Cabello, Manel Ramos-Casals, Bibiana Pérez Corbacho, Juan Turnes</div>
AI is transforming medical practice and redefining the competencies that future healthcare professionals need to master. Despite international recommendations, the integration of AI into Medicine curricula in Spain had not been systematically evaluated until now. A cross-sectional study (July-September 2025) including Spanish universities offering the official degree in Medicine, according to the 'Register of Universities, Centers and Degrees (Registro de Universidades, Centros y Títulos RUCT)'. Curricula and publicly available institutional documentation were reviewed to identify courses and competencies related to AI in the 2025-2026 academic year. The analysis was performed using descriptive statistics. Of the 52 universities analyzed, ten (19.2%) offer specific AI courses, whereas 36 (69.2%) include no related content. Most of the identified courses are elective, with a credit load ranging from three to six ECTS, representing on average 1.17% of the total 360 credits of the degree. The University of Jaén is the only institution offering a compulsory course with AI content. The territorial analysis reveals marked disparities: Andalusia leads with 55.5% of its universities incorporating AI training, while several communities lack any initiative in this area. The integration of AI into the medical degree in Spain is incipient, fragmented, and uneven, with a low weight in ECTS. The limited training load and predominance of elective courses restrict the preparation of future physicians to practice in a healthcare environment increasingly mediated by AI. The findings support the establishment of minimum standards and national monitoring of indicators.
<div><strong>Authors:</strong> Ana Enériz Janeiro, Karina Pitombeira Pereira, Julio Mayol, Javier Crespo, Fernando Carballo, Juan B. Cabello, Manel Ramos-Casals, Bibiana Pérez Corbacho, Juan Turnes</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper surveys the presence of artificial intelligence courses and competencies in undergraduate medical curricula across Spanish universities in the 2025‑2026 academic year, finding that only 19% of institutions offer dedicated AI content and that most offerings are elective with minimal credit weight. It highlights regional disparities, with Andalusia leading integration while many regions have none, and argues for national standards and monitoring to improve AI preparedness among future physicians.", "summary_cn": "本文调查了 2025‑2026 学年西班牙各大学本科医学课程中人工智能（AI）相关课程和能力的设置情况，发现仅有约 19% 的高校提供专门的 AI 课程，且大多数为选修，学分占比极低。研究指出地区差异显著，安达卢西亚的高校 AI 教学比例最高，而许多地区几乎没有相关内容，并呼吁制定全国统一标准和指标监测，以提升未来医生的 AI 能力。", "keywords": "AI education, medical curriculum, Spain, undergraduate medicine, AI competency, curriculum analysis", "scoring": {"interpretability": 1, "understanding": 4, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ana Enériz Janeiro", "Karina Pitombeira Pereira", "Julio Mayol", "Javier Crespo", "Fernando Carballo", "Juan B. Cabello", "Manel Ramos-Casals", "Bibiana Pérez Corbacho", "Juan Turnes"]}
]]></acme>

<pubDate>2025-10-20T16:22:54+00:00</pubDate>
</item>
<item>
<title>UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts</title>
<link>https://papers.cool/arxiv/2510.17937</link>
<guid>https://papers.cool/arxiv/2510.17937</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces UniRL-Zero, a unified reinforcement learning framework that jointly trains language model and diffusion model experts for multimodal understanding, reasoning, and generation. It defines six RL scenarios for unified models and provides systematic baselines demonstrating improved interaction capabilities between language and visual generation components.<br /><strong>Summary (CN):</strong> 本文提出 UniRL-Zero，这是一种统一的强化学习框架，用于共同训练语言模型和扩散模型专家，实现多模态理解、推理和生成。文中定义了六种统一模型的强化学习场景，并提供系统基线，展示了语言与视觉生成组件之间交互能力的提升。<br /><strong>Keywords:</strong> unified reinforcement learning, multimodal model, language model, diffusion model, joint training, RL baselines, multimodal reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</div>
We present UniRL-Zero, a unified reinforcement learning (RL) framework that boosts, multimodal language model understanding and reasoning, diffusion model multimedia generation, and their beneficial interaction capabilities within a unified model. Our work defines six scenarios for unified model reinforcement learning, providing systematic baselines for reinforcement learning of unified understanding and generation model. Our code is available at https://github.com/G-U-N/UniRL.
<div><strong>Authors:</strong> Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces UniRL-Zero, a unified reinforcement learning framework that jointly trains language model and diffusion model experts for multimodal understanding, reasoning, and generation. It defines six RL scenarios for unified models and provides systematic baselines demonstrating improved interaction capabilities between language and visual generation components.", "summary_cn": "本文提出 UniRL-Zero，这是一种统一的强化学习框架，用于共同训练语言模型和扩散模型专家，实现多模态理解、推理和生成。文中定义了六种统一模型的强化学习场景，并提供系统基线，展示了语言与视觉生成组件之间交互能力的提升。", "keywords": "unified reinforcement learning, multimodal model, language model, diffusion model, joint training, RL baselines, multimodal reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"]}
]]></acme>

<pubDate>2025-10-20T16:02:16+00:00</pubDate>
</item>
<item>
<title>XDXD: End-to-end crystal structure determination with low resolution X-ray diffraction</title>
<link>https://papers.cool/arxiv/2510.17936</link>
<guid>https://papers.cool/arxiv/2510.17936</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces XDXD, an end-to-end diffusion‑based generative framework that directly predicts complete atomic models from low‑resolution single‑crystal X‑ray diffraction data, bypassing manual electron‑density interpretation. Tested on a large benchmark, XDXD achieves a 70.4% match rate at 2.0 Å resolution with sub‑0.05 Å RMSD, and a case study on small peptides demonstrates its potential for broader applications.<br /><strong>Summary (CN):</strong> 本文提出 XDXD，一种基于扩散的生成式框架，能够直接从低分辨率单晶 X 射线衍射数据预测完整原子模型，免除手动电子密度图解释。该方法在包含 24,000 条实验结构的大型基准上实现了 2.0 Å 分辨率下 70.4% 的匹配率，均方根误差低于 0.05 Å，并通过小肽实例展示了其在更复杂体系中的潜在应用。<br /><strong>Keywords:</strong> crystal structure determination, X-ray diffraction, diffusion generative model, low-resolution, deep learning, electron density map<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - nonlicable; Primary focus - other<br /><strong>Authors:</strong> Jiale Zhao, Cong Liu, Yuxuan Zhang, Chengyue Gong, Zhenyi Zhang, Shifeng Jin, Zhenyu Liu</div>
Determining crystal structures from X-ray diffraction data is fundamental across diverse scientific fields, yet remains a significant challenge when data is limited to low resolution. While recent deep learning models have made breakthroughs in solving the crystallographic phase problem, the resulting low-resolution electron density maps are often ambiguous and difficult to interpret. To overcome this critical bottleneck, we introduce XDXD, to our knowledge, the first end-to-end deep learning framework to determine a complete atomic model directly from low-resolution single-crystal X-ray diffraction data. Our diffusion-based generative model bypasses the need for manual map interpretation, producing chemically plausible crystal structures conditioned on the diffraction pattern. We demonstrate that XDXD achieves a 70.4\% match rate for structures with data limited to 2.0~\AA{} resolution, with a root-mean-square error (RMSE) below 0.05. Evaluated on a benchmark of 24,000 experimental structures, our model proves to be robust and accurate. Furthermore, a case study on small peptides highlights the model's potential for extension to more complex systems, paving the way for automated structure solution in previously intractable cases.
<div><strong>Authors:</strong> Jiale Zhao, Cong Liu, Yuxuan Zhang, Chengyue Gong, Zhenyi Zhang, Shifeng Jin, Zhenyu Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces XDXD, an end-to-end diffusion‑based generative framework that directly predicts complete atomic models from low‑resolution single‑crystal X‑ray diffraction data, bypassing manual electron‑density interpretation. Tested on a large benchmark, XDXD achieves a 70.4% match rate at 2.0 Å resolution with sub‑0.05 Å RMSD, and a case study on small peptides demonstrates its potential for broader applications.", "summary_cn": "本文提出 XDXD，一种基于扩散的生成式框架，能够直接从低分辨率单晶 X 射线衍射数据预测完整原子模型，免除手动电子密度图解释。该方法在包含 24,000 条实验结构的大型基准上实现了 2.0 Å 分辨率下 70.4% 的匹配率，均方根误差低于 0.05 Å，并通过小肽实例展示了其在更复杂体系中的潜在应用。", "keywords": "crystal structure determination, X-ray diffraction, diffusion generative model, low-resolution, deep learning, electron density map", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "nonlicable", "primary_focus": "other"}, "authors": ["Jiale Zhao", "Cong Liu", "Yuxuan Zhang", "Chengyue Gong", "Zhenyi Zhang", "Shifeng Jin", "Zhenyu Liu"]}
]]></acme>

<pubDate>2025-10-20T15:50:21+00:00</pubDate>
</item>
<item>
<title>AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM</title>
<link>https://papers.cool/arxiv/2510.17934</link>
<guid>https://papers.cool/arxiv/2510.17934</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AtlasKV introduces a parametric knowledge integration approach that augments large language models (LLMs) with billion‑scale knowledge graphs (e.g., 1 billion triples) while using less than 20 GB of VRAM. The method combines KG2KV and HiKVP to embed KG triples into the model with sub‑linear time and memory complexity, preserving strong grounding and generalization without external retrievers or additional retraining for new knowledge.<br /><strong>Summary (CN):</strong> AtlasKV 提出了一种参数化知识集成方法，将大型语言模型（LLM）与十亿级规模的知识图谱（如 10⁹ 条三元组）进行融合，显著降低显存需求至 20 GB 以下。该方法通过 KG2KV 与 HiKVP 将 KG 条目嵌入模型内部，实现次线性时间和内存复杂度，保持强知识定位和泛化能力，无需外部检索器或在添加新知识时重新训练。<br /><strong>Keywords:</strong> knowledge graph, parametric knowledge integration, LLM augmentation, KG2KV, HiKVP, sublinear memory, VRAM efficiency, retrieval‑augmented generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
Retrieval-augmented generation (RAG) has shown some success in augmenting large language models (LLMs) with external knowledge. However, as a non-parametric knowledge integration paradigm for LLMs, RAG methods heavily rely on external retrieval modules and the retrieved textual context prior. Especially for very large scale knowledge augmentation, they would introduce substantial inference latency due to expensive searches and much longer relevant context. In this paper, we propose a parametric knowledge integration method, called \textbf{AtlasKV}, a scalable, effective, and general way to augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with sub-linear time and memory complexity. It maintains strong knowledge grounding and generalization performance using the LLMs' inherent attention mechanism, and requires no external retrievers, long context priors, or retraining when adapting to new knowledge.
<div><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AtlasKV introduces a parametric knowledge integration approach that augments large language models (LLMs) with billion‑scale knowledge graphs (e.g., 1 billion triples) while using less than 20 GB of VRAM. The method combines KG2KV and HiKVP to embed KG triples into the model with sub‑linear time and memory complexity, preserving strong grounding and generalization without external retrievers or additional retraining for new knowledge.", "summary_cn": "AtlasKV 提出了一种参数化知识集成方法，将大型语言模型（LLM）与十亿级规模的知识图谱（如 10⁹ 条三元组）进行融合，显著降低显存需求至 20 GB 以下。该方法通过 KG2KV 与 HiKVP 将 KG 条目嵌入模型内部，实现次线性时间和内存复杂度，保持强知识定位和泛化能力，无需外部检索器或在添加新知识时重新训练。", "keywords": "knowledge graph, parametric knowledge integration, LLM augmentation, KG2KV, HiKVP, sublinear memory, VRAM efficiency, retrieval‑augmented generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyu Huang", "Hong Ting Tsang", "Jiaxin Bai", "Xi Peng", "Gong Zhang", "Yangqiu Song"]}
]]></acme>

<pubDate>2025-10-20T15:40:14+00:00</pubDate>
</item>
<item>
<title>From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference</title>
<link>https://papers.cool/arxiv/2510.17933</link>
<guid>https://papers.cool/arxiv/2510.17933</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Parameter‑Space Changepoint Detection (Param‑CPD), a two‑stage framework that first amortizes Bayesian inference of governing parameters of a nonlinear dynamical system using a neural posterior estimator trained via simulation‑based inference, and then applies a standard changepoint detection algorithm to the resulting parameter trajectory. Experiments on piecewise‑constant Lorenz‑63 systems show significant improvements in F1 score, localization error, and false‑positive rates compared to observation‑space baselines, and analyses demonstrate identifiability, calibration, and robustness of the method.<br /><strong>Summary (CN):</strong> 本文提出参数空间突变点检测（Param‑CPD）框架，首先利用基于仿真推断训练的神经后验估计器对非线性动力系统的控制参数进行贝叶斯推断，然后在得到的参数轨迹上应用常规突变点检测算法。实验在分段常数的 Lorenz‑63 系统上显示出相较于观测空间基线在 F1、定位误差和误报率方面的显著提升，并通过可辨识性、校准性以及鲁棒性分析验证了方法的有效性。<br /><strong>Keywords:</strong> changepoint detection, simulation-based inference, Bayesian parameter estimation, nonlinear dynamics, Lorenz-63, neural posterior estimator, interpretability, time series analysis<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Xiangbo Deng, Cheng Chen, Peng Yang</div>
Detecting regime shifts in chaotic time series is hard because observation-space signals are entangled with intrinsic variability. We propose Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework that first amortizes Bayesian inference of governing parameters with a neural posterior estimator trained by simulation-based inference, and then applies a standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63 with piecewise-constant parameters, Param--CPD improves F1, reduces localization error, and lowers false positives compared to observation--space baselines. We further verify identifiability and calibration of the inferred posteriors on stationary trajectories, explaining why parameter space offers a cleaner detection signal. Robustness analyses over tolerance, window length, and noise indicate consistent gains. Our results show that operating in a physically interpretable parameter space enables accurate and interpretable changepoint detection in nonlinear dynamical systems.
<div><strong>Authors:</strong> Xiangbo Deng, Cheng Chen, Peng Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Parameter‑Space Changepoint Detection (Param‑CPD), a two‑stage framework that first amortizes Bayesian inference of governing parameters of a nonlinear dynamical system using a neural posterior estimator trained via simulation‑based inference, and then applies a standard changepoint detection algorithm to the resulting parameter trajectory. Experiments on piecewise‑constant Lorenz‑63 systems show significant improvements in F1 score, localization error, and false‑positive rates compared to observation‑space baselines, and analyses demonstrate identifiability, calibration, and robustness of the method.", "summary_cn": "本文提出参数空间突变点检测（Param‑CPD）框架，首先利用基于仿真推断训练的神经后验估计器对非线性动力系统的控制参数进行贝叶斯推断，然后在得到的参数轨迹上应用常规突变点检测算法。实验在分段常数的 Lorenz‑63 系统上显示出相较于观测空间基线在 F1、定位误差和误报率方面的显著提升，并通过可辨识性、校准性以及鲁棒性分析验证了方法的有效性。", "keywords": "changepoint detection, simulation-based inference, Bayesian parameter estimation, nonlinear dynamics, Lorenz-63, neural posterior estimator, interpretability, time series analysis", "scoring": {"interpretability": 6, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xiangbo Deng", "Cheng Chen", "Peng Yang"]}
]]></acme>

<pubDate>2025-10-20T15:29:31+00:00</pubDate>
</item>
<item>
<title>From Charts to Code: A Hierarchical Benchmark for Multimodal Models</title>
<link>https://papers.cool/arxiv/2510.17932</link>
<guid>https://papers.cool/arxiv/2510.17932</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Chart2Code, a hierarchical benchmark for assessing large multimodal models' ability to understand charts and generate corresponding code. It defines three difficulty levels—chart reproduction, chart editing, and long‑table‑to‑chart generation—covering 2,023 tasks across 22 chart types, and evaluates 25 state‑of‑the‑art LMMs, revealing substantial performance gaps. The authors release the dataset and evaluation code to encourage progress in multimodal reasoning.<br /><strong>Summary (CN):</strong> 本文提出了 Chart2Code，一套层次化基准，用于评估大型多模态模型（LMM）对图表的理解和代码生成能力。基准包含三个难度层级——图表复现、图表编辑和长表格到图表生成，共 2,023 条任务，覆盖 22 种图表类型，并对 25 种最新 LMM（包括 GPT-5、Qwen2.5‑VL 等）进行评估，显示出显著的性能差距。作者公开了数据和评估代码，以推动多模态推理的进步。<br /><strong>Keywords:</strong> multimodal models, chart understanding, code generation, benchmark, Chart2Code, visual reasoning, evaluation metrics, LMM, dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jiahao Tang, Henry Hengyuan Zhao, Lijian Wu, Yifei Tao, Dongxing Mao, Yang Wan, Jingru Tan, Min Zeng, Min Li, Alex Jinpeng Wang</div>
We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.
<div><strong>Authors:</strong> Jiahao Tang, Henry Hengyuan Zhao, Lijian Wu, Yifei Tao, Dongxing Mao, Yang Wan, Jingru Tan, Min Zeng, Min Li, Alex Jinpeng Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Chart2Code, a hierarchical benchmark for assessing large multimodal models' ability to understand charts and generate corresponding code. It defines three difficulty levels—chart reproduction, chart editing, and long‑table‑to‑chart generation—covering 2,023 tasks across 22 chart types, and evaluates 25 state‑of‑the‑art LMMs, revealing substantial performance gaps. The authors release the dataset and evaluation code to encourage progress in multimodal reasoning.", "summary_cn": "本文提出了 Chart2Code，一套层次化基准，用于评估大型多模态模型（LMM）对图表的理解和代码生成能力。基准包含三个难度层级——图表复现、图表编辑和长表格到图表生成，共 2,023 条任务，覆盖 22 种图表类型，并对 25 种最新 LMM（包括 GPT-5、Qwen2.5‑VL 等）进行评估，显示出显著的性能差距。作者公开了数据和评估代码，以推动多模态推理的进步。", "keywords": "multimodal models, chart understanding, code generation, benchmark, Chart2Code, visual reasoning, evaluation metrics, LMM, dataset", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jiahao Tang", "Henry Hengyuan Zhao", "Lijian Wu", "Yifei Tao", "Dongxing Mao", "Yang Wan", "Jingru Tan", "Min Zeng", "Min Li", "Alex Jinpeng Wang"]}
]]></acme>

<pubDate>2025-10-20T15:11:56+00:00</pubDate>
</item>
<item>
<title>Attracting Commercial Artificial Intelligence Firms to Support National Security through Collaborative Contracts</title>
<link>https://papers.cool/arxiv/2510.17931</link>
<guid>https://papers.cool/arxiv/2510.17931</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper examines why commercial artificial intelligence firms decide to work with the U.S. Department of Defense despite contractual and procurement obstacles, proposing an "optimal buyer" framework based on social exchange theory to explain these decisions. Interviews reveal that firms view the DoD as an attractive customer when contract terms align with their business and technology needs, and the work suggests best practices for adapting contract law to better match commercial AI development cycles.<br /><strong>Summary (CN):</strong> 本文研究了商业人工智能公司在尽管面临合同和采购障碍的情况下为何仍选择与美国国防部合作，提出基于社会交换理论的“最佳买方”框架来解释此类决策。访谈显示，当合同条款符合公司业务和技术需求时，国防部被视为有吸引力的客户，文章还提供了利用现合同法（主要是其他交易授权）以更好匹配机器学习研发与部署生命周期的最佳实践。<br /><strong>Keywords:</strong> commercial AI, defense contracts, national security, procurement law, optimal buyer theory, social exchange theory<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 4, Safety: 3, Technicality: 5, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Andrew Bowne</div>
Unlike other military technologies driven by national security needs and developed with federal funding, AI is predominantly funded and advanced by commercial industry for civilian applications. However, there is a lack of understanding of the reasons commercial AI firms decide to work with the DoD or choose to abstain from the defence market. This thesis argues that the contract law and procurement framework are among the most significant obstacles. This research indicates that the commercial AI industry actually views the DoD as an attractive customer. However, this attraction is despite the obstacles presented by traditional contract law and procurement practices used to solicit and award contracts. Drawing on social exchange theory, this thesis introduces a theoretical framework, optimal buyer theory, to understand the factors that influence a commercial decision to engage with the DoD. Interviews from a sample of the participants explain why the AI industry holds such perceptions, opinions, and preferences about contracts generally and the DoD, specifically, in its role as a customer. This thesis concludes that commercial AI firms are attracted to contracts that are consistent with their business and technology considerations. Additionally, it develops best practices for leveraging existing contract law, primarily other transaction authority, to align contracting practices with commercial preferences and the machine learning development and deployment lifecycle.
<div><strong>Authors:</strong> Andrew Bowne</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper examines why commercial artificial intelligence firms decide to work with the U.S. Department of Defense despite contractual and procurement obstacles, proposing an \"optimal buyer\" framework based on social exchange theory to explain these decisions. Interviews reveal that firms view the DoD as an attractive customer when contract terms align with their business and technology needs, and the work suggests best practices for adapting contract law to better match commercial AI development cycles.", "summary_cn": "本文研究了商业人工智能公司在尽管面临合同和采购障碍的情况下为何仍选择与美国国防部合作，提出基于社会交换理论的“最佳买方”框架来解释此类决策。访谈显示，当合同条款符合公司业务和技术需求时，国防部被视为有吸引力的客户，文章还提供了利用现合同法（主要是其他交易授权）以更好匹配机器学习研发与部署生命周期的最佳实践。", "keywords": "commercial AI, defense contracts, national security, procurement law, optimal buyer theory, social exchange theory", "scoring": {"interpretability": 1, "understanding": 4, "safety": 3, "technicality": 5, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Andrew Bowne"]}
]]></acme>

<pubDate>2025-10-20T15:11:04+00:00</pubDate>
</item>
<item>
<title>Diagnosing Representation Dynamics in NER Model Extension</title>
<link>https://papers.cool/arxiv/2510.17930</link>
<guid>https://papers.cool/arxiv/2510.17930</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how extending a BERT-based NER model with new PII entity types affects its existing semantic entity predictions, finding that the model can largely preserve original performance through independent semantic and morphological feature mechanisms. Two diagnostic insights are reported: (1) the LOC entity is especially vulnerable due to representation overlap with PII patterns, and (2) a "reverse O-tag representation drift" where the background class blocks learning unless its classifier is unfrozen. These findings provide a mechanistic diagnosis of representation dynamics during NER model adaptation.<br /><strong>Summary (CN):</strong> 本文研究在 BERT 基础的 NER 模型中加入新的 PII 实体类型（如 EMAIL、PHONE）如何影响原有语义实体的预测，发现模型可通过语义特征与形态特征的独立机制保持原有性能。诊断得到两点关键发现：（1）LOC 实体因与 PII 的模式特征重叠而特别脆弱；（2）出现 "逆 O 标记表示漂移"，即背景类 O 会阻碍新学习，除非解冻 O 标记的分类器以使其适应。此工作提供了 NER 模型扩展过程中的表征动力学机制性诊断。<br /><strong>Keywords:</strong> NER, representation dynamics, incremental learning, feature independence, representation overlap, O-tag plasticity, BERT, PII, semantic drift<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
Extending Named Entity Recognition (NER) models to new PII entities in noisy spoken-language data is a common need. We find that jointly fine-tuning a BERT model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII (EMAIL, PHONE) results in minimal degradation for original classes. We investigate this "peaceful coexistence," hypothesizing that the model uses independent semantic vs. morphological feature mechanisms. Using an incremental learning setup as a diagnostic tool, we measure semantic drift and find two key insights. First, the LOC (location) entity is uniquely vulnerable due to a representation overlap with new PII, as it shares pattern-like features (e.g., postal codes). Second, we identify a "reverse O-tag representation drift." The model, initially trained to map PII patterns to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's classifier, allowing the background class to adapt and "release" these patterns. This work provides a mechanistic diagnosis of NER model adaptation, highlighting feature independence, representation overlap, and 'O' tag plasticity.
<div><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how extending a BERT-based NER model with new PII entity types affects its existing semantic entity predictions, finding that the model can largely preserve original performance through independent semantic and morphological feature mechanisms. Two diagnostic insights are reported: (1) the LOC entity is especially vulnerable due to representation overlap with PII patterns, and (2) a \"reverse O-tag representation drift\" where the background class blocks learning unless its classifier is unfrozen. These findings provide a mechanistic diagnosis of representation dynamics during NER model adaptation.", "summary_cn": "本文研究在 BERT 基础的 NER 模型中加入新的 PII 实体类型（如 EMAIL、PHONE）如何影响原有语义实体的预测，发现模型可通过语义特征与形态特征的独立机制保持原有性能。诊断得到两点关键发现：（1）LOC 实体因与 PII 的模式特征重叠而特别脆弱；（2）出现 \"逆 O 标记表示漂移\"，即背景类 O 会阻碍新学习，除非解冻 O 标记的分类器以使其适应。此工作提供了 NER 模型扩展过程中的表征动力学机制性诊断。", "keywords": "NER, representation dynamics, incremental learning, feature independence, representation overlap, O-tag plasticity, BERT, PII, semantic drift", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Xirui Zhang", "Philippe de La Chevasnerie", "Benoit Fabre"]}
]]></acme>

<pubDate>2025-10-20T14:53:42+00:00</pubDate>
</item>
<item>
<title>EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</title>
<link>https://papers.cool/arxiv/2510.17928</link>
<guid>https://papers.cool/arxiv/2510.17928</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> EvoSyn proposes an evolutionary, task-agnostic framework that jointly generates problems, diverse candidate solutions, and verification artifacts, using a consistency-based evaluator to discover strategies that enforce agreement between human annotations and strategy-induced checks. By turning filtering into principled synthesis, the method produces coherent, verifiable training instances that improve performance on RL with verifiable rewards and model distillation tasks such as LiveCodeBench and AgentBench-OS.<br /><strong>Summary (CN):</strong> EvoSyn 引入一种进化式、任务无关的数据合成框架，能够同时生成问题、多个候选解以及验证工件，并通过一致性评估器发现策略，使人工标注与策略生成的检查保持一致。该方法将过滤升级为原则化合成，生成可靠的可验证训练实例，显著提升在 RLVR 和模型蒸馏训练范式下的 LiveCodeBench 与 AgentBench-OS 等任务表现。<br /><strong>Keywords:</strong> evolutionary data synthesis, verifiable learning, RL with verifiable rewards, strategy-guided generation, consistency evaluator, data distillation, LiveCodeBench, AgentBench<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</div>
Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.
<div><strong>Authors:</strong> He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "EvoSyn proposes an evolutionary, task-agnostic framework that jointly generates problems, diverse candidate solutions, and verification artifacts, using a consistency-based evaluator to discover strategies that enforce agreement between human annotations and strategy-induced checks. By turning filtering into principled synthesis, the method produces coherent, verifiable training instances that improve performance on RL with verifiable rewards and model distillation tasks such as LiveCodeBench and AgentBench-OS.", "summary_cn": "EvoSyn 引入一种进化式、任务无关的数据合成框架，能够同时生成问题、多个候选解以及验证工件，并通过一致性评估器发现策略，使人工标注与策略生成的检查保持一致。该方法将过滤升级为原则化合成，生成可靠的可验证训练实例，显著提升在 RLVR 和模型蒸馏训练范式下的 LiveCodeBench 与 AgentBench-OS 等任务表现。", "keywords": "evolutionary data synthesis, verifiable learning, RL with verifiable rewards, strategy-guided generation, consistency evaluator, data distillation, LiveCodeBench, AgentBench", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["He Du", "Bowen Li", "Aijun Yang", "Siyang He", "Qipeng Guo", "Dacheng Tao"]}
]]></acme>

<pubDate>2025-10-20T11:56:35+00:00</pubDate>
</item>
<item>
<title>SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion</title>
<link>https://papers.cool/arxiv/2510.17925</link>
<guid>https://papers.cool/arxiv/2510.17925</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SpecAgent is an indexing-time agent that proactively explores repository files to build speculative context, improving both code-generation quality and inference latency for large language models in realistic software repositories. The approach also identifies and mitigates future context leakage in existing benchmarks, providing a synthetic leakage-free benchmark for fair evaluation. Experiments show 9-11% absolute gains over strong baselines while significantly reducing latency.<br /><strong>Summary (CN):</strong> SpecAgent 在索引阶段主动遍历代码库文件，构建预测未来编辑的投机上下文，从而在保持低推理延迟的同时提升代码生成质量。该方法还发现并解决了现有基准中未来上下文泄漏的问题，构建了一个合成的无泄漏基准用于更真实的评估。实验表明，相较于最强基线，SpecAgent 可实现 9-11% 的绝对提升，并显著降低推理延迟。<br /><strong>Keywords:</strong> speculative retrieval, code completion, large language models, repository context, inference latency, leakage-free benchmark<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> George Ma, Anurag Koul, Qi Chen, Yawen Wu, Sachit Kuhar, Yu Yu, Aritra Sengupta, Varun Kumar, Murali Krishna Ramanathan</div>
Large Language Models (LLMs) excel at code-related tasks but often struggle in realistic software repositories, where project-specific APIs and cross-file dependencies are crucial. Retrieval-augmented methods mitigate this by injecting repository context at inference time. The low inference-time latency budget affects either retrieval quality or the added latency adversely impacts user experience. We address this limitation with SpecAgent, an agent that improves both latency and code-generation quality by proactively exploring repository files during indexing and constructing speculative context that anticipates future edits in each file. This indexing-time asynchrony allows thorough context computation, masking latency, and the speculative nature of the context improves code-generation quality. Additionally, we identify the problem of future context leakage in existing benchmarks, which can inflate reported performance. To address this, we construct a synthetic, leakage-free benchmark that enables a more realistic evaluation of our agent against baselines. Experiments show that SpecAgent consistently achieves absolute gains of 9-11% (48-58% relative) compared to the best-performing baselines, while significantly reducing inference latency.
<div><strong>Authors:</strong> George Ma, Anurag Koul, Qi Chen, Yawen Wu, Sachit Kuhar, Yu Yu, Aritra Sengupta, Varun Kumar, Murali Krishna Ramanathan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SpecAgent is an indexing-time agent that proactively explores repository files to build speculative context, improving both code-generation quality and inference latency for large language models in realistic software repositories. The approach also identifies and mitigates future context leakage in existing benchmarks, providing a synthetic leakage-free benchmark for fair evaluation. Experiments show 9-11% absolute gains over strong baselines while significantly reducing latency.", "summary_cn": "SpecAgent 在索引阶段主动遍历代码库文件，构建预测未来编辑的投机上下文，从而在保持低推理延迟的同时提升代码生成质量。该方法还发现并解决了现有基准中未来上下文泄漏的问题，构建了一个合成的无泄漏基准用于更真实的评估。实验表明，相较于最强基线，SpecAgent 可实现 9-11% 的绝对提升，并显著降低推理延迟。", "keywords": "speculative retrieval, code completion, large language models, repository context, inference latency, leakage-free benchmark", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["George Ma", "Anurag Koul", "Qi Chen", "Yawen Wu", "Sachit Kuhar", "Yu Yu", "Aritra Sengupta", "Varun Kumar", "Murali Krishna Ramanathan"]}
]]></acme>

<pubDate>2025-10-20T08:04:51+00:00</pubDate>
</item>
<item>
<title>Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs</title>
<link>https://papers.cool/arxiv/2510.17924</link>
<guid>https://papers.cool/arxiv/2510.17924</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a comprehensive comparative study of NLP methods—including embedding‑based models, fine‑tuned transformers,‑shot/few‑shot LLM prompting, and retrieval‑augmented generation—for automated toxicity detection in online gaming chats. It evaluates each approach on accuracy, latency, and computational cost, and proposes a hybrid moderation architecture that balances automated detection with human oversight while enabling continuous learning. Experiments show that a fine‑tuned DistilBERT model offers the best accuracy‑cost trade‑off for deployment in dynamic gaming environments.<br /><strong>Summary (CN):</strong> 本文对多种自然语言处理方法（包括基于嵌入的传统模型、微调的 Transformer、零样本/少样本的 LLM 提示以及检索增强生成）在在线游戏聊天中的自动毒性检测进行全面比较。通过评估准确率、处理速度和计算成本三个关键维度，并提出一种融合自动检测与人工审核、支持持续学习的混合 moderation 系统架构。实验表明，微调的 DistilBERT 在精度与成本之间实现了最佳平衡，适合在动态游戏环境中部署。<br /><strong>Keywords:</strong> toxicity detection, gaming chat moderation, embeddings, fine-tuned transformer, LLM zero-shot, retrieval-augmented generation, cost-effective moderation, comparative analysis, natural language processing<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer models, and retrieval-augmented generation (RAG) approaches are evaluated. The evaluation framework assesses three critical dimensions: classification accuracy, processing speed, and computational costs. A hybrid moderation system architecture is proposed that optimizes human moderator workload through automated detection and incorporates continuous learning mechanisms. The experimental results demonstrate significant performance variations across methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs. The findings provide empirical evidence for deploying cost-effective, efficient content moderation systems in dynamic online gaming environments.
<div><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a comprehensive comparative study of NLP methods—including embedding‑based models, fine‑tuned transformers,‑shot/few‑shot LLM prompting, and retrieval‑augmented generation—for automated toxicity detection in online gaming chats. It evaluates each approach on accuracy, latency, and computational cost, and proposes a hybrid moderation architecture that balances automated detection with human oversight while enabling continuous learning. Experiments show that a fine‑tuned DistilBERT model offers the best accuracy‑cost trade‑off for deployment in dynamic gaming environments.", "summary_cn": "本文对多种自然语言处理方法（包括基于嵌入的传统模型、微调的 Transformer、零样本/少样本的 LLM 提示以及检索增强生成）在在线游戏聊天中的自动毒性检测进行全面比较。通过评估准确率、处理速度和计算成本三个关键维度，并提出一种融合自动检测与人工审核、支持持续学习的混合 moderation 系统架构。实验表明，微调的 DistilBERT 在精度与成本之间实现了最佳平衡，适合在动态游戏环境中部署。", "keywords": "toxicity detection, gaming chat moderation, embeddings, fine-tuned transformer, LLM zero-shot, retrieval-augmented generation, cost-effective moderation, comparative analysis, natural language processing", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"]}
]]></acme>

<pubDate>2025-10-20T08:03:28+00:00</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.17923</link>
<guid>https://papers.cool/arxiv/2510.17923</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces COMPASS, a composite path-and-answer self-scoring reward mechanism for test-time reinforcement learning of large language models that operates without external supervision. COMPASS combines a Dual-Calibration Answer Reward to create trustworthy pseudo‑labels and a Decisive Path Reward that directly optimizes the quality of reasoning chains, leading to consistent performance gains across various reasoning tasks. Experiments demonstrate that jointly reinforcing reliable consensus answers and decisive reasoning processes improves LLM analytical capabilities while addressing scalability bottlenecks of traditional RL approaches.<br /><strong>Summary (CN):</strong> 本文提出了 COMPASS，一种在测试时强化学习中用于大语言模型的复合路径与答案自评分奖励机制，无需外部监督。COMPASS 将双校准答案奖励（通过置信度与可信度校准生成可靠的伪标签）与决定性路径奖励（直接优化推理链质量）相结合，显著提升了模型在多种推理任务上的表现。实验表明，该方法在强化可靠共识答案和高决策性推理过程方面均取得了一致的性能提升，缓解了传统强化学习对人工偏好数据的规模瓶颈。<br /><strong>Keywords:</strong> test-time reinforcement learning, self-scoring reward, composite path reward, large language models, autonomous reward learning, reasoning chain optimization<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</div>
Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
<div><strong>Authors:</strong> Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces COMPASS, a composite path-and-answer self-scoring reward mechanism for test-time reinforcement learning of large language models that operates without external supervision. COMPASS combines a Dual-Calibration Answer Reward to create trustworthy pseudo‑labels and a Decisive Path Reward that directly optimizes the quality of reasoning chains, leading to consistent performance gains across various reasoning tasks. Experiments demonstrate that jointly reinforcing reliable consensus answers and decisive reasoning processes improves LLM analytical capabilities while addressing scalability bottlenecks of traditional RL approaches.", "summary_cn": "本文提出了 COMPASS，一种在测试时强化学习中用于大语言模型的复合路径与答案自评分奖励机制，无需外部监督。COMPASS 将双校准答案奖励（通过置信度与可信度校准生成可靠的伪标签）与决定性路径奖励（直接优化推理链质量）相结合，显著提升了模型在多种推理任务上的表现。实验表明，该方法在强化可靠共识答案和高决策性推理过程方面均取得了一致的性能提升，缓解了传统强化学习对人工偏好数据的规模瓶颈。", "keywords": "test-time reinforcement learning, self-scoring reward, composite path reward, large language models, autonomous reward learning, reasoning chain optimization", "scoring": {"interpretability": 4, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Chenwei Tang", "Jingyu Xing", "Xinyu Liu", "Wei Ju", "Jiancheng Lv", "Deng Xiong", "Ziyue Qiao"]}
]]></acme>

<pubDate>2025-10-20T07:53:51+00:00</pubDate>
</item>
<item>
<title>Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17922</link>
<guid>https://papers.cool/arxiv/2510.17922</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates task decomposition methods for large language models, analyzing factors such as approach categories, task characteristics, and model configurations that affect performance and cost. Based on this analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition approach and adds a verification step to achieve an optimal performance‑cost trade‑off, achieving Pareto‑optimal results on several benchmarks.<br /><strong>Summary (CN):</strong> 本文研究了大语言模型的任务分解方法，分析了方法类别、任务特性和模型配置等因素对性能和成本的影响。基于此分析，提出了 Select-Then-Decompose 策略，可动态选择最合适的分解方式并通过验证模块提升可靠性，在多项基准上实现了性能‑成本的 Pareto 最优平衡。<br /><strong>Keywords:</strong> task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, LLM planning, benchmark evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose.
<div><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates task decomposition methods for large language models, analyzing factors such as approach categories, task characteristics, and model configurations that affect performance and cost. Based on this analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition approach and adds a verification step to achieve an optimal performance‑cost trade‑off, achieving Pareto‑optimal results on several benchmarks.", "summary_cn": "本文研究了大语言模型的任务分解方法，分析了方法类别、任务特性和模型配置等因素对性能和成本的影响。基于此分析，提出了 Select-Then-Decompose 策略，可动态选择最合适的分解方式并通过验证模块提升可靠性，在多项基准上实现了性能‑成本的 Pareto 最优平衡。", "keywords": "task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, LLM planning, benchmark evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"]}
]]></acme>

<pubDate>2025-10-20T07:28:15+00:00</pubDate>
</item>
<item>
<title>CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections</title>
<link>https://papers.cool/arxiv/2510.17921</link>
<guid>https://papers.cool/arxiv/2510.17921</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CLAWS introduces a white‑box method that leverages attention weights across prompt sections and model outputs to automatically classify mathematical solutions from LLMs as typical, creative, or hallucinated, eliminating the need for human evaluation. The approach outperforms five existing white‑box detection baselines on several 7‑8B RL‑fine‑tuned math models and is validated on 4,545 problems from major math contests.<br /><strong>Summary (CN):</strong> CLAWS 提出一种基于注意力权重跨提示段落与输出的白盒方法，自动将 LLM 生成的数学解答划分为典型、创造性或幻觉类别，免除人工评估。该方法在多个 7‑8B RL‑微调数学模型上优于五种现有白盒检测基准，并在 4,545 道来自主要数学竞赛的题目上完成验证。<br /><strong>Keywords:</strong> creativity detection, hallucination detection, attention analysis, large language models, math reasoning, white‑box interpretability, RL fine‑tuning, model evaluation<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a method that defines and classifies mathematical solutions into typical, creative, and hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems collected from 181 math contests (AJHSME, AMC, AIME).
<div><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CLAWS introduces a white‑box method that leverages attention weights across prompt sections and model outputs to automatically classify mathematical solutions from LLMs as typical, creative, or hallucinated, eliminating the need for human evaluation. The approach outperforms five existing white‑box detection baselines on several 7‑8B RL‑fine‑tuned math models and is validated on 4,545 problems from major math contests.", "summary_cn": "CLAWS 提出一种基于注意力权重跨提示段落与输出的白盒方法，自动将 LLM 生成的数学解答划分为典型、创造性或幻觉类别，免除人工评估。该方法在多个 7‑8B RL‑微调数学模型上优于五种现有白盒检测基准，并在 4,545 道来自主要数学竞赛的题目上完成验证。", "keywords": "creativity detection, hallucination detection, attention analysis, large language models, math reasoning, white‑box interpretability, RL fine‑tuning, model evaluation", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"]}
]]></acme>

<pubDate>2025-10-20T06:59:37+00:00</pubDate>
</item>
<item>
<title>CBINNS: Cancer Biology-Informed Neural Network for Unknown Parameter Estimation and Missing Physics Identification</title>
<link>https://papers.cool/arxiv/2510.17920</link>
<guid>https://papers.cool/arxiv/2510.17920</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces the Cancer Biology‑Informed Neural Network (CBINN), a physics‑informed neural network framework designed to estimate unknown parameters and uncover missing terms in tumor‑immune interaction models from sparse, noisy data. Experiments on three nonlinear compartmental ODE models demonstrate that CBINN can accurately recover both model parameters and the underlying governing equations despite measurement noise. The results suggest that the approach is robust and generalizable for discovering hidden biological dynamics in complex tumor microenvironments.<br /><strong>Summary (CN):</strong> 本文提出了癌症生物学驱动神经网络（CBINN），一种基于物理信息的神经网络框架，用于从稀疏且噪声较大的实验数据中估计肿瘤‑免疫相互作用模型的未知参数并发现缺失的动力学项。通过在三个非线性区室模型上的实验，CBINN 能在噪声环境下准确恢复模型参数及其 governing equations（支配方程），展示了对复杂肿瘤微环境隐藏生物动力学的鲁棒性和通用性。<br /><strong>Keywords:</strong> physics-informed neural networks, parameter estimation, missing physics discovery, tumor-immune modeling, inverse modeling, ODE, compartmental models<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 2, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Bishal Chhetri, B. V. Rathish Kumar</div>
The dynamics of tumor-immune interactions within a complex tumor microenvironment are typically modeled using a system of ordinary differential equations or partial differential equations. These models introduce some unknown parameters that need to be estimated accurately and efficiently from the limited and noisy experimental data. Moreover, due to the intricate biological complexity and limitations in experimental measurements, tumor-immune dynamics are not fully understood, and therefore, only partial knowledge of the underlying physics may be available, resulting in unknown or missing terms within the system of equations. In this study, we develop a cancer biology-informed neural network model(CBINN) to infer the unknown parameters in the system of equations as well as to discover the missing physics from sparse and noisy measurements. We test the performance of the CBINN model on three distinct nonlinear compartmental tumor-immune models and evaluate its robustness across multiple synthetic noise levels. By harnessing these highly nonlinear dynamics, our CBINN framework effectively estimates the unknown model parameters and uncovers the underlying physical laws or mathematical structures that govern these biological systems, even from scattered and noisy measurements. The models chosen here represent the dynamic patterns commonly observed in compartmental models of tumor-immune interactions, thereby validating the generalizability and efficacy of our methodology.
<div><strong>Authors:</strong> Bishal Chhetri, B. V. Rathish Kumar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces the Cancer Biology‑Informed Neural Network (CBINN), a physics‑informed neural network framework designed to estimate unknown parameters and uncover missing terms in tumor‑immune interaction models from sparse, noisy data. Experiments on three nonlinear compartmental ODE models demonstrate that CBINN can accurately recover both model parameters and the underlying governing equations despite measurement noise. The results suggest that the approach is robust and generalizable for discovering hidden biological dynamics in complex tumor microenvironments.", "summary_cn": "本文提出了癌症生物学驱动神经网络（CBINN），一种基于物理信息的神经网络框架，用于从稀疏且噪声较大的实验数据中估计肿瘤‑免疫相互作用模型的未知参数并发现缺失的动力学项。通过在三个非线性区室模型上的实验，CBINN 能在噪声环境下准确恢复模型参数及其 governing equations（支配方程），展示了对复杂肿瘤微环境隐藏生物动力学的鲁棒性和通用性。", "keywords": "physics-informed neural networks, parameter estimation, missing physics discovery, tumor-immune modeling, inverse modeling, ODE, compartmental models", "scoring": {"interpretability": 4, "understanding": 2, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Bishal Chhetri", "B. V. Rathish Kumar"]}
]]></acme>

<pubDate>2025-10-20T04:33:00+00:00</pubDate>
</item>
<item>
<title>ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection</title>
<link>https://papers.cool/arxiv/2510.17919</link>
<guid>https://papers.cool/arxiv/2510.17919</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ParaVul introduces a parallel large language model combined with a retrieval-augmented generation framework to detect vulnerabilities in smart contracts. It employs Sparse Low-Rank Adaptation (SLoRA) to fine‑tune LLMs efficiently, a hybrid dense/BM25 retrieval system, and a meta‑learning module that fuses LLM and RAG outputs, achieving high F1 scores on single‑label and multi‑label detection tasks.<br /><strong>Summary (CN):</strong> ParaVul 提出了一种并行大型语言模型与检索增强生成（RAG）相结合的框架，用于智能合约漏洞检测。该方法通过稀疏低秩适配（SLoRA）高效微调 LLM，结合稠密检索与 BM25 的混合检索系统以及元学习模型融合两者输出，并在单标签和多标签检测任务中实现了高 F1 分数。<br /><strong>Keywords:</strong> smart contract security, vulnerability detection, large language model, retrieval-augmented generation, SLoRA, meta-learning, chain-of-thought prompting, blockchain<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Tenghui Huang, Jinbo Wen, Jiawen Kang, Siyong Chen, Zhengtao Li, Tao Zhang, Dongning Liu, Jiacheng Wang, Chengjun Cai, Yinqiu Liu, Dusit Niyato</div>
Smart contracts play a significant role in automating blockchain services. Nevertheless, vulnerabilities in smart contracts pose serious threats to blockchain security. Currently, traditional detection methods primarily rely on static analysis and formal verification, which can result in high false-positive rates and poor scalability. Large Language Models (LLMs) have recently made significant progress in smart contract vulnerability detection. However, they still face challenges such as high inference costs and substantial computational overhead. In this paper, we propose ParaVul, a parallel LLM and retrieval-augmented framework to improve the reliability and accuracy of smart contract vulnerability detection. Specifically, we first develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA introduces sparsification by incorporating a sparse matrix into quantized LoRA-based LLMs, thereby reducing computational overhead and resource requirements while enhancing their ability to understand vulnerability-related issues. We then construct a vulnerability contract dataset and develop a hybrid Retrieval-Augmented Generation (RAG) system that integrates dense retrieval with Best Matching 25 (BM25), assisting in verifying the results generated by the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of the RAG system and the LLM, thereby generating the final detection results. After completing vulnerability detection, we design chain-of-thought prompts to guide LLMs to generate comprehensive vulnerability detection reports. Simulation results demonstrate the superiority of ParaVul, especially in terms of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for multi-label detection.
<div><strong>Authors:</strong> Tenghui Huang, Jinbo Wen, Jiawen Kang, Siyong Chen, Zhengtao Li, Tao Zhang, Dongning Liu, Jiacheng Wang, Chengjun Cai, Yinqiu Liu, Dusit Niyato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ParaVul introduces a parallel large language model combined with a retrieval-augmented generation framework to detect vulnerabilities in smart contracts. It employs Sparse Low-Rank Adaptation (SLoRA) to fine‑tune LLMs efficiently, a hybrid dense/BM25 retrieval system, and a meta‑learning module that fuses LLM and RAG outputs, achieving high F1 scores on single‑label and multi‑label detection tasks.", "summary_cn": "ParaVul 提出了一种并行大型语言模型与检索增强生成（RAG）相结合的框架，用于智能合约漏洞检测。该方法通过稀疏低秩适配（SLoRA）高效微调 LLM，结合稠密检索与 BM25 的混合检索系统以及元学习模型融合两者输出，并在单标签和多标签检测任务中实现了高 F1 分数。", "keywords": "smart contract security, vulnerability detection, large language model, retrieval-augmented generation, SLoRA, meta-learning, chain-of-thought prompting, blockchain", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Tenghui Huang", "Jinbo Wen", "Jiawen Kang", "Siyong Chen", "Zhengtao Li", "Tao Zhang", "Dongning Liu", "Jiacheng Wang", "Chengjun Cai", "Yinqiu Liu", "Dusit Niyato"]}
]]></acme>

<pubDate>2025-10-20T03:23:41+00:00</pubDate>
</item>
<item>
<title>JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</title>
<link>https://papers.cool/arxiv/2510.17918</link>
<guid>https://papers.cool/arxiv/2510.17918</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> JT‑Safe introduces a data‑centric approach that enriches pre‑training corpora with world‑context information (DWC) to anchor tokens in real‑world scenarios, aiming to reduce hallucinations and improve safety and trustworthiness of large language models. The authors continue pre‑training a JT‑35B‑Base checkpoint on 1.5 trillion DWC tokens and apply post‑training procedures, achieving a 1.79 % gain on safety‑trustworthiness benchmarks compared to a comparable Qwen model while using fewer total tokens.<br /><strong>Summary (CN):</strong> JT‑Safe 通过在预训练语料中加入世界上下文信息（DWC），将文本更好地锚定到真实场景，以降低幻觉并提升大型语言模型的安全性和可信度。作者对 JT‑35B‑Base 继续进行 1.5 万亿 DWC 令牌的预训练，并使用后训练技术，使模型在安全可信评估基准上比规模相近的 Qwen 提升 1.79%，且总训练令牌更少。<br /><strong>Keywords:</strong> world-context data, pretraining augmentation, hallucination mitigation, safety, trustworthiness, large language models, data grounding, JT‑Safe, DWC, benchmark improvement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.
<div><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "JT‑Safe introduces a data‑centric approach that enriches pre‑training corpora with world‑context information (DWC) to anchor tokens in real‑world scenarios, aiming to reduce hallucinations and improve safety and trustworthiness of large language models. The authors continue pre‑training a JT‑35B‑Base checkpoint on 1.5 trillion DWC tokens and apply post‑training procedures, achieving a 1.79 % gain on safety‑trustworthiness benchmarks compared to a comparable Qwen model while using fewer total tokens.", "summary_cn": "JT‑Safe 通过在预训练语料中加入世界上下文信息（DWC），将文本更好地锚定到真实场景，以降低幻觉并提升大型语言模型的安全性和可信度。作者对 JT‑35B‑Base 继续进行 1.5 万亿 DWC 令牌的预训练，并使用后训练技术，使模型在安全可信评估基准上比规模相近的 Qwen 提升 1.79%，且总训练令牌更少。", "keywords": "world-context data, pretraining augmentation, hallucination mitigation, safety, trustworthiness, large language models, data grounding, JT‑Safe, DWC, benchmark improvement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Junlan Feng", "Fanyu Meng", "Chong Long", "Pengyu Cong", "Duqing Wang", "Yan Zheng", "Yuyao Zhang", "Xuanchang Gao", "Ye Yuan", "Yunfei Ma", "Zhijie Ren", "Fan Yang", "Na Wu", "Di Jin", "Chao Deng"]}
]]></acme>

<pubDate>2025-10-20T02:12:49+00:00</pubDate>
</item>
<item>
<title>Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection</title>
<link>https://papers.cool/arxiv/2510.17917</link>
<guid>https://papers.cool/arxiv/2510.17917</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a time‑frequency selective approach for data unlearning in diffusion models, arguing that forgetting is uneven across diffusion timesteps and frequency bands. By targeting specific time‑frequency ranges during the unlearning process, the method improves generation quality while achieving more effective removal of training samples, and a normalized SSCD metric is proposed for evaluating deletion and quality.<br /><strong>Summary (CN):</strong> 本文提出在扩散模型中进行数据消除时，使用时频选择策略，指出忘记过程在不同扩散时间步和频率上分布不均。通过仅在特定的时频范围内进行消除，可提升生成质量并更彻底地去除训练样本，另外提出归一化的 SSCD 指标用于评估删除效果和生成质量。<br /><strong>Keywords:</strong> data unlearning, diffusion models, time-frequency selection, privacy, generative modeling, SSCD evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Jinseong Park, Mijung Park</div>
Data unlearning aims to remove the influence of specific training samples from a trained model without requiring full retraining. Unlike concept unlearning, data unlearning in diffusion models remains underexplored and often suffers from quality degradation or incomplete forgetting. To address this, we first observe that most existing methods attempt to unlearn the samples at all diffusion time steps equally, leading to poor-quality generation. We argue that forgetting occurs disproportionately across time and frequency, depending on the model and scenarios. By selectively focusing on specific time-frequency ranges during training, we achieve samples with higher aesthetic quality and lower noise. We validate this improvement by applying our time-frequency selective approach to diverse settings, including gradient-based and preference optimization objectives, as well as both image-level and text-to-image tasks. Finally, to evaluate both deletion and quality of unlearned data samples, we propose a simple normalized version of SSCD. Together, our analysis and methods establish a clearer understanding of the unique challenges in data unlearning for diffusion models, providing practical strategies to improve both evaluation and unlearning performance.
<div><strong>Authors:</strong> Jinseong Park, Mijung Park</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a time‑frequency selective approach for data unlearning in diffusion models, arguing that forgetting is uneven across diffusion timesteps and frequency bands. By targeting specific time‑frequency ranges during the unlearning process, the method improves generation quality while achieving more effective removal of training samples, and a normalized SSCD metric is proposed for evaluating deletion and quality.", "summary_cn": "本文提出在扩散模型中进行数据消除时，使用时频选择策略，指出忘记过程在不同扩散时间步和频率上分布不均。通过仅在特定的时频范围内进行消除，可提升生成质量并更彻底地去除训练样本，另外提出归一化的 SSCD 指标用于评估删除效果和生成质量。", "keywords": "data unlearning, diffusion models, time-frequency selection, privacy, generative modeling, SSCD evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Jinseong Park", "Mijung Park"]}
]]></acme>

<pubDate>2025-10-20T02:00:12+00:00</pubDate>
</item>
<item>
<title>Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy</title>
<link>https://papers.cool/arxiv/2510.17916</link>
<guid>https://papers.cool/arxiv/2510.17916</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a constructive proof that the Free Energy Principle can be realized via exact local credit assignment using a hierarchical gradient decomposition: spatial credit through feedback alignment, temporal credit via eligibility traces, and structural credit through a Trophic Field Map that predicts gradient magnitudes. Empirical results show the Trophic Field Map closely matches oracle gradients and enables capabilities such as high retention after task interference, autonomous recovery from structural damage, self‑organized criticality, and sample‑efficient continuous‑control reinforcement learning without replay buffers. The work unifies concepts from dissipative structures, free energy minimization, and attractor dynamics, demonstrating that exact hierarchical inference over network topology can be achieved with biologically plausible rules.<br /><strong>Summary (CN):</strong> 本文提供了一个建构性证明，展示自由能原理（Free Energy Principle）可以通过精确的局部信用分配实现，采用层次梯度分解：空间信用使用反馈对齐（feedback alignment），时间信用通过资格迹（eligibility traces），结构信用则利用营养场映射（Trophic Field Map）来估计每个连接块的梯度幅度。实验结果表明，营养场映射与真实梯度的皮尔逊相关系数高达 0.9693，并且系统表现出任务干扰后 98.6% 的记忆保持、对 75% 结构损伤的自主恢复、自组织临界性以及在连续控制强化学习任务中无需回放缓冲区的样本效率提升。该架构将普里高津的耗散结构、弗里斯顿的自由能最小化以及霍普金斯的吸引子动力学统一起来，展示了通过局部、生物可实现的规则对网络拓扑进行精确层次推断的可能性。<br /><strong>Keywords:</strong> free energy principle, hierarchical gradient, feedback alignment, eligibility traces, trophic field map, biologically plausible learning, credit assignment, dissipative structures, reinforcement learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Michael James McCulloch</div>
The Free Energy Principle (FEP) states that self-organizing systems must minimize variational free energy to persist, but the path from principle to implementable algorithm has remained unclear. We present a constructive proof that the FEP can be realized through exact local credit assignment. The system decomposes gradient computation hierarchically: spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map (TFM) that estimates expected gradient magnitude for each connection block. We prove these mechanisms are exact at their respective levels and validate the central claim empirically: the TFM achieves 0.9693 Pearson correlation with oracle gradients. This exactness produces emergent capabilities including 98.6% retention after task interference, autonomous recovery from 75% structural damage, self-organized criticality (spectral radius p ~= 1.0$), and sample-efficient reinforcement learning on continuous control tasks without replay buffers. The architecture unifies Prigogine's dissipative structures, Friston's free energy minimization, and Hopfield's attractor dynamics, demonstrating that exact hierarchical inference over network topology can be implemented with local, biologically plausible rules.
<div><strong>Authors:</strong> Michael James McCulloch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a constructive proof that the Free Energy Principle can be realized via exact local credit assignment using a hierarchical gradient decomposition: spatial credit through feedback alignment, temporal credit via eligibility traces, and structural credit through a Trophic Field Map that predicts gradient magnitudes. Empirical results show the Trophic Field Map closely matches oracle gradients and enables capabilities such as high retention after task interference, autonomous recovery from structural damage, self‑organized criticality, and sample‑efficient continuous‑control reinforcement learning without replay buffers. The work unifies concepts from dissipative structures, free energy minimization, and attractor dynamics, demonstrating that exact hierarchical inference over network topology can be achieved with biologically plausible rules.", "summary_cn": "本文提供了一个建构性证明，展示自由能原理（Free Energy Principle）可以通过精确的局部信用分配实现，采用层次梯度分解：空间信用使用反馈对齐（feedback alignment），时间信用通过资格迹（eligibility traces），结构信用则利用营养场映射（Trophic Field Map）来估计每个连接块的梯度幅度。实验结果表明，营养场映射与真实梯度的皮尔逊相关系数高达 0.9693，并且系统表现出任务干扰后 98.6% 的记忆保持、对 75% 结构损伤的自主恢复、自组织临界性以及在连续控制强化学习任务中无需回放缓冲区的样本效率提升。该架构将普里高津的耗散结构、弗里斯顿的自由能最小化以及霍普金斯的吸引子动力学统一起来，展示了通过局部、生物可实现的规则对网络拓扑进行精确层次推断的可能性。", "keywords": "free energy principle, hierarchical gradient, feedback alignment, eligibility traces, trophic field map, biologically plausible learning, credit assignment, dissipative structures, reinforcement learning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Michael James McCulloch"]}
]]></acme>

<pubDate>2025-10-20T00:19:32+00:00</pubDate>
</item>
<item>
<title>Uncertainty-Aware Post-Hoc Calibration: Mitigating Confidently Incorrect Predictions Beyond Calibration Metrics</title>
<link>https://papers.cool/arxiv/2510.17915</link>
<guid>https://papers.cool/arxiv/2510.17915</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a post‑hoc calibration framework that first separates predictions into likely‑correct and likely‑incorrect groups using proximity‑based conformal prediction, then applies two different isotonic regression calibrations—standard for the former and an under‑confidence regularized version for the latter—to reduce confidently wrong outputs while keeping overall calibration competitive. Experiments on CIFAR‑10/100 with BiT and CoAtNet show fewer confidently incorrect predictions and comparable Expected Calibration Error (ECE) relative to isotonic and focal‑loss baselines, without any model retraining. The method links calibration improvement with better uncertainty‑aware decision‑making.<br /><strong>Summary (CN):</strong> 本文提出一种后置校准框架，利用基于相似度的(Conformal Prediction)将预测划分为可能正确和可能错误两组，然后对前者使用标准(Isotonic Regression)进行校准，对后者使用降低置信度的正则化(Isotonic Regression)使其趋向均匀分布，从而降低自信错误预测的数量且保持整体校准质量。实验在 CIFAR‑10/100 上使用 BiT 与 CoAtNet 骨干网络，展示了相较于标准等距和 focal‑loss 基线，能够显著减少自信错误预测且在 Expected Calibration Error 上保持竞争力，且无需重新训练模型。该方法将校准提升与更有效的“不确定性感知”决策相结合。<br /><strong>Keywords:</strong> calibration, uncertainty quantification, isotonic, conformal prediction, post-hoc adaptation, confidently incorrect predictions, CIFAR<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Hassan Gharoun, Mohammad Sadegh Khorshidi, Kasra Ranjbarigderi, Fang Chen, Amir H. Gandomi</div>
Despite extensive research on neural network calibration, existing methods typically apply global transformations that treat all predictions uniformly, overlooking the heterogeneous reliability of individual predictions. Furthermore, the relationship between improved calibration and effective uncertainty-aware decision-making remains largely unexplored. This paper presents a post-hoc calibration framework that leverages prediction reliability assessment to jointly enhance calibration quality and uncertainty-aware decision-making. The framework employs proximity-based conformal prediction to stratify calibration samples into putatively correct and putatively incorrect groups based on semantic similarity in feature space. A dual calibration strategy is then applied: standard isotonic regression calibrated confidence in putatively correct predictions, while underconfidence-regularized isotonic regression reduces confidence toward uniform distributions for putatively incorrect predictions, facilitating their identification for further investigations. A comprehensive evaluation is conducted using calibration metrics, uncertainty-aware performance measures, and empirical conformal coverage. Experiments on CIFAR-10 and CIFAR-100 with BiT and CoAtNet backbones show that the proposed method achieves lower confidently incorrect predictions, and competitive Expected Calibration Error compared with isotonic and focal-loss baselines. This work bridges calibration and uncertainty quantification through instance-level adaptivity, offering a practical post-hoc solution that requires no model retraining while improving both probability alignment and uncertainty-aware decision-making.
<div><strong>Authors:</strong> Hassan Gharoun, Mohammad Sadegh Khorshidi, Kasra Ranjbarigderi, Fang Chen, Amir H. Gandomi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a post‑hoc calibration framework that first separates predictions into likely‑correct and likely‑incorrect groups using proximity‑based conformal prediction, then applies two different isotonic regression calibrations—standard for the former and an under‑confidence regularized version for the latter—to reduce confidently wrong outputs while keeping overall calibration competitive. Experiments on CIFAR‑10/100 with BiT and CoAtNet show fewer confidently incorrect predictions and comparable Expected Calibration Error (ECE) relative to isotonic and focal‑loss baselines, without any model retraining. The method links calibration improvement with better uncertainty‑aware decision‑making.", "summary_cn": "本文提出一种后置校准框架，利用基于相似度的(Conformal Prediction)将预测划分为可能正确和可能错误两组，然后对前者使用标准(Isotonic Regression)进行校准，对后者使用降低置信度的正则化(Isotonic Regression)使其趋向均匀分布，从而降低自信错误预测的数量且保持整体校准质量。实验在 CIFAR‑10/100 上使用 BiT 与 CoAtNet 骨干网络，展示了相较于标准等距和 focal‑loss 基线，能够显著减少自信错误预测且在 Expected Calibration Error 上保持竞争力，且无需重新训练模型。该方法将校准提升与更有效的“不确定性感知”决策相结合。", "keywords": "calibration, uncertainty quantification, isotonic, conformal prediction, post-hoc adaptation, confidently incorrect predictions, CIFAR", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"]}
]]></acme>

<pubDate>2025-10-19T23:55:36+00:00</pubDate>
</item>
<item>
<title>NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</title>
<link>https://papers.cool/arxiv/2510.17914</link>
<guid>https://papers.cool/arxiv/2510.17914</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> NeuCo-Bench is a benchmark framework for evaluating lossy neural compression and representation learning using fixed-size embeddings in Earth Observation (EO) tasks. It provides an evaluation pipeline, a hidden-task challenge leaderboard to reduce pretraining bias, and a scoring system that balances accuracy with stability, along with a curated multispectral, multitemporal EO dataset (SSL4EO-S12-downstream). Initial results from a CVPR EARTHVISION workshop challenge and ablations with foundation models are reported.<br /><strong>Summary (CN):</strong> NeuCo-Bench 是一个用于评估地球观测 (EO) 任务中（有损）神经压缩与表征学习的基准框架，采用固定大小的嵌入向量作为任务无关的紧凑表示。它包括可复用的评估管线、用于降低预训练偏差的隐藏任务排行榜以及在准确性与稳定性之间平衡的评分体系，并发布了多光谱、多时相的 EO 数据集 SSL4EO-S12-downstream。论文展示了 2025 年 CVPR EARTHVISION 工作坊挑战赛的初步结果以及对最新基础模型的消融实验。<br /><strong>Keywords:</strong> neural embeddings, Earth observation, benchmark, representation learning, lossy compression, downstream tasks, SSL4EO, challenge leaderboard<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy) neural compression and representation learning in the context of Earth Observation (EO). Our approach builds on fixed-size embeddings that act as compact, task-agnostic representations applicable to a broad range of downstream tasks. NeuCo-Bench comprises three core components: (i) an evaluation pipeline built around reusable embeddings, (ii) a new challenge mode with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii) a scoring system that balances accuracy and stability. To support reproducibility, we release SSL4EO-S12-downstream, a curated multispectral, multitemporal EO dataset. We present initial results from a public challenge at the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art foundation models. NeuCo-Bench provides a first step towards community-driven, standardized evaluation of neural embeddings for EO and beyond.
<div><strong>Authors:</strong> Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "NeuCo-Bench is a benchmark framework for evaluating lossy neural compression and representation learning using fixed-size embeddings in Earth Observation (EO) tasks. It provides an evaluation pipeline, a hidden-task challenge leaderboard to reduce pretraining bias, and a scoring system that balances accuracy with stability, along with a curated multispectral, multitemporal EO dataset (SSL4EO-S12-downstream). Initial results from a CVPR EARTHVISION workshop challenge and ablations with foundation models are reported.", "summary_cn": "NeuCo-Bench 是一个用于评估地球观测 (EO) 任务中（有损）神经压缩与表征学习的基准框架，采用固定大小的嵌入向量作为任务无关的紧凑表示。它包括可复用的评估管线、用于降低预训练偏差的隐藏任务排行榜以及在准确性与稳定性之间平衡的评分体系，并发布了多光谱、多时相的 EO 数据集 SSL4EO-S12-downstream。论文展示了 2025 年 CVPR EARTHVISION 工作坊挑战赛的初步结果以及对最新基础模型的消融实验。", "keywords": "neural embeddings, Earth observation, benchmark, representation learning, lossy compression, downstream tasks, SSL4EO, challenge leaderboard", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rikard Vinge", "Isabelle Wittmann", "Jannik Schneider", "Michael Marszalek", "Luis Gilch", "Thomas Brunschwiler", "Conrad M Albrecht"]}
]]></acme>

<pubDate>2025-10-19T23:47:33+00:00</pubDate>
</item>
<item>
<title>TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education</title>
<link>https://papers.cool/arxiv/2510.17913</link>
<guid>https://papers.cool/arxiv/2510.17913</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents TACLA, a multi-agent system that uses large language models to simulate the three ego states of Transactional Analysis (Parent, Adult, Child). An orchestrator selects which ego state is active based on context and a scripted life story, enabling realistic shifts in student agents during educational interactions, such as conflict escalation and de-escalation. Experiments in a classroom scenario show the system produces credible, psychologically grounded conversations and can model the effects of different teacher strategies.<br /><strong>Summary (CN):</strong> 本文提出 TACLA，这是一种基于大语言模型的多智能体系统，模拟交易分析（Transactional Analysis）的三种自我状态：父母（Parent）、成人（Adult）和儿童（Child）。系统通过一个协调者智能体，根据情境触发和角色的生活脚本决定激活的自我状态，从而在学生智能体的对话中实现真实的自我状态转换，能够表现冲突的升级和缓和。教育场景的实验表明，该方法生成的对话具有较高的可信度和心理真实性，并能评估不同教师干预策略的效果。<br /><strong>Keywords:</strong> Transactional Analysis, multi-agent systems, large language models, ego states, educational simulation, psychological modeling, orchestrator agent, conflict escalation, dialogue credibility<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Monika Zamojska, Jarosław A. Chudziak</div>
Simulating nuanced human social dynamics with Large Language Models (LLMs) remains a significant challenge, particularly in achieving psychological depth and consistent persona behavior crucial for high-fidelity training tools. This paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a novel Multi-Agent architecture designed to overcome these limitations. TACLA integrates core principles of Transactional Analysis (TA) by modeling agents as an orchestrated system of distinct Parent, Adult, and Child ego states, each with its own pattern memory. An Orchestrator Agent prioritizes ego state activation based on contextual triggers and an agent's life script, ensuring psychologically authentic responses. Validated in an educational scenario, TACLA demonstrates realistic ego state shifts in Student Agents, effectively modeling conflict de-escalation and escalation based on different teacher intervention strategies. Evaluation shows high conversational credibility and confirms TACLA's capacity to create dynamic, psychologically-grounded social simulations, advancing the development of effective AI tools for education and beyond.
<div><strong>Authors:</strong> Monika Zamojska, Jarosław A. Chudziak</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents TACLA, a multi-agent system that uses large language models to simulate the three ego states of Transactional Analysis (Parent, Adult, Child). An orchestrator selects which ego state is active based on context and a scripted life story, enabling realistic shifts in student agents during educational interactions, such as conflict escalation and de-escalation. Experiments in a classroom scenario show the system produces credible, psychologically grounded conversations and can model the effects of different teacher strategies.", "summary_cn": "本文提出 TACLA，这是一种基于大语言模型的多智能体系统，模拟交易分析（Transactional Analysis）的三种自我状态：父母（Parent）、成人（Adult）和儿童（Child）。系统通过一个协调者智能体，根据情境触发和角色的生活脚本决定激活的自我状态，从而在学生智能体的对话中实现真实的自我状态转换，能够表现冲突的升级和缓和。教育场景的实验表明，该方法生成的对话具有较高的可信度和心理真实性，并能评估不同教师干预策略的效果。", "keywords": "Transactional Analysis, multi-agent systems, large language models, ego states, educational simulation, psychological modeling, orchestrator agent, conflict escalation, dialogue credibility", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Monika Zamojska", "Jarosław A. Chudziak"]}
]]></acme>

<pubDate>2025-10-19T21:39:12+00:00</pubDate>
</item>
<item>
<title>Interpretability Framework for LLMs in Undergraduate Calculus</title>
<link>https://papers.cool/arxiv/2510.17910</link>
<guid>https://papers.cool/arxiv/2510.17910</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a novel interpretability framework for analyzing large language model (LLM) solutions to undergraduate calculus problems, focusing on extracting reasoning flows, semantically labeling operations, and using prompt‑ablation studies to measure input salience and output stability. Structured metrics such as reasoning complexity, phrase sensitivity, and robustness are applied to real exam data, revealing that LLMs often produce fluent but conceptually flawed solutions that are highly sensitive to prompt phrasing. The framework aims to diagnose reasoning failures, align with curriculum goals, and support the development of transparent, responsible AI‑assisted feedback tools in STEM education.<br /><strong>Summary (CN):</strong> 本文提出一种新颖的可解释性框架，用于分析大型语言模型（LLM）在本科微积分题目中的解题过程，重点是提取推理流程、对操作进行语义标记，并通过提示消融实验评估输入显著性和输出稳健性。作者在真实的微积分考试数据上使用推理复杂度、短语敏感度和稳健性等结构化指标，发现 LLM 虽然语言流畅，却常出现概念性错误，且对提示措辞高度敏感。该框架旨在诊断推理失误、与课程目标保持一致，并为 STEM 教育中的透明、负责任的 AI 辅助反馈工具提供支持。<br /><strong>Keywords:</strong> LLM interpretability, calculus reasoning, symbolic reasoning, prompt ablation, reasoning complexity, AI education, pedagogical alignment, robustness evaluation, semantic decomposition, educational AI safety<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
Large Language Models (LLMs) are increasingly being used in education, yet their correctness alone does not capture the quality, reliability, or pedagogical validity of their problem-solving behavior, especially in mathematics, where multistep logic, symbolic reasoning, and conceptual clarity are critical. Conventional evaluation methods largely focus on final answer accuracy and overlook the reasoning process. To address this gap, we introduce a novel interpretability framework for analyzing LLM-generated solutions using undergraduate calculus problems as a representative domain. Our approach combines reasoning flow extraction and decomposing solutions into semantically labeled operations and concepts with prompt ablation analysis to assess input salience and output stability. Using structured metrics such as reasoning complexity, phrase sensitivity, and robustness, we evaluated the model behavior on real Calculus I to III university exams. Our findings revealed that LLMs often produce syntactically fluent yet conceptually flawed solutions, with reasoning patterns sensitive to prompt phrasing and input variation. This framework enables fine-grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI-assisted feedback tools. This is the first study to offer a structured, quantitative, and pedagogically grounded framework for interpreting LLM reasoning in mathematics education, laying the foundation for the transparent and responsible deployment of AI in STEM learning environments.
<div><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a novel interpretability framework for analyzing large language model (LLM) solutions to undergraduate calculus problems, focusing on extracting reasoning flows, semantically labeling operations, and using prompt‑ablation studies to measure input salience and output stability. Structured metrics such as reasoning complexity, phrase sensitivity, and robustness are applied to real exam data, revealing that LLMs often produce fluent but conceptually flawed solutions that are highly sensitive to prompt phrasing. The framework aims to diagnose reasoning failures, align with curriculum goals, and support the development of transparent, responsible AI‑assisted feedback tools in STEM education.", "summary_cn": "本文提出一种新颖的可解释性框架，用于分析大型语言模型（LLM）在本科微积分题目中的解题过程，重点是提取推理流程、对操作进行语义标记，并通过提示消融实验评估输入显著性和输出稳健性。作者在真实的微积分考试数据上使用推理复杂度、短语敏感度和稳健性等结构化指标，发现 LLM 虽然语言流畅，却常出现概念性错误，且对提示措辞高度敏感。该框架旨在诊断推理失误、与课程目标保持一致，并为 STEM 教育中的透明、负责任的 AI 辅助反馈工具提供支持。", "keywords": "LLM interpretability, calculus reasoning, symbolic reasoning, prompt ablation, reasoning complexity, AI education, pedagogical alignment, robustness evaluation, semantic decomposition, educational AI safety", "scoring": {"interpretability": 8, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sagnik Dakshit", "Sushmita Sinha Roy"]}
]]></acme>

<pubDate>2025-10-19T17:20:36+00:00</pubDate>
</item>
<item>
<title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title>
<link>https://papers.cool/arxiv/2510.17904</link>
<guid>https://papers.cool/arxiv/2510.17904</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BreakFun is a jailbreak technique that exploits LLMs’ tendency to follow structured schemas by embedding a Trojan Schema within a three‑part prompt, achieving up to 100 % attack success on several models. The authors also propose a defense, Adversarial Prompt Deconstruction, which uses a secondary LLM to transcribe the prompt literally and reveal hidden malicious intent, showing strong mitigation performance.<br /><strong>Summary (CN):</strong> BreakFun 利用 LLM 对结构化 schema 的强依赖，在三段式提示中植入 Trojan Schema，实现对模型的越狱攻击，部分模型攻击成功率达 100%。作者提出使用二级 LLM 的 “Literal Transcription” 防御机制，通过字面转录揭示隐藏的有害意图，从而有效减弱此类攻击。<br /><strong>Keywords:</strong> jailbreak, schema exploitation, large language models, adversarial prompting, safety guardrails, chain-of-thought distraction, Trojan schema, prompt engineering<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 8, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.
<div><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BreakFun is a jailbreak technique that exploits LLMs’ tendency to follow structured schemas by embedding a Trojan Schema within a three‑part prompt, achieving up to 100 % attack success on several models. The authors also propose a defense, Adversarial Prompt Deconstruction, which uses a secondary LLM to transcribe the prompt literally and reveal hidden malicious intent, showing strong mitigation performance.", "summary_cn": "BreakFun 利用 LLM 对结构化 schema 的强依赖，在三段式提示中植入 Trojan Schema，实现对模型的越狱攻击，部分模型攻击成功率达 100%。作者提出使用二级 LLM 的 “Literal Transcription” 防御机制，通过字面转录揭示隐藏的有害意图，从而有效减弱此类攻击。", "keywords": "jailbreak, schema exploitation, large language models, adversarial prompting, safety guardrails, chain-of-thought distraction, Trojan schema, prompt engineering", "scoring": {"interpretability": 6, "understanding": 7, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Amirkia Rafiei Oskooei", "Mehmet S. Aktas"]}
]]></acme>

<pubDate>2025-10-19T11:27:44+00:00</pubDate>
</item>
<item>
<title>The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</title>
<link>https://papers.cool/arxiv/2510.17901</link>
<guid>https://papers.cool/arxiv/2510.17901</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Sherpa.ai Blind Vertical Federated Learning (SBVFL) paradigm, which decouples most client updates from the central server to dramatically reduce communication in vertical federated learning. Experiments demonstrate about a 99% reduction in communication compared to standard VFL while preserving model accuracy and robustness across domains such as healthcare, finance, and manufacturing. The approach also enhances privacy and security by limiting data exchange between parties.<br /><strong>Summary (CN):</strong> 本文提出 Sherpa.ai Blind Vertical Federated Learning（SBVFL）范式，通过将大多数客户端更新与中心服务器解耦，大幅降低垂直联邦学习中的通信量。实验显示，与标准 VFL 相比，通信量降低约 99%，同时保持模型准确性和鲁棒性，适用于医疗、金融、制造等敏感领域。该方法通过限制数据交换，进一步提升了隐私和安全性。<br /><strong>Keywords:</strong> vertical federated learning, communication reduction, privacy, SBVFL, distributed training, robustness, federated learning, blind VFL<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</div>
Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications. In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.
<div><strong>Authors:</strong> Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Sherpa.ai Blind Vertical Federated Learning (SBVFL) paradigm, which decouples most client updates from the central server to dramatically reduce communication in vertical federated learning. Experiments demonstrate about a 99% reduction in communication compared to standard VFL while preserving model accuracy and robustness across domains such as healthcare, finance, and manufacturing. The approach also enhances privacy and security by limiting data exchange between parties.", "summary_cn": "本文提出 Sherpa.ai Blind Vertical Federated Learning（SBVFL）范式，通过将大多数客户端更新与中心服务器解耦，大幅降低垂直联邦学习中的通信量。实验显示，与标准 VFL 相比，通信量降低约 99%，同时保持模型准确性和鲁棒性，适用于医疗、金融、制造等敏感领域。该方法通过限制数据交换，进一步提升了隐私和安全性。", "keywords": "vertical federated learning, communication reduction, privacy, SBVFL, distributed training, robustness, federated learning, blind VFL", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Alex Acero", "Daniel M. Jimenez-Gutierrez", "Dario Pighin", "Enrique Zuazua", "Joaquin Del Rio", "Xabi Uribe-Etxebarria"]}
]]></acme>

<pubDate>2025-10-19T10:27:07+00:00</pubDate>
</item>
<item>
<title>Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning</title>
<link>https://papers.cool/arxiv/2510.17900</link>
<guid>https://papers.cool/arxiv/2510.17900</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a benchmark for assessing large language models on Indian legal examinations, covering multiple-choice objective tests and long-form answers graded by lawyers in a blind study. Results show that frontier models meet historical cutoff scores on objective sections but lag behind top human performers on comprehensive legal reasoning tasks, revealing specific failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work outlines where LLMs can assist legal workflows and where human expertise remains indispensable.<br /><strong>Summary (CN):</strong> 本文构建了针对印度法律考试的基准，以评估大型语言模型在客观选择题和由律师盲评的长篇答案中的表现。结果显示，前沿模型能够达标客观题的历史分数线，但在全面法律推理上仍落后于顶尖人类考生，暴露出程序合规性、引用规范以及法庭语言恰当性等三类可靠性失效模式。文章指出了 LLM 可辅助的环节以及仍需人类主导的领域。<br /><strong>Keywords:</strong> legal reasoning, large language models, benchmark, Indian law, exam evaluation, courtroom AI, procedural compliance, citation discipline, AI safety, model assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
Large language models (LLMs) are entering legal workflows, yet we lack a jurisdiction-specific framework to assess their baseline competence therein. We use India's public legal examinations as a transparent proxy. Our multi-year benchmark assembles objective screens from top national and state exams and evaluates open and frontier LLMs under real-world exam conditions. To probe beyond multiple-choice questions, we also include a lawyer-graded, paired-blinded study of long-form answers from the Supreme Court's Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded, India-specific yardstick for LLM court-readiness released with datasets and protocols. Our work shows that while frontier systems consistently clear historical cutoffs and often match or exceed recent top-scorer bands on objective exams, none surpasses the human topper on long-form reasoning. Grader notes converge on three reliability failure modes: procedural or format compliance, authority or citation discipline, and forum-appropriate voice and structure. These findings delineate where LLMs can assist (checks, cross-statute consistency, statute and precedent lookups) and where human leadership remains essential: forum-specific drafting and filing, procedural and relief strategy, reconciling authorities and exceptions, and ethical, accountable judgment.
<div><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a benchmark for assessing large language models on Indian legal examinations, covering multiple-choice objective tests and long-form answers graded by lawyers in a blind study. Results show that frontier models meet historical cutoff scores on objective sections but lag behind top human performers on comprehensive legal reasoning tasks, revealing specific failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work outlines where LLMs can assist legal workflows and where human expertise remains indispensable.", "summary_cn": "本文构建了针对印度法律考试的基准，以评估大型语言模型在客观选择题和由律师盲评的长篇答案中的表现。结果显示，前沿模型能够达标客观题的历史分数线，但在全面法律推理上仍落后于顶尖人类考生，暴露出程序合规性、引用规范以及法庭语言恰当性等三类可靠性失效模式。文章指出了 LLM 可辅助的环节以及仍需人类主导的领域。", "keywords": "legal reasoning, large language models, benchmark, Indian law, exam evaluation, courtroom AI, procedural compliance, citation discipline, AI safety, model assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Kush Juvekar", "Arghya Bhattacharya", "Sai Khadloya", "Utkarsh Saxena"]}
]]></acme>

<pubDate>2025-10-19T10:04:29+00:00</pubDate>
</item>
<item>
<title>Automated Algorithm Design for Auto-Tuning Optimizers</title>
<link>https://papers.cool/arxiv/2510.17899</link>
<guid>https://papers.cool/arxiv/2510.17899</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a framework that uses large language models to automatically generate specialized optimization algorithms for auto-tuning tasks, leveraging problem descriptions and search-space characteristics to create and iteratively improve optimizers. Experiments on four real-world applications across six hardware platforms show that LLM-generated optimizers can achieve up to a 72.4% improvement over state-of-the-art human-designed methods.<br /><strong>Summary (CN):</strong> 本文提出一种框架，利用大型语言模型（LLM）根据问题描述和搜索空间特征自动生成针对自动调优任务的专用优化算法，并通过迭代改进。实验在四个真实应用和六个平台上表明，LLM 生成的优化器相较于最先进的人类设计方法可实现最高约 72.4% 的性能提升。<br /><strong>Keywords:</strong> auto-tuning, optimizer generation, large language model, performance optimization, algorithm design, hardware acceleration, meta-optimization<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</div>
Automatic performance tuning (auto-tuning) is essential for optimizing high-performance applications, where vast and irregular parameter spaces make manual exploration infeasible. Traditionally, auto-tuning relies on well-established optimization algorithms such as evolutionary algorithms, annealing methods, or surrogate model-based optimizers to efficiently find near-optimal configurations. However, designing effective optimizers remains challenging, as no single method performs best across all tuning tasks. In this work, we explore a new paradigm: using large language models (LLMs) to automatically generate optimization algorithms tailored to auto-tuning problems. We introduce a framework that prompts LLMs with problem descriptions and search-space characteristics results to produce specialized optimization strategies, which are iteratively examined and improved. These generated algorithms are evaluated on four real-world auto-tuning applications across six hardware platforms and compared against the state-of-the-art in optimization algorithms of two contemporary auto-tuning frameworks. The evaluation demonstrates that providing additional application- and search space-specific information in the generation stage results in an average performance improvement of 30.7\% and 14.6\%, respectively. In addition, our results show that LLM-generated optimizers can rival, and in various cases outperform, existing human-designed algorithms, with our best-performing generated optimization algorithms achieving, on average, 72.4\% improvement over state-of-the-art optimizers for auto-tuning.
<div><strong>Authors:</strong> Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a framework that uses large language models to automatically generate specialized optimization algorithms for auto-tuning tasks, leveraging problem descriptions and search-space characteristics to create and iteratively improve optimizers. Experiments on four real-world applications across six hardware platforms show that LLM-generated optimizers can achieve up to a 72.4% improvement over state-of-the-art human-designed methods.", "summary_cn": "本文提出一种框架，利用大型语言模型（LLM）根据问题描述和搜索空间特征自动生成针对自动调优任务的专用优化算法，并通过迭代改进。实验在四个真实应用和六个平台上表明，LLM 生成的优化器相较于最先进的人类设计方法可实现最高约 72.4% 的性能提升。", "keywords": "auto-tuning, optimizer generation, large language model, performance optimization, algorithm design, hardware acceleration, meta-optimization", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Floris-Jan Willemsen", "Niki van Stein", "Ben van Werkhoven"]}
]]></acme>

<pubDate>2025-10-19T09:38:15+00:00</pubDate>
</item>
<item>
<title>L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts</title>
<link>https://papers.cool/arxiv/2510.17898</link>
<guid>https://papers.cool/arxiv/2510.17898</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces L‑MoE, a lightweight mixture-of-experts architecture where each expert is a low-rank LoRA adapter. A differentiable gating network learns to compose these adapters by weighting their parameters for each token, enabling end-to-end training of a parameter-efficient, modular language model. The authors present the mathematical formulation, routing mechanism, and joint optimization objective.<br /><strong>Summary (CN):</strong> 本文提出 L‑MoE，一种轻量化的 Mixture of Experts 架构，将每个专家定义为低秩 LoRA 适配器。通过可微分的门控网络对每个 token 的适配器参数进行加权组合，实现端到端训练的高参数效率模块化语言模型。作者给出了数学框架、路由机制以及联合优化目标。<br /><strong>Keywords:</strong> Mixture of Experts, LoRA, low-rank adaptation, parameter-efficient fine-tuning, gating network, modular language model, dynamic composition<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shihao Ji, Zihui Song</div>
The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In this work, we unify these two paradigms into a novel, end-to-end trainable framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines MoE experts not as dense feed-forward networks, but as a collection of task-specialized, low-rank adapters. A lightweight gating network, trained jointly with the experts, learns to dynamically compose these LoRA adapters by computing a weighted average of their parameters for each input token. This composition is fully differentiable, allowing gradients from a standard auto-regressive language modeling objective to flow back through the entire architecture, simultaneously refining both the expert adapters and the routing strategy. This approach creates a highly parameter-efficient MoE model that is modular by design, allows for dynamic skill composition, and is trainable from end-to-end. We present the formal mathematical framework for L-MoE, detailing the differentiable routing mechanism and the joint optimization objective, thereby providing a new path toward building more efficient, scalable, and specialized language models.
<div><strong>Authors:</strong> Shihao Ji, Zihui Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces L‑MoE, a lightweight mixture-of-experts architecture where each expert is a low-rank LoRA adapter. A differentiable gating network learns to compose these adapters by weighting their parameters for each token, enabling end-to-end training of a parameter-efficient, modular language model. The authors present the mathematical formulation, routing mechanism, and joint optimization objective.", "summary_cn": "本文提出 L‑MoE，一种轻量化的 Mixture of Experts 架构，将每个专家定义为低秩 LoRA 适配器。通过可微分的门控网络对每个 token 的适配器参数进行加权组合，实现端到端训练的高参数效率模块化语言模型。作者给出了数学框架、路由机制以及联合优化目标。", "keywords": "Mixture of Experts, LoRA, low-rank adaptation, parameter-efficient fine-tuning, gating network, modular language model, dynamic composition", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shihao Ji", "Zihui Song"]}
]]></acme>

<pubDate>2025-10-19T08:44:25+00:00</pubDate>
</item>
<item>
<title>Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism</title>
<link>https://papers.cool/arxiv/2510.17896</link>
<guid>https://papers.cool/arxiv/2510.17896</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a unified benchmark that evaluates both attention kernel optimizations and distributed context‑parallel mechanisms for long‑context training of transformer models. It assesses performance across diverse attention mask patterns, sequence lengths, and device scales up to 96 GPUs, providing reproducible comparisons and practical design guidance.<br /><strong>Summary (CN):</strong> 本文提出一个统一基准，用于评估长上下文 Transformer 模型的注意力内核优化和分布式上下文并行机制。基准在不同注意力掩码模式、序列长度以及最多 96 张 GPU 的规模下进行性能评估，提供可复现的比较结果并给出实际设计建议。<br /><strong>Keywords:</strong> long-context attention, benchmark, kernel optimization, distributed attention, context parallelism, LLM scalability, GPU clusters<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</div>
Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose a unified benchmark that integrates representative attention kernels and context parallel mechanisms with a modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights method-specific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training.
<div><strong>Authors:</strong> Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a unified benchmark that evaluates both attention kernel optimizations and distributed context‑parallel mechanisms for long‑context training of transformer models. It assesses performance across diverse attention mask patterns, sequence lengths, and device scales up to 96 GPUs, providing reproducible comparisons and practical design guidance.", "summary_cn": "本文提出一个统一基准，用于评估长上下文 Transformer 模型的注意力内核优化和分布式上下文并行机制。基准在不同注意力掩码模式、序列长度以及最多 96 张 GPU 的规模下进行性能评估，提供可复现的比较结果并给出实际设计建议。", "keywords": "long-context attention, benchmark, kernel optimization, distributed attention, context parallelism, LLM scalability, GPU clusters", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tao Bu", "Qiangang Wang", "Bowen Zeng", "Hanwen Sun", "Yunpeng Huang", "Chun Cao", "Jingwei Xu"]}
]]></acme>

<pubDate>2025-10-19T07:07:37+00:00</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17895</link>
<guid>https://papers.cool/arxiv/2510.17895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a hierarchical federated unlearning framework for large language models (LLMs) that enables scalable, privacy‑preserving removal of undesirable knowledge in decentralized settings. By decoupling unlearning from retention through task‑specific adapter learning and employing a hierarchical merging strategy, the method mitigates inter‑ and intra‑domain interference while maintaining model performance, as demonstrated on benchmarks such as WMDP, MUSE and TOFU.<br /><strong>Summary (CN):</strong> 本文提出了一种层次化联邦消除（federated unlearning）框架，用于在分布式环境中对大语言模型（LLM）进行可扩展、隐私保护的知识删除。通过任务特定的适配器学习将消除与保留解耦，并采用层次合并策略减轻域间和域内干扰，同时在 WMDP、MUSE、TOFU 等基准上保持模型效能。<br /><strong>Keywords:</strong> federated unlearning, large language models, privacy, adapter learning, hierarchical merging, continual unlearning, decentralized data<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
<div><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a hierarchical federated unlearning framework for large language models (LLMs) that enables scalable, privacy‑preserving removal of undesirable knowledge in decentralized settings. By decoupling unlearning from retention through task‑specific adapter learning and employing a hierarchical merging strategy, the method mitigates inter‑ and intra‑domain interference while maintaining model performance, as demonstrated on benchmarks such as WMDP, MUSE and TOFU.", "summary_cn": "本文提出了一种层次化联邦消除（federated unlearning）框架，用于在分布式环境中对大语言模型（LLM）进行可扩展、隐私保护的知识删除。通过任务特定的适配器学习将消除与保留解耦，并采用层次合并策略减轻域间和域内干扰，同时在 WMDP、MUSE、TOFU 等基准上保持模型效能。", "keywords": "federated unlearning, large language models, privacy, adapter learning, hierarchical merging, continual unlearning, decentralized data", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yisheng Zhong", "Zhengbang Yang", "Zhuangdi Zhu"]}
]]></acme>

<pubDate>2025-10-19T04:24:51+00:00</pubDate>
</item>
<item>
<title>MIN-Merging: Merge the Important Neurons for Model Merging</title>
<link>https://papers.cool/arxiv/2510.17890</link>
<guid>https://papers.cool/arxiv/2510.17890</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MIN-Merging, a router-based framework that selects and merges the most important neurons from multiple pretrained models to alleviate parameter conflicts when combining them. Experiments on computer vision and natural language processing benchmarks demonstrate that this approach consistently improves in-domain performance while preserving out-of-domain generalization. The results suggest that selective neuron merging is an effective practical solution for model merging.<br /><strong>Summary (CN):</strong> 本文提出 MIN-Merging，一种基于路由器的框架，通过挑选并合并多个预训练模型中最重要的神经元，以减少模型合并时的参数冲突。在计算机视觉和自然语言处理基准上的实验显示，该方法能够在提升领域内任务表现的同时，保持领域外的泛化能力。结果表明，选择性神经元合并是解决模型合并冲突的实用方案。<br /><strong>Keywords:</strong> model merging, neuron importance, router-based framework, parameter conflict, computer vision, natural language processing, neural network integration, transfer learning<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yunfei Liang</div>
Recent advances in deep learning have led to a surge of open-source models across diverse domains. While model merging offers a promising way to combine their strengths, existing approaches often suffer from parameter conflicts that degrade performance on domain-specific tasks. We propose MIN-Merging, a router-based framework that selectively merges the most important neurons to reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent gains on in-domain tasks while retaining the generalization ability of pretrained models on out-of-domain tasks. These results highlight its effectiveness as a practical solution to the parameter conflict problem in model merging.
<div><strong>Authors:</strong> Yunfei Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MIN-Merging, a router-based framework that selects and merges the most important neurons from multiple pretrained models to alleviate parameter conflicts when combining them. Experiments on computer vision and natural language processing benchmarks demonstrate that this approach consistently improves in-domain performance while preserving out-of-domain generalization. The results suggest that selective neuron merging is an effective practical solution for model merging.", "summary_cn": "本文提出 MIN-Merging，一种基于路由器的框架，通过挑选并合并多个预训练模型中最重要的神经元，以减少模型合并时的参数冲突。在计算机视觉和自然语言处理基准上的实验显示，该方法能够在提升领域内任务表现的同时，保持领域外的泛化能力。结果表明，选择性神经元合并是解决模型合并冲突的实用方案。", "keywords": "model merging, neuron importance, router-based framework, parameter conflict, computer vision, natural language processing, neural network integration, transfer learning", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yunfei Liang"]}
]]></acme>

<pubDate>2025-10-18T15:23:36+00:00</pubDate>
</item>
<item>
<title>Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp</title>
<link>https://papers.cool/arxiv/2510.17889</link>
<guid>https://papers.cool/arxiv/2510.17889</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a complete Lisp implemented using a vector‑symbolic architecture based on holographic reduced representations, describing how the five elementary Lisp functions, lambda expressions, and auxiliary operations can be encoded to achieve Turing‑completeness. It demonstrates that such architectures are Cartesian‑closed and highlights the role of cleanup‑memory tables in the representation. The work discusses the mathematical implications and significance of this construction for vector‑symbolic computing.<br /><strong>Summary (CN):</strong> 本文展示了利用全息降维表征（HRR）的向量符号架构实现完整的 Lisp，包括五个基本函数、lambda 表达式及其它辅助函数的向量表示，从而实现图灵完备。作者证明了该架构的笛卡尔闭合性并强调了查找表清理记忆在规格说明中的重要性。文中讨论了这一构造的数学意义及其在向量符号计算中的意义。<br /><strong>Keywords:</strong> vector-symbolic architecture, holographic reduced representations, Lisp, Cartesian-closed category, cleanup memory, Turing-completeness, symbolic reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Eilene Tomkins-Flanagan, Mary A. Kelly</div>
Kanerva (2014) suggested that it would be possible to construct a complete Lisp out of a vector-symbolic architecture. We present the general form of a vector-symbolic representation of the five Lisp elementary functions, lambda expressions, and other auxiliary functions, found in the Lisp 1.5 specification McCarthy (1960), which is near minimal and sufficient for Turing-completeness. Our specific implementation uses holographic reduced representations Plate (1995), with a lookup table cleanup memory. Lisp, as all Turing-complete languages, is a Cartesian closed category, unusual in its proximity to the mathematical abstraction. We discuss the mathematics, the purpose, and the significance of demonstrating vector-symbolic architectures' Cartesian-closure, as well as the importance of explicitly including cleanup memories in the specification of the architecture.
<div><strong>Authors:</strong> Eilene Tomkins-Flanagan, Mary A. Kelly</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a complete Lisp implemented using a vector‑symbolic architecture based on holographic reduced representations, describing how the five elementary Lisp functions, lambda expressions, and auxiliary operations can be encoded to achieve Turing‑completeness. It demonstrates that such architectures are Cartesian‑closed and highlights the role of cleanup‑memory tables in the representation. The work discusses the mathematical implications and significance of this construction for vector‑symbolic computing.", "summary_cn": "本文展示了利用全息降维表征（HRR）的向量符号架构实现完整的 Lisp，包括五个基本函数、lambda 表达式及其它辅助函数的向量表示，从而实现图灵完备。作者证明了该架构的笛卡尔闭合性并强调了查找表清理记忆在规格说明中的重要性。文中讨论了这一构造的数学意义及其在向量符号计算中的意义。", "keywords": "vector-symbolic architecture, holographic reduced representations, Lisp, Cartesian-closed category, cleanup memory, Turing-completeness, symbolic reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Eilene Tomkins-Flanagan", "Mary A. Kelly"]}
]]></acme>

<pubDate>2025-10-18T14:42:36+00:00</pubDate>
</item>
<item>
<title>Metrics and evaluations for computational and sustainable AI efficiency</title>
<link>https://papers.cool/arxiv/2510.17885</link>
<guid>https://papers.cool/arxiv/2510.17885</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified, reproducible methodology for evaluating AI model inference that combines computational and environmental metrics such as latency, throughput, energy use, and location-adjusted carbon emissions while keeping accuracy constant. It demonstrates the framework across a range of hardware (e.g., GH200, RTX 4090) and software stacks (PyTorch, TensorRT, ONNX Runtime), delivering Pareto frontiers that clarify trade‑offs between accuracy, speed, energy, and carbon impact, and provides open‑source tools for independent verification.<br /><strong>Summary (CN):</strong> 本文提出了一套统一且可复现的 AI 推理评估方法，将计算指标（延迟、吞吐量）与环境指标（能耗、地点校正的碳排放）结合，并在保持准确率不变的前提下进行比较。作者在多种硬件（如 GH200、RTX 4090）和软件栈（PyTorch、TensorRT、ONNX Runtime）上验证该框架，生成展示准确率、速度、能耗和碳排放权衡的 Pareto 前沿，并提供开源代码以便独立验证。<br /><strong>Keywords:</strong> AI efficiency, carbon emissions, energy consumption, latency, throughput, benchmarking, sustainable AI, multi-precision, hardware evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
The rapid advancement of Artificial Intelligence (AI) has created unprecedented demands for computational power, yet methods for evaluating the performance, efficiency, and environmental impact of deployed models remain fragmented. Current approaches often fail to provide a holistic view, making it difficult to compare and optimise systems across heterogeneous hardware, software stacks, and numeric precisions. To address this gap, we propose a unified and reproducible methodology for AI model inference that integrates computational and environmental metrics under realistic serving conditions. Our framework provides a pragmatic, carbon-aware evaluation by systematically measuring latency and throughput distributions, energy consumption, and location-adjusted carbon emissions, all while maintaining matched accuracy constraints for valid comparisons. We apply this methodology to multi-precision models across diverse hardware platforms, from data-centre accelerators like the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream software stacks including PyTorch, TensorRT, and ONNX Runtime. By systematically categorising these factors, our work establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying the trade-offs between accuracy, latency, energy, and carbon. The accompanying open-source code enables independent verification and facilitates adoption, empowering researchers and practitioners to make evidence-based decisions for sustainable AI deployment.
<div><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified, reproducible methodology for evaluating AI model inference that combines computational and environmental metrics such as latency, throughput, energy use, and location-adjusted carbon emissions while keeping accuracy constant. It demonstrates the framework across a range of hardware (e.g., GH200, RTX 4090) and software stacks (PyTorch, TensorRT, ONNX Runtime), delivering Pareto frontiers that clarify trade‑offs between accuracy, speed, energy, and carbon impact, and provides open‑source tools for independent verification.", "summary_cn": "本文提出了一套统一且可复现的 AI 推理评估方法，将计算指标（延迟、吞吐量）与环境指标（能耗、地点校正的碳排放）结合，并在保持准确率不变的前提下进行比较。作者在多种硬件（如 GH200、RTX 4090）和软件栈（PyTorch、TensorRT、ONNX Runtime）上验证该框架，生成展示准确率、速度、能耗和碳排放权衡的 Pareto 前沿，并提供开源代码以便独立验证。", "keywords": "AI efficiency, carbon emissions, energy consumption, latency, throughput, benchmarking, sustainable AI, multi-precision, hardware evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}
]]></acme>

<pubDate>2025-10-18T03:30:15+00:00</pubDate>
</item>
<item>
<title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title>
<link>https://papers.cool/arxiv/2510.17884</link>
<guid>https://papers.cool/arxiv/2510.17884</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper empirically evaluates the ability of several open-source large language models (TinyLLaMA, Falcon-RW-1B, Flan‑T5) to guess passwords from synthetic user profiles, finding that all models achieve less than 1.5% Hit@10 and far underperform traditional rule‑based cracking methods. The authors analyze the generative reasoning limitations of LLMs in this security‑critical task and discuss implications for adversarial misuse and the need for more robust, privacy‑preserving password modeling.<br /><strong>Summary (CN):</strong> 本文对多款开源大语言模型（TinyLLaMA、Falcon‑RW‑1B、Flan‑T5）在基于合成用户属性的密码猜测任务上的表现进行实验评估，结果显示所有模型的 Hit@10 低于 1.5%，远逊于传统规则/组合破解方法。作者分析了 LLM 在此安全敏感任务中的生成推理局限，并探讨了对抗性滥用的影响以及实现更稳健、隐私保护的密码建模的必要性。<br /><strong>Keywords:</strong> password cracking, large language models, cybersecurity, adversarial misuse, synthetic user profiles, Hit@k evaluation, model robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</div>
The remarkable capabilities of Large Language Models (LLMs) in natural language understanding and generation have sparked interest in their potential for cybersecurity applications, including password guessing. In this study, we conduct an empirical investigation into the efficacy of pre-trained LLMs for password cracking using synthetic user profiles. Specifically, we evaluate the performance of state-of-the-art open-source LLMs such as TinyLLaMA, Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords based on structured user attributes (e.g., name, birthdate, hobbies). Our results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext and SHA-256 hash comparisons, reveal consistently poor performance, with all models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional rule-based and combinator-based cracking methods demonstrate significantly higher success rates. Through detailed analysis and visualization, we identify key limitations in the generative reasoning of LLMs when applied to the domain-specific task of password guessing. Our findings suggest that, despite their linguistic prowess, current LLMs lack the domain adaptation and memorization capabilities required for effective password inference, especially in the absence of supervised fine-tuning on leaked password datasets. This study provides critical insights into the limitations of LLMs in adversarial contexts and lays the groundwork for future efforts in secure, privacy-preserving, and robust password modeling.
<div><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper empirically evaluates the ability of several open-source large language models (TinyLLaMA, Falcon-RW-1B, Flan‑T5) to guess passwords from synthetic user profiles, finding that all models achieve less than 1.5% Hit@10 and far underperform traditional rule‑based cracking methods. The authors analyze the generative reasoning limitations of LLMs in this security‑critical task and discuss implications for adversarial misuse and the need for more robust, privacy‑preserving password modeling.", "summary_cn": "本文对多款开源大语言模型（TinyLLaMA、Falcon‑RW‑1B、Flan‑T5）在基于合成用户属性的密码猜测任务上的表现进行实验评估，结果显示所有模型的 Hit@10 低于 1.5%，远逊于传统规则/组合破解方法。作者分析了 LLM 在此安全敏感任务中的生成推理局限，并探讨了对抗性滥用的影响以及实现更稳健、隐私保护的密码建模的必要性。", "keywords": "password cracking, large language models, cybersecurity, adversarial misuse, synthetic user profiles, Hit@k evaluation, model robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Mohammad Abdul Rehman", "Syed Imad Ali Shah", "Abbas Anwar", "Noor Islam"]}
]]></acme>

<pubDate>2025-10-18T02:15:28+00:00</pubDate>
</item>
<item>
<title>From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15</title>
<link>https://papers.cool/arxiv/2510.17883</link>
<guid>https://papers.cool/arxiv/2510.17883</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper investigates whether zero‑ and few‑shot large language models can detect network intrusions by converting raw network flows into compact textual records enriched with lightweight, domain‑inspired boolean flags. Using a grammar‑constrained, calibrated prompting pipeline, the authors compare zero‑shot, instruction‑guided, and few‑shot prompting against strong tabular and neural baselines on the UNSW‑NB15 dataset, reporting accuracy, precision, recall, F1 and macro scores. Results show that instruction‑guided prompting with flags substantially improves detection quality, though performance degrades as the evaluation set grows, highlighting sensitivity to coverage and prompting strategy.<br /><strong>Summary (CN):</strong> 本文探讨了在不进行微调的情况下，零‑shot 和 few‑shot 大语言模型能否通过将网络流转换为紧凑的文本记录并加入轻量布尔特征（如不对称、突发率、TTL 异常等）来检测网络入侵。采用基于语法约束的校准提示方法，作者在 UNSW‑NB15 数据集上比较了不同提示方式与强基线的准确率、精确率、召回率、F1 和宏分数。实验表明，指令引导加特征的提示显著提升检测效果，但随评估规模扩大性能下降，揭示了覆盖率和提示敏感性的问题。<br /><strong>Keywords:</strong> zero-shot LLM, few-shot prompting, intrusion detection, network flow to text, grammar-constrained prompting, calibration, UNSW-NB15, boolean flags, cybersecurity<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas n=Anwar, Noor Islam</div>
Large Language Models (LLMs) can reason over natural-language inputs, but their role in intrusion detection without fine-tuning remains uncertain. This study evaluates a prompt-only approach on UNSW-NB15 by converting each network flow to a compact textual record and augmenting it with lightweight, domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer anomalies, rare service/state, short bursts). To reduce output drift and support measurement, the model is constrained to produce structured, grammar-valid responses, and a single decision threshold is calibrated on a small development split. We compare zero-shot, instruction-guided, and few-shot prompting to strong tabular and neural baselines under identical splits, reporting accuracy, precision, recall, F1, and macro scores. Empirically, unguided prompting is unreliable, while instructions plus flags substantially improve detection quality; adding calibrated scoring further stabilizes results. On a balanced subset of two hundred flows, a 7B instruction-tuned model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot cues and calibration attains F1 near 0.68 on one thousand examples. As the evaluation set grows to two thousand flows, decision quality decreases, revealing sensitivity to coverage and prompting. Tabular baselines remain more stable and faster, yet the prompt-only pipeline requires no gradient training, produces readable artifacts, and adapts easily through instructions and flags. Contributions include a flow-to-text protocol with interpretable cues, a calibration method for thresholding, a systematic baseline comparison, and a reproducibility bundle with prompts, grammar, metrics, and figures.
<div><strong>Authors:</strong> Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas n=Anwar, Noor Islam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper investigates whether zero‑ and few‑shot large language models can detect network intrusions by converting raw network flows into compact textual records enriched with lightweight, domain‑inspired boolean flags. Using a grammar‑constrained, calibrated prompting pipeline, the authors compare zero‑shot, instruction‑guided, and few‑shot prompting against strong tabular and neural baselines on the UNSW‑NB15 dataset, reporting accuracy, precision, recall, F1 and macro scores. Results show that instruction‑guided prompting with flags substantially improves detection quality, though performance degrades as the evaluation set grows, highlighting sensitivity to coverage and prompting strategy.", "summary_cn": "本文探讨了在不进行微调的情况下，零‑shot 和 few‑shot 大语言模型能否通过将网络流转换为紧凑的文本记录并加入轻量布尔特征（如不对称、突发率、TTL 异常等）来检测网络入侵。采用基于语法约束的校准提示方法，作者在 UNSW‑NB15 数据集上比较了不同提示方式与强基线的准确率、精确率、召回率、F1 和宏分数。实验表明，指令引导加特征的提示显著提升检测效果，但随评估规模扩大性能下降，揭示了覆盖率和提示敏感性的问题。", "keywords": "zero-shot LLM, few-shot prompting, intrusion detection, network flow to text, grammar-constrained prompting, calibration, UNSW-NB15, boolean flags, cybersecurity", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Mohammad Abdul Rehman", "Syed Imad Ali Shah", "Abbas n=Anwar", "Noor Islam"]}
]]></acme>

<pubDate>2025-10-18T02:11:50+00:00</pubDate>
</item>
<item>
<title>Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints</title>
<link>https://papers.cool/arxiv/2510.17882</link>
<guid>https://papers.cool/arxiv/2510.17882</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a large-scale empirical analysis of over 2.1 million preprints from 2016 to 2025 to investigate how generative large language models (LLMs) affect scholarly publishing. Using interrupted time‑series, collaboration metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission and revision cycles, modestly increase linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields, thereby acting as selective catalysts rather than universal disruptors.<br /><strong>Summary (CN):</strong> 本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，探讨生成式大语言模型（LLM）对学术出版的影响。通过中断时间序列、合作与生产力指标、语言特征分析和主题建模等方法，发现 LLM 加快了提交和修订周期，略微提升了语言复杂度，并在计算密集型领域显著扩大了人工智能相关主题的比例，表现出选择性催化而非全局颠覆的特征。<br /><strong>Keywords:</strong> generative AI, preprints, scholarly publishing, large language models, linguistic analysis, topic modeling, academic impact, governance<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
Preprint repositories become central infrastructures for scholarly communication. Their expansion transforms how research is circulated and evaluated before journal publication. Generative large language models (LLMs) introduce a further potential disruption by altering how manuscripts are written. While speculation abounds, systematic evidence of whether and how LLMs reshape scientific publishing remains limited. This paper addresses the gap through a large-scale analysis of more than 2.1 million preprints spanning 2016--2025 (115 months) across four major repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a multi-level analytical framework that integrates interrupted time-series models, collaboration and productivity metrics, linguistic profiling, and topic modeling to assess changes in volume, authorship, style, and disciplinary orientation. Our findings reveal that LLMs have accelerated submission and revision cycles, modestly increased linguistic complexity, and disproportionately expanded AI-related topics, while computationally intensive fields benefit more than others. These results show that LLMs act less as universal disruptors than as selective catalysts, amplifying existing strengths and widening disciplinary divides. By documenting these dynamics, the paper provides the first empirical foundation for evaluating the influence of generative AI on academic publishing and highlights the need for governance frameworks that preserve trust, fairness, and accountability in an AI-enabled research ecosystem.
<div><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a large-scale empirical analysis of over 2.1 million preprints from 2016 to 2025 to investigate how generative large language models (LLMs) affect scholarly publishing. Using interrupted time‑series, collaboration metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission and revision cycles, modestly increase linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields, thereby acting as selective catalysts rather than universal disruptors.", "summary_cn": "本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，探讨生成式大语言模型（LLM）对学术出版的影响。通过中断时间序列、合作与生产力指标、语言特征分析和主题建模等方法，发现 LLM 加快了提交和修订周期，略微提升了语言复杂度，并在计算密集型领域显著扩大了人工智能相关主题的比例，表现出选择性催化而非全局颠覆的特征。", "keywords": "generative AI, preprints, scholarly publishing, large language models, linguistic analysis, topic modeling, academic impact, governance", "scoring": {"interpretability": 1, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Minfeng Qi", "Zhongmin Cao", "Qin Wang", "Ningran Li", "Tianqing Zhu"]}
]]></acme>

<pubDate>2025-10-18T01:37:40+00:00</pubDate>
</item>
<item>
<title>POPI: Personalizing LLMs via Optimized Natural Language Preference Inference</title>
<link>https://papers.cool/arxiv/2510.17881</link>
<guid>https://papers.cool/arxiv/2510.17881</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> POPI proposes a framework that learns a preference inference model to convert heterogeneous user signals into concise natural language summaries, which are then used to condition a shared LLM for personalized generation. The system jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy while drastically reducing context size, and the learned summaries can be transferred to frozen off‑the‑shelf models without weight updates.<br /><strong>Summary (CN):</strong> POPI提出一个框架，通过偏好推断模型将多样的用户信号提炼为简洁的自然语言摘要，再将这些摘要作为条件输入共享的大语言模型，实现个性化生成。该框架在强化学习下同时优化推断模型和生成模型，显著提升个性化准确率并大幅降低上下文开销，且生成的摘要可以直接迁移到冻结的现成 LLM，做到无需权重更新的即插即用个性化。<br /><strong>Keywords:</strong> personalized LLM, preference inference, natural language summary, reinforcement learning, plug-and-play personalization, RLHF, DPO, user preference modeling<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.
<div><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "POPI proposes a framework that learns a preference inference model to convert heterogeneous user signals into concise natural language summaries, which are then used to condition a shared LLM for personalized generation. The system jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy while drastically reducing context size, and the learned summaries can be transferred to frozen off‑the‑shelf models without weight updates.", "summary_cn": "POPI提出一个框架，通过偏好推断模型将多样的用户信号提炼为简洁的自然语言摘要，再将这些摘要作为条件输入共享的大语言模型，实现个性化生成。该框架在强化学习下同时优化推断模型和生成模型，显著提升个性化准确率并大幅降低上下文开销，且生成的摘要可以直接迁移到冻结的现成 LLM，做到无需权重更新的即插即用个性化。", "keywords": "personalized LLM, preference inference, natural language summary, reinforcement learning, plug-and-play personalization, RLHF, DPO, user preference modeling", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yizhuo Chen", "Xin Liu", "Ruijie Wang", "Zheng Li", "Pei Chen", "Changlong Yu", "Priyanka Nigam", "Meng Jiang", "Bing Yin"]}
]]></acme>

<pubDate>2025-10-17T23:07:57+00:00</pubDate>
</item>
<item>
<title>Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</title>
<link>https://papers.cool/arxiv/2510.17880</link>
<guid>https://papers.cool/arxiv/2510.17880</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models (LLMs) use emotion to guide fairness enforcement by conducting a large-scale study of altruistic third‑party punishment, comparing over 4,000 LLM agents to human participants. It finds that LLMs often prioritize emotion over monetary cost, punishing unfairness in an almost all‑or‑none fashion, while humans balance fairness with cost, and that prompting self‑reported emotion increases punishment. The authors suggest that LLM moral decision‑making follows a developmental trajectory and call for future models to integrate emotion with context‑sensitive reasoning for safer alignment.<br /><strong>Summary (CN):</strong> 本文通过大规模的第三方惩罚实验，比较了 4,068 个大语言模型（LLM）代理与 1,159 名成人在公平执​​行中的决策，探讨了 LLM 是否像人类一样受情绪驱动。研究发现，LLM 往往在面对不公平时优先考虑情绪而非成本，以几乎全有或全无的方式进行惩罚，而人类会在公平与成本之间进行权衡，并且提示模型报告情绪会因果性地提升惩罚力度。作者认为 LLM 的道德决策呈现类似人类发展的轨迹，呼吁未来模型将情绪与情境推理相结合，以实现更安全的对齐。<br /><strong>Keywords:</strong> emotion, fairness, large language models, third-party punishment, cost-sensitivity, moral decision, AI alignment, safety, LLM behavior, human comparison<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.
<div><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models (LLMs) use emotion to guide fairness enforcement by conducting a large-scale study of altruistic third‑party punishment, comparing over 4,000 LLM agents to human participants. It finds that LLMs often prioritize emotion over monetary cost, punishing unfairness in an almost all‑or‑none fashion, while humans balance fairness with cost, and that prompting self‑reported emotion increases punishment. The authors suggest that LLM moral decision‑making follows a developmental trajectory and call for future models to integrate emotion with context‑sensitive reasoning for safer alignment.", "summary_cn": "本文通过大规模的第三方惩罚实验，比较了 4,068 个大语言模型（LLM）代理与 1,159 名成人在公平执​​行中的决策，探讨了 LLM 是否像人类一样受情绪驱动。研究发现，LLM 往往在面对不公平时优先考虑情绪而非成本，以几乎全有或全无的方式进行惩罚，而人类会在公平与成本之间进行权衡，并且提示模型报告情绪会因果性地提升惩罚力度。作者认为 LLM 的道德决策呈现类似人类发展的轨迹，呼吁未来模型将情绪与情境推理相结合，以实现更安全的对齐。", "keywords": "emotion, fairness, large language models, third-party punishment, cost-sensitivity, moral decision, AI alignment, safety, LLM behavior, human comparison", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"]}
]]></acme>

<pubDate>2025-10-17T08:41:36+00:00</pubDate>
</item>
<item>
<title>Decoding Listeners Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer</title>
<link>https://papers.cool/arxiv/2510.17879</link>
<guid>https://papers.cool/arxiv/2510.17879</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a lightweight spiking transformer built on spiking neural networks for EEG-based person identification, aiming to achieve high accuracy with low energy consumption. Experiments on the EEG-Music Emotion Recognition Challenge dataset report 100% classification accuracy while using less than 10% of the energy required by traditional deep neural networks. The approach demonstrates a promising direction for efficient brain‑computer interfaces and secure biometric authentication.<br /><strong>Summary (CN):</strong> 本文提出一种基于脉冲神经网络（SNN）的轻量级脉冲Transformer，用于EEG信号的人体身份识别，以实现高精度和低能耗。 在EEG‑Music情感识别挑战数据集上的实验显示，该模型在保持传统深度网络不足10%能耗的情况下取得100%分类准确率。 该方法为高效脑-机接口和安全生物特征认证提供了有前景的方向。<br /><strong>Keywords:</strong> EEG, person identification, spiking neural networks, spiking transformer, energy-efficient, brain-computer interface, biometrics, lightweight model, temporal dynamics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zheyuan Lin, Siqi Cai, Haizhou Li</div>
EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs. The source code is available at https://github.com/PatrickZLin/Decode-ListenerIdentity.
<div><strong>Authors:</strong> Zheyuan Lin, Siqi Cai, Haizhou Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a lightweight spiking transformer built on spiking neural networks for EEG-based person identification, aiming to achieve high accuracy with low energy consumption. Experiments on the EEG-Music Emotion Recognition Challenge dataset report 100% classification accuracy while using less than 10% of the energy required by traditional deep neural networks. The approach demonstrates a promising direction for efficient brain‑computer interfaces and secure biometric authentication.", "summary_cn": "本文提出一种基于脉冲神经网络（SNN）的轻量级脉冲Transformer，用于EEG信号的人体身份识别，以实现高精度和低能耗。 在EEG‑Music情感识别挑战数据集上的实验显示，该模型在保持传统深度网络不足10%能耗的情况下取得100%分类准确率。 该方法为高效脑-机接口和安全生物特征认证提供了有前景的方向。", "keywords": "EEG, person identification, spiking neural networks, spiking transformer, energy-efficient, brain-computer interface, biometrics, lightweight model, temporal dynamics", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zheyuan Lin", "Siqi Cai", "Haizhou Li"]}
]]></acme>

<pubDate>2025-10-17T08:20:01+00:00</pubDate>
</item>
<item>
<title>DRL-Based Resource Allocation for Energy-Efficient IRS-Assisted UAV Spectrum Sharing Systems</title>
<link>https://papers.cool/arxiv/2510.17877</link>
<guid>https://papers.cool/arxiv/2510.17877</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a deep reinforcement learning (DRL) actor‑critic framework to jointly optimize beamforming, subcarrier allocation, IRS phase shifts, and UAV trajectory for maximizing the energy efficiency of a secondary network in an IRS‑assisted UAV spectrum sharing system with OFDM. By incorporating a realistic propulsion‑energy model and its upper bound, the method transforms a highly non‑convex, time‑coupled problem into a tractable DRL formulation, showing significant EE gains over benchmark schemes in extensive simulations.<br /><strong>Summary (CN):</strong> 本文提出了一种基于深度强化学习（DRL）演员‑评论家框架的方案，联合优化波束成形、子载波分配、IRS 相位移以及 UAV 轨迹，以最大化 IRS‑辅助 UAV 频谱共享系统中次级网络的能效（EE），并采用 OFDM 技术。通过引入实际的推进能耗模型及其上界，将高度非凸、时序耦合的资源分配问题转化为可求解的 DRL 形式，实验表明该方法相较于多个基准方案显著提升能效。<br /><strong>Keywords:</strong> IRS, UAV, spectrum sharing, energy efficiency, deep reinforcement learning, resource allocation, OFDM, trajectory optimization<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yiheng Wang</div>
Intelligent reflecting surface (IRS) assisted unmanned aerial vehicle (UAV) systems provide a new paradigm for reconfigurable and flexible wireless communications. To enable more energy efficient and spectrum efficient IRS assisted UAV wireless communications, this paper introduces a novel IRS-assisted UAV enabled spectrum sharing system with orthogonal frequency division multiplexing (OFDM). The goal is to maximize the energy efficiency (EE) of the secondary network by jointly optimizing the beamforming, subcarrier allocation, IRS phase shifts, and the UAV trajectory subject to practical transmit power and passive reflection constraints as well as UAV physical limitations. A physically grounded propulsion-energy model is adopted, with its tight upper bound used to form a tractable EE lower bound for the spectrum sharing system. To handle highly non convex, time coupled optimization problems with a mixed continuous and discrete policy space, we develop a deep reinforcement learning (DRL) approach based on the actor critic framework. Extended experiments show the significant EE improvement of the proposed DRL-based approach compared to several benchmark schemes, thus demonstrating the effectiveness and robustness of the proposed approach with mobility.
<div><strong>Authors:</strong> Yiheng Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a deep reinforcement learning (DRL) actor‑critic framework to jointly optimize beamforming, subcarrier allocation, IRS phase shifts, and UAV trajectory for maximizing the energy efficiency of a secondary network in an IRS‑assisted UAV spectrum sharing system with OFDM. By incorporating a realistic propulsion‑energy model and its upper bound, the method transforms a highly non‑convex, time‑coupled problem into a tractable DRL formulation, showing significant EE gains over benchmark schemes in extensive simulations.", "summary_cn": "本文提出了一种基于深度强化学习（DRL）演员‑评论家框架的方案，联合优化波束成形、子载波分配、IRS 相位移以及 UAV 轨迹，以最大化 IRS‑辅助 UAV 频谱共享系统中次级网络的能效（EE），并采用 OFDM 技术。通过引入实际的推进能耗模型及其上界，将高度非凸、时序耦合的资源分配问题转化为可求解的 DRL 形式，实验表明该方法相较于多个基准方案显著提升能效。", "keywords": "IRS, UAV, spectrum sharing, energy efficiency, deep reinforcement learning, resource allocation, OFDM, trajectory optimization", "scoring": {"interpretability": 1, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yiheng Wang"]}
]]></acme>

<pubDate>2025-10-17T04:18:17+00:00</pubDate>
</item>
<item>
<title>3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</title>
<link>https://papers.cool/arxiv/2510.17875</link>
<guid>https://papers.cool/arxiv/2510.17875</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a 3D weakly supervised semantic segmentation framework that improves pseudo-label quality by first applying a class-aware label refinement module and then a geometry-aware refinement step that enforces 3D geometric plausibility, followed by a self‑training based label update strategy to expand coverage. Experiments on ScanNet and S3DIS demonstrate state‑of‑the‑art performance and good generalisation in unsupervised settings.<br /><strong>Summary (CN):</strong> 本文提出了一种 3D 弱监督语义分割方法，先通过类别感知标签细化模块提升伪标签的平衡性和准确性，再利用几何感知细化组件融合 3D 几何约束过滤不符合几何合理性的低置信度伪标签，并通过自训练的标签更新策略将标签传播到未标注区域，从而提升伪标签质量和覆盖率。实验在 ScanNet 与 S3DIS 数据集上实现了最先进的性能，并在无监督设置下展现出良好的泛化能力。<br /><strong>Keywords:</strong> 3D weakly supervised semantic segmentation, pseudo-label refinement, class-aware guidance, geometry-aware constraints, self-training, ScanNet, S3DIS<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</div>
3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.
<div><strong>Authors:</strong> Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a 3D weakly supervised semantic segmentation framework that improves pseudo-label quality by first applying a class-aware label refinement module and then a geometry-aware refinement step that enforces 3D geometric plausibility, followed by a self‑training based label update strategy to expand coverage. Experiments on ScanNet and S3DIS demonstrate state‑of‑the‑art performance and good generalisation in unsupervised settings.", "summary_cn": "本文提出了一种 3D 弱监督语义分割方法，先通过类别感知标签细化模块提升伪标签的平衡性和准确性，再利用几何感知细化组件融合 3D 几何约束过滤不符合几何合理性的低置信度伪标签，并通过自训练的标签更新策略将标签传播到未标注区域，从而提升伪标签质量和覆盖率。实验在 ScanNet 与 S3DIS 数据集上实现了最先进的性能，并在无监督设置下展现出良好的泛化能力。", "keywords": "3D weakly supervised semantic segmentation, pseudo-label refinement, class-aware guidance, geometry-aware constraints, self-training, ScanNet, S3DIS", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xiaoxu Xu", "Xuexun Liu", "Jinlong Li", "Yitian Yuan", "Qiudan Zhang", "Lin Ma", "Nicu Sebe", "Xu Wang"]}
]]></acme>

<pubDate>2025-10-17T03:53:43+00:00</pubDate>
</item>
<item>
<title>Repairing Tool Calls Using Post-tool Execution Reflection and RAG</title>
<link>https://papers.cool/arxiv/2510.17874</link>
<guid>https://papers.cool/arxiv/2510.17874</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a post‑tool execution reflection component that combines large language model‑based reasoning with domain‑specific retrieval‑augmented generation to fix syntactic and semantic errors in tool calls, focusing on the kubectl command line tool. Empirical results show that the RAG‑based reflection improves successful execution rates for a majority of evaluated models and increases correct user query answers, with troubleshooting documents offering additional performance gains over official documentation.<br /><strong>Summary (CN):</strong> 本文提出了一种后工具执行反思模块，融合大型语言模型推理和针对特定工具的检索增强生成（RAG），用于修复工具调用中的语法和语义错误，实验聚焦于 kubectl 命令行工具。实验证明，相比仅使用官方文档，利用故障排除文档的 RAG 反思能够显著提升模型的执行成功率和用户查询的正确回答率。<br /><strong>Keywords:</strong> tool call repair, reflection, retrieval-augmented generation, kubectl, LLM, agentic systems, troubleshooting documents<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Jason Tsay, Zidane Wright, Gaodan Fang, Kiran Kate, Saurabh Jha, Yara Rizk</div>
Agentic systems interact with external systems by calling tools such as Python functions, REST API endpoints, or command line tools such as kubectl in Kubernetes. These tool calls often fail for various syntactic and semantic reasons. Some less obvious semantic errors can only be identified and resolved after analyzing the tool's response. To repair these errors, we develop a post-tool execution reflection component that combines large language model (LLM)-based reflection with domain-specific retrieval-augmented generation (RAG) using documents describing both the specific tool being called and troubleshooting documents related to the tool. For this paper, we focus on the use case of the kubectl command line tool to manage Kubernetes, a platform for orchestrating cluster applications. Through a larger empirical study and a smaller manual evaluation, we find that our RAG-based reflection will repair kubectl commands such that they are both more likely to successfully execute (pass rate) for 55% of our models evaluated and 36% more likely to correctly answer the user query on average. We find that troubleshooting documents improve pass rate compared to official documentation by an average of 10%.
<div><strong>Authors:</strong> Jason Tsay, Zidane Wright, Gaodan Fang, Kiran Kate, Saurabh Jha, Yara Rizk</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a post‑tool execution reflection component that combines large language model‑based reasoning with domain‑specific retrieval‑augmented generation to fix syntactic and semantic errors in tool calls, focusing on the kubectl command line tool. Empirical results show that the RAG‑based reflection improves successful execution rates for a majority of evaluated models and increases correct user query answers, with troubleshooting documents offering additional performance gains over official documentation.", "summary_cn": "本文提出了一种后工具执行反思模块，融合大型语言模型推理和针对特定工具的检索增强生成（RAG），用于修复工具调用中的语法和语义错误，实验聚焦于 kubectl 命令行工具。实验证明，相比仅使用官方文档，利用故障排除文档的 RAG 反思能够显著提升模型的执行成功率和用户查询的正确回答率。", "keywords": "tool call repair, reflection, retrieval-augmented generation, kubectl, LLM, agentic systems, troubleshooting documents", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Jason Tsay", "Zidane Wright", "Gaodan Fang", "Kiran Kate", "Saurabh Jha", "Yara Rizk"]}
]]></acme>

<pubDate>2025-10-17T03:50:37+00:00</pubDate>
</item>
<item>
<title>Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach</title>
<link>https://papers.cool/arxiv/2510.17873</link>
<guid>https://papers.cool/arxiv/2510.17873</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper audits five popular gender classification datasets, uncovering severe intersectional underrepresentation, and demonstrates that classifiers trained on even the most balanced datasets still exhibit significant gender and racial bias. It then introduces BalancedFace, a new dataset that equalizes subgroup representation across 189 age‑race‑gender intersections, and shows that training on this data markedly reduces fairness gaps with minimal loss in overall accuracy. The results highlight the effectiveness of data‑centric interventions for mitigating bias in vision models.<br /><strong>Summary (CN):</strong> 本文审计了五个常用的人脸性别分类数据集，发现它们在交叉人口子组上严重不足，并展示即使在最为平衡的数据上训练的模型仍然存在显著的性别与种族偏差。随后构建了 BalancedFace 数据集，针对年龄、种族、性别的 189 种交叉子组实现了均衡，并证明在该数据上训练的模型能够显著降低公平性差距，且整体准确率几乎不受影响。研究强调了以数据为中心的干预在减轻视觉模型偏差方面的价值。<br /><strong>Keywords:</strong> gender classification, bias auditing, data-centric fairness, BalancedFace, intersectional representation, fairness metrics, demographic imbalance<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - alignment<br /><strong>Authors:</strong> Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges, Dheeraj Kumar Singh</div>
Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying female faces at a higher rate than male faces and amplifying existing racial skew. To counter these data-induced biases, we construct BalancedFace, a new public dataset created by blending images from FairFace and UTKFace, supplemented with images from other collections to fill missing demographic gaps. It is engineered to equalize subgroup shares across 189 intersections of age, race, and gender using only real, unedited images. When a standard classifier is trained on BalancedFace, it reduces the maximum True Positive Rate gap across racial subgroups by over 50% and brings the average Disparate Impact score 63% closer to the ideal of 1.0 compared to the next-best dataset, all with a minimal loss of overall accuracy. These results underline the profound value of data-centric interventions and provide an openly available resource for fair gender classification research.
<div><strong>Authors:</strong> Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges, Dheeraj Kumar Singh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper audits five popular gender classification datasets, uncovering severe intersectional underrepresentation, and demonstrates that classifiers trained on even the most balanced datasets still exhibit significant gender and racial bias. It then introduces BalancedFace, a new dataset that equalizes subgroup representation across 189 age‑race‑gender intersections, and shows that training on this data markedly reduces fairness gaps with minimal loss in overall accuracy. The results highlight the effectiveness of data‑centric interventions for mitigating bias in vision models.", "summary_cn": "本文审计了五个常用的人脸性别分类数据集，发现它们在交叉人口子组上严重不足，并展示即使在最为平衡的数据上训练的模型仍然存在显著的性别与种族偏差。随后构建了 BalancedFace 数据集，针对年龄、种族、性别的 189 种交叉子组实现了均衡，并证明在该数据上训练的模型能够显著降低公平性差距，且整体准确率几乎不受影响。研究强调了以数据为中心的干预在减轻视觉模型偏差方面的价值。", "keywords": "gender classification, bias auditing, data-centric fairness, BalancedFace, intersectional representation, fairness metrics, demographic imbalance", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "alignment"}, "authors": ["Tadesse K Bahiru", "Natnael Tilahun Sinshaw", "Teshager Hailemariam Moges", "Dheeraj Kumar Singh"]}
]]></acme>

<pubDate>2025-10-17T02:09:17+00:00</pubDate>
</item>
<item>
<title>A Survey of Recursive and Recurrent Neural Networks</title>
<link>https://papers.cool/arxiv/2510.17867</link>
<guid>https://papers.cool/arxiv/2510.17867</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a comprehensive taxonomy of recursive and recurrent neural network architectures, categorizing them by structure, training objectives, and learning algorithms into three broad groups and describing their variants and applications. It reviews the principles, structural details, research progress, and typical use cases for each model, and concludes with prospects and summarization of the field.<br /><strong>Summary (CN):</strong> 本文对递归神经网络和循环神经网络的各种架构进行详尽的分类，依据网络结构、训练目标函数和学习算法划分为三大类，并对每类的变体及其应用进行描述。文章回顾了各模型的原理、结构细节、研究进展和典型应用，并对该领域的未来发展进行展望和总结。<br /><strong>Keywords:</strong> recursive neural networks, recurrent neural networks, LSTM, hierarchical RNN, graph RNN, survey, sequence modeling, architecture taxonomy, deep learning, neural network survey<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 5, Surprisal: 2<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jian-wei Liu, Bing-rong Xu, Zhi-yan Song</div>
In this paper, the branches of recursive and recurrent neural networks are classified in detail according to the network structure, training objective function and learning algorithm implementation. They are roughly divided into three categories: The first category is General Recursive and Recurrent Neural Networks, including Basic Recursive and Recurrent Neural Networks, Long Short Term Memory Recursive and Recurrent Neural Networks, Convolutional Recursive and Recurrent Neural Networks, Differential Recursive and Recurrent Neural Networks, One-Layer Recursive and Recurrent Neural Networks, High-Order Recursive and Recurrent Neural Networks, Highway Networks, Multidimensional Recursive and Recurrent Neural Networks, Bidirectional Recursive and Recurrent Neural Networks; the second category is Structured Recursive and Recurrent Neural Networks, including Grid Recursive and Recurrent Neural Networks, Graph Recursive and Recurrent Neural Networks, Temporal Recursive and Recurrent Neural Networks, Lattice Recursive and Recurrent Neural Networks, Hierarchical Recursive and Recurrent Neural Networks, Tree Recursive and Recurrent Neural Networks; the third category is Other Recursive and Recurrent Neural Networks, including Array Long Short Term Memory, Nested and Stacked Recursive and Recurrent Neural Networks, Memory Recursive and Recurrent Neural Networks. Various networks cross each other and even rely on each other to form a complex network of relationships. In the context of the development and convergence of various networks, many complex sequence, speech and image problems are solved. After a detailed description of the principle and structure of the above model and model deformation, the research progress and application of each model are described, and finally the recursive and recurrent neural network models are prospected and summarized.
<div><strong>Authors:</strong> Jian-wei Liu, Bing-rong Xu, Zhi-yan Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a comprehensive taxonomy of recursive and recurrent neural network architectures, categorizing them by structure, training objectives, and learning algorithms into three broad groups and describing their variants and applications. It reviews the principles, structural details, research progress, and typical use cases for each model, and concludes with prospects and summarization of the field.", "summary_cn": "本文对递归神经网络和循环神经网络的各种架构进行详尽的分类，依据网络结构、训练目标函数和学习算法划分为三大类，并对每类的变体及其应用进行描述。文章回顾了各模型的原理、结构细节、研究进展和典型应用，并对该领域的未来发展进行展望和总结。", "keywords": "recursive neural networks, recurrent neural networks, LSTM, hierarchical RNN, graph RNN, survey, sequence modeling, architecture taxonomy, deep learning, neural network survey", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 5, "surprisal": 2}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jian-wei Liu", "Bing-rong Xu", "Zhi-yan Song"]}
]]></acme>

<pubDate>2025-10-16T03:12:48+00:00</pubDate>
</item>
<item>
<title>MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation</title>
<link>https://papers.cool/arxiv/2510.17866</link>
<guid>https://papers.cool/arxiv/2510.17866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes MUSE, a training-free framework for zero-shot 2D object detection and segmentation that uses multi-view 2D templates rendered from unseen 3D objects and query image proposals. It combines class and patch embeddings with GeM pooling and a joint similarity metric, refined by an uncertainty-aware object prior, achieving state-of-the-art results on the BOP Challenge 2025 without additional training.<br /><strong>Summary (CN):</strong> 本文提出 MUSE，一种无需训练的零样本 2D 目标检测与分割框架，利用从未见过的 3D 物体渲染的多视角 2D 模板以及输入图像的目标提议。系统通过类和局部特征嵌入（局部特征使用 GeM 池化）并结合绝对与相对相似度的联合度量，再利用不确定性感知的目标先验进行分数校正，在 BOP Challenge 2025 中实现了领先的性能。<br /><strong>Keywords:</strong> zero-shot detection, similarity estimation, uncertainty-aware, model-based, multi-view templates, GeM pooling, object segmentation, training-free<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sungmin Cho, Sungbum Park, Insoo Oh</div>
In this work, we introduce MUSE (Model-based Uncertainty-aware Similarity Estimation), a training-free framework designed for model-based zero-shot 2D object detection and segmentation. MUSE leverages 2D multi-view templates rendered from 3D unseen objects and 2D object proposals extracted from input query images. In the embedding stage, it integrates class and patch embeddings, where the patch embeddings are normalized using generalized mean pooling (GeM) to capture both global and local representations efficiently. During the matching stage, MUSE employs a joint similarity metric that combines absolute and relative similarity scores, enhancing the robustness of matching under challenging scenarios. Finally, the similarity score is refined through an uncertainty-aware object prior that adjusts for proposal reliability. Without any additional training or fine-tuning, MUSE achieves state-of-the-art performance on the BOP Challenge 2025, ranking first across the Classic Core, H3, and Industrial tracks. These results demonstrate that MUSE offers a powerful and generalizable framework for zero-shot 2D object detection and segmentation.
<div><strong>Authors:</strong> Sungmin Cho, Sungbum Park, Insoo Oh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes MUSE, a training-free framework for zero-shot 2D object detection and segmentation that uses multi-view 2D templates rendered from unseen 3D objects and query image proposals. It combines class and patch embeddings with GeM pooling and a joint similarity metric, refined by an uncertainty-aware object prior, achieving state-of-the-art results on the BOP Challenge 2025 without additional training.", "summary_cn": "本文提出 MUSE，一种无需训练的零样本 2D 目标检测与分割框架，利用从未见过的 3D 物体渲染的多视角 2D 模板以及输入图像的目标提议。系统通过类和局部特征嵌入（局部特征使用 GeM 池化）并结合绝对与相对相似度的联合度量，再利用不确定性感知的目标先验进行分数校正，在 BOP Challenge 2025 中实现了领先的性能。", "keywords": "zero-shot detection, similarity estimation, uncertainty-aware, model-based, multi-view templates, GeM pooling, object segmentation, training-free", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sungmin Cho", "Sungbum Park", "Insoo Oh"]}
]]></acme>

<pubDate>2025-10-15T22:16:09+00:00</pubDate>
</item>
<item>
<title>Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis</title>
<link>https://papers.cool/arxiv/2510.17852</link>
<guid>https://papers.cool/arxiv/2510.17852</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a framework for migrating large-scale atmospheric and oceanic AI models such as FourCastNet and AI-GOMS from PyTorch to MindSpore and optimizing them for Chinese domestic chips, focusing on software‑hardware adaptation, memory management, and parallelism. Experimental evaluations show that the migrated models retain their original accuracy while achieving faster training and inference speeds and improved energy efficiency compared to GPU baselines. The work offers practical guidance for reducing system dependencies and enhancing operational efficiency on Chinese hardware platforms.<br /><strong>Summary (CN):</strong> 本文提出了一套将 FourCastNet、AI‑GOMS 等大规模大气与海洋 AI 模型从 PyTorch 迁移至 MindSpore 并针对国产芯片进行优化的框架，重点在于软件‑硬件适配、内存管理和并行化。实验结果表明，迁移后的模型在保持原有精度的同时，相比 GPU 基准实现了更快的训练与推理速度以及更高的能效。该工作为在国产芯片上开展气候与天气 AI 计算提供了实用的技术路线和效率提升方案。<br /><strong>Keywords:</strong> climate AI, atmospheric modeling, FourCastNet, AI-GOMS, model migration, MindSpore, Chinese chips, performance optimization, hardware independence, energy efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</div>
With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.
<div><strong>Authors:</strong> Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a framework for migrating large-scale atmospheric and oceanic AI models such as FourCastNet and AI-GOMS from PyTorch to MindSpore and optimizing them for Chinese domestic chips, focusing on software‑hardware adaptation, memory management, and parallelism. Experimental evaluations show that the migrated models retain their original accuracy while achieving faster training and inference speeds and improved energy efficiency compared to GPU baselines. The work offers practical guidance for reducing system dependencies and enhancing operational efficiency on Chinese hardware platforms.", "summary_cn": "本文提出了一套将 FourCastNet、AI‑GOMS 等大规模大气与海洋 AI 模型从 PyTorch 迁移至 MindSpore 并针对国产芯片进行优化的框架，重点在于软件‑硬件适配、内存管理和并行化。实验结果表明，迁移后的模型在保持原有精度的同时，相比 GPU 基准实现了更快的训练与推理速度以及更高的能效。该工作为在国产芯片上开展气候与天气 AI 计算提供了实用的技术路线和效率提升方案。", "keywords": "climate AI, atmospheric modeling, FourCastNet, AI-GOMS, model migration, MindSpore, Chinese chips, performance optimization, hardware independence, energy efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"]}
]]></acme>

<pubDate>2025-10-14T02:41:56+00:00</pubDate>
</item>
<item>
<title>Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model</title>
<link>https://papers.cool/arxiv/2510.17851</link>
<guid>https://papers.cool/arxiv/2510.17851</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a latent diffusion model that generates post‑treatment glioblastoma MRI scans from pre‑treatment scans, conditioned on tumor localization and survival information via classifier‑free guidance. Trained on a dataset of 140 patients, the approach aims to provide early visual prediction of treatment response, potentially aiding personalized medicine. Results show that the model can capture tumor evolution patterns across the Stupp protocol.<br /><strong>Summary (CN):</strong> 本文提出一种潜在扩散模型，通过连接式条件（包含术前 MRI 与肿瘤定位）以及基于生存信息的 classifier‑free guidance，生成术后 GBM MRI，以实现早期视觉化的治疗响应预测。模型在140例患者的数据上进行训练和测试，旨在捕捉肿瘤在 Stupp 方案下的演化过程，从而帮助个体化治疗。实验表明该模型能够较好地重建术后肿瘤变化。<br /><strong>Keywords:</strong> latent diffusion model, MRI prediction, glioblastoma, treatment response prediction, disease progression modeling, classifier-free guidance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</div>
Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.
<div><strong>Authors:</strong> Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a latent diffusion model that generates post‑treatment glioblastoma MRI scans from pre‑treatment scans, conditioned on tumor localization and survival information via classifier‑free guidance. Trained on a dataset of 140 patients, the approach aims to provide early visual prediction of treatment response, potentially aiding personalized medicine. Results show that the model can capture tumor evolution patterns across the Stupp protocol.", "summary_cn": "本文提出一种潜在扩散模型，通过连接式条件（包含术前 MRI 与肿瘤定位）以及基于生存信息的 classifier‑free guidance，生成术后 GBM MRI，以实现早期视觉化的治疗响应预测。模型在140例患者的数据上进行训练和测试，旨在捕捉肿瘤在 Stupp 方案下的演化过程，从而帮助个体化治疗。实验表明该模型能够较好地重建术后肿瘤变化。", "keywords": "latent diffusion model, MRI prediction, glioblastoma, treatment response prediction, disease progression modeling, classifier-free guidance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandre G. Leclercq", "Sébastien Bougleux", "Noémie N. Moreau", "Alexis Desmonts", "Romain Hérault", "Aurélien Corroyer-Dulmont"]}
]]></acme>

<pubDate>2025-10-13T20:32:06+00:00</pubDate>
</item>
<item>
<title>CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings</title>
<link>https://papers.cool/arxiv/2510.17846</link>
<guid>https://papers.cool/arxiv/2510.17846</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents CARLE, a hybrid deep‑shallow framework that combines Res‑CNN, Res‑LSTM with multi‑head attention and a Random Forest Regressor to estimate the useful life of rolling element bearings. It includes a compact preprocessing pipeline using Gaussian filtering and continuous wavelet transform, and demonstrates improved robustness and generalization across bearing datasets, with additional analysis of model transparency using LIME and SHAP.<br /><strong>Summary (CN):</strong> 本文提出 CARLE 框架，将 Res‑CNN、Res‑LSTM（带多头注意力）与随机森林回归器相结合，用于滚动轴承的剩余寿命预测，并采用高斯滤波和连续小波变换进行预处理。实验在 XJTU‑SY 与 PRONOSTIA 数据集上展示了其在动态工况下的鲁棒性和泛化能力，同时通过 LIME 与 SHAP 对模型透明度进行了解释性分析。<br /><strong>Keywords:</strong> remaining useful life, bearing prognostics, hybrid deep-shallow learning, Res-CNN, Res-LSTM, multi-head attention, Random Forest Regressor, CWT, LIME, SHAP<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Waleed Razzaq, Yun-Bo Zhao</div>
Prognostic Health Management (PHM) systems monitor and predict equipment health. A key task is Remaining Useful Life (RUL) estimation, which predicts how long a component, such as a rolling element bearing, will operate before failure. Many RUL methods exist but often lack generalizability and robustness under changing operating conditions. This paper introduces CARLE, a hybrid AI framework that combines deep and shallow learning to address these challenges. CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual connections to capture spatial and temporal degradation patterns, and a Random Forest Regressor (RFR) for stable, accurate RUL prediction. A compact preprocessing pipeline applies Gaussian filtering for noise reduction and Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies measure each component's contribution, while noise and cross-domain experiments test robustness and generalization. Comparative results show CARLE outperforms several state-of-the-art methods, especially under dynamic conditions. Finally, we analyze model interpretability with LIME and SHAP to assess transparency and trustworthiness.
<div><strong>Authors:</strong> Waleed Razzaq, Yun-Bo Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents CARLE, a hybrid deep‑shallow framework that combines Res‑CNN, Res‑LSTM with multi‑head attention and a Random Forest Regressor to estimate the useful life of rolling element bearings. It includes a compact preprocessing pipeline using Gaussian filtering and continuous wavelet transform, and demonstrates improved robustness and generalization across bearing datasets, with additional analysis of model transparency using LIME and SHAP.", "summary_cn": "本文提出 CARLE 框架，将 Res‑CNN、Res‑LSTM（带多头注意力）与随机森林回归器相结合，用于滚动轴承的剩余寿命预测，并采用高斯滤波和连续小波变换进行预处理。实验在 XJTU‑SY 与 PRONOSTIA 数据集上展示了其在动态工况下的鲁棒性和泛化能力，同时通过 LIME 与 SHAP 对模型透明度进行了解释性分析。", "keywords": "remaining useful life, bearing prognostics, hybrid deep-shallow learning, Res-CNN, Res-LSTM, multi-head attention, Random Forest Regressor, CWT, LIME, SHAP", "scoring": {"interpretability": 5, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Waleed Razzaq", "Yun-Bo Zhao"]}
]]></acme>

<pubDate>2025-10-10T21:43:26+00:00</pubDate>
</item>
<item>
<title>MAT-Agent: Adaptive Multi-Agent Training Optimization</title>
<link>https://papers.cool/arxiv/2510.17845</link>
<guid>https://papers.cool/arxiv/2510.17845</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> MAT-Agent introduces a collaborative multi-agent framework that dynamically adjusts data augmentation, optimizers, learning rates, and loss functions during multi-label image classification training using non‑stationary multi‑armed bandit algorithms. The system combines a composite reward balancing accuracy, rare‑class performance, and training stability, and demonstrates superior mAP and F1 scores on Pascal VOC, COCO, and VG‑256 compared to static baselines. Additional components such as dual‑rate EMA smoothing and mixed‑precision training improve convergence speed and robustness across domains.<br /><strong>Summary (CN):</strong> MAT-Agent 提出一种协作式多智能体框架，在多标签图像分类训练过程中通过非平稳多臂赌博机算法动态调节数据增强、优化器、学习率和损失函数。系统使用综合奖励同时考虑准确率、稀有类别表现和训练稳定性，并在 Pascal VOC、COCO 和 VG‑256 数据集上实现了比传统静态方法更高的 mAP 与 F1 分数。双速率 EMA 平滑和混合精度训练等技术进一步提升了收敛速度和跨域鲁棒性。<br /><strong>Keywords:</strong> multi-label classification, adaptive training, multi-agent optimization, non-stationary bandit, data augmentation tuning, optimizer scheduling, mixed-precision training, exponential moving average, cross-domain generalization, visual semantics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.
<div><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "MAT-Agent introduces a collaborative multi-agent framework that dynamically adjusts data augmentation, optimizers, learning rates, and loss functions during multi-label image classification training using non‑stationary multi‑armed bandit algorithms. The system combines a composite reward balancing accuracy, rare‑class performance, and training stability, and demonstrates superior mAP and F1 scores on Pascal VOC, COCO, and VG‑256 compared to static baselines. Additional components such as dual‑rate EMA smoothing and mixed‑precision training improve convergence speed and robustness across domains.", "summary_cn": "MAT-Agent 提出一种协作式多智能体框架，在多标签图像分类训练过程中通过非平稳多臂赌博机算法动态调节数据增强、优化器、学习率和损失函数。系统使用综合奖励同时考虑准确率、稀有类别表现和训练稳定性，并在 Pascal VOC、COCO 和 VG‑256 数据集上实现了比传统静态方法更高的 mAP 与 F1 分数。双速率 EMA 平滑和混合精度训练等技术进一步提升了收敛速度和跨域鲁棒性。", "keywords": "multi-label classification, adaptive training, multi-agent optimization, non-stationary bandit, data augmentation tuning, optimizer scheduling, mixed-precision training, exponential moving average, cross-domain generalization, visual semantics", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Ningyuan Liu", "Keze Wang"]}
]]></acme>

<pubDate>2025-10-10T19:41:50+00:00</pubDate>
</item>
<item>
<title>Modeling Layered Consciousness with Multi-Agent Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17844</link>
<guid>https://papers.cool/arxiv/2510.17844</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multi‑agent framework that models artificial consciousness in large language models based on psychoanalytic concepts of self‑awareness, pre‑consciousness, and unconsciousness. A Psychodynamic Model combines a Personalization Module with fixed traits and dynamic needs, and parameter‑efficient fine‑tuning on emotionally rich dialogues is used; evaluations across eight personalized conditions show a 71.2 % preference for the fine‑tuned model in a judge‑LLM assessment, indicating increased emotional depth and reduced output variance.<br /><strong>Summary (CN):</strong> 本文提出一种多智能体框架，用心理分析理论中的自我意识、前意识和无意识概念来建模大语言模型的人工意识。通过 Psychodynamic Model 将固定特质与动态需求相结合的 Personalization Module，并在情感丰富的对话上进行参数高效微调；在八种个性化条件下的评估显示，在 LLM 判别者的测试中，微调模型获得 71.2% 的偏好，表明其情感深度提升且输出方差降低。<br /><strong>Keywords:</strong> artificial consciousness, multi-agent LLM, psychodynamic model, personalization module, emotional dialogue fine-tuning, introspective modeling, layered cognition<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.
<div><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multi‑agent framework that models artificial consciousness in large language models based on psychoanalytic concepts of self‑awareness, pre‑consciousness, and unconsciousness. A Psychodynamic Model combines a Personalization Module with fixed traits and dynamic needs, and parameter‑efficient fine‑tuning on emotionally rich dialogues is used; evaluations across eight personalized conditions show a 71.2 % preference for the fine‑tuned model in a judge‑LLM assessment, indicating increased emotional depth and reduced output variance.", "summary_cn": "本文提出一种多智能体框架，用心理分析理论中的自我意识、前意识和无意识概念来建模大语言模型的人工意识。通过 Psychodynamic Model 将固定特质与动态需求相结合的 Personalization Module，并在情感丰富的对话上进行参数高效微调；在八种个性化条件下的评估显示，在 LLM 判别者的测试中，微调模型获得 71.2% 的偏好，表明其情感深度提升且输出方差降低。", "keywords": "artificial consciousness, multi-agent LLM, psychodynamic model, personalization module, emotional dialogue fine-tuning, introspective modeling, layered cognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"]}
]]></acme>

<pubDate>2025-10-10T07:08:34+00:00</pubDate>
</item>
<item>
<title>GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing</title>
<link>https://papers.cool/arxiv/2510.17843</link>
<guid>https://papers.cool/arxiv/2510.17843</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> GRETEL introduces an agentic framework that validates semantically retrieved tools through sandboxed plan‑execute‑evaluate cycles, using execution‑grounded evidence to filter out functionally inoperative tools. Experiments on the ToolBench benchmark show substantial gains in pass rate, recall, and NDCG, demonstrating that execution‑based validation narrows the semantic‑functional gap and improves tool selection for LLM agents.<br /><strong>Summary (CN):</strong> GRETEL 提出了一种基于代理工作流的框架，通过沙盒中的计划‑执行‑评估循环，对语义检索得到的工具进行执行层面的验证，以排除功能不可用的工具。 在 ToolBench 基准上的实验显示，Pass Rate、Recall 和 NDCG 均显著提升，证明执行‑基于验证能够缩小语义‑功能差距，提升 LLM 代理的工具选择可靠性。<br /><strong>Keywords:</strong> LLM tool selection, execution-based validation, semantic-functional gap, agentic workflow, ToolBench, functional verification, sandboxed execution, retrieval robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Zongze Wu, Yani Guo, Churong Liang, Runnan Li</div>
Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.
<div><strong>Authors:</strong> Zongze Wu, Yani Guo, Churong Liang, Runnan Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "GRETEL introduces an agentic framework that validates semantically retrieved tools through sandboxed plan‑execute‑evaluate cycles, using execution‑grounded evidence to filter out functionally inoperative tools. Experiments on the ToolBench benchmark show substantial gains in pass rate, recall, and NDCG, demonstrating that execution‑based validation narrows the semantic‑functional gap and improves tool selection for LLM agents.", "summary_cn": "GRETEL 提出了一种基于代理工作流的框架，通过沙盒中的计划‑执行‑评估循环，对语义检索得到的工具进行执行层面的验证，以排除功能不可用的工具。 在 ToolBench 基准上的实验显示，Pass Rate、Recall 和 NDCG 均显著提升，证明执行‑基于验证能够缩小语义‑功能差距，提升 LLM 代理的工具选择可靠性。", "keywords": "LLM tool selection, execution-based validation, semantic-functional gap, agentic workflow, ToolBench, functional verification, sandboxed execution, retrieval robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Zongze Wu", "Yani Guo", "Churong Liang", "Runnan Li"]}
]]></acme>

<pubDate>2025-10-10T00:12:51+00:00</pubDate>
</item>
<item>
<title>Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage</title>
<link>https://papers.cool/arxiv/2510.17833</link>
<guid>https://papers.cool/arxiv/2510.17833</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This review synthesizes findings from 25 fMRI studies (2023‑2025) that compare human brain activations with internal representations of large language models. It evaluates two hypotheses: the Platonic Representation Hypothesis, which posits that scaled models converge toward a true representation of the world, and the Intermediate-Layer Advantage, which suggests that mid‑depth model layers capture richer, more generalizable features. The authors report converging evidence that brains and models share abstract representational structures, supporting both hypotheses and encouraging further research on brain‑model alignment.<br /><strong>Summary (CN):</strong> 本文综述了 2023‑2025 年间的 25 项基于 fMRI 的研究，比较了人脑激活与大型语言模型内部表征。文章检验了两个假设：① 柏拉图表征假设——模型随规模增长会趋向于对真实世界的表征；② 中间层优势——模型的中层往往编码更丰富、更具通用性的特征。作者发现大脑与模型在抽象表征结构上存在一致性，支持上述假设，并推动进一步的脑‑模型对齐研究。<br /><strong>Keywords:</strong> brain-model alignment, fMRI, language models, Platonic representation hypothesis, intermediate-layer advantage, neural representation, mechanistic interpretability, cognitive neuroscience, multimodal alignment, representation convergence<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Ángela López-Cardona, Sebastián Idesis, Mireia Masias-Bruns, Sergi Abadal, Ioannis Arapakis</div>
Do brains and language models converge toward the same internal representations of the world? Recent years have seen a rise in studies of neural activations and model alignment. In this work, we review 25 fMRI-based studies published between 2023 and 2025 and explicitly confront their findings with two key hypotheses: (i) the Platonic Representation Hypothesis -- that as models scale and improve, they converge to a representation of the real world, and (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth) layers often encode richer, more generalizable features. Our findings provide converging evidence that models and brains may share abstract representational structures, supporting both hypotheses and motivating further research on brain-model alignment.
<div><strong>Authors:</strong> Ángela López-Cardona, Sebastián Idesis, Mireia Masias-Bruns, Sergi Abadal, Ioannis Arapakis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This review synthesizes findings from 25 fMRI studies (2023‑2025) that compare human brain activations with internal representations of large language models. It evaluates two hypotheses: the Platonic Representation Hypothesis, which posits that scaled models converge toward a true representation of the world, and the Intermediate-Layer Advantage, which suggests that mid‑depth model layers capture richer, more generalizable features. The authors report converging evidence that brains and models share abstract representational structures, supporting both hypotheses and encouraging further research on brain‑model alignment.", "summary_cn": "本文综述了 2023‑2025 年间的 25 项基于 fMRI 的研究，比较了人脑激活与大型语言模型内部表征。文章检验了两个假设：① 柏拉图表征假设——模型随规模增长会趋向于对真实世界的表征；② 中间层优势——模型的中层往往编码更丰富、更具通用性的特征。作者发现大脑与模型在抽象表征结构上存在一致性，支持上述假设，并推动进一步的脑‑模型对齐研究。", "keywords": "brain-model alignment, fMRI, language models, Platonic representation hypothesis, intermediate-layer advantage, neural representation, mechanistic interpretability, cognitive neuroscience, multimodal alignment, representation convergence", "scoring": {"interpretability": 7, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Ángela López-Cardona", "Sebastián Idesis", "Mireia Masias-Bruns", "Sergi Abadal", "Ioannis Arapakis"]}
]]></acme>

<pubDate>2025-10-03T16:33:09+00:00</pubDate>
</item>
<item>
<title>Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks</title>
<link>https://papers.cool/arxiv/2510.17832</link>
<guid>https://papers.cool/arxiv/2510.17832</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a method for generating synthetic EEG signals for motor imagery tasks using Denoising Diffusion Probabilistic Models. After preprocessing real EEG data, the diffusion model learns to reconstruct EEG channels from noise, and the generated signals are evaluated with signal-level and task-level metrics, achieving high classification accuracy. The results suggest that diffusion‑based synthetic EEG can augment BCI datasets and mitigate data scarcity issues.<br /><strong>Summary (CN):</strong> 本文提出了一种基于扩散概率模型（DDPM）生成运动意象任务合成 EEG 信号的方法。通过对真实 EEG 数据的预处理后训练模型，从噪声重建各通道信号，并使用信号层面和任务层面的指标评估生成质量，分类准确率超过 95%。结果表明，利用扩散模型生成的合成 EEG 可有效补充 BCI 数据集，缓解数据稀缺问题。<br /><strong>Keywords:</strong> synthetic EEG, diffusion probabilistic models, motor imagery, brain-computer interface, data augmentation, DDPM, classification<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Henrique de Lima Alexandre, Clodoaldo Aparecido de Moraes Lima</div>
Electroencephalography (EEG) is a widely used, non-invasive method for capturing brain activity, and is particularly relevant for applications in Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data remains a major challenge due to sensor costs, acquisition time, and inter-subject variability. To address these limitations, this study proposes a methodology for generating synthetic EEG signals associated with motor imagery brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves preprocessing real EEG data, training a diffusion model to reconstruct EEG channels from noise, and evaluating the quality of the generated signals through both signal-level and task-level metrics. For validation, we employed classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks (CNN), and U-Net to compare the performance of synthetic data against real data in classification tasks. The generated data achieved classification accuracies above 95%, with low mean squared error and high correlation with real signals. Our results demonstrate that synthetic EEG signals produced by diffusion models can effectively complement datasets, improving classification performance in EEG-based BCIs and addressing data scarcity.
<div><strong>Authors:</strong> Henrique de Lima Alexandre, Clodoaldo Aparecido de Moraes Lima</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a method for generating synthetic EEG signals for motor imagery tasks using Denoising Diffusion Probabilistic Models. After preprocessing real EEG data, the diffusion model learns to reconstruct EEG channels from noise, and the generated signals are evaluated with signal-level and task-level metrics, achieving high classification accuracy. The results suggest that diffusion‑based synthetic EEG can augment BCI datasets and mitigate data scarcity issues.", "summary_cn": "本文提出了一种基于扩散概率模型（DDPM）生成运动意象任务合成 EEG 信号的方法。通过对真实 EEG 数据的预处理后训练模型，从噪声重建各通道信号，并使用信号层面和任务层面的指标评估生成质量，分类准确率超过 95%。结果表明，利用扩散模型生成的合成 EEG 可有效补充 BCI 数据集，缓解数据稀缺问题。", "keywords": "synthetic EEG, diffusion probabilistic models, motor imagery, brain-computer interface, data augmentation, DDPM, classification", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Henrique de Lima Alexandre", "Clodoaldo Aparecido de Moraes Lima"]}
]]></acme>

<pubDate>2025-10-03T02:02:05+00:00</pubDate>
</item>
<item>
<title>Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy</title>
<link>https://papers.cool/arxiv/2510.17830</link>
<guid>https://papers.cool/arxiv/2510.17830</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper presents a multi‑agent design assistant that integrates large language reasoning models with high‑order multiphysics inertial fusion simulation codes, enabling autonomous exploration, manipulation, and inverse‑design optimization of fusion capsule geometry to achieve simulated ignition. The system uses natural‑language interaction to coordinate agents that run physics‑accurate models, demonstrating collaborative and autonomous capsule design under extreme conditions.<br /><strong>Summary (CN):</strong> 本文构建了一个多智能体系统，将大语言模型与高阶多物理惯性聚变仿真代码结合，实现对融合燃料胶囊几何形状的自主探索、操控和逆向设计优化，以模拟点火。系统通过自然语言交互协调智能体，展示了在极端条件下的协同与自主胶囊设计能力。<br /><strong>Keywords:</strong> multi-agent systems, inertial confinement fusion, inverse design, physics-informed AI, capsule geometry optimization, multiphysics simulation, autonomous design assistant, high-fidelity modeling, natural language interaction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Meir H. Shachar, Dane M. Sterbentz, Harshitha Menon, Charles F. Jekel, M. Giselle Fernández-Godino, Yue Hao, Kevin Korner, Robert Rieben, Daniel A. White, William J. Schill, Jonathan L. Belof</div>
Inertial fusion energy promises nearly unlimited, clean power if it can be achieved. However, the design and engineering of fusion systems requires controlling and manipulating matter at extreme energies and timescales; the shock physics and radiation transport governing the physical behavior under these conditions are complex requiring the development, calibration, and use of predictive multiphysics codes to navigate the highly nonlinear and multi-faceted design landscape. We hypothesize that artificial intelligence reasoning models can be combined with physics codes and emulators to autonomously design fusion fuel capsules. In this article, we construct a multi-agent system where natural language is utilized to explore the complex physics regimes around fusion energy. The agentic system is capable of executing a high-order multiphysics inertial fusion computational code. We demonstrate the capacity of the multi-agent design assistant to both collaboratively and autonomously manipulate, navigate, and optimize capsule geometry while accounting for high fidelity physics that ultimately achieve simulated ignition via inverse design.
<div><strong>Authors:</strong> Meir H. Shachar, Dane M. Sterbentz, Harshitha Menon, Charles F. Jekel, M. Giselle Fernández-Godino, Yue Hao, Kevin Korner, Robert Rieben, Daniel A. White, William J. Schill, Jonathan L. Belof</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper presents a multi‑agent design assistant that integrates large language reasoning models with high‑order multiphysics inertial fusion simulation codes, enabling autonomous exploration, manipulation, and inverse‑design optimization of fusion capsule geometry to achieve simulated ignition. The system uses natural‑language interaction to coordinate agents that run physics‑accurate models, demonstrating collaborative and autonomous capsule design under extreme conditions.", "summary_cn": "本文构建了一个多智能体系统，将大语言模型与高阶多物理惯性聚变仿真代码结合，实现对融合燃料胶囊几何形状的自主探索、操控和逆向设计优化，以模拟点火。系统通过自然语言交互协调智能体，展示了在极端条件下的协同与自主胶囊设计能力。", "keywords": "multi-agent systems, inertial confinement fusion, inverse design, physics-informed AI, capsule geometry optimization, multiphysics simulation, autonomous design assistant, high-fidelity modeling, natural language interaction", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Meir H. Shachar", "Dane M. Sterbentz", "Harshitha Menon", "Charles F. Jekel", "M. Giselle Fernández-Godino", "Yue Hao", "Kevin Korner", "Robert Rieben", "Daniel A. White", "William J. Schill", "Jonathan L. Belof"]}
]]></acme>

<pubDate>2025-10-02T19:15:09+00:00</pubDate>
</item>
<item>
<title>Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis</title>
<link>https://papers.cool/arxiv/2510.17826</link>
<guid>https://papers.cool/arxiv/2510.17826</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces "Speak to a Protein," an AI system that enables interactive multimodal dialogue for protein analysis, retrieving literature, structures, and ligand data while grounding answers in a live 3D scene and generating executable code. It demonstrates real-time hypothesis testing on proteins, reducing the time from question to evidence and lowering the barrier to advanced structural analysis. The system is freely available for use.<br /><strong>Summary (CN):</strong> 本文推出了 "Speak to a Protein" 系统，通过多模态对话方式帮助科研人员进行蛋白质分析，能够自动检索文献、结构和配体信息，并在实时 3D 场景中给出答案，同时生成可运行的代码并提供可视化解释。实验展示了该系统在结合结合口袋、构象变化及结构-活性关系等方面的即时假设验证能力，大幅缩短了从提问到获取证据的时间，降低了进行高级结构分析的门槛。系统已公开免费使用。<br /><strong>Keywords:</strong> protein analysis, multimodal dialogue, AI co-scientist, structural biology, interactive 3D visualization, code generation, literature retrieval, hypothesis generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Carles Navarro, Mariona Torrens, Philipp Thölke, Stefan Doerr, Gianni De Fabritiis</div>
Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.
<div><strong>Authors:</strong> Carles Navarro, Mariona Torrens, Philipp Thölke, Stefan Doerr, Gianni De Fabritiis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces \"Speak to a Protein,\" an AI system that enables interactive multimodal dialogue for protein analysis, retrieving literature, structures, and ligand data while grounding answers in a live 3D scene and generating executable code. It demonstrates real-time hypothesis testing on proteins, reducing the time from question to evidence and lowering the barrier to advanced structural analysis. The system is freely available for use.", "summary_cn": "本文推出了 \"Speak to a Protein\" 系统，通过多模态对话方式帮助科研人员进行蛋白质分析，能够自动检索文献、结构和配体信息，并在实时 3D 场景中给出答案，同时生成可运行的代码并提供可视化解释。实验展示了该系统在结合结合口袋、构象变化及结构-活性关系等方面的即时假设验证能力，大幅缩短了从提问到获取证据的时间，降低了进行高级结构分析的门槛。系统已公开免费使用。", "keywords": "protein analysis, multimodal dialogue, AI co-scientist, structural biology, interactive 3D visualization, code generation, literature retrieval, hypothesis generation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Carles Navarro", "Mariona Torrens", "Philipp Thölke", "Stefan Doerr", "Gianni De Fabritiis"]}
]]></acme>

<pubDate>2025-10-01T22:12:34+00:00</pubDate>
</item>
<item>
<title>Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin</title>
<link>https://papers.cool/arxiv/2510.17825</link>
<guid>https://papers.cool/arxiv/2510.17825</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a carbon‑aware orchestration framework for Integrated Satellite Aerial Terrestrial Networks (ISATNs) that utilizes Digital Twin technology to minimize grams of CO₂‑equivalent per transmitted bit. It implements a multi‑timescale Plan‑Do‑Check‑Act loop combining day‑ahead forecasting with real‑time adaptive optimization, exploiting control knobs such as carbon‑aware handovers, UAV duty cycling, and renewable‑aware edge placement. Simulations with real carbon intensity data demonstrate up to 29% reduction in gCO₂/bit compared to QoS‑only orchestration while improving renewable utilization and resilience.<br /><strong>Summary (CN):</strong> 本文提出了一种碳感知的集成卫星‑空中‑地面网络 (ISATN) 编排框架，利用数字孪生技术以每比特二氧化碳当量克数 (gCO₂/bit) 为主要可持续性指标。该框架通过多时间尺度的计划‑执行‑检查‑行动 (PDCA) 循环，将日前预测与实时自适应优化相结合，利用碳感知切换、无人机工作周期调节和可再生能源感知边缘部署等控制手段。基于真实碳强度数据的仿真显示，相较于仅关注 QoS 的编排方案，可实现最高 29% 的 gCO₂/bit 减少，同时提升可再生能源利用率和系统韧性。<br /><strong>Keywords:</strong> carbon-aware orchestration, integrated satellite aerial terrestrial networks, digital twin, sustainability, green communications, PDCA optimization, UAV duty cycling, renewable-aware edge placement<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Shumaila Javaid, Nasir Saeed</div>
Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as key enablers of 6G, providing global connectivity for applications such as autonomous transportation, Industrial IoT, and disaster response. Their large-scale deployment, however, risks unsustainable energy use and carbon emissions. This work advances prior energy-aware studies by proposing a carbon-aware orchestration framework for ISATNs that leverages Digital Twin (DT) technology. The framework adopts grams of CO$_2$-equivalent per bit (gCO$_2$/bit) as a primary sustainability metric and implements a multi timescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting with real-time adaptive optimization. ISATN-specific control knobs, including carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement, are exploited to reduce emissions. Simulation results with real carbon intensity data show up to 29\% lower gCO$_2$/bit than QoS-only orchestration, while improving renewable utilization and resilience under adverse events.
<div><strong>Authors:</strong> Shumaila Javaid, Nasir Saeed</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a carbon‑aware orchestration framework for Integrated Satellite Aerial Terrestrial Networks (ISATNs) that utilizes Digital Twin technology to minimize grams of CO₂‑equivalent per transmitted bit. It implements a multi‑timescale Plan‑Do‑Check‑Act loop combining day‑ahead forecasting with real‑time adaptive optimization, exploiting control knobs such as carbon‑aware handovers, UAV duty cycling, and renewable‑aware edge placement. Simulations with real carbon intensity data demonstrate up to 29% reduction in gCO₂/bit compared to QoS‑only orchestration while improving renewable utilization and resilience.", "summary_cn": "本文提出了一种碳感知的集成卫星‑空中‑地面网络 (ISATN) 编排框架，利用数字孪生技术以每比特二氧化碳当量克数 (gCO₂/bit) 为主要可持续性指标。该框架通过多时间尺度的计划‑执行‑检查‑行动 (PDCA) 循环，将日前预测与实时自适应优化相结合，利用碳感知切换、无人机工作周期调节和可再生能源感知边缘部署等控制手段。基于真实碳强度数据的仿真显示，相较于仅关注 QoS 的编排方案，可实现最高 29% 的 gCO₂/bit 减少，同时提升可再生能源利用率和系统韧性。", "keywords": "carbon-aware orchestration, integrated satellite aerial terrestrial networks, digital twin, sustainability, green communications, PDCA optimization, UAV duty cycling, renewable-aware edge placement", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Shumaila Javaid", "Nasir Saeed"]}
]]></acme>

<pubDate>2025-10-01T06:49:42+00:00</pubDate>
</item>
<item>
<title>A Biophysical-Model-Informed Source Separation Framework For EMG Decomposition</title>
<link>https://papers.cool/arxiv/2510.17822</link>
<guid>https://papers.cool/arxiv/2510.17822</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes the Biophysical-Model-Informed Source Separation (BMISS) framework, which incorporates anatomically accurate forward EMG models into blind source separation to improve motor unit decomposition from surface EMG. Using MRI-based reconstructions and generative modeling, BMISS directly inverts the biophysical model to estimate neural drive and motor neuron properties unsupervised, achieving higher fidelity and lower computational cost in simulated experiments. This approach enables personalized, non‑invasive neuromuscular assessment with applications in diagnostics, prosthetic control, and rehabilitation.<br /><strong>Summary (CN):</strong> 本文提出了生物物理模型驱动的源分离框架（BMISS），将基于 MRI 的解剖重建和生成模型融入 EMG 正向模型，以在盲源分离中加入生物物理约束，从而提升表面 EMG 对运动单元的分解精度。该方法通过直接逆推生物物理模型，实现对神经驱动和运动神经元属性的无监督估计，在模拟实验中表现出更高的分解保真度并显著降低计算成本。此框架为非侵入性、个性化的神经肌肉评估提供了新途径，可用于临床诊断、假肢控制和神经康复。<br /><strong>Keywords:</strong> EMG decomposition, source separation, biophysical model, motor unit, neural drive, MRI-based anatomy, unsupervised inversion<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> D. Halatsis, P. Mamidanna, J. Pereira, D. Farina</div>
Recent advances in neural interfacing have enabled significant improvements in human-computer interaction, rehabilitation, and neuromuscular diagnostics. Motor unit (MU) decomposition from surface electromyography (sEMG) is a key technique for extracting neural drive information, but traditional blind source separation (BSS) methods fail to incorporate biophysical constraints, limiting their accuracy and interpretability. In this work, we introduce a novel Biophysical-Model-Informed Source Separation (BMISS) framework, which integrates anatomically accurate forward EMG models into the decomposition process. By leveraging MRI-based anatomical reconstructions and generative modeling, our approach enables direct inversion of a biophysically accurate forward model to estimate both neural drive and motor neuron properties in an unsupervised manner. Empirical validation in a controlled simulated setting demonstrates that BMISS achieves higher fidelity motor unit estimation while significantly reducing computational cost compared to traditional methods. This framework paves the way for non-invasive, personalized neuromuscular assessments, with potential applications in clinical diagnostics, prosthetic control, and neurorehabilitation.
<div><strong>Authors:</strong> D. Halatsis, P. Mamidanna, J. Pereira, D. Farina</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes the Biophysical-Model-Informed Source Separation (BMISS) framework, which incorporates anatomically accurate forward EMG models into blind source separation to improve motor unit decomposition from surface EMG. Using MRI-based reconstructions and generative modeling, BMISS directly inverts the biophysical model to estimate neural drive and motor neuron properties unsupervised, achieving higher fidelity and lower computational cost in simulated experiments. This approach enables personalized, non‑invasive neuromuscular assessment with applications in diagnostics, prosthetic control, and rehabilitation.", "summary_cn": "本文提出了生物物理模型驱动的源分离框架（BMISS），将基于 MRI 的解剖重建和生成模型融入 EMG 正向模型，以在盲源分离中加入生物物理约束，从而提升表面 EMG 对运动单元的分解精度。该方法通过直接逆推生物物理模型，实现对神经驱动和运动神经元属性的无监督估计，在模拟实验中表现出更高的分解保真度并显著降低计算成本。此框架为非侵入性、个性化的神经肌肉评估提供了新途径，可用于临床诊断、假肢控制和神经康复。", "keywords": "EMG decomposition, source separation, biophysical model, motor unit, neural drive, MRI-based anatomy, unsupervised inversion", "scoring": {"interpretability": 4, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["D. Halatsis", "P. Mamidanna", "J. Pereira", "D. Farina"]}
]]></acme>

<pubDate>2025-09-29T17:53:52+00:00</pubDate>
</item>
<item>
<title>LLM Assisted Alpha Fairness for 6 GHz WiFi and NR_U Coexistence: An Agentic Orchestrator for Throughput, Energy, and SLA</title>
<link>https://papers.cool/arxiv/2510.17814</link>
<guid>https://papers.cool/arxiv/2510.17814</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an agentic orchestrator that uses a large language model (LLM) to generate interpretable fairness knobs (α‑fairness index, duty‑cycle caps, class weights) for joint Wi‑Fi and 5G NR‑U coexistence in the unlicensed 6 GHz band, then applies a deterministic optimizer to compute a feasible α‑fair allocation that balances throughput, energy consumption, and service‑level objectives while clamping unsafe policies and falling back to a rule‑based baseline. Simulations with two 160 MHz channels demonstrate up to 35 % energy reduction and up to a 3.5 % increase in total bits compared to the baseline, with competitive throughput. Code, per‑epoch logs, and plotting utilities are released for reproducibility.<br /><strong>Summary (CN):</strong> 本文提出一种代理式控制器，利用大语言模型（LLM）生成可解释的公平参数（α‑公平指数、通道占空比上限、类别权重），并通过确定性优化器求解满足吞吐量、能耗和服务水平目标的 α‑公平分配，同时对不安全的策略进行限制并回退到规则基线。在两条 160 MHz 频道的 6 GHz 仿真中，该方法实现了最高 35 % 的能耗降低并提升 3.5 % 的总比特量，吞吐量保持竞争力。作者发布代码、每轮日志和绘图工具以保证可复现性。<br /><strong>Keywords:</strong> LLM, alpha-fairness, Wi-Fi, NR-U, 6GHz coexistence, energy efficiency, throughput, policy orchestration, deterministic optimizer, agentic controller<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Qun Wang, Yingzhou Lu, Guiran Liu, Binrong Zhu, Yang Liu</div>
Unlicensed 6GHz is becoming a primary workhorse for high-capacity access, with Wi-Fi and 5G NR-U competing for the same channels under listen-before-talk (LBT) rules. Operating in this regime requires decisions that jointly trade throughput, energy, and service-level objectives while remaining safe and auditable. We present an agentic controller that separates {policy} from {execution}. At the start of each scheduling epoch the agent summarizes telemetry (per-channel busy and baseline LBT failure; per-user CQI, backlog, latency, battery, priority, and power mode) and invokes a large language model (LLM) to propose a small set of interpretable knobs: a fairness index \alpha, per-channel duty-cycle caps for Wi-Fi/NR-U, and class weights. A deterministic optimizer then enforces feasibility and computes an \alpha-fair allocation that internalizes LBT losses and energy cost; malformed or unsafe policies are clamped and fall back to a rule baseline. In a 6GHz simulator with two 160MHz channels and mixed Wi-Fi/NR-U users, LLM-assisted policies consistently improve energy efficiency while keeping throughput competitive with a strong rule baseline. One LLM lowers total energy by 35.3% at modest throughput loss, and another attains the best overall trade-off, finishing with higher total bits (+3.5%) and higher bits/J (+12.2%) than the baseline. We release code, per-epoch logs, and plotting utilities to reproduce all figures and numbers, illustrating how transparent, policy-level LLM guidance can safely improve wireless coexistence.
<div><strong>Authors:</strong> Qun Wang, Yingzhou Lu, Guiran Liu, Binrong Zhu, Yang Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an agentic orchestrator that uses a large language model (LLM) to generate interpretable fairness knobs (α‑fairness index, duty‑cycle caps, class weights) for joint Wi‑Fi and 5G NR‑U coexistence in the unlicensed 6 GHz band, then applies a deterministic optimizer to compute a feasible α‑fair allocation that balances throughput, energy consumption, and service‑level objectives while clamping unsafe policies and falling back to a rule‑based baseline. Simulations with two 160 MHz channels demonstrate up to 35 % energy reduction and up to a 3.5 % increase in total bits compared to the baseline, with competitive throughput. Code, per‑epoch logs, and plotting utilities are released for reproducibility.", "summary_cn": "本文提出一种代理式控制器，利用大语言模型（LLM）生成可解释的公平参数（α‑公平指数、通道占空比上限、类别权重），并通过确定性优化器求解满足吞吐量、能耗和服务水平目标的 α‑公平分配，同时对不安全的策略进行限制并回退到规则基线。在两条 160 MHz 频道的 6 GHz 仿真中，该方法实现了最高 35 % 的能耗降低并提升 3.5 % 的总比特量，吞吐量保持竞争力。作者发布代码、每轮日志和绘图工具以保证可复现性。", "keywords": "LLM, alpha-fairness, Wi-Fi, NR-U, 6GHz coexistence, energy efficiency, throughput, policy orchestration, deterministic optimizer, agentic controller", "scoring": {"interpretability": 4, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Qun Wang", "Yingzhou Lu", "Guiran Liu", "Binrong Zhu", "Yang Liu"]}
]]></acme>

<pubDate>2025-09-26T15:34:45+00:00</pubDate>
</item>
<item>
<title>Visual Space Optimization for Zero-shot Learning</title>
<link>https://papers.cool/arxiv/1907.00330</link>
<guid>https://papers.cool/arxiv/1907.00330</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes optimizing the visual embedding space for zero-shot learning by (1) learning a visual prototype for each class and (2) using a multilayer perceptron to create an intermediate embedding that makes visual data structures more distinctive. Experiments on four benchmark datasets show that these strategies improve the alignment between semantic vectors and visual features, achieving state-of-the-art performance for prototype-based methods.<br /><strong>Summary (CN):</strong> 本文提出通过两种方式优化零样本学习的视觉嵌入空间：（1）为每个类别学习视觉原型，使类别由原型特征表示；（2）利用多层感知机构建中间嵌入，使视觉数据结构更具区分度。四个基准数据集的实验表明，这些方法提升了语义向量与视觉特征的匹配，并在基于原型的方法中取得了最新的性能。<br /><strong>Keywords:</strong> zero-shot learning, visual space optimization, visual prototype, embedding space, multilayer perceptron<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xinsheng Wang, Shanmin Pang, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Yaochen Li</div>
Zero-shot learning, which aims to recognize new categories that are not included in the training set, has gained popularity owing to its potential ability in the real-word applications. Zero-shot learning models rely on learning an embedding space, where both semantic descriptions of classes and visual features of instances can be embedded for nearest neighbor search. Recently, most of the existing works consider the visual space formulated by deep visual features as an ideal choice of the embedding space. However, the discrete distribution of instances in the visual space makes the data structure unremarkable. We argue that optimizing the visual space is crucial as it allows semantic vectors to be embedded into the visual space more effectively. In this work, we propose two strategies to accomplish this purpose. One is the visual prototype based method, which learns a visual prototype for each visual class, so that, in the visual space, a class can be represented by a prototype feature instead of a series of discrete visual features. The other is to optimize the visual feature structure in an intermediate embedding space, and in this method we successfully devise a multilayer perceptron framework based algorithm that is able to learn the common intermediate embedding space and meanwhile to make the visual data structure more distinctive. Through extensive experimental evaluation on four benchmark datasets, we demonstrate that optimizing visual space is beneficial for zero-shot learning. Besides, the proposed prototype based method achieves the new state-of-the-art performance.
<div><strong>Authors:</strong> Xinsheng Wang, Shanmin Pang, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Yaochen Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes optimizing the visual embedding space for zero-shot learning by (1) learning a visual prototype for each class and (2) using a multilayer perceptron to create an intermediate embedding that makes visual data structures more distinctive. Experiments on four benchmark datasets show that these strategies improve the alignment between semantic vectors and visual features, achieving state-of-the-art performance for prototype-based methods.", "summary_cn": "本文提出通过两种方式优化零样本学习的视觉嵌入空间：（1）为每个类别学习视觉原型，使类别由原型特征表示；（2）利用多层感知机构建中间嵌入，使视觉数据结构更具区分度。四个基准数据集的实验表明，这些方法提升了语义向量与视觉特征的匹配，并在基于原型的方法中取得了最新的性能。", "keywords": "zero-shot learning, visual space optimization, visual prototype, embedding space, multilayer perceptron", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xinsheng Wang", "Shanmin Pang", "Jihua Zhu", "Zhongyu Li", "Zhiqiang Tian", "Yaochen Li"]}
]]></acme>

<pubDate>2019-06-30T06:44:24+00:00</pubDate>
</item>
</channel>
</rss>