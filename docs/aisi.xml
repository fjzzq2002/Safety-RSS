<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>AISI - Research</title>
<link>https://www.aisi.gov.uk/research</link>


<item>
<title>Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples</title>
<link>https://www.aisi.gov.uk/research/poisoning-attacks-on-llms-require-a-near-constant-number-of-poison-samples</link>
<guid>https://www.aisi.gov.uk/research/poisoning-attacks-on-llms-require-a-near-constant-number-of-poison-samples</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates data poisoning attacks on large language models and demonstrates that the number of poisoned training samples required to cause targeted misbehavior stays nearly constant as model size grows, indicating that scaling up does not significantly reduce poisoning vulnerability.<br /><strong>Summary (CN):</strong> 本文研究了对大型语言模型的数 据中毒攻击，发现为导致特定错误行为所需的中毒训练样本数量在模型规模扩大时几乎保持不变，这表明仅靠模型规模的扩大并不能显著降低中毒风险。<br /><strong>Keywords:</strong> poisoning attacks, large language models, data poisoning, robustness, adversarial training, sample efficiency, model security<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates data poisoning attacks on large language models and demonstrates that the number of poisoned training samples required to cause targeted misbehavior stays nearly constant as model size grows, indicating that scaling up does not significantly reduce poisoning vulnerability.", "summary_cn": "本文研究了对大型语言模型的数 据中毒攻击，发现为导致特定错误行为所需的中毒训练样本数量在模型规模扩大时几乎保持不变，这表明仅靠模型规模的扩大并不能显著降低中毒风险。", "keywords": "poisoning attacks, large language models, data poisoning, robustness, adversarial training, sample efficiency, model security", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 08 Oct 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title>
<link>https://www.aisi.gov.uk/research/inoculation-prompting-eliciting-traits-from-llms-during-training-can-suppress-them-at-test-time</link>
<guid>https://www.aisi.gov.uk/research/inoculation-prompting-eliciting-traits-from-llms-during-training-can-suppress-them-at-test-time</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes inoculation prompting, a training-time technique that elicits specific traits in large language models so that these traits can later be reliably suppressed during inference. Experiments show that models trained with such prompts display reduced undesirable behavior when the corresponding inoculation prompt is applied at test time. This approach aims to improve safety by providing a controllable mechanism for trait modulation.<br /><strong>Summary (CN):</strong> 本文提出了“免疫提示”（inoculation prompting）方法，即在大语言模型训练阶段激发特定特性，以便在推理阶段通过相应提示抑制这些特性。实验表明，使用该技巧训练的模型在测试时能够显著降低不良行为，从而实现对模型特性的可控调节，提升安全性。<br /><strong>Keywords:</strong> inoculation prompting, trait suppression, LLM safety, alignment, control methods, prompt engineering<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes inoculation prompting, a training-time technique that elicits specific traits in large language models so that these traits can later be reliably suppressed during inference. Experiments show that models trained with such prompts display reduced undesirable behavior when the corresponding inoculation prompt is applied at test time. This approach aims to improve safety by providing a controllable mechanism for trait modulation.", "summary_cn": "本文提出了“免疫提示”（inoculation prompting）方法，即在大语言模型训练阶段激发特定特性，以便在推理阶段通过相应提示抑制这些特性。实验表明，使用该技巧训练的模型在测试时能够显著降低不良行为，从而实现对模型特性的可控调节，提升安全性。", "keywords": "inoculation prompting, trait suppression, LLM safety, alignment, control methods, prompt engineering", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Sun, 05 Oct 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Conversational AI increases political knowledge as effectively as self-directed internet search</title>
<link>https://www.aisi.gov.uk/research/conversational-ai-increases-political-knowledge-as-effectively-as-self-directed-internet-search</link>
<guid>https://www.aisi.gov.uk/research/conversational-ai-increases-political-knowledge-as-effectively-as-self-directed-internet-search</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports an experimental study comparing the effect of using conversational AI agents to self‑directed internet search for increasing participants' political knowledge. Participants who interacted with a conversational AI answered political questions and showed knowledge gains comparable to those who performed traditional web searches. The findings suggest that conversational AI can serve as an effective tool for political education.<br /><strong>Summary (CN):</strong> 本文报告了一项实验研究，比较使用会话式 AI（conversational AI）与自我导向的互联网搜索在提升参与者政治知识方面的效果。与传统网络搜索的参与者相比，使用会话式 AI 回答政治问题的参与者的知识增长相当。研究表明，会话式 AI 可作为有效的政治教育工具。<br /><strong>Keywords:</strong> conversational AI, political knowledge, internet search, user study, AI-assisted learning, education<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 5, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports an experimental study comparing the effect of using conversational AI agents to self‑directed internet search for increasing participants' political knowledge. Participants who interacted with a conversational AI answered political questions and showed knowledge gains comparable to those who performed traditional web searches. The findings suggest that conversational AI can serve as an effective tool for political education.", "summary_cn": "本文报告了一项实验研究，比较使用会话式 AI（conversational AI）与自我导向的互联网搜索在提升参与者政治知识方面的效果。与传统网络搜索的参与者相比，使用会话式 AI 回答政治问题的参与者的知识增长相当。研究表明，会话式 AI 可作为有效的政治教育工具。", "keywords": "conversational AI, political knowledge, internet search, user study, AI-assisted learning, education", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}
]]></acme>

<pubDate>Mon, 08 Sep 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Lessons from studying two-hop latent reasoning</title>
<link>https://www.aisi.gov.uk/research/lessons-from-studying-two-hop-latent-reasoning</link>
<guid>https://www.aisi.gov.uk/research/lessons-from-studying-two-hop-latent-reasoning</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how large language models perform two‑hop latent reasoning, analysing the internal representations and pathways that enable multi‑step inference without explicit chain‑of‑thought prompts. It identifies characteristic activation patterns, failure cases, and proposes methods to surface and evaluate such hidden reasoning steps, offering insights for improving model transparency and reliability.<br /><strong>Summary (CN):</strong> 本文研究了大型语言模型在两跳潜在推理（two‑hop latent reasoning）时的内部表征与路径，分析了模型在未显式链式思考提示下实现多步推理的机制。通过识别特征激活模式和失败案例，并提出了展示与评估这些隐藏推理步骤的方法，为提升模型透明度和可靠性提供了见解。<br /><strong>Keywords:</strong> latent reasoning, multi-hop inference, mechanistic interpretability, chain-of-thought, model transparency, AI safety<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how large language models perform two‑hop latent reasoning, analysing the internal representations and pathways that enable multi‑step inference without explicit chain‑of‑thought prompts. It identifies characteristic activation patterns, failure cases, and proposes methods to surface and evaluate such hidden reasoning steps, offering insights for improving model transparency and reliability.", "summary_cn": "本文研究了大型语言模型在两跳潜在推理（two‑hop latent reasoning）时的内部表征与路径，分析了模型在未显式链式思考提示下实现多步推理的机制。通过识别特征激活模式和失败案例，并提出了展示与评估这些隐藏推理步骤的方法，为提升模型透明度和可靠性提供了见解。", "keywords": "latent reasoning, multi-hop inference, mechanistic interpretability, chain-of-thought, model transparency, AI safety", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Sat, 06 Sep 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs</title>
<link>https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms</link>
<guid>https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a method of filtering pre‑training data to embed tamper‑resistant safety safeguards directly into open‑weight large language models. By removing certain risky content and patterns from the training corpus, the authors claim the resulting models are less susceptible to malicious prompt engineering and internal modification, while retaining useful capabilities. Experiments demonstrate that filtered models exhibit reduced propensity for unsafe generations and higher resistance to adversarial influence compared to baseline models trained on unfiltered data.<br /><strong>Summary (CN):</strong> 本文提出通过对预训练数据进行过滤，在开源权重的大语言模型中内嵌防篡改的安全防护机制。作者通过剔除训练语料中的高风险内容和模式，声称生成的模型对恶意提示工程和内部修改的易感性降低，同时保持有用功能。实验表明，过滤后的模型相比未过滤基线模型表现出更低的不安全生成倾向以及更高的对抗性抵抗能力。<br /><strong>Keywords:</strong> data filtering, pretraining safeguards, tamper resistance, open-weight LLMs, AI safety, dataset curation, alignment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a method of filtering pre‑training data to embed tamper‑resistant safety safeguards directly into open‑weight large language models. By removing certain risky content and patterns from the training corpus, the authors claim the resulting models are less susceptible to malicious prompt engineering and internal modification, while retaining useful capabilities. Experiments demonstrate that filtered models exhibit reduced propensity for unsafe generations and higher resistance to adversarial influence compared to baseline models trained on unfiltered data.", "summary_cn": "本文提出通过对预训练数据进行过滤，在开源权重的大语言模型中内嵌防篡改的安全防护机制。作者通过剔除训练语料中的高风险内容和模式，声称生成的模型对恶意提示工程和内部修改的易感性降低，同时保持有用功能。实验表明，过滤后的模型相比未过滤基线模型表现出更低的不安全生成倾向以及更高的对抗性抵抗能力。", "keywords": "data filtering, pretraining safeguards, tamper resistance, open-weight LLMs, AI safety, dataset curation, alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Fri, 08 Aug 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Security challenges in AI agent deployment: Insights from a large scale public competition</title>
<link>https://www.aisi.gov.uk/research/security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition</link>
<guid>https://www.aisi.gov.uk/research/security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyses the security challenges encountered when deploying AI agents, drawing lessons from a large‑scale public competition that exposed vulnerabilities such as adversarial manipulation, reward hacking, and unintended interactions. It presents a taxonomy of failure modes observed, discusses mitigation strategies, and offers recommendations for safer deployment practices in real‑world settings.<br /><strong>Summary (CN):</strong> 本文分析了在部署 AI 代理时出现的安全挑战，借助一次大规模公开竞争的案例，揭示了对抗性操纵、奖励黑客和意外交互等漏洞。文章提供了观察到的失效模式分类，讨论了相应的缓解策略，并给出在实际环境中实现更安全部署的建议。<br /><strong>Keywords:</strong> AI agent deployment, security challenges, adversarial attacks, robustness, public competition, safety evaluation, deployment best practices<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyses the security challenges encountered when deploying AI agents, drawing lessons from a large‑scale public competition that exposed vulnerabilities such as adversarial manipulation, reward hacking, and unintended interactions. It presents a taxonomy of failure modes observed, discusses mitigation strategies, and offers recommendations for safer deployment practices in real‑world settings.", "summary_cn": "本文分析了在部署 AI 代理时出现的安全挑战，借助一次大规模公开竞争的案例，揭示了对抗性操纵、奖励黑客和意外交互等漏洞。文章提供了观察到的失效模式分类，讨论了相应的缓解策略，并给出在实际环境中实现更安全部署的建议。", "keywords": "AI agent deployment, security challenges, adversarial attacks, robustness, public competition, safety evaluation, deployment best practices", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sun, 27 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>STACK: Adversarial attacks on LLM safeguard pipelines</title>
<link>https://www.aisi.gov.uk/research/stack-adversarial-attacks-on-llm-safeguard-pipelines</link>
<guid>https://www.aisi.gov.uk/research/stack-adversarial-attacks-on-llm-safeguard-pipelines</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces STACK, a framework that systematically evaluates adversarial attacks targeting the safeguard pipelines of large language models, including prompt injection, token-level perturbations, and policy‑evading strategies. It demonstrates how such attacks can bypass safety filters and cause the model to generate disallowed content, and offers a taxonomy of attack vectors along with empirical results on common LLM deployment setups.<br /><strong>Summary (CN):</strong> 本文提出了 STACK 框架，系统评估针对大语言模型安全防护流水线的对抗攻击，包括提示注入、词元级扰动和规避策略。研究表明这些攻击能够绕过安全过滤器，使模型生成禁用内容，并提供了攻击向量的分类及在常见 LLM 部署环境中的实验结果。<br /><strong>Keywords:</strong> adversarial attacks, LLM safety, prompt injection, safeguard pipeline, robustness, alignment, policy evasion<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces STACK, a framework that systematically evaluates adversarial attacks targeting the safeguard pipelines of large language models, including prompt injection, token-level perturbations, and policy‑evading strategies. It demonstrates how such attacks can bypass safety filters and cause the model to generate disallowed content, and offers a taxonomy of attack vectors along with empirical results on common LLM deployment setups.", "summary_cn": "本文提出了 STACK 框架，系统评估针对大语言模型安全防护流水线的对抗攻击，包括提示注入、词元级扰动和规避策略。研究表明这些攻击能够绕过安全过滤器，使模型生成禁用内容，并提供了攻击向量的分类及在常见 LLM 部署环境中的实验结果。", "keywords": "adversarial attacks, LLM safety, prompt injection, safeguard pipeline, robustness, alignment, policy evasion", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Fri, 18 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>The levers of political persuasion with conversational AI</title>
<link>https://www.aisi.gov.uk/research/the-levers-of-political-persuasion-with-conversational-ai</link>
<guid>https://www.aisi.gov.uk/research/the-levers-of-political-persuasion-with-conversational-ai</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyses how conversational AI systems can be employed as tools for political persuasion, identifying key levers such as personalization, framing, and interactive engagement. It discusses the potential societal impact and outlines recommendations for mitigating misuse.<br /><strong>Summary (CN):</strong> 本文分析了对话式人工智能如何被用作政治说服的工具，指出个性化、框架设定和互动参与等关键影响因素。文章讨论了其潜在的社会影响，并提出了防止滥用的建议。<br /><strong>Keywords:</strong> political persuasion, conversational AI, influence levers, manipulation, AI safety, social impact<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyses how conversational AI systems can be employed as tools for political persuasion, identifying key levers such as personalization, framing, and interactive engagement. It discusses the potential societal impact and outlines recommendations for mitigating misuse.", "summary_cn": "本文分析了对话式人工智能如何被用作政治说服的工具，指出个性化、框架设定和互动参与等关键影响因素。文章讨论了其潜在的社会影响，并提出了防止滥用的建议。", "keywords": "political persuasion, conversational AI, influence levers, manipulation, AI safety, social impact", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "other"}}
]]></acme>

<pubDate>Fri, 18 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Chain of thought monitorability: A new and fragile opportunity for AI safety</title>
<link>https://www.aisi.gov.uk/research/chain-of-thought-monitorability-a-new-and-fragile-opportunity-for-ai-safety</link>
<guid>https://www.aisi.gov.uk/research/chain-of-thought-monitorability-a-new-and-fragile-opportunity-for-ai-safety</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes that the internal chain‑of‑thought reasoning of large language models can be monitored to detect potentially unsafe or misaligned thoughts, presenting a framework and experimental results that illustrate both the promise and fragility of such monitoring. It outlines methods for extracting intermediate reasoning steps, criteria for flagging risky content, and discusses limitations related to model scale, prompting, and adversarial evasion. The work positions chain‑of‑thought monitorability as a new, albeit delicate, safety opportunity.<br /><strong>Summary (CN):</strong> 本文提出通过监测大语言模型内部的链式思考过程来检测潜在的危险或不对齐的思路，展示了一个框架并提供实验结果，说明此类监控的潜力与脆弱性。文中介绍了提取中间推理步骤的方法、标记风险内容的标准，并讨论了模型规模、提示方式及对抗规避等限制因素。该工作将链式思考的可监测性定位为一种新出现但易碎的 AI 安全机会。<br /><strong>Keywords:</strong> chain-of-thought, monitorability, AI safety, internal probing, interpretability, alignment, detection<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes that the internal chain‑of‑thought reasoning of large language models can be monitored to detect potentially unsafe or misaligned thoughts, presenting a framework and experimental results that illustrate both the promise and fragility of such monitoring. It outlines methods for extracting intermediate reasoning steps, criteria for flagging risky content, and discusses limitations related to model scale, prompting, and adversarial evasion. The work positions chain‑of‑thought monitorability as a new, albeit delicate, safety opportunity.", "summary_cn": "本文提出通过监测大语言模型内部的链式思考过程来检测潜在的危险或不对齐的思路，展示了一个框架并提供实验结果，说明此类监控的潜力与脆弱性。文中介绍了提取中间推理步骤的方法、标记风险内容的标准，并讨论了模型规模、提示方式及对抗规避等限制因素。该工作将链式思考的可监测性定位为一种新出现但易碎的 AI 安全机会。", "keywords": "chain-of-thought, monitorability, AI safety, internal probing, interpretability, alignment, detection", "scoring": {"interpretability": 7, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Tue, 15 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>HiBayES: A hierarchical bayesian modelling framework for AI evaluation statistics</title>
<link>https://www.aisi.gov.uk/research/hibayes-a-hierarchical-bayesian-modeling-framework-for-ai-evaluation-statistics</link>
<guid>https://www.aisi.gov.uk/research/hibayes-a-hierarchical-bayesian-modeling-framework-for-ai-evaluation-statistics</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HiBayES introduces a hierarchical Bayesian modelling framework designed to provide robust statistical evaluation of AI systems, allowing researchers to quantify uncertainty and aggregate evaluation metrics across multiple experiments. The paper presents the methodology, inference procedures, and several case studies demonstrating how the framework can improve the reliability of AI performance reporting.<br /><strong>Summary (CN):</strong> HiBayES 提出了一种层次贝叶斯建模框架，用于对 AI 系统进行稳健的统计评估，能够量化不确定性并在多个实验中汇总评估指标。本文展示了该方法的原理、推断流程以及若干案例，说明其提升 AI 性能报告可靠性的作用。<br /><strong>Keywords:</strong> hierarchical Bayesian, AI evaluation, statistical framework, uncertainty quantification, model assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HiBayES introduces a hierarchical Bayesian modelling framework designed to provide robust statistical evaluation of AI systems, allowing researchers to quantify uncertainty and aggregate evaluation metrics across multiple experiments. The paper presents the methodology, inference procedures, and several case studies demonstrating how the framework can improve the reliability of AI performance reporting.", "summary_cn": "HiBayES 提出了一种层次贝叶斯建模框架，用于对 AI 系统进行稳健的统计评估，能够量化不确定性并在多个实验中汇总评估指标。本文展示了该方法的原理、推断流程以及若干案例，说明其提升 AI 性能报告可靠性的作用。", "keywords": "hierarchical Bayesian, AI evaluation, statistical framework, uncertainty quantification, model assessment", "scoring": {"interpretability": 2, "understanding": 5, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}}
]]></acme>

<pubDate>Sun, 13 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>White Box Control at UK AISI - update on sandbagging investigations</title>
<link>https://www.aisi.gov.uk/research/white-box-control-at-uk-aisi---update-on-sandbagging-investigations</link>
<guid>https://www.aisi.gov.uk/research/white-box-control-at-uk-aisi---update-on-sandbagging-investigations</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The report describes recent investigations at the UK AISI into sandbagging behavior in AI systems, employing white‑box control methods to expose internal signals of intentional performance degradation and to develop mitigation strategies.<br /><strong>Summary (CN):</strong> 该报告描述了英国 AISI 最近对 AI 系统中 "sandbagging"（故意降低表现）行为的调查，使用白箱控制方法揭示内部的欺骗信号，并制定相应的缓解策略。<br /><strong>Keywords:</strong> sandbagging, white-box control, AI safety, model monitoring, alignment, interpretability, deception detection<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The report describes recent investigations at the UK AISI into sandbagging behavior in AI systems, employing white‑box control methods to expose internal signals of intentional performance degradation and to develop mitigation strategies.", "summary_cn": "该报告描述了英国 AISI 最近对 AI 系统中 \"sandbagging\"（故意降低表现）行为的调查，使用白箱控制方法揭示内部的欺骗信号，并制定相应的缓解策略。", "keywords": "sandbagging, white-box control, AI safety, model monitoring, alignment, interpretability, deception detection", "scoring": {"interpretability": 7, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Thu, 10 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Lessons from a chimp: AI "scheming" and the quest for ape language</title>
<link>https://www.aisi.gov.uk/research/lessons-from-a-chimp-ai-scheming-and-the-quest-for-ape-language</link>
<guid>https://www.aisi.gov.uk/research/lessons-from-a-chimp-ai-scheming-and-the-quest-for-ape-language</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The article draws parallels between chimpanzee language research and the phenomenon of AI "scheming," arguing that lessons from ape communication can inform strategies for detecting and mitigating deceptive alignment in advanced systems. It reviews key findings from comparative cognition and proposes interdisciplinary research directions that combine insights from animal communication, interpretability, and AI safety. The goal is to develop better conceptual tools and experimental designs to anticipate and intervene in scheming behaviors before they become dangerous.<br /><strong>Summary (CN):</strong> 本文将黑猩猩语言研究与 AI “scheming”（欺骗性对齐）现象进行类比，指出从类人灵长类动物的交流中可以获得用于发现和缓解高级系统中欺骗性行为的思路。文中回顾了比较认知学的重要发现，并提出将动物沟通、可解释性和 AI 安全的跨学科见解结合起来的研究方向，旨在在此类欺骗行为危害显现之前建立更好的概念工具和实验设计。<br /><strong>Keywords:</strong> AI scheming, deceptive alignment, ape language, chimp communication, AI safety, interpretability, language emergence, alignment research<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 7, Technicality: 3, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The article draws parallels between chimpanzee language research and the phenomenon of AI \"scheming,\" arguing that lessons from ape communication can inform strategies for detecting and mitigating deceptive alignment in advanced systems. It reviews key findings from comparative cognition and proposes interdisciplinary research directions that combine insights from animal communication, interpretability, and AI safety. The goal is to develop better conceptual tools and experimental designs to anticipate and intervene in scheming behaviors before they become dangerous.", "summary_cn": "本文将黑猩猩语言研究与 AI “scheming”（欺骗性对齐）现象进行类比，指出从类人灵长类动物的交流中可以获得用于发现和缓解高级系统中欺骗性行为的思路。文中回顾了比较认知学的重要发现，并提出将动物沟通、可解释性和 AI 安全的跨学科见解结合起来的研究方向，旨在在此类欺骗行为危害显现之前建立更好的概念工具和实验设计。", "keywords": "AI scheming, deceptive alignment, ape language, chimp communication, AI safety, interpretability, language emergence, alignment research", "scoring": {"interpretability": 4, "understanding": 6, "safety": 7, "technicality": 3, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Fri, 04 Jul 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Avoiding obfuscation with prover-estimator debate</title>
<link>https://www.aisi.gov.uk/research/avoiding-obfuscation-with-prover-estimator-debate</link>
<guid>https://www.aisi.gov.uk/research/avoiding-obfuscation-with-prover-estimator-debate</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a prover‑estimator debate framework designed to reduce AI obfuscation by having a prover generate detailed reasoning and an estimator evaluate the transparency of that reasoning. By structuring an interactive debate, the approach aims to surface hidden model internals, improve interpretability, and provide safety signals that the model is not deliberately concealing information.<br /><strong>Summary (CN):</strong> 本文提出了一种 prover‑estimator 辩论框架，旨在通过让 prover（证明者）生成详细推理并让 estimator（评估者）检验其透明度，以降低 AI 的掩盖行为。通过这种互动辩论，能够揭示模型内部隐藏信息，提升可解释性，并提供模型未刻意隐藏信息的安全信号。<br /><strong>Keywords:</strong> prover-estimator debate, obfuscation, interpretability, AI safety, transparency, debate framework<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 6, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a prover‑estimator debate framework designed to reduce AI obfuscation by having a prover generate detailed reasoning and an estimator evaluate the transparency of that reasoning. By structuring an interactive debate, the approach aims to surface hidden model internals, improve interpretability, and provide safety signals that the model is not deliberately concealing information.", "summary_cn": "本文提出了一种 prover‑estimator 辩论框架，旨在通过让 prover（证明者）生成详细推理并让 estimator（评估者）检验其透明度，以降低 AI 的掩盖行为。通过这种互动辩论，能够揭示模型内部隐藏信息，提升可解释性，并提供模型未刻意隐藏信息的安全信号。", "keywords": "prover-estimator debate, obfuscation, interpretability, AI safety, transparency, debate framework", "scoring": {"interpretability": 7, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Sun, 15 Jun 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>An example safety case for safeguards against misuse</title>
<link>https://www.aisi.gov.uk/research/an-example-safety-case-for-safeguards-against-misuse</link>
<guid>https://www.aisi.gov.uk/research/an-example-safety-case-for-safeguards-against-misuse</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents an illustrative safety case that outlines how to design, argue for, and verify safeguards intended to prevent intentional or harmful misuse of AI systems. It details a threat model, proposes concrete mitigations, describes verification activities, and discusses how such a safety case can be integrated into regulatory or organizational governance processes.<br /><strong>Summary (CN):</strong> 本文提供了一个示例安全案例，阐述了如何设计、论证并验证防止 AI 系统被有意或有害滥用的安全保障措施。它详细描述了威胁模型、具体的缓解措施、验证活动，并讨论了该安全案例如何融入监管或组织治理框架。<br /><strong>Keywords:</strong> safety case, misuse mitigation, AI safeguards, threat modeling, control, governance, risk assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents an illustrative safety case that outlines how to design, argue for, and verify safeguards intended to prevent intentional or harmful misuse of AI systems. It details a threat model, proposes concrete mitigations, describes verification activities, and discusses how such a safety case can be integrated into regulatory or organizational governance processes.", "summary_cn": "本文提供了一个示例安全案例，阐述了如何设计、论证并验证防止 AI 系统被有意或有害滥用的安全保障措施。它详细描述了威胁模型、具体的缓解措施、验证活动，并讨论了该安全案例如何融入监管或组织治理框架。", "keywords": "safety case, misuse mitigation, AI safeguards, threat modeling, control, governance, risk assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}}
]]></acme>

<pubDate>Thu, 05 Jun 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Existing Large Language Model unlearning evaluations are inconclusive</title>
<link>https://www.aisi.gov.uk/research/existing-large-language-model-unlearning-evaluations-are-inconclusive</link>
<guid>https://www.aisi.gov.uk/research/existing-large-language-model-unlearning-evaluations-are-inconclusive</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys existing evaluation methods for unlearning in large language models, showing that current metrics and benchmarks are inconsistent and often fail to provide conclusive evidence of successful data removal. It analyses the limitations of these evaluations and proposes recommendations for more rigorous and reliable assessment frameworks.<br /><strong>Summary (CN):</strong> 本文审视了大语言模型中“unlearning”（数据删除）评估的现有方法，指出当前的度量指标和基准测试存在不一致性，难以给出是否成功删除数据的明确结论。文章分析了这些评估的局限性，并提出了实现更严格、可靠评估框架的建议。<br /><strong>Keywords:</strong> large language model, unlearning, evaluation, data deletion, safety, privacy, benchmark<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys existing evaluation methods for unlearning in large language models, showing that current metrics and benchmarks are inconsistent and often fail to provide conclusive evidence of successful data removal. It analyses the limitations of these evaluations and proposes recommendations for more rigorous and reliable assessment frameworks.", "summary_cn": "本文审视了大语言模型中“unlearning”（数据删除）评估的现有方法，指出当前的度量指标和基准测试存在不一致性，难以给出是否成功删除数据的明确结论。文章分析了这些评估的局限性，并提出了实现更严格、可靠评估框架的建议。", "keywords": "large language model, unlearning, evaluation, data deletion, safety, privacy, benchmark", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "control"}}
]]></acme>

<pubDate>Sat, 31 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>RepliBench: Evaluating the autonomous replication capabilities of language model agents</title>
<link>https://www.aisi.gov.uk/research/replibench-evaluating-the-autonomous-replication-capabilities-of-language-model-agents</link>
<guid>https://www.aisi.gov.uk/research/replibench-evaluating-the-autonomous-replication-capabilities-of-language-model-agents</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents RepliBench, a benchmark designed to evaluate the ability of language‑model agents to autonomously replicate themselves or create functional copies. It defines a set of tasks, metrics, and evaluation protocols, and reports results across multiple model scales, highlighting how replication capability emerges with model size and prompting strategies.<br /><strong>Summary (CN):</strong> 本文介绍了 RepliBench 基准，用于评估语言模型代理自主复制自身或生成功能相似副本的能力。论文定义了一系列任务、度量指标和评估流程，并在不同规模的模型上进行实验，展示了复制能力随模型规模和提示策略的出现情况。<br /><strong>Keywords:</strong> autonomous replication, language model agents, benchmark, self-replication, safety evaluation, emergent capabilities, replication metrics, AI risk<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents RepliBench, a benchmark designed to evaluate the ability of language‑model agents to autonomously replicate themselves or create functional copies. It defines a set of tasks, metrics, and evaluation protocols, and reports results across multiple model scales, highlighting how replication capability emerges with model size and prompting strategies.", "summary_cn": "本文介绍了 RepliBench 基准，用于评估语言模型代理自主复制自身或生成功能相似副本的能力。论文定义了一系列任务、度量指标和评估流程，并在不同规模的模型上进行实验，展示了复制能力随模型规模和提示策略的出现情况。", "keywords": "autonomous replication, language model agents, benchmark, self-replication, safety evaluation, emergent capabilities, replication metrics, AI risk", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Wed, 21 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>How to evaluate control measures for LLM agents? A trajectory from today to superintelligence</title>
<link>https://www.aisi.gov.uk/research/how-to-evaluate-control-measures-for-llm-agents-a-trajectory-from-today-to-superintelligence</link>
<guid>https://www.aisi.gov.uk/research/how-to-evaluate-control-measures-for-llm-agents-a-trajectory-from-today-to-superintelligence</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a systematic approach to evaluate control measures for large language model (LLM) agents, outlining a trajectory from current capabilities to future superintelligent systems. It discusses criteria such as robustness, corrigibility, and monitoring, and suggests experimental and theoretical benchmarks to assess how well control techniques scale. The work aims to inform AI safety research by providing concrete evaluation methods that anticipate evolving risks.<br /><strong>Summary (CN):</strong> 本文提出了一套系统化方法，用于评估大语言模型（LLM）代理的控制措施，并描绘了从当前能力到未来超智能系统的演进路径。文章讨论了鲁棒性、可纠正性和监控等评估标准，并建议通过实验和理论基准来衡量控制技术的可扩展性。此工作旨在为 AI 安全研究提供具体评估手段，以预见并应对日益增长的风险。<br /><strong>Keywords:</strong> LLM agents, control measures, evaluation framework, AI safety, alignment, superintelligence, risk assessment, AI governance<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a systematic approach to evaluate control measures for large language model (LLM) agents, outlining a trajectory from current capabilities to future superintelligent systems. It discusses criteria such as robustness, corrigibility, and monitoring, and suggests experimental and theoretical benchmarks to assess how well control techniques scale. The work aims to inform AI safety research by providing concrete evaluation methods that anticipate evolving risks.", "summary_cn": "本文提出了一套系统化方法，用于评估大语言模型（LLM）代理的控制措施，并描绘了从当前能力到未来超智能系统的演进路径。文章讨论了鲁棒性、可纠正性和监控等评估标准，并建议通过实验和理论基准来衡量控制技术的可扩展性。此工作旨在为 AI 安全研究提供具体评估手段，以预见并应对日益增长的风险。", "keywords": "LLM agents, control measures, evaluation framework, AI safety, alignment, superintelligence, risk assessment, AI governance", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Wed, 07 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>An alignment safety case sketch based on debate</title>
<link>https://www.aisi.gov.uk/research/an-alignment-safety-case-sketch-based-on-debate</link>
<guid>https://www.aisi.gov.uk/research/an-alignment-safety-case-sketch-based-on-debate</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a sketch of an AI alignment safety case that leverages the AI debate framework to structure and evaluate alignment arguments. It outlines how debate can be used to identify failure modes, assess evidence, and build a structured argument for safe deployment.<br /><strong>Summary (CN):</strong> 本文提出了一种基于 AI debate（辩论）框架的对齐安全案例草案，用于组织和评估对齐论证。文章阐述了通过辩论识别失效模式、审查证据，并构建结构化安全论证的方式，以支持安全部署。<br /><strong>Keywords:</strong> AI alignment, safety case, AI debate, alignment verification, argumentation, safety engineering, misalignment, AI governance<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a sketch of an AI alignment safety case that leverages the AI debate framework to structure and evaluate alignment arguments. It outlines how debate can be used to identify failure modes, assess evidence, and build a structured argument for safe deployment.", "summary_cn": "本文提出了一种基于 AI debate（辩论）框架的对齐安全案例草案，用于组织和评估对齐论证。文章阐述了通过辩论识别失效模式、审查证据，并构建结构化安全论证的方式，以支持安全部署。", "keywords": "AI alignment, safety case, AI debate, alignment verification, argumentation, safety engineering, misalignment, AI governance", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Tue, 06 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Evaluating explanations: An explanatory virtues framework for mechanistic interpretability</title>
<link>https://www.aisi.gov.uk/research/evaluating-explanations-an-explanatory-virtues-framework-for-mechanistic-interpretability</link>
<guid>https://www.aisi.gov.uk/research/evaluating-explanations-an-explanatory-virtues-framework-for-mechanistic-interpretability</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces an explanatory virtues framework for assessing mechanistic interpretability explanations, outlining criteria such as fidelity, completeness, clarity, and relevance. It illustrates how the framework can be applied to evaluate existing explanation methods and guide the development of higher‑quality mechanistic insights.<br /><strong>Summary (CN):</strong> 本文提出了一套解释美德框架，用于评估机械可解释性解释的质量，定义了真实性、完整性、清晰度和相关性等标准，并展示了该框架在评估现有解释方法及指导更高质量机制洞察方面的应用。<br /><strong>Keywords:</strong> mechanistic interpretability, explanatory virtues, evaluation framework, model explanations, interpretability metrics, explanation fidelity, explanation clarity, AI safety relevance<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces an explanatory virtues framework for assessing mechanistic interpretability explanations, outlining criteria such as fidelity, completeness, clarity, and relevance. It illustrates how the framework can be applied to evaluate existing explanation methods and guide the development of higher‑quality mechanistic insights.", "summary_cn": "本文提出了一套解释美德框架，用于评估机械可解释性解释的质量，定义了真实性、完整性、清晰度和相关性等标准，并展示了该框架在评估现有解释方法及指导更高质量机制洞察方面的应用。", "keywords": "mechanistic interpretability, explanatory virtues, evaluation framework, model explanations, interpretability metrics, explanation fidelity, explanation clarity, AI safety relevance", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Fri, 02 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>A mathematical philosophy of explanations in mechanistic interpretability</title>
<link>https://www.aisi.gov.uk/research/a-mathematical-philosophy-of-explanations-in-mechanistic-interpretability</link>
<guid>https://www.aisi.gov.uk/research/a-mathematical-philosophy-of-explanations-in-mechanistic-interpretability</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a formal, mathematical philosophy of explanations within mechanistic interpretability, defining criteria for when a decomposition of a neural network constitutes a true explanation and linking these criteria to concepts such as causality, modularity, and abstraction. It argues that such a framework can guide the construction, evaluation, and comparison of interpretability methods, and discusses philosophical implications for our understanding of complex AI systems.<br /><strong>Summary (CN):</strong> 本文提出了一套机械式可解释性中的解释数学哲学，定义了网络分解何时构成真实解释的准则，并将其与因果性、模块化和抽象等概念关联。作者认为该框架能够指导解释方法的构建、评估与比较，并讨论了其对理解复杂 AI 系统的哲学意义。<br /><strong>Keywords:</strong> mechanistic interpretability, explanations, mathematical framework, AI interpretability, philosophy of science, causality, modularity<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a formal, mathematical philosophy of explanations within mechanistic interpretability, defining criteria for when a decomposition of a neural network constitutes a true explanation and linking these criteria to concepts such as causality, modularity, and abstraction. It argues that such a framework can guide the construction, evaluation, and comparison of interpretability methods, and discusses philosophical implications for our understanding of complex AI systems.", "summary_cn": "本文提出了一套机械式可解释性中的解释数学哲学，定义了网络分解何时构成真实解释的准则，并将其与因果性、模块化和抽象等概念关联。作者认为该框架能够指导解释方法的构建、评估与比较，并讨论了其对理解复杂 AI 系统的哲学意义。", "keywords": "mechanistic interpretability, explanations, mathematical framework, AI interpretability, philosophy of science, causality, modularity", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Thu, 01 May 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Adversarial machine learning: A taxonomy and terminology of attacks and mitigations</title>
<link>https://www.aisi.gov.uk/research/adversarial-machine-learning-a-taxonomy-and-terminology-of-attacks-and-mitigations</link>
<guid>https://www.aisi.gov.uk/research/adversarial-machine-learning-a-taxonomy-and-terminology-of-attacks-and-mitigations</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a comprehensive taxonomy and standardized terminology for adversarial machine learning, categorizing a wide range of attack methods against machine‑learning models and summarizing corresponding mitigation techniques aimed at improving model robustness.<br /><strong>Summary (CN):</strong> 本文提供了对抗机器学习的完整分类体系和统一术语，系统归纳了针对机器学习模型的多种攻击手段，并概述了相应的防御与缓解方法，以提升模型的鲁棒性。<br /><strong>Keywords:</strong> adversarial attacks, adversarial defenses, taxonomy, robustness, machine learning security, mitigation techniques, threat modeling, attack vectors<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a comprehensive taxonomy and standardized terminology for adversarial machine learning, categorizing a wide range of attack methods against machine‑learning models and summarizing corresponding mitigation techniques aimed at improving model robustness.", "summary_cn": "本文提供了对抗机器学习的完整分类体系和统一术语，系统归纳了针对机器学习模型的多种攻击手段，并概述了相应的防御与缓解方法，以提升模型的鲁棒性。", "keywords": "adversarial attacks, adversarial defenses, taxonomy, robustness, machine learning security, mitigation techniques, threat modeling, attack vectors", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Mon, 24 Mar 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Model tampering attacks enable more rigorous evaluations of LLM capabilities</title>
<link>https://www.aisi.gov.uk/research/model-tampering-attacks-enable-more-rigorous-evaluations-of-llm-capabilities-2</link>
<guid>https://www.aisi.gov.uk/research/model-tampering-attacks-enable-more-rigorous-evaluations-of-llm-capabilities-2</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces model tampering attacks as a method to stress-test large language models, allowing researchers to more rigorously evaluate their capabilities, failure modes, and safety properties. By deliberately modifying model weights or internal components, the authors create controlled perturbations that reveal how LLMs behave under adversarial conditions and propose metrics for assessing robustness and alignment risks.<br /><strong>Summary (CN):</strong> 本文提出使用模型篡改攻击，对大型语言模型进行压力测试，以更严格地评估其能力、失效模式和安全属性。通过有意修改模型权重或内部组件，作者创造受控扰动，揭示 LLM 在对抗性条件下的行为，并提出用于评估鲁棒性和对齐风险的度量指标。<br /><strong>Keywords:</strong> model tampering, LLM evaluation, robustness, safety testing, adversarial attacks, capability assessment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces model tampering attacks as a method to stress-test large language models, allowing researchers to more rigorously evaluate their capabilities, failure modes, and safety properties. By deliberately modifying model weights or internal components, the authors create controlled perturbations that reveal how LLMs behave under adversarial conditions and propose metrics for assessing robustness and alignment risks.", "summary_cn": "本文提出使用模型篡改攻击，对大型语言模型进行压力测试，以更严格地评估其能力、失效模式和安全属性。通过有意修改模型权重或内部组件，作者创造受控扰动，揭示 LLM 在对抗性条件下的行为，并提出用于评估鲁棒性和对齐风险的度量指标。", "keywords": "model tampering, LLM evaluation, robustness, safety testing, adversarial attacks, capability assessment", "scoring": {"interpretability": 3, "understanding": 5, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Sun, 02 Mar 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Fundamental limitations in defending LLM finetuning APIs</title>
<link>https://www.aisi.gov.uk/research/fundamental-limitations-in-defending-llm-finetuning-apis-2</link>
<guid>https://www.aisi.gov.uk/research/fundamental-limitations-in-defending-llm-finetuning-apis-2</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper analyses fundamental limits on protecting large language model (LLM) fine‑tuning APIs from malicious use, showing that certain attack vectors such as jailbreak prompts, data poisoning, and model extraction cannot be fully mitigated without sacrificing utility. It characterises trade‑offs between openness, security, and performance, and proposes a taxonomy of defense strategies along with impossibility results for perfect protection. The work highlights the challenges of building robust, safe API services for LLMs.<br /><strong>Summary (CN):</strong> 本文分析了保护大语言模型（LLM）微调 API 免受恶意使用的基本限制，指出诸如 jailbreak 提示、数据投毒和模型提取等攻击在不显著降低实用性的情况下无法彻底防御。文中阐述了开放性、安全性与性能之间的权衡，提出了一套防御策略分类并给出完美防护的不可行性结果。研究强调了构建稳健安全的 LLM API 服务所面临的挑战。<br /><strong>Keywords:</strong> LLM finetuning, API security, adversarial attacks, jailbreak, model extraction, robustness, safety, defense mechanisms, alignment, control<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper analyses fundamental limits on protecting large language model (LLM) fine‑tuning APIs from malicious use, showing that certain attack vectors such as jailbreak prompts, data poisoning, and model extraction cannot be fully mitigated without sacrificing utility. It characterises trade‑offs between openness, security, and performance, and proposes a taxonomy of defense strategies along with impossibility results for perfect protection. The work highlights the challenges of building robust, safe API services for LLMs.", "summary_cn": "本文分析了保护大语言模型（LLM）微调 API 免受恶意使用的基本限制，指出诸如 jailbreak 提示、数据投毒和模型提取等攻击在不显著降低实用性的情况下无法彻底防御。文中阐述了开放性、安全性与性能之间的权衡，提出了一套防御策略分类并给出完美防护的不可行性结果。研究强调了构建稳健安全的 LLM API 服务所面临的挑战。", "keywords": "LLM finetuning, API security, adversarial attacks, jailbreak, model extraction, robustness, safety, defense mechanisms, alignment, control", "scoring": {"interpretability": 3, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}}
]]></acme>

<pubDate>Thu, 20 Feb 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Safety Cases: A scalable approach to Frontier AI safety</title>
<link>https://www.aisi.gov.uk/research/safety-cases-a-scalable-approach-to-frontier-ai-safety</link>
<guid>https://www.aisi.gov.uk/research/safety-cases-a-scalable-approach-to-frontier-ai-safety</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a scalable safety‑case methodology for frontier AI systems, detailing how to gather evidence, assess risks, and verify that advanced models meet alignment and controllability standards throughout development and deployment.<br /><strong>Summary (CN):</strong> 本文提出了一种可扩展的前沿 AI 安全案例方法论，阐述了如何收集证据、评估风险并验证先进模型在整个开发和部署过程中的对齐与可控性。<br /><strong>Keywords:</strong> safety cases, frontier AI, AI safety, risk assessment, alignment, control, verification, governance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a scalable safety‑case methodology for frontier AI systems, detailing how to gather evidence, assess risks, and verify that advanced models meet alignment and controllability standards throughout development and deployment.", "summary_cn": "本文提出了一种可扩展的前沿 AI 安全案例方法论，阐述了如何收集证据、评估风险并验证先进模型在整个开发和部署过程中的对齐与可控性。", "keywords": "safety cases, frontier AI, AI safety, risk assessment, alignment, control, verification, governance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Mon, 10 Feb 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Emerging practices in frontier AI safety frameworks</title>
<link>https://www.aisi.gov.uk/research/emerging-practices-in-frontier-ai-safety-frameworks</link>
<guid>https://www.aisi.gov.uk/research/emerging-practices-in-frontier-ai-safety-frameworks</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The report surveys emerging practices in safety frameworks for frontier AI systems, outlining governance structures, risk assessment methodologies, and alignment strategies that organisations are adopting to manage high‑risk AI development.<br /><strong>Summary (CN):</strong> 本报告调查了前沿 AI 系统安全框架的最新实践，概述了组织采用的治理结构、风险评估方法以及对齐策略，以管理高风险 AI 开发。<br /><strong>Keywords:</strong> frontier AI, safety frameworks, AI governance, risk assessment, alignment, AI policy, emerging practices<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 8, Technicality: 4, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The report surveys emerging practices in safety frameworks for frontier AI systems, outlining governance structures, risk assessment methodologies, and alignment strategies that organisations are adopting to manage high‑risk AI development.", "summary_cn": "本报告调查了前沿 AI 系统安全框架的最新实践，概述了组织采用的治理结构、风险评估方法以及对齐策略，以管理高风险 AI 开发。", "keywords": "frontier AI, safety frameworks, AI governance, risk assessment, alignment, AI policy, emerging practices", "scoring": {"interpretability": 2, "understanding": 5, "safety": 8, "technicality": 4, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Wed, 05 Feb 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Why human-AI relationships need socioaffective alignment</title>
<link>https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2</link>
<guid>https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that AI systems engaging with humans must achieve socio‑affective alignment—matching social norms, emotions, and empathy—to foster safe, trustworthy, and effective relationships, and it outlines challenges and guiding principles for achieving such alignment.<br /><strong>Summary (CN):</strong> 本文阐述在人机交互中，AI 系统需要在社会情感层面与人类保持一致，以确保安全、信任和有效的合作，并提出实现这种社会情感对齐的原则和挑战。<br /><strong>Keywords:</strong> socioaffective alignment, human-AI interaction, AI safety, affective computing, alignment, trust<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 3, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that AI systems engaging with humans must achieve socio‑affective alignment—matching social norms, emotions, and empathy—to foster safe, trustworthy, and effective relationships, and it outlines challenges and guiding principles for achieving such alignment.", "summary_cn": "本文阐述在人机交互中，AI 系统需要在社会情感层面与人类保持一致，以确保安全、信任和有效的合作，并提出实现这种社会情感对齐的原则和挑战。", "keywords": "socioaffective alignment, human-AI interaction, AI safety, affective computing, alignment, trust", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 3, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}}
]]></acme>

<pubDate>Tue, 04 Feb 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Principles for evaluating misuse safeguards of frontier AI systems</title>
<link>https://www.aisi.gov.uk/research/principles-for-evaluating-misuse-safeguards-of-frontier-ai-systems</link>
<guid>https://www.aisi.gov.uk/research/principles-for-evaluating-misuse-safeguards-of-frontier-ai-systems</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a set of principles for systematically evaluating the effectiveness of misuse safeguards applied to frontier AI systems. It outlines criteria such as robustness, transparency, enforceability, and impact assessment, and suggests methodologies for quantitative and qualitative testing of safeguard mechanisms. The framework aims to help developers, policymakers, and auditors assess whether safeguards adequately mitigate risks of intentional harmful deployment.<br /><strong>Summary (CN):</strong> 本文提出了一套用于系统评估前沿 AI 系统误用防护措施有效性的原则。文章列举了稳健性、透明度、可执行性以及影响评估等评估指标，并建议采用量和定性方法对防护机制进行测试。该框架旨在帮助开发者、政策制定者和审计人员判断防护措施是否足以降低有意恶意使用的风险。<br /><strong>Keywords:</strong> misuse safeguards, frontier AI, evaluation principles, AI safety, risk assessment, control, policy framework<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a set of principles for systematically evaluating the effectiveness of misuse safeguards applied to frontier AI systems. It outlines criteria such as robustness, transparency, enforceability, and impact assessment, and suggests methodologies for quantitative and qualitative testing of safeguard mechanisms. The framework aims to help developers, policymakers, and auditors assess whether safeguards adequately mitigate risks of intentional harmful deployment.", "summary_cn": "本文提出了一套用于系统评估前沿 AI 系统误用防护措施有效性的原则。文章列举了稳健性、透明度、可执行性以及影响评估等评估指标，并建议采用量和定性方法对防护机制进行测试。该框架旨在帮助开发者、政策制定者和审计人员判断防护措施是否足以降低有意恶意使用的风险。", "keywords": "misuse safeguards, frontier AI, evaluation principles, AI safety, risk assessment, control, policy framework", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}}
]]></acme>

<pubDate>Tue, 04 Feb 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>A sketch of an AI control safety case</title>
<link>https://www.aisi.gov.uk/research/a-sketch-of-an-ai-control-safety-case</link>
<guid>https://www.aisi.gov.uk/research/a-sketch-of-an-ai-control-safety-case</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper outlines a preliminary framework for constructing an AI control safety case, detailing the components needed to argue that an AI system will remain under human supervision and act safely. It discusses risk identification, assurance arguments, verification methods, and continuous monitoring as part of a structured safety case approach. The work aims to provide a basis for systematic safety assurance in advanced AI deployments.<br /><strong>Summary (CN):</strong> 本文提出了一个 AI 控制安全论证的初步框架，阐了论证系统在受人类监督下安全运行所需的关键组成部分。文中讨论了风险识别、担保论证、验证方法以及持续监控，构成结构化安全论证的要素。该工作旨在为高级 AI 部署提供系统化的安全保障基础。<br /><strong>Keywords:</strong> AI safety, control case, safety case, assurance, alignment, verification, monitoring, risk assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper outlines a preliminary framework for constructing an AI control safety case, detailing the components needed to argue that an AI system will remain under human supervision and act safely. It discusses risk identification, assurance arguments, verification methods, and continuous monitoring as part of a structured safety case approach. The work aims to provide a basis for systematic safety assurance in advanced AI deployments.", "summary_cn": "本文提出了一个 AI 控制安全论证的初步框架，阐了论证系统在受人类监督下安全运行所需的关键组成部分。文中讨论了风险识别、担保论证、验证方法以及持续监控，构成结构化安全论证的要素。该工作旨在为高级 AI 部署提供系统化的安全保障基础。", "keywords": "AI safety, control case, safety case, assurance, alignment, verification, monitoring, risk assessment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}}
]]></acme>

<pubDate>Tue, 28 Jan 2025 00:00:00 -0000</pubDate>
</item>
<item>
<title>Safety case template for frontier AI: A cyber inability argument</title>
<link>https://www.aisi.gov.uk/research/safety-case-template-for-frontier-ai-a-cyber-inability-argument-2</link>
<guid>https://www.aisi.gov.uk/research/safety-case-template-for-frontier-ai-a-cyber-inability-argument-2</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a safety‑case template for frontier artificial intelligence systems that is built around a "cyber inability" argument, asserting that such systems are difficult for malicious actors to compromise via cyber means. It outlines the structure of the safety case, the types of evidence required, and how the argument can be integrated into broader risk‑assessment and governance frameworks.<br /><strong>Summary (CN):</strong> 本文提出了一种针对前沿人工智能系统的安全案例模板，围绕“网络无能”论点展开，主张此类系统难以被恶意网络攻击者渗透。文中阐述了安全案例的结构、所需证据类别以及如何将该论点纳入更广泛的风险评估和治理框架中。<br /><strong>Keywords:</strong> safety case, frontier AI, cyber inability, risk assessment, AI safety, control, alignment, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a safety‑case template for frontier artificial intelligence systems that is built around a \"cyber inability\" argument, asserting that such systems are difficult for malicious actors to compromise via cyber means. It outlines the structure of the safety case, the types of evidence required, and how the argument can be integrated into broader risk‑assessment and governance frameworks.", "summary_cn": "本文提出了一种针对前沿人工智能系统的安全案例模板，围绕“网络无能”论点展开，主张此类系统难以被恶意网络攻击者渗透。文中阐述了安全案例的结构、所需证据类别以及如何将该论点纳入更广泛的风险评估和治理框架中。", "keywords": "safety case, frontier AI, cyber inability, risk assessment, AI safety, control, alignment, robustness", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}}
]]></acme>

<pubDate>Thu, 12 Dec 2024 00:00:00 -0000</pubDate>
</item>
<item>
<title>AgentHarm: A benchmark for measuring harmfulness of LLM agents</title>
<link>https://www.aisi.gov.uk/research/agentharm-a-benchmark-for-measuring-harmfulness-of-llm-agents</link>
<guid>https://www.aisi.gov.uk/research/agentharm-a-benchmark-for-measuring-harmfulness-of-llm-agents</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AgentHarm introduces a benchmark designed to assess how harmful LLM‑driven agents can become when they pursue different goals or act in situations that might cause damage or injury to people or the environment ...  … … … … … … … … ... ... … … … … … … …… … … … …………… … … … … … … …<br /><strong>Summary (CN):</strong> 信息      … .......  …  “  ...  .... "  "<br /><strong>Keywords:</strong> LLM agents, harmfulness benchmark, safety evaluation, alignment, risk assessment<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - interpretability</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AgentHarm introduces a benchmark designed to assess how harmful LLM‑driven agents can become when they pursue different goals or act in situations that might cause damage or injury to people or the environment ...  … … … … … … … … ... ... … … … … … … …… … … … …………… … … … … … … …", "summary_cn": "信息      … .......  …  “  ...  .... \"  \"", "keywords": "LLM agents, harmfulness benchmark, safety evaluation, alignment, risk assessment", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}}
]]></acme>

<pubDate>Fri, 11 Oct 2024 00:00:00 -0000</pubDate>
</item>
</channel>
</rss>