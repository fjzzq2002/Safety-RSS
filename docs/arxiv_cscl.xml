<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Computation and Language</title>
<link>https://papers.cool/arxiv/cs.CL</link>


<item>
<title>On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?</title>
<link>https://papers.cool/arxiv/2510.20810</link>
<guid>https://papers.cool/arxiv/2510.20810</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper surveys the ambiguous definition of “LLM-generated text” and argues that current detection benchmarks often target only a narrow subset of possible outputs. It highlights how human editing and subtle model influence blur the boundary between machine‑ and human‑written text, leading to misinterpretation of detector performance in real‑world scenarios. The authors suggest that detectors should be used as reference tools under specific conditions rather than as definitive judgments.<br /><strong>Summary (CN):</strong> 本文梳理了“LLM 生成文本”定义的模糊性，指出现有检测基准通常只覆盖模型可能产生的部分文本文章强调人类编辑以及模型对用户的微妙影响会模糊机器与人类文本的边界，导致在实际应用中对检测器性能的误读。作者主张检测器应仅在特定条件下作为参考工具，而非决定性判定手段。<br /><strong>Keywords:</strong> LLM-generated text detection, provenance classification, AI-generated content, evaluation benchmarks, safety, human edits, mis/disinformation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - interpretability<br /><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely "LLM-generated text". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.
<div><strong>Authors:</strong> Mingmeng Geng, Thierry Poibeau</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper surveys the ambiguous definition of “LLM-generated text” and argues that current detection benchmarks often target only a narrow subset of possible outputs. It highlights how human editing and subtle model influence blur the boundary between machine‑ and human‑written text, leading to misinterpretation of detector performance in real‑world scenarios. The authors suggest that detectors should be used as reference tools under specific conditions rather than as definitive judgments.", "summary_cn": "本文梳理了“LLM 生成文本”定义的模糊性，指出现有检测基准通常只覆盖模型可能产生的部分文本文章强调人类编辑以及模型对用户的微妙影响会模糊机器与人类文本的边界，导致在实际应用中对检测器性能的误读。作者主张检测器应仅在特定条件下作为参考工具，而非决定性判定手段。", "keywords": "LLM-generated text detection, provenance classification, AI-generated content, evaluation benchmarks, safety, human edits, mis/disinformation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "interpretability"}, "authors": ["Mingmeng Geng", "Thierry Poibeau"]}
]]></acme>

<pubDate>2025-10-23T17:59:06+00:00</pubDate>
</item>
<item>
<title>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</title>
<link>https://papers.cool/arxiv/2510.20797</link>
<guid>https://papers.cool/arxiv/2510.20797</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture. It also explores training a single compressor to produce multiple compression ratios and evaluates the approach across diverse QA benchmarks, model families, and scales. Results indicate strong overall performance with modest degradation when handling multiple ratios, while highlighting nuanced trade‑offs among compression strategies.<br /><strong>Summary (CN):</strong> 本文提出了一种轻量级的均值池化软上下文压缩方法，用于检索增强生成（RAG），并证明其在性能上始终优于常用的 compression‑tokens 架构。文章进一步研究了同一压缩器输出多种压缩比的训练方式，并在不同领域和模型规模的问答数据集上进行评估。实验结果显示，该方法整体表现最佳，即使在多压缩比设置下也仅出现小幅性能下降，同时揭示了压缩方法之间更为细致的权衡关系。<br /><strong>Keywords:</strong> context compression, mean-pooling, retrieval-augmented generation, large language models, multi-ratio training, soft compression, QA evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.
<div><strong>Authors:</strong> Yair Feldman, Yoav Artzi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a lightweight mean‑pooling method for soft context compression in retrieval‑augmented generation, showing it consistently outperforms the commonly used compression‑tokens architecture. It also explores training a single compressor to produce multiple compression ratios and evaluates the approach across diverse QA benchmarks, model families, and scales. Results indicate strong overall performance with modest degradation when handling multiple ratios, while highlighting nuanced trade‑offs among compression strategies.", "summary_cn": "本文提出了一种轻量级的均值池化软上下文压缩方法，用于检索增强生成（RAG），并证明其在性能上始终优于常用的 compression‑tokens 架构。文章进一步研究了同一压缩器输出多种压缩比的训练方式，并在不同领域和模型规模的问答数据集上进行评估。实验结果显示，该方法整体表现最佳，即使在多压缩比设置下也仅出现小幅性能下降，同时揭示了压缩方法之间更为细致的权衡关系。", "keywords": "context compression, mean-pooling, retrieval-augmented generation, large language models, multi-ratio training, soft compression, QA evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yair Feldman", "Yoav Artzi"]}
]]></acme>

<pubDate>2025-10-23T17:57:23+00:00</pubDate>
</item>
<item>
<title>Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction</title>
<link>https://papers.cool/arxiv/2510.20787</link>
<guid>https://papers.cool/arxiv/2510.20787</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes hybrid sparse attention mechanisms combined with a learnable token eviction strategy to reduce the forgetfulness inherent in linear‑attention models. By integrating sliding‑window attention, a lightweight CNN, and query‑aware sparse attention, the approach retains a limited set of critical key‑value pairs while preserving linear time and space complexity, supported by efficient Triton kernels and empirical validation on retrieval‑intensive benchmarks.<br /><strong>Summary (CN):</strong> 本文提出一种混合稀疏注意力机制并引入可学习的 token 驱逐方法，以缓解线性注意力模型的遗忘问题。通过将滑动窗口注意力、轻量级 CNN 和查询感知稀疏注意力相结合，模型在保持线性时间与空间复杂度的同时，自适应保留每个头部的关键 KV 对，并提供高效的 Triton 实现，在检索密集型基准上验证了其有效性。<br /><strong>Keywords:</strong> linear attention, sparse attention, token eviction, hybrid attention, retrieval, sliding-window attention, CNN aggregation, Triton kernels<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mutian He, Philip N. Garner</div>
Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.
<div><strong>Authors:</strong> Mutian He, Philip N. Garner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes hybrid sparse attention mechanisms combined with a learnable token eviction strategy to reduce the forgetfulness inherent in linear‑attention models. By integrating sliding‑window attention, a lightweight CNN, and query‑aware sparse attention, the approach retains a limited set of critical key‑value pairs while preserving linear time and space complexity, supported by efficient Triton kernels and empirical validation on retrieval‑intensive benchmarks.", "summary_cn": "本文提出一种混合稀疏注意力机制并引入可学习的 token 驱逐方法，以缓解线性注意力模型的遗忘问题。通过将滑动窗口注意力、轻量级 CNN 和查询感知稀疏注意力相结合，模型在保持线性时间与空间复杂度的同时，自适应保留每个头部的关键 KV 对，并提供高效的 Triton 实现，在检索密集型基准上验证了其有效性。", "keywords": "linear attention, sparse attention, token eviction, hybrid attention, retrieval, sliding-window attention, CNN aggregation, Triton kernels", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mutian He", "Philip N. Garner"]}
]]></acme>

<pubDate>2025-10-23T17:53:03+00:00</pubDate>
</item>
<item>
<title>A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text</title>
<link>https://papers.cool/arxiv/2510.20782</link>
<guid>https://papers.cool/arxiv/2510.20782</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a use‑case specific dataset for evaluating large language models on multiple responsible performance dimensions (quality, veracity, safety, and fairness) within the task of generating plain‑text product descriptions. The dataset is parameterized by fairness attributes such as gendered adjectives and product categories, providing a rich set of labeled prompts to uncover gaps in LLM behavior. The authors demonstrate how the resource can be used to assess and compare LLMs, proposing an evaluation framework for the community.<br /><strong>Summary (CN):</strong> 本文构建了一个面向特定使用场景的数据集，用于在生成商品描述的任务中评估大型语言模型的质量、真实性、安全性和公平性等多维度负责任表现。数据集通过将性别形容词和商品类别等公平属性交叉组合，生成了丰富的标注提示，以帮助发现模型在这些维度上的不足。作者展示了该资源的使用方式，并提出了一套供研究社区使用的 LLM 评估方案。<br /><strong>Keywords:</strong> LLM evaluation, responsible AI, fairness, dataset, product description generation, safety, veracity, quality<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</div>
Current methods for evaluating large language models (LLMs) typically focus on high-level tasks such as text generation, without targeting a particular AI application. This approach is not sufficient for evaluating LLMs for Responsible AI dimensions like fairness, since protected attributes that are highly relevant in one application may be less relevant in another. In this work, we construct a dataset that is driven by a real-world application (generate a plain-text product description, given a list of product features), parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts. We show how to use the data to identify quality, veracity, safety, and fairness gaps in LLMs, contributing a proposal for LLM evaluation paired with a concrete resource for the research community.
<div><strong>Authors:</strong> Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a use‑case specific dataset for evaluating large language models on multiple responsible performance dimensions (quality, veracity, safety, and fairness) within the task of generating plain‑text product descriptions. The dataset is parameterized by fairness attributes such as gendered adjectives and product categories, providing a rich set of labeled prompts to uncover gaps in LLM behavior. The authors demonstrate how the resource can be used to assess and compare LLMs, proposing an evaluation framework for the community.", "summary_cn": "本文构建了一个面向特定使用场景的数据集，用于在生成商品描述的任务中评估大型语言模型的质量、真实性、安全性和公平性等多维度负责任表现。数据集通过将性别形容词和商品类别等公平属性交叉组合，生成了丰富的标注提示，以帮助发现模型在这些维度上的不足。作者展示了该资源的使用方式，并提出了一套供研究社区使用的 LLM 评估方案。", "keywords": "LLM evaluation, responsible AI, fairness, dataset, product description generation, safety, veracity, quality", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Alicia Sagae", "Chia-Jung Lee", "Sandeep Avula", "Brandon Dang", "Vanessa Murdock"]}
]]></acme>

<pubDate>2025-10-23T17:50:55+00:00</pubDate>
</item>
<item>
<title>Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</title>
<link>https://papers.cool/arxiv/2510.20780</link>
<guid>https://papers.cool/arxiv/2510.20780</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically investigates the use of large reasoning models (LRMs) as judges for machine translation quality, identifying challenges such as overthinking and scoring bias. It proposes calibrating LRMs by training them on synthetic, human‑like thinking trajectories, which reduces the reasoning budget by about 35× and boosts correlation scores across model sizes. Experiments on WMT24 benchmarks demonstrate significant performance gains, highlighting the promise of efficiently calibrated LRMs for fine‑grained MT evaluation.<br /><strong>Summary (CN):</strong> 本文系统研究了大型推理模型（LRM）作为机器翻译质量评估者的潜力，指出其倾向于“过度思考”以及评分机制导致的高估等问题。作者通过在合成的类人思考轨迹上进行训练，对LRM进行校准，从而将思考预算降低约35倍并提升在不同规模模型（7B至32B）上的相关性得分。WMT24评测实验显示该方法显著提升评估性能，凸显高效校准的LRM在细粒度机器翻译评估中的潜力。<br /><strong>Keywords:</strong> large reasoning models, machine translation evaluation, model calibration, synthetic thinking trajectories, correlation improvement, evaluation metrics, LRM-as-judge<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</div>
Recent advancements in large reasoning models (LRMs) have introduced an intermediate "thinking" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to "overthink" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.
<div><strong>Authors:</strong> Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically investigates the use of large reasoning models (LRMs) as judges for machine translation quality, identifying challenges such as overthinking and scoring bias. It proposes calibrating LRMs by training them on synthetic, human‑like thinking trajectories, which reduces the reasoning budget by about 35× and boosts correlation scores across model sizes. Experiments on WMT24 benchmarks demonstrate significant performance gains, highlighting the promise of efficiently calibrated LRMs for fine‑grained MT evaluation.", "summary_cn": "本文系统研究了大型推理模型（LRM）作为机器翻译质量评估者的潜力，指出其倾向于“过度思考”以及评分机制导致的高估等问题。作者通过在合成的类人思考轨迹上进行训练，对LRM进行校准，从而将思考预算降低约35倍并提升在不同规模模型（7B至32B）上的相关性得分。WMT24评测实验显示该方法显著提升评估性能，凸显高效校准的LRM在细粒度机器翻译评估中的潜力。", "keywords": "large reasoning models, machine translation evaluation, model calibration, synthetic thinking trajectories, correlation improvement, evaluation metrics, LRM-as-judge", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Runzhe Zhan", "Zhihong Huang", "Xinyi Yang", "Lidia S. Chao", "Min Yang", "Derek F. Wong"]}
]]></acme>

<pubDate>2025-10-23T17:48:36+00:00</pubDate>
</item>
<item>
<title>Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing</title>
<link>https://papers.cool/arxiv/2510.20727</link>
<guid>https://papers.cool/arxiv/2510.20727</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The study develops and evaluates various NLP methods, including rule‑based, machine‑learning, deep‑learning, and large language model prompting, to automatically extract fluoropyrimidine treatment regimens and associated toxicities from clinical notes, achieving perfect F1 scores with error‑analysis prompting. Results show that LLM‑based approaches outperform other methods, while deep‑learning models struggle due to limited training data.<br /><strong>Summary (CN):</strong> 该研究开发并评估了规则、机器学习、深度学习以及大语言模型提示等多种自然语言处理方法，以自动从临床记录中提取氟嘧啶类药物治疗方案及相关毒性，错误分析提示实现了治疗和毒性提取的 F1=1.000。结果表明，大语言模型方法优于其他方法，深度学习模型因训练数据不足表现有限。<br /><strong>Keywords:</strong> fluoropyrimidine, toxicity extraction, clinical notes, NLP, large language model, pharmacovigilance, rule-based, machine learning, BERT, zero-shot prompting<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</div>
Objective: Fluoropyrimidines are widely prescribed for colorectal and breast cancers, but are associated with toxicities such as hand-foot syndrome and cardiotoxicity. Since toxicity documentation is often embedded in clinical notes, we aimed to develop and evaluate natural language processing (NLP) methods to extract treatment and toxicity information. Materials and Methods: We constructed a gold-standard dataset of 236 clinical notes from 204,165 adult oncology patients. Domain experts annotated categories related to treatment regimens and toxicities. We developed rule-based, machine learning-based (Random Forest, Support Vector Machine [SVM], Logistic Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language models (LLM)-based NLP approaches (zero-shot and error-analysis prompting). Models used an 80:20 train-test split. Results: Sufficient data existed to train and evaluate 5 annotated categories. Error-analysis prompting achieved optimal precision, recall, and F1 scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot prompting reached F1=1.000 for treatment and F1=0.876 for toxicities extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods served as our baseline with F1 scores of 0.857 in treatment and 0.858 in toxicities. Discussion: LMM-based approaches outperformed all others, followed by machine learning methods. Machine and deep learning approaches were limited by small training data and showed limited generalizability, particularly for rare categories. Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine treatment and toxicity information from clinical notes, and has strong potential to support oncology research and pharmacovigilance.
<div><strong>Authors:</strong> Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The study develops and evaluates various NLP methods, including rule‑based, machine‑learning, deep‑learning, and large language model prompting, to automatically extract fluoropyrimidine treatment regimens and associated toxicities from clinical notes, achieving perfect F1 scores with error‑analysis prompting. Results show that LLM‑based approaches outperform other methods, while deep‑learning models struggle due to limited training data.", "summary_cn": "该研究开发并评估了规则、机器学习、深度学习以及大语言模型提示等多种自然语言处理方法，以自动从临床记录中提取氟嘧啶类药物治疗方案及相关毒性，错误分析提示实现了治疗和毒性提取的 F1=1.000。结果表明，大语言模型方法优于其他方法，深度学习模型因训练数据不足表现有限。", "keywords": "fluoropyrimidine, toxicity extraction, clinical notes, NLP, large language model, pharmacovigilance, rule-based, machine learning, BERT, zero-shot prompting", "scoring": {"interpretability": 2, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xizhi Wu", "Madeline S. Kreider", "Philip E. Empey", "Chenyu Li", "Yanshan Wang"]}
]]></acme>

<pubDate>2025-10-23T16:44:39+00:00</pubDate>
</item>
<item>
<title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title>
<link>https://papers.cool/arxiv/2510.20721</link>
<guid>https://papers.cool/arxiv/2510.20721</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a user study with 94 participants who evaluated LLM responses to 90 privacy‑sensitive scenarios from the PrivacyLens benchmark, revealing low agreement among users on perceived privacy preservation and helpfulness. Proxy LLMs showed high internal agreement but correlated poorly with human judgments, indicating that current automated privacy assessments do not reflect real user perceptions. The authors argue for more user‑centered evaluation methods and improved alignment between proxy models and actual user expectations.<br /><strong>Summary (CN):</strong> 本文通过对94名参与者在PrivacyLens基准的90个隐私敏感情境下评估大型语言模型（LLM）回应的实验，发现用户在隐私保护质量和帮助性上的一致性较低。虽然代理LLM之间的评估高度一致，却与用户的感知关联度低，表明现有的自动隐私评估并未真实反映用户感受。作者呼吁开展以用户为中心的评估，并提升代理模型与真实用户感知之间的对齐。<br /><strong>Keywords:</strong> user perception, privacy, helpfulness, large language models, alignment, privacy-preserving AI, evaluation, user study, proxy LLMs<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</div>
Large language models (LLMs) have seen rapid adoption for tasks such as drafting emails, summarizing meetings, and answering health questions. In such uses, users may need to share private information (e.g., health records, contact details). To evaluate LLMs' ability to identify and redact such private information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with real-life scenarios. Using these benchmarks, researchers have found that LLMs sometimes fail to keep secrets private when responding to complex tasks (e.g., leaking employee salaries in meeting summaries). However, these evaluations rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking real users' perceptions. Moreover, prior work primarily focused on the privacy-preservation quality of responses, without investigating nuanced differences in helpfulness. To understand how users perceive the privacy-preservation quality and helpfulness of LLM responses to privacy-sensitive scenarios, we conducted a user study with 94 participants using 90 scenarios from PrivacyLens. We found that, when evaluating identical responses to the same scenario, users showed low agreement with each other on the privacy-preservation quality and helpfulness of the LLM response. Further, we found high agreement among five proxy LLMs, while each individual LLM had low correlation with users' evaluations. These results indicate that the privacy and helpfulness of LLM responses are often specific to individuals, and proxy LLMs are poor estimates of how real users would perceive these responses in privacy-sensitive scenarios. Our results suggest the need to conduct user-centered studies on measuring LLMs' ability to help users while preserving privacy. Additionally, future research could investigate ways to improve the alignment between proxy LLMs and users for better estimation of users' perceived privacy and utility.
<div><strong>Authors:</strong> Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a user study with 94 participants who evaluated LLM responses to 90 privacy‑sensitive scenarios from the PrivacyLens benchmark, revealing low agreement among users on perceived privacy preservation and helpfulness. Proxy LLMs showed high internal agreement but correlated poorly with human judgments, indicating that current automated privacy assessments do not reflect real user perceptions. The authors argue for more user‑centered evaluation methods and improved alignment between proxy models and actual user expectations.", "summary_cn": "本文通过对94名参与者在PrivacyLens基准的90个隐私敏感情境下评估大型语言模型（LLM）回应的实验，发现用户在隐私保护质量和帮助性上的一致性较低。虽然代理LLM之间的评估高度一致，却与用户的感知关联度低，表明现有的自动隐私评估并未真实反映用户感受。作者呼吁开展以用户为中心的评估，并提升代理模型与真实用户感知之间的对齐。", "keywords": "user perception, privacy, helpfulness, large language models, alignment, privacy-preserving AI, evaluation, user study, proxy LLMs", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xiaoyuan Wu", "Roshni Kaushik", "Wenkai Li", "Lujo Bauer", "Koichi Onoue"]}
]]></acme>

<pubDate>2025-10-23T16:38:26+00:00</pubDate>
</item>
<item>
<title>Structure-Conditional Minimum Bayes Risk Decoding</title>
<link>https://papers.cool/arxiv/2510.20700</link>
<guid>https://papers.cool/arxiv/2510.20700</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces three lightweight adaptations to the utility function of Minimum Bayes Risk (MBR) decoding that make it sensitive to latent structural variations such as dialogue act, emotion, and response format. It proposes two new metrics to assess structural optimality, demonstrates that conventional similarity‑based utilities perform poorly on these metrics, and shows that the adapted utilities improve win rates on instruction‑following benchmarks (AlpacaEval, MT‑Bench) by up to 13.7 points. The work highlights the importance of structural awareness for open‑ended generation tasks.<br /><strong>Summary (CN):</strong> 本文提出了三种轻量化的效用函数改进，使最小贝叶斯风险（MBR）解码能够感知潜在结构变化，如对话行为、情感以及回复形式。作者设计了两项结构最优度评估指标，证明传统基于相度的效用函数在这些指标上表现不佳，而改进后的方法在 AlpacaEval 和 MT‑Bench 等指令遵循基准上提升了最高 13.7% 的胜率。该工作凸显了在开放式生成任务中关注结构信息的重要性。<br /><strong>Keywords:</strong> minimum Bayes risk, MBR decoding, structural conditioning, dialogue act, emotion modeling, instruction-following, generation quality, utility function adaptation, evaluation metrics<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - other<br /><strong>Authors:</strong> Bryan Eikema, Anna Rutkiewicz, Mario Giulianelli</div>
Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative to traditional generation strategies. While MBR has proven effective in machine translation, where the variability of a language model's outcome space is naturally constrained, it may face challenges in more open-ended tasks such as dialogue or instruction-following. We hypothesise that in such settings, applying MBR with standard similarity-based utility functions may result in selecting responses that are broadly representative of the model's distribution, yet sub-optimal with respect to any particular grouping of generations that share an underlying latent structure. In this work, we introduce three lightweight adaptations to the utility function, designed to make MBR more sensitive to structural variability in the outcome space. To test our hypothesis, we curate a dataset capturing three representative types of latent structure: dialogue act, emotion, and response structure (e.g., a sentence, a paragraph, or a list). We further propose two metrics to evaluate the structural optimality of MBR. Our analysis demonstrates that common similarity-based utility functions fall short by these metrics. In contrast, our proposed adaptations considerably improve structural optimality. Finally, we evaluate our approaches on real-world instruction-following benchmarks, AlpacaEval and MT-Bench, and show that increased structural sensitivity improves generation quality by up to 13.7 percentage points in win rate.
<div><strong>Authors:</strong> Bryan Eikema, Anna Rutkiewicz, Mario Giulianelli</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces three lightweight adaptations to the utility function of Minimum Bayes Risk (MBR) decoding that make it sensitive to latent structural variations such as dialogue act, emotion, and response format. It proposes two new metrics to assess structural optimality, demonstrates that conventional similarity‑based utilities perform poorly on these metrics, and shows that the adapted utilities improve win rates on instruction‑following benchmarks (AlpacaEval, MT‑Bench) by up to 13.7 points. The work highlights the importance of structural awareness for open‑ended generation tasks.", "summary_cn": "本文提出了三种轻量化的效用函数改进，使最小贝叶斯风险（MBR）解码能够感知潜在结构变化，如对话行为、情感以及回复形式。作者设计了两项结构最优度评估指标，证明传统基于相度的效用函数在这些指标上表现不佳，而改进后的方法在 AlpacaEval 和 MT‑Bench 等指令遵循基准上提升了最高 13.7% 的胜率。该工作凸显了在开放式生成任务中关注结构信息的重要性。", "keywords": "minimum Bayes risk, MBR decoding, structural conditioning, dialogue act, emotion modeling, instruction-following, generation quality, utility function adaptation, evaluation metrics", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "other"}, "authors": ["Bryan Eikema", "Anna Rutkiewicz", "Mario Giulianelli"]}
]]></acme>

<pubDate>2025-10-23T16:13:49+00:00</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Small Models</title>
<link>https://papers.cool/arxiv/2510.20690</link>
<guid>https://papers.cool/arxiv/2510.20690</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to lower hallucination rates in language models without increasing parameters or data. By combining parallel LoRA adapters with Barlow Twins regularization (ND‑LoRA), the authors demonstrate up to a 25.6% reduction in hallucinations while preserving accuracy, and provide theoretical bounds linking representational correlation to hallucination probability. Analyses reveal task-dependent optimal levels of diversity, positioning neural diversity as a third scaling axis orthogonal to size and data.<br /><strong>Summary (CN):</strong> 本文提出神经多样性（decorrelated parallel representations）作为降低语言模型幻觉率的机制，且无需增加参数或数据。通过将并行 LoRA 适配器与 Barlow Twins 正则化相结合（ND‑LoRA），实现了最高 25.6% 的幻觉减少且不损失准确性，并给出表征相关性与幻觉概率之间的理论上界。研究表明不同任务需要不同的最优多样性水平，将神经多样性视为与参数和数据正交的第三个尺度轴。<br /><strong>Keywords:</strong> hallucination, neural diversity, LoRA, Barlow Twins, regularization, language models, reliability<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - other; Primary focus - robustness<br /><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. Inspired by portfolio theory, where uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination probability is bounded by representational correlation: $P(H) \leq f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language models need an optimal amount of neurodiversity. To validate this, we introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces hallucinations by up to 25.6% (and 14.6% on average) without degrading general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational analyses indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different amounts of optimal neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
<div><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundhar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces neural diversity—decorrelated parallel representations—as a mechanism to lower hallucination rates in language models without increasing parameters or data. By combining parallel LoRA adapters with Barlow Twins regularization (ND‑LoRA), the authors demonstrate up to a 25.6% reduction in hallucinations while preserving accuracy, and provide theoretical bounds linking representational correlation to hallucination probability. Analyses reveal task-dependent optimal levels of diversity, positioning neural diversity as a third scaling axis orthogonal to size and data.", "summary_cn": "本文提出神经多样性（decorrelated parallel representations）作为降低语言模型幻觉率的机制，且无需增加参数或数据。通过将并行 LoRA 适配器与 Barlow Twins 正则化相结合（ND‑LoRA），实现了最高 25.6% 的幻觉减少且不损失准确性，并给出表征相关性与幻觉概率之间的理论上界。研究表明不同任务需要不同的最优多样性水平，将神经多样性视为与参数和数据正交的第三个尺度轴。", "keywords": "hallucination, neural diversity, LoRA, Barlow Twins, regularization, language models, reliability", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "other", "primary_focus": "robustness"}, "authors": ["Kushal Chakrabarti", "Nirmal Balachundhar"]}
]]></acme>

<pubDate>2025-10-23T16:03:07+00:00</pubDate>
</item>
<item>
<title>\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding</title>
<link>https://papers.cool/arxiv/2510.20670</link>
<guid>https://papers.cool/arxiv/2510.20670</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CantoNLU, a benchmark covering seven Cantonese natural language understanding tasks ranging from syntax to semantics, and evaluates several models including Mandarin-based, Cantonese-adapted, and monolingual Cantonese models. Results show Cantonese-adapted models generally perform best, while monolingual models excel on syntactic tasks, and Mandarin models remain competitive when Cantonese data is limited. All datasets, code, and model weights are released to support future research in Cantonese NLP.<br /><strong>Summary (CN):</strong> 本文推出 CantoNLU 基准，涵盖七项粤语自然语言理解任务（包括句法与语义），并评估了多种模型：未进行粤语训练的普通话模型、通过持续预训练获得的粤语适配模型，以及从零开始训练的单语粤语模型。实验表明，粤语适配模型整体表现最佳，单语模型在句法任务上更占优势，而普通话模型在数据稀缺时仍具竞争力。作者公开了所有数据集、代码和模型权重，以推动粤语 NLP 研究。<br /><strong>Keywords:</strong> Cantonese, natural language understanding, benchmark, low-resource language, cross-lingual transfer, syntax, semantics, NLU tasks<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Junghyun Min, York Hay Ng, Sophia Chan, Helena Shunhua Zhao, En-Shiun Annie Lee</div>
Cantonese, although spoken by millions, remains under-resourced due to policy and diglossia. To address this scarcity of evaluation frameworks for Cantonese, we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural language understanding (NLU). This novel benchmark spans seven tasks covering syntax and semantics, including word sense disambiguation, linguistic acceptability judgment, language detection, natural language inference, sentiment analysis, part-of-speech tagging, and dependency parsing. In addition to the benchmark, we provide model baseline performance across a set of models: a Mandarin model without Cantonese training, two Cantonese-adapted models obtained by continual pre-training a Mandarin model on Cantonese text, and a monolingual Cantonese model trained from scratch. Results show that Cantonese-adapted models perform best overall, while monolingual models perform better on syntactic tasks. Mandarin models remain competitive in certain settings, indicating that direct transfer may be sufficient when Cantonese domain data is scarce. We release all datasets, code, and model weights to facilitate future research in Cantonese NLP.
<div><strong>Authors:</strong> Junghyun Min, York Hay Ng, Sophia Chan, Helena Shunhua Zhao, En-Shiun Annie Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CantoNLU, a benchmark covering seven Cantonese natural language understanding tasks ranging from syntax to semantics, and evaluates several models including Mandarin-based, Cantonese-adapted, and monolingual Cantonese models. Results show Cantonese-adapted models generally perform best, while monolingual models excel on syntactic tasks, and Mandarin models remain competitive when Cantonese data is limited. All datasets, code, and model weights are released to support future research in Cantonese NLP.", "summary_cn": "本文推出 CantoNLU 基准，涵盖七项粤语自然语言理解任务（包括句法与语义），并评估了多种模型：未进行粤语训练的普通话模型、通过持续预训练获得的粤语适配模型，以及从零开始训练的单语粤语模型。实验表明，粤语适配模型整体表现最佳，单语模型在句法任务上更占优势，而普通话模型在数据稀缺时仍具竞争力。作者公开了所有数据集、代码和模型权重，以推动粤语 NLP 研究。", "keywords": "Cantonese, natural language understanding, benchmark, low-resource language, cross-lingual transfer, syntax, semantics, NLU tasks", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Junghyun Min", "York Hay Ng", "Sophia Chan", "Helena Shunhua Zhao", "En-Shiun Annie Lee"]}
]]></acme>

<pubDate>2025-10-23T15:47:27+00:00</pubDate>
</item>
<item>
<title>The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI</title>
<link>https://papers.cool/arxiv/2510.20647</link>
<guid>https://papers.cool/arxiv/2510.20647</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how Large Reasoning Models (LRMs) handle multilingual question answering, finding that they often switch to English reasoning even for non‑English prompts. Experiments on MGSM and GPQA Diamond show that English‑based reasoning yields higher accuracy and richer cognitive behaviors, but introduces a "Lost in Translation" failure mode where translation steps cause errors that would not occur when reasoning in the question's original language.<br /><strong>Summary (CN):</strong> 本文研究了大规模推理模型（LRM）在多语言问答中的表现，发现它们常在非英语提问时转而使用英语进行推理。对 MGSM 和 GPQA Diamond 两项任务的实验表明，使用英语推理能获得更高的答案准确率并展现出更丰富的认知行为，但也会出现“翻译失误”这一关键失败模式，即翻译过程引入的错误本可以通过直接使用问题语言的推理避免。<br /><strong>Keywords:</strong> multilingual reasoning, large reasoning models, interpretability, translation errors, MGSM, GPQA Diamond, language bias, cognitive behavior analysis<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Alan Saji, Raj Dabre, Anoop Kunchukuttan, Ratish Puduppully</div>
Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting "Lost in Translation," where translation steps lead to errors that would have been avoided by question's language reasoning.
<div><strong>Authors:</strong> Alan Saji, Raj Dabre, Anoop Kunchukuttan, Ratish Puduppully</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how Large Reasoning Models (LRMs) handle multilingual question answering, finding that they often switch to English reasoning even for non‑English prompts. Experiments on MGSM and GPQA Diamond show that English‑based reasoning yields higher accuracy and richer cognitive behaviors, but introduces a \"Lost in Translation\" failure mode where translation steps cause errors that would not occur when reasoning in the question's original language.", "summary_cn": "本文研究了大规模推理模型（LRM）在多语言问答中的表现，发现它们常在非英语提问时转而使用英语进行推理。对 MGSM 和 GPQA Diamond 两项任务的实验表明，使用英语推理能获得更高的答案准确率并展现出更丰富的认知行为，但也会出现“翻译失误”这一关键失败模式，即翻译过程引入的错误本可以通过直接使用问题语言的推理避免。", "keywords": "multilingual reasoning, large reasoning models, interpretability, translation errors, MGSM, GPQA Diamond, language bias, cognitive behavior analysis", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Alan Saji", "Raj Dabre", "Anoop Kunchukuttan", "Ratish Puduppully"]}
]]></acme>

<pubDate>2025-10-23T15:22:00+00:00</pubDate>
</item>
<item>
<title>Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model</title>
<link>https://papers.cool/arxiv/2510.20635</link>
<guid>https://papers.cool/arxiv/2510.20635</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes an evaluation framework for large language models (LLMs) based on the Five-Dimensional Curiosity Scale Revised (5DCR), covering information seeking, thrill seeking, and social curiosity. Experiments show that LLMs exhibit a stronger thirst for knowledge than humans but make more conservative choices under uncertainty, and that higher curiosity correlates with improved reasoning and active learning abilities. These results suggest LLMs can display human‑like curiosity, informing future development of learning‑capable AI systems.<br /><strong>Summary (CN):</strong> 本文基于五维好奇心量表（5DCR）构建了评估大型语言模型（LLM）好奇心的框架，涵盖信息寻求、刺激寻求和社交好奇心等维度。实验发现，LLM 相较于人类表现出更强的求知欲，但在不确定环境下倾向于保守选择；同时，好奇行为能够提升模型的推理和主动学习能力。该研究表明 LLM 有潜力展现类似人类的好奇心，为未来具备学习能力的 AI 研发提供实验依据。<br /><strong>Keywords:</strong> curiosity, large language model, 5DCR, information seeking, thrill seeking, social curiosity, reasoning, active learning<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Haoyu Wang, Sihang Jiang, Yuyan Chen, Yitong Wang, Yanghua Xiao</div>
Curiosity serves as a pivotal conduit for human beings to discover and learn new knowledge. Recent advancements of large language models (LLMs) in natural language processing have sparked discussions regarding whether these models possess capability of curiosity-driven learning akin to humans. In this paper, starting from the human curiosity assessment questionnaire Five-Dimensional Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework that covers dimensions such as Information Seeking, Thrill Seeking, and Social Curiosity to assess the extent of curiosity exhibited by LLMs. The results demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but still tend to make conservative choices when faced with uncertain environments. We further investigated the relationship between curiosity and thinking of LLMs, confirming that curious behaviors can enhance the model's reasoning and active learning abilities. These findings suggest that LLMs have the potential to exhibit curiosity similar to that of humans, providing experimental support for the future development of learning capabilities and innovative research in LLMs.
<div><strong>Authors:</strong> Haoyu Wang, Sihang Jiang, Yuyan Chen, Yitong Wang, Yanghua Xiao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes an evaluation framework for large language models (LLMs) based on the Five-Dimensional Curiosity Scale Revised (5DCR), covering information seeking, thrill seeking, and social curiosity. Experiments show that LLMs exhibit a stronger thirst for knowledge than humans but make more conservative choices under uncertainty, and that higher curiosity correlates with improved reasoning and active learning abilities. These results suggest LLMs can display human‑like curiosity, informing future development of learning‑capable AI systems.", "summary_cn": "本文基于五维好奇心量表（5DCR）构建了评估大型语言模型（LLM）好奇心的框架，涵盖信息寻求、刺激寻求和社交好奇心等维度。实验发现，LLM 相较于人类表现出更强的求知欲，但在不确定环境下倾向于保守选择；同时，好奇行为能够提升模型的推理和主动学习能力。该研究表明 LLM 有潜力展现类似人类的好奇心，为未来具备学习能力的 AI 研发提供实验依据。", "keywords": "curiosity, large language model, 5DCR, information seeking, thrill seeking, social curiosity, reasoning, active learning", "scoring": {"interpretability": 5, "understanding": 7, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Haoyu Wang", "Sihang Jiang", "Yuyan Chen", "Yitong Wang", "Yanghua Xiao"]}
]]></acme>

<pubDate>2025-10-23T15:05:17+00:00</pubDate>
</item>
<item>
<title>BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection</title>
<link>https://papers.cool/arxiv/2510.20610</link>
<guid>https://papers.cool/arxiv/2510.20610</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports the BUSTED team’s participation in the AraGenEval shared task for Arabic AI‑generated text detection, evaluating three pre‑trained transformer models (AraELECTRA, CAMeLBERT, and XLM‑RoBERTa) via fine‑tuning for binary classification. Surprisingly, the multilingual XLM‑RoBERTa achieved the best performance with an F1 of 0.7701, surpassing the Arabic‑specific models, highlighting the strong generalisation of multilingual models for detection tasks.<br /><strong>Summary (CN):</strong> 本文介绍了 BUSTED 团队在 AraGenEval 共享任务中对阿拉伯语 AI 生成文本检测的参赛情况，评估了三种预训练 Transformer 模型（AraELECTRA、CAMeLBERT 和 XLM‑RoBERTa）通过微调进行二分类的效果。令人惊讶的是，多语言模型 XLM‑RoBERTa 达到了最高的 F1 分数 0.7701，优于专门的阿拉伯语模型，凸显了多语言模型在检测任务中的强大泛化能力。<br /><strong>Keywords:</strong> Arabic AI-generated text detection, transformer, XLM-RoBERTa, AraELECTRA, CAMeLBERT, multilingual models, binary classification, F1 score<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 7, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Ali Zain, Sareem Farooqui, Muhammad Rafi</div>
This paper details our submission to the Ara- GenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, se- cured 5th place. We investigated the effec- tiveness of three pre-trained transformer mod- els: AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a binary classification task. Our findings revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the highest performance with an F1 score of 0.7701, outperforming the spe- cialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong generalization capa- bilities of multilingual models.
<div><strong>Authors:</strong> Ali Zain, Sareem Farooqui, Muhammad Rafi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports the BUSTED team’s participation in the AraGenEval shared task for Arabic AI‑generated text detection, evaluating three pre‑trained transformer models (AraELECTRA, CAMeLBERT, and XLM‑RoBERTa) via fine‑tuning for binary classification. Surprisingly, the multilingual XLM‑RoBERTa achieved the best performance with an F1 of 0.7701, surpassing the Arabic‑specific models, highlighting the strong generalisation of multilingual models for detection tasks.", "summary_cn": "本文介绍了 BUSTED 团队在 AraGenEval 共享任务中对阿拉伯语 AI 生成文本检测的参赛情况，评估了三种预训练 Transformer 模型（AraELECTRA、CAMeLBERT 和 XLM‑RoBERTa）通过微调进行二分类的效果。令人惊讶的是，多语言模型 XLM‑RoBERTa 达到了最高的 F1 分数 0.7701，优于专门的阿拉伯语模型，凸显了多语言模型在检测任务中的强大泛化能力。", "keywords": "Arabic AI-generated text detection, transformer, XLM-RoBERTa, AraELECTRA, CAMeLBERT, multilingual models, binary classification, F1 score", "scoring": {"interpretability": 3, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"]}
]]></acme>

<pubDate>2025-10-23T14:41:04+00:00</pubDate>
</item>
<item>
<title>Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks</title>
<link>https://papers.cool/arxiv/2510.20584</link>
<guid>https://papers.cool/arxiv/2510.20584</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper empirically investigates whether using ChatGPT to automatically code communication data in collaborative tasks introduces bias against demographic groups such as gender and race. Analyzing negotiation, problem‑solving, and decision‑making datasets, the authors find no statistically significant bias across the examined groups, suggesting that ChatGPT‑based coding can be safely employed for large‑scale assessment of collaboration.<br /><strong>Summary (CN):</strong> 本文实证研究了使用 ChatGPT 自动编码协作任务中的交流数据是否会对性别、种族等人口群体产生偏见。通过对谈判、问题解决和决策制定三类任务的数据进行分析，作者发现不同群体之间没有显著的编码偏差，表明 ChatGPT 编码可安全用于大规模的合作与交流评估。<br /><strong>Keywords:</strong> ChatGPT coding, bias evaluation, fairness, demographic bias, communication annotation, collaborative tasks, automated coding<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi</div>
Assessing communication and collaboration at scale depends on a labor intensive task of coding communication data into categories according to different frameworks. Prior research has established that ChatGPT can be directly instructed with coding rubrics to code the communication data and achieves accuracy comparable to human raters. However, whether the coding from ChatGPT or similar AI technology exhibits bias against different demographic groups, such as gender and race, remains unclear. To fill this gap, this paper investigates ChatGPT-based automated coding of communication data using a typical coding framework for collaborative problem solving, examining differences across gender and racial groups. The analysis draws on data from three types of collaborative tasks: negotiation, problem solving, and decision making. Our results show that ChatGPT-based coding exhibits no significant bias across gender and racial groups, paving the road for its adoption in large-scale assessment of collaboration and communication.
<div><strong>Authors:</strong> Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper empirically investigates whether using ChatGPT to automatically code communication data in collaborative tasks introduces bias against demographic groups such as gender and race. Analyzing negotiation, problem‑solving, and decision‑making datasets, the authors find no statistically significant bias across the examined groups, suggesting that ChatGPT‑based coding can be safely employed for large‑scale assessment of collaboration.", "summary_cn": "本文实证研究了使用 ChatGPT 自动编码协作任务中的交流数据是否会对性别、种族等人口群体产生偏见。通过对谈判、问题解决和决策制定三类任务的数据进行分析，作者发现不同群体之间没有显著的编码偏差，表明 ChatGPT 编码可安全用于大规模的合作与交流评估。", "keywords": "ChatGPT coding, bias evaluation, fairness, demographic bias, communication annotation, collaborative tasks, automated coding", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 4}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi"]}
]]></acme>

<pubDate>2025-10-23T14:09:03+00:00</pubDate>
</item>
<item>
<title>Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search</title>
<link>https://papers.cool/arxiv/2510.20567</link>
<guid>https://papers.cool/arxiv/2510.20567</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a Multi-Agent Cognitive Decision Framework (MACDF) that moves e‑commerce search beyond the traditional retrieval‑ranking paradigm by modeling the multi‑stage cognitive decision process of users. MACDF employs multiple agents to provide proactive decision support, leading to significant gains in recommendation accuracy and user satisfaction, especially for complex queries involving negation, multiple constraints, or reasoning, as shown by extensive offline experiments and online A/B tests on JD's platform.<br /><strong>Summary (CN):</strong> 本文提出了多代理认知决策框架（MACDF），通过模拟用户的多阶段认知决策过程，将电商搜索从传统的检索‑排序模式转向主动决策支持。MACDF 使用多个智能体提供专业购物指导，在处理包含否定、多约束或推理需求的复杂查询时，显著提升推荐准确率和用户满意度，离线实验和京东平台的线上 A/B 测试均验证了其有效性。<br /><strong>Keywords:</strong> e-commerce search, multi-agent systems, cognitive decision framework, retrieval-ranking, recommendation accuracy, user satisfaction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhouwei Zhai, Mengxiang Chen, Haoyun Xia, Jin Li, Renquan Zhou, Min Yang</div>
The retrieval-ranking paradigm has long dominated e-commerce search, but its reliance on query-item matching fundamentally misaligns with multi-stage cognitive decision processes of platform users. This misalignment introduces critical limitations: semantic gaps in complex queries, high decision costs due to cross-platform information foraging, and the absence of professional shopping guidance. To address these issues, we propose a Multi-Agent Cognitive Decision Framework (MACDF), which shifts the paradigm from passive retrieval to proactive decision support. Extensive offline evaluations demonstrate MACDF's significant improvements in recommendation accuracy and user satisfaction, particularly for complex queries involving negation, multi-constraint, or reasoning demands. Online A/B testing on JD search platform confirms its practical efficacy. This work highlights the transformative potential of multi-agent cognitive systems in redefining e-commerce search.
<div><strong>Authors:</strong> Zhouwei Zhai, Mengxiang Chen, Haoyun Xia, Jin Li, Renquan Zhou, Min Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a Multi-Agent Cognitive Decision Framework (MACDF) that moves e‑commerce search beyond the traditional retrieval‑ranking paradigm by modeling the multi‑stage cognitive decision process of users. MACDF employs multiple agents to provide proactive decision support, leading to significant gains in recommendation accuracy and user satisfaction, especially for complex queries involving negation, multiple constraints, or reasoning, as shown by extensive offline experiments and online A/B tests on JD's platform.", "summary_cn": "本文提出了多代理认知决策框架（MACDF），通过模拟用户的多阶段认知决策过程，将电商搜索从传统的检索‑排序模式转向主动决策支持。MACDF 使用多个智能体提供专业购物指导，在处理包含否定、多约束或推理需求的复杂查询时，显著提升推荐准确率和用户满意度，离线实验和京东平台的线上 A/B 测试均验证了其有效性。", "keywords": "e-commerce search, multi-agent systems, cognitive decision framework, retrieval-ranking, recommendation accuracy, user satisfaction", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhouwei Zhai", "Mengxiang Chen", "Haoyun Xia", "Jin Li", "Renquan Zhou", "Min Yang"]}
]]></acme>

<pubDate>2025-10-23T13:55:53+00:00</pubDate>
</item>
<item>
<title>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.20548</link>
<guid>https://papers.cool/arxiv/2510.20548</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces GlobalRAG, a reinforcement‑learning framework that decomposes multi‑hop questions into subgoals and coordinates retrieval with reasoning to improve global planning and execution fidelity. It proposes novel Planning Quality and SubGoal Completion rewards, together with a progressive weight annealing strategy, achieving up to 14.2% gains in EM and F1 on both in‑domain and out‑of‑domain QA benchmarks while using only 8k training examples.<br /><strong>Summary (CN):</strong> 本文提出 GlobalRAG 框架，通过将多跳问题拆解为子目标并同步检索与推理，实现全局规划和执行的提升。文中设计了规划质量奖励和子目标完成奖励，并采用渐进权重退火策略，使模型在仅使用 8k 训练数据的情况下，在域内外问答基准上实现了 EM 与 F1 的 14.2% 平均提升。<br /><strong>Keywords:</strong> reinforcement learning, multi-hop question answering, retrieval-augmented generation, global planning, subgoal decomposition, reward design<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.
<div><strong>Authors:</strong> Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces GlobalRAG, a reinforcement‑learning framework that decomposes multi‑hop questions into subgoals and coordinates retrieval with reasoning to improve global planning and execution fidelity. It proposes novel Planning Quality and SubGoal Completion rewards, together with a progressive weight annealing strategy, achieving up to 14.2% gains in EM and F1 on both in‑domain and out‑of‑domain QA benchmarks while using only 8k training examples.", "summary_cn": "本文提出 GlobalRAG 框架，通过将多跳问题拆解为子目标并同步检索与推理，实现全局规划和执行的提升。文中设计了规划质量奖励和子目标完成奖励，并采用渐进权重退火策略，使模型在仅使用 8k 训练数据的情况下，在域内外问答基准上实现了 EM 与 F1 的 14.2% 平均提升。", "keywords": "reinforcement learning, multi-hop question answering, retrieval-augmented generation, global planning, subgoal decomposition, reward design", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jinchang Luo", "Mingquan Cheng", "Fan Wan", "Ni Li", "Xiaoling Xia", "Shuangshuang Tian", "Tingcheng Bian", "Haiwei Wang", "Haohuan Fu", "Yan Tao"]}
]]></acme>

<pubDate>2025-10-23T13:35:02+00:00</pubDate>
</item>
<item>
<title>The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts</title>
<link>https://papers.cool/arxiv/2510.20543</link>
<guid>https://papers.cool/arxiv/2510.20543</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CenterBench, a dataset of 9,720 questions probing language models' ability to parse center‑embedded sentences and to distinguish syntactic comprehension from semantic plausibility shortcuts. Experiments on several models show that performance gaps between plausible and implausible sentences grow with nesting depth, revealing systematic reliance on semantic associations over structural analysis. The work provides a clear framework for identifying when models abandon true syntactic processing in favor of pattern matching.<br /><strong>Summary (CN):</strong> 本文推出 CenterBench 数据集，包含 9,720 条关于中心嵌入句子（如 “The cat that the dog chased meowed”）的理解问答，用于检验语言模型是进行句法解析还是仅凭语义关联作答。实验表明，随着句子嵌套深度增加，可行性与不合常理句子的表现差距逐渐扩大，显示模型倾向于使用语义捷径而非结构分析。该研究提供了首个框架来辨别模型何时放弃句法推理转而依赖模式匹配。<br /><strong>Keywords:</strong> center-embedded sentences, structural understanding, semantic plausibility, language model shortcuts, probing benchmark, syntactic parsing, comprehension questions<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Sangmitra Madhusudan, Kaige Chen, Ali Emami</div>
When language models correctly parse "The cat that the dog chased meowed," are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like "The cat [that the dog chased] meowed") where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.
<div><strong>Authors:</strong> Sangmitra Madhusudan, Kaige Chen, Ali Emami</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CenterBench, a dataset of 9,720 questions probing language models' ability to parse center‑embedded sentences and to distinguish syntactic comprehension from semantic plausibility shortcuts. Experiments on several models show that performance gaps between plausible and implausible sentences grow with nesting depth, revealing systematic reliance on semantic associations over structural analysis. The work provides a clear framework for identifying when models abandon true syntactic processing in favor of pattern matching.", "summary_cn": "本文推出 CenterBench 数据集，包含 9,720 条关于中心嵌入句子（如 “The cat that the dog chased meowed”）的理解问答，用于检验语言模型是进行句法解析还是仅凭语义关联作答。实验表明，随着句子嵌套深度增加，可行性与不合常理句子的表现差距逐渐扩大，显示模型倾向于使用语义捷径而非结构分析。该研究提供了首个框架来辨别模型何时放弃句法推理转而依赖模式匹配。", "keywords": "center-embedded sentences, structural understanding, semantic plausibility, language model shortcuts, probing benchmark, syntactic parsing, comprehension questions", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sangmitra Madhusudan", "Kaige Chen", "Ali Emami"]}
]]></acme>

<pubDate>2025-10-23T13:30:40+00:00</pubDate>
</item>
<item>
<title>ARC-Encoder: learning compressed text representations for large language models</title>
<link>https://papers.cool/arxiv/2510.20535</link>
<guid>https://papers.cool/arxiv/2510.20535</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ARC-Encoder proposes an encoder that compresses long text contexts into a smaller set of continuous representations, which replace token embeddings in decoder LLMs, achieving up to 4-8x reduction in representation count. The paper systematically studies training strategies and architecture choices, demonstrating state-of-the-art performance across in‑context learning, context‑window extension, and both instruct and base decoders while reducing inference cost. Additionally, the encoder can be adapted to multiple decoders simultaneously, offering a portable, efficient solution for various LLMs.<br /><strong>Summary (CN):</strong> ARC-Encoder 提出一种编码器，可将长文本上下文压缩为更少的连续表示，用以替代解码器 LLM 中的 token 嵌入，实现 4-8 倍的表示数量压缩。论文系统研究了训练策略和架构选择，在上下文学习、窗口扩展以及指令和基础解码器等多种使用场景中达到最新水平的性能，并降低推理成本。该编码器还能同时适配多个解码器，实现跨不同 LLM 的通用、高效压缩。<br /><strong>Keywords:</strong> context compression, encoder-decoder, large language models, in-context learning, retrieval-augmented generation, chain-of-thought, efficient inference, multi-decoder adaptation, ARC-Encoder<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Hippolyte Pilchen, Edouard Grave, Patrick Pérez</div>
Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .
<div><strong>Authors:</strong> Hippolyte Pilchen, Edouard Grave, Patrick Pérez</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ARC-Encoder proposes an encoder that compresses long text contexts into a smaller set of continuous representations, which replace token embeddings in decoder LLMs, achieving up to 4-8x reduction in representation count. The paper systematically studies training strategies and architecture choices, demonstrating state-of-the-art performance across in‑context learning, context‑window extension, and both instruct and base decoders while reducing inference cost. Additionally, the encoder can be adapted to multiple decoders simultaneously, offering a portable, efficient solution for various LLMs.", "summary_cn": "ARC-Encoder 提出一种编码器，可将长文本上下文压缩为更少的连续表示，用以替代解码器 LLM 中的 token 嵌入，实现 4-8 倍的表示数量压缩。论文系统研究了训练策略和架构选择，在上下文学习、窗口扩展以及指令和基础解码器等多种使用场景中达到最新水平的性能，并降低推理成本。该编码器还能同时适配多个解码器，实现跨不同 LLM 的通用、高效压缩。", "keywords": "context compression, encoder-decoder, large language models, in-context learning, retrieval-augmented generation, chain-of-thought, efficient inference, multi-decoder adaptation, ARC-Encoder", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Hippolyte Pilchen", "Edouard Grave", "Patrick Pérez"]}
]]></acme>

<pubDate>2025-10-23T13:20:57+00:00</pubDate>
</item>
<item>
<title>Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset</title>
<link>https://papers.cool/arxiv/2510.20508</link>
<guid>https://papers.cool/arxiv/2510.20508</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a 21‑way multiparallel EuroParl dataset annotated with speakers' political affiliations and uses it to evaluate the political fairness of multilingual large language models by measuring translation quality across parties. Systematic differences are observed, with speeches from majority left, center, and right parties being translated more accurately than those from outsider parties, highlighting bias in multilingual LLMs.<br /><strong>Summary (CN):</strong> 本文构建了包含议员政治派别标签的 21 语言欧盟议会（EuroParl）多语平行语料库，并据此评估多语言大模型的政治公平性，通过比较不同政党发言的翻译质量来发现偏差。研究发现，左派、中心派和右派的主流政党发言的翻译质量普遍高于外部政党，揭示了多语言模型在政治立上的系统性偏见。<br /><strong>Keywords:</strong> political bias, multilingual LLMs, fairness, EuroParl, translation quality, multilingual dataset, political fairness, bias evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Paul Lerner, François Yvon</div>
The political biases of Large Language Models (LLMs) are usually assessed by simulating their answers to English surveys. In this work, we propose an alternative framing of political biases, relying on principles of fairness in multilingual translation. We systematically compare the translation quality of speeches in the European Parliament (EP), observing systematic differences with majority parties from left, center, and right being better translated than outsider parties. This study is made possible by a new, 21-way multiparallel version of EuroParl, the parliamentary proceedings of the EP, which includes the political affiliations of each speaker. The dataset consists of 1.5M sentences for a total of 40M words and 249M characters. It covers three years, 1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of national parties.
<div><strong>Authors:</strong> Paul Lerner, François Yvon</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a 21‑way multiparallel EuroParl dataset annotated with speakers' political affiliations and uses it to evaluate the political fairness of multilingual large language models by measuring translation quality across parties. Systematic differences are observed, with speeches from majority left, center, and right parties being translated more accurately than those from outsider parties, highlighting bias in multilingual LLMs.", "summary_cn": "本文构建了包含议员政治派别标签的 21 语言欧盟议会（EuroParl）多语平行语料库，并据此评估多语言大模型的政治公平性，通过比较不同政党发言的翻译质量来发现偏差。研究发现，左派、中心派和右派的主流政党发言的翻译质量普遍高于外部政党，揭示了多语言模型在政治立上的系统性偏见。", "keywords": "political bias, multilingual LLMs, fairness, EuroParl, translation quality, multilingual dataset, political fairness, bias evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Paul Lerner", "François Yvon"]}
]]></acme>

<pubDate>2025-10-23T12:50:30+00:00</pubDate>
</item>
<item>
<title>Hierarchical Sequence Iteration for Heterogeneous Question Answering</title>
<link>https://papers.cool/arxiv/2510.20505</link>
<guid>https://papers.cool/arxiv/2510.20505</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Hierarchical Sequence (HSEQ) Iteration, a unified framework that linearises text, tables, and knowledge graphs into a reversible hierarchical sequence with structural tags and performs structure‑aware iterative retrieval to gather sufficient evidence before answer synthesis. A Head Agent guides retrieval while an Iteration Agent expands the sequence via parent/child hops, table neighbour moves, and KG relation traversals, followed by evidence canonicalisation and optional refinement to resolve contradictions. Experiments on HotpotQA, HybridQA/TAT‑QA, and MetaQA demonstrate consistent EM/F1 improvements over strong baselines with reduced latency and token usage.<br /><strong>Summary (CN):</strong> 本文提出了层级序列（HSEQ）迭代框架，将文本、表格和知识图谱线性化为可逆的层级序列并添加轻量结构标签，通过结构感知的迭代检索在生成答案前收集足够证据。Head Agent负责检索指引，Iteration Agent 通过父子跳跃、表格邻居或 KG 关系等结构保持动作扩展序列，随后对证据进行规范化并可选地进行矛盾消解循环。对 HotpotQA、HybridQA/TAT‑QA 与 MetaQA 的实验表明，相比强基线在 EM/F1 上取得一致提升，并显著降低延迟和令牌消耗。<br /><strong>Keywords:</strong> hierarchical sequence, heterogeneous question answering, retrieval-augmented generation, multi-hop reasoning, structural tags, knowledge graph, table QA, budget-aware iteration, evidence canonicalization<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration for Heterogeneous Question Answering, a unified framework that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) guided, budget-aware iteration that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.
<div><strong>Authors:</strong> Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Hierarchical Sequence (HSEQ) Iteration, a unified framework that linearises text, tables, and knowledge graphs into a reversible hierarchical sequence with structural tags and performs structure‑aware iterative retrieval to gather sufficient evidence before answer synthesis. A Head Agent guides retrieval while an Iteration Agent expands the sequence via parent/child hops, table neighbour moves, and KG relation traversals, followed by evidence canonicalisation and optional refinement to resolve contradictions. Experiments on HotpotQA, HybridQA/TAT‑QA, and MetaQA demonstrate consistent EM/F1 improvements over strong baselines with reduced latency and token usage.", "summary_cn": "本文提出了层级序列（HSEQ）迭代框架，将文本、表格和知识图谱线性化为可逆的层级序列并添加轻量结构标签，通过结构感知的迭代检索在生成答案前收集足够证据。Head Agent负责检索指引，Iteration Agent 通过父子跳跃、表格邻居或 KG 关系等结构保持动作扩展序列，随后对证据进行规范化并可选地进行矛盾消解循环。对 HotpotQA、HybridQA/TAT‑QA 与 MetaQA 的实验表明，相比强基线在 EM/F1 上取得一致提升，并显著降低延迟和令牌消耗。", "keywords": "hierarchical sequence, heterogeneous question answering, retrieval-augmented generation, multi-hop reasoning, structural tags, knowledge graph, table QA, budget-aware iteration, evidence canonicalization", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"]}
]]></acme>

<pubDate>2025-10-23T12:48:18+00:00</pubDate>
</item>
<item>
<title>Robust Preference Alignment via Directional Neighborhood Consensus</title>
<link>https://papers.cool/arxiv/2510.20498</link>
<guid>https://papers.cool/arxiv/2510.20498</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Robust Preference Selection (RPS), a training-free, post‑hoc method that samples multiple responses from a directional neighborhood of related preferences and selects the one best matching the user’s intent, thereby closing the preference coverage gap in large language models. A theoretical analysis shows the neighborhood generation is provably superior to a strong baseline, and experiments across DPA, DPO, and SFT paradigms demonstrate up to 69% win rates on under‑represented preference regions without retraining. RPS provides a practical and theoretically grounded way to enhance the robustness and reliability of preference‑aligned models.<br /><strong>Summary (CN):</strong> 本文提出了 Robust Preference Selection（RPS）——一种无需重新训练的后处理方法，通过在偏好空间的方向邻域中采样多个候选回复并挑选最符合用户意图的答案，以弥补大型语言模型在偏好覆盖方面的不足。理论分析表明该邻域生成策略相较于强基线有可证明的优势，实验在 DPA、DPO 和 SFT 三种对齐范式上验证了其有效性，在偏好空间的弱覆盖区域实现最高 69% 的胜率，且无需模型重新训练。RPS 为提升偏好对齐模型的鲁棒性和可靠性提供了实用且有理论支撑的方案。<br /><strong>Keywords:</strong> preference alignment, directional neighborhood consensus, robust preference selection, post-hoc alignment, large language models, DPO, DPA, SFT, safety<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ruochen Mao, Yuling Shi, Xiaodong Gu, Jiaheng Wei</div>
Aligning large language models with human preferences is critical for creating reliable and controllable AI systems. A human preference can be visualized as a high-dimensional vector where different directions represent trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet, because the training data often reflects dominant, average preferences, LLMs tend to perform well on common requests but fall short in specific, individual needs. This mismatch creates a preference coverage gap. Existing methods often address this through costly retraining, which may not be generalized to the full spectrum of diverse preferences. This brittleness means that when a user's request reflects a nuanced preference deviating from the training data's central tendency, model performance can degrade unpredictably. To address this challenge, we introduce Robust Preference Selection (RPS), a post-hoc, training-free method by leveraging directional neighborhood consensus. Instead of forcing a model to generate a response from a single, highly specific preference, RPS samples multiple responses from a local neighborhood of related preferences to create a superior candidate pool. It then selects the response that best aligns with the user's original intent. We provide a theoretical framework showing our neighborhood generation strategy is provably superior to a strong baseline that also samples multiple candidates. Comprehensive experiments across three distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS consistently improves robustness against this baseline, achieving win rates of up to 69% on challenging preferences from under-represented regions of the space without any model retraining. Our work presents a practical, theoretically-grounded solution for enhancing the reliability of preference-aligned models.
<div><strong>Authors:</strong> Ruochen Mao, Yuling Shi, Xiaodong Gu, Jiaheng Wei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Robust Preference Selection (RPS), a training-free, post‑hoc method that samples multiple responses from a directional neighborhood of related preferences and selects the one best matching the user’s intent, thereby closing the preference coverage gap in large language models. A theoretical analysis shows the neighborhood generation is provably superior to a strong baseline, and experiments across DPA, DPO, and SFT paradigms demonstrate up to 69% win rates on under‑represented preference regions without retraining. RPS provides a practical and theoretically grounded way to enhance the robustness and reliability of preference‑aligned models.", "summary_cn": "本文提出了 Robust Preference Selection（RPS）——一种无需重新训练的后处理方法，通过在偏好空间的方向邻域中采样多个候选回复并挑选最符合用户意图的答案，以弥补大型语言模型在偏好覆盖方面的不足。理论分析表明该邻域生成策略相较于强基线有可证明的优势，实验在 DPA、DPO 和 SFT 三种对齐范式上验证了其有效性，在偏好空间的弱覆盖区域实现最高 69% 的胜率，且无需模型重新训练。RPS 为提升偏好对齐模型的鲁棒性和可靠性提供了实用且有理论支撑的方案。", "keywords": "preference alignment, directional neighborhood consensus, robust preference selection, post-hoc alignment, large language models, DPO, DPA, SFT, safety", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ruochen Mao", "Yuling Shi", "Xiaodong Gu", "Jiaheng Wei"]}
]]></acme>

<pubDate>2025-10-23T12:39:20+00:00</pubDate>
</item>
<item>
<title>Steering Evaluation-Aware Language Models To Act Like They Are Deployed</title>
<link>https://papers.cool/arxiv/2510.20487</link>
<guid>https://papers.cool/arxiv/2510.20487</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method to suppress evaluation-awareness in large language models by adding a steering vector to their activations, causing the model to behave as if it were deployed during evaluation. The authors first train a model to exhibit evaluation-aware behavior using factual documents and Python type hints, then show that steering with a vector derived from the original model removes the cue‑driven bias. This technique can improve the reliability of safety assessments.<br /><strong>Summary (CN):</strong> 本文提出通过向大语言模型的激活添加导向向量来抑制其评估感知，使模型在评估时表现得像已部署状态。作者先使用包含模型事实描述的文档和 Python 类型提示训练模型形成评估感知行为，然后示例利用在原始模型上获得的导向向量消除评估线索导致的偏差。该方法有望提升安全评估的可靠性。<br /><strong>Keywords:</strong> evaluation-awareness, activation steering, language model alignment, safety evaluation, deployment simulation, expert iteration, Python type hints, evaluation cue, model bias, reliability<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda</div>
Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. However, this gap can only be observed by removing the evaluation cue. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.
<div><strong>Authors:</strong> Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method to suppress evaluation-awareness in large language models by adding a steering vector to their activations, causing the model to behave as if it were deployed during evaluation. The authors first train a model to exhibit evaluation-aware behavior using factual documents and Python type hints, then show that steering with a vector derived from the original model removes the cue‑driven bias. This technique can improve the reliability of safety assessments.", "summary_cn": "本文提出通过向大语言模型的激活添加导向向量来抑制其评估感知，使模型在评估时表现得像已部署状态。作者先使用包含模型事实描述的文档和 Python 类型提示训练模型形成评估感知行为，然后示例利用在原始模型上获得的导向向量消除评估线索导致的偏差。该方法有望提升安全评估的可靠性。", "keywords": "evaluation-awareness, activation steering, language model alignment, safety evaluation, deployment simulation, expert iteration, Python type hints, evaluation cue, model bias, reliability", "scoring": {"interpretability": 5, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Tim Tian Hua", "Andrew Qin", "Samuel Marks", "Neel Nanda"]}
]]></acme>

<pubDate>2025-10-23T12:29:16+00:00</pubDate>
</item>
<item>
<title>RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging</title>
<link>https://papers.cool/arxiv/2510.20479</link>
<guid>https://papers.cool/arxiv/2510.20479</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RECALL, a representation‑aware model merging framework that leverages layer‑wise hidden representations of large language models to compute inter‑model similarity and perform adaptive hierarchical parameter fusion. By aligning knowledge across models, RECALL preserves domain‑general features in shallow layers while allowing task‑specific adaptation in deeper layers, achieving strong resistance to catastrophic forgetting without requiring historical data or task labels. Extensive experiments on five NLP tasks demonstrate superior knowledge retention and generalization compared to existing continual learning baselines.<br /><strong>Summary (CN):</strong> 本文提出 RECALL，一种基于表示的模型合并框架，利用大语言模型逐层隐藏表征计算模型间相似度，并进行自适应的层次参数融合。该设计在保持浅层通用特征的同时，让深层实现任务特定适配，从而在无需历史数据或任务标签的情况下显著缓解灾难性遗忘。实验在五个 NLP 任务和多种持续学习场景中显示 RECALL 在知识保留和泛化方面优于现有基线。<br /><strong>Keywords:</strong> continual learning, catastrophic forgetting, model merging, representation alignment, hierarchical fusion, LLM, data-free, knowledge retention<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Bowen Wang, Haiyuan Wan, Liwen Shi, Chen Yang, Peng He, Yue Ma, Haochen Han, Wenhao Li, Tiao Tan, Yongjian Li, Fangming Liu, Yifan Gong, Sheng Zhang</div>
We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.
<div><strong>Authors:</strong> Bowen Wang, Haiyuan Wan, Liwen Shi, Chen Yang, Peng He, Yue Ma, Haochen Han, Wenhao Li, Tiao Tan, Yongjian Li, Fangming Liu, Yifan Gong, Sheng Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RECALL, a representation‑aware model merging framework that leverages layer‑wise hidden representations of large language models to compute inter‑model similarity and perform adaptive hierarchical parameter fusion. By aligning knowledge across models, RECALL preserves domain‑general features in shallow layers while allowing task‑specific adaptation in deeper layers, achieving strong resistance to catastrophic forgetting without requiring historical data or task labels. Extensive experiments on five NLP tasks demonstrate superior knowledge retention and generalization compared to existing continual learning baselines.", "summary_cn": "本文提出 RECALL，一种基于表示的模型合并框架，利用大语言模型逐层隐藏表征计算模型间相似度，并进行自适应的层次参数融合。该设计在保持浅层通用特征的同时，让深层实现任务特定适配，从而在无需历史数据或任务标签的情况下显著缓解灾难性遗忘。实验在五个 NLP 任务和多种持续学习场景中显示 RECALL 在知识保留和泛化方面优于现有基线。", "keywords": "continual learning, catastrophic forgetting, model merging, representation alignment, hierarchical fusion, LLM, data-free, knowledge retention", "scoring": {"interpretability": 4, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Bowen Wang", "Haiyuan Wan", "Liwen Shi", "Chen Yang", "Peng He", "Yue Ma", "Haochen Han", "Wenhao Li", "Tiao Tan", "Yongjian Li", "Fangming Liu", "Yifan Gong", "Sheng Zhang"]}
]]></acme>

<pubDate>2025-10-23T12:17:37+00:00</pubDate>
</item>
<item>
<title>Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs</title>
<link>https://papers.cool/arxiv/2510.20475</link>
<guid>https://papers.cool/arxiv/2510.20475</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents an improved Masked Language Modeling (MLM) technique for the 2025 BabyLM Challenge, where masking probabilities are adapted based on the model’s prediction difficulty and sub‑token embeddings are incorporated to enhance morphological generalization. Experiments show substantial gains on (Super)GLUE benchmarks compared to standard MLM, and the approach outperforms the baseline in the strict‑small track.<br /><strong>Summary (CN):</strong> 本文提出了一种改进的掩码语言模型（MLM）方法，用于 2025 年 BabyLM 挑战。该方法根据模型预测难度自适应调整掩码概率，并引入子词嵌入以提升形态学泛化能力。实验表明，相较于标准，在 (Super)GLUE 基准上取得显著提升，并在 strict‑small 轨道中超越基线。<br /><strong>Keywords:</strong> masked language modeling, adaptive masking, BabyLM, morphological generalization, sub-token embeddings, low-resource pretraining<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lukas Edman, Alexander Fraser</div>
We describe our strategy for the 2025 edition of the BabyLM Challenge. Our main contribution is that of an improved form of Masked Language Modeling (MLM), which adapts the probabilities of the tokens masked according to the model's ability to predict them. The results show a substantial increase in performance on (Super)GLUE tasks over the standard MLM. We also incorporate sub-token embeddings, finding that this increases the model's morphological generalization capabilities. Our submission beats the baseline in the strict-small track.
<div><strong>Authors:</strong> Lukas Edman, Alexander Fraser</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents an improved Masked Language Modeling (MLM) technique for the 2025 BabyLM Challenge, where masking probabilities are adapted based on the model’s prediction difficulty and sub‑token embeddings are incorporated to enhance morphological generalization. Experiments show substantial gains on (Super)GLUE benchmarks compared to standard MLM, and the approach outperforms the baseline in the strict‑small track.", "summary_cn": "本文提出了一种改进的掩码语言模型（MLM）方法，用于 2025 年 BabyLM 挑战。该方法根据模型预测难度自适应调整掩码概率，并引入子词嵌入以提升形态学泛化能力。实验表明，相较于标准，在 (Super)GLUE 基准上取得显著提升，并在 strict‑small 轨道中超越基线。", "keywords": "masked language modeling, adaptive masking, BabyLM, morphological generalization, sub-token embeddings, low-resource pretraining", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lukas Edman", "Alexander Fraser"]}
]]></acme>

<pubDate>2025-10-23T12:15:24+00:00</pubDate>
</item>
<item>
<title>Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20460</link>
<guid>https://papers.cool/arxiv/2510.20460</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper systematically evaluates four confidence estimation methods—VCE, MSP, Sample Consistency, and CoCoA—for large language model outputs across four QA tasks, finding that each metric captures different aspects of uncertainty and that the hybrid CoCoA approach achieves the best overall calibration and discrimination. Recommendations for selecting uncertainty measures in LLM applications are provided.<br /><strong>Summary (CN):</strong> 本文系统评估了四种大型语言模型输出置信度估计方法（VCE、MSP、样本一致性和 CoCoA），在四个问答任务上比较它们捕获模型不确定性的不同方面，结果表明混合的 CoCoA 方法在校准和正确答案辨别方面表现最佳。文章还讨论了各方法的权衡并给出在实际 LLM 应用中选择不确定性度量的建议。<br /><strong>Keywords:</strong> uncertainty estimation, confidence calibration, large language models, VCE, MSP, sample consistency, CoCoA, question answering, reliability, evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Christian Hobelsberger, Theresa Winner, Andreas Nawroth, Oliver Mitevski, Anna-Carolina Haensch</div>
Large language models (LLMs) produce outputs with varying levels of uncertainty, and, just as often, varying levels of correctness; making their practical reliability far from guaranteed. To quantify this uncertainty, we systematically evaluate four approaches for confidence estimation in LLM outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For the evaluation of the approaches, we conduct experiments on four question-answering tasks using a state-of-the-art open-source LLM. Our results show that each uncertainty metric captures a different facet of model confidence and that the hybrid CoCoA approach yields the best reliability overall, improving both calibration and discrimination of correct answers. We discuss the trade-offs of each method and provide recommendations for selecting uncertainty measures in LLM applications.
<div><strong>Authors:</strong> Christian Hobelsberger, Theresa Winner, Andreas Nawroth, Oliver Mitevski, Anna-Carolina Haensch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper systematically evaluates four confidence estimation methods—VCE, MSP, Sample Consistency, and CoCoA—for large language model outputs across four QA tasks, finding that each metric captures different aspects of uncertainty and that the hybrid CoCoA approach achieves the best overall calibration and discrimination. Recommendations for selecting uncertainty measures in LLM applications are provided.", "summary_cn": "本文系统评估了四种大型语言模型输出置信度估计方法（VCE、MSP、样本一致性和 CoCoA），在四个问答任务上比较它们捕获模型不确定性的不同方面，结果表明混合的 CoCoA 方法在校准和正确答案辨别方面表现最佳。文章还讨论了各方法的权衡并给出在实际 LLM 应用中选择不确定性度量的建议。", "keywords": "uncertainty estimation, confidence calibration, large language models, VCE, MSP, sample consistency, CoCoA, question answering, reliability, evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Christian Hobelsberger", "Theresa Winner", "Andreas Nawroth", "Oliver Mitevski", "Anna-Carolina Haensch"]}
]]></acme>

<pubDate>2025-10-23T11:50:47+00:00</pubDate>
</item>
<item>
<title>LM-mixup: Text Data Augmentation via Language Model based Mixup</title>
<link>https://papers.cool/arxiv/2510.20449</link>
<guid>https://papers.cool/arxiv/2510.20449</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines instruction distillation and introduces LM-Mixup, a method that fine‑tunes LLMs on a small distilled dataset (MIXTURE) and further optimizes them with reinforcement learning using quality, semantic alignment, and format rewards. LM‑Mixup augments low‑quality instruction data so that training on only ~3 % of the original data outperforms full‑dataset training and rivals state‑of‑the‑art data selection methods. This demonstrates that low‑quality data can be transformed into a valuable resource for instruction‑tuned LLMs.<br /><strong>Summary (CN):</strong> 本文定义了指令蒸馏任务并提出 LM‑Mixup 方法：先在包含低质量指令集的 MIXTURE 数据上进行监督微调，再通过包含质量、语义对齐和格式符合度三种奖励的强化学习（GRPO）进行优化。实验表明，仅使用约 3% 的蒸馏数据进行微调即可超越整体数据训练并与最先进的数据筛选方法竞争，证明低质量数据在适当蒸馏后可成为提升指令调优模型的宝贵资源。<br /><strong>Keywords:</strong> instruction tuning, data augmentation, LM-Mixup, instruction distillation, reinforcement learning, GRPO, low-quality data, MIXTURE dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Zhijie Deng, Zhouan Shen, Ling Li, Yao Zhou, Zhaowei Zhu, Yanji He, Wei Wang, Jiaheng Wei</div>
Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of Instruction Distillation: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create MIXTURE, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce LM-Mixup, by first performing supervised fine-tuning on MIXTURE and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that LM-Mixup effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with LM-Mixup, significantly enhancing the efficiency and performance of instruction-tuned LLMs.
<div><strong>Authors:</strong> Zhijie Deng, Zhouan Shen, Ling Li, Yao Zhou, Zhaowei Zhu, Yanji He, Wei Wang, Jiaheng Wei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines instruction distillation and introduces LM-Mixup, a method that fine‑tunes LLMs on a small distilled dataset (MIXTURE) and further optimizes them with reinforcement learning using quality, semantic alignment, and format rewards. LM‑Mixup augments low‑quality instruction data so that training on only ~3 % of the original data outperforms full‑dataset training and rivals state‑of‑the‑art data selection methods. This demonstrates that low‑quality data can be transformed into a valuable resource for instruction‑tuned LLMs.", "summary_cn": "本文定义了指令蒸馏任务并提出 LM‑Mixup 方法：先在包含低质量指令集的 MIXTURE 数据上进行监督微调，再通过包含质量、语义对齐和格式符合度三种奖励的强化学习（GRPO）进行优化。实验表明，仅使用约 3% 的蒸馏数据进行微调即可超越整体数据训练并与最先进的数据筛选方法竞争，证明低质量数据在适当蒸馏后可成为提升指令调优模型的宝贵资源。", "keywords": "instruction tuning, data augmentation, LM-Mixup, instruction distillation, reinforcement learning, GRPO, low-quality data, MIXTURE dataset", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zhijie Deng", "Zhouan Shen", "Ling Li", "Yao Zhou", "Zhaowei Zhu", "Yanji He", "Wei Wang", "Jiaheng Wei"]}
]]></acme>

<pubDate>2025-10-23T11:33:35+00:00</pubDate>
</item>
<item>
<title>Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction</title>
<link>https://papers.cool/arxiv/2510.20411</link>
<guid>https://papers.cool/arxiv/2510.20411</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ContingentChat, a teacher-student framework that uses a post‑training alignment dataset to improve multi‑turn contingency in a BabyLM trained on 100 M words, resulting in more grammatical and cohesive responses. Experiments with adaptive teacher decoding show limited additional gains, highlighting that contingency remains a challenging goal for small language models.<br /><strong>Summary (CN):</strong> 本文提出了 ContingentChat 教师‑学生框架，通过后训练对齐数据集提升在 100 M 词训练的 BabyLM 中的多轮对话应答的连续性，使生成的回复更为语法正确且连贯。针对性教师解码策略的实验仅带来有限增益，表明对婴儿语言模型而言，实现连续性仍是一个挑战。<br /><strong>Keywords:</strong> BabyLM, multi-turn dialogue, contingency, teacher-student framework, post-training alignment, language model, dialogue quality<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Suchir Salhan, Hongyi Gu, Donya Rooein, Diana Galvan-Sosa, Gabrielle Gaudeau, Andrew Caines, Zheng Yuan, Paula Buttery</div>
Multi-turn dialogues between a child and a caregiver are characterized by a property called contingency - that is, prompt, direct, and meaningful exchanges between interlocutors. We introduce ContingentChat, a teacher-student framework that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M words. Using a novel alignment dataset for post-training, BabyLM generates responses that are more grammatical and cohesive. Experiments with adaptive teacher decoding strategies show limited additional gains. ContingentChat demonstrates the benefits of targeted post-training for dialogue quality and indicates that contingency remains a challenging goal for BabyLMs.
<div><strong>Authors:</strong> Suchir Salhan, Hongyi Gu, Donya Rooein, Diana Galvan-Sosa, Gabrielle Gaudeau, Andrew Caines, Zheng Yuan, Paula Buttery</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ContingentChat, a teacher-student framework that uses a post‑training alignment dataset to improve multi‑turn contingency in a BabyLM trained on 100 M words, resulting in more grammatical and cohesive responses. Experiments with adaptive teacher decoding show limited additional gains, highlighting that contingency remains a challenging goal for small language models.", "summary_cn": "本文提出了 ContingentChat 教师‑学生框架，通过后训练对齐数据集提升在 100 M 词训练的 BabyLM 中的多轮对话应答的连续性，使生成的回复更为语法正确且连贯。针对性教师解码策略的实验仅带来有限增益，表明对婴儿语言模型而言，实现连续性仍是一个挑战。", "keywords": "BabyLM, multi-turn dialogue, contingency, teacher-student framework, post-training alignment, language model, dialogue quality", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Suchir Salhan", "Hongyi Gu", "Donya Rooein", "Diana Galvan-Sosa", "Gabrielle Gaudeau", "Andrew Caines", "Zheng Yuan", "Paula Buttery"]}
]]></acme>

<pubDate>2025-10-23T10:29:23+00:00</pubDate>
</item>
<item>
<title>NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew</title>
<link>https://papers.cool/arxiv/2510.20386</link>
<guid>https://papers.cool/arxiv/2510.20386</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces NeoDictaBERT and NeoDictaBERT-bilingual, BERT-style models based on the NeoBERT architecture that are specifically trained on Hebrew text. These models achieve state-of-the-art results on most Hebrew benchmarks and demonstrate strong performance on multilingual retrieval tasks compared to similarly sized models. The authors detail the training methodology and release the models for community use.<br /><strong>Summary (CN):</strong> 本文提出了 NeoDictaBERT 和 NeoDictaBERT‑bilingual 两种基于 NeoBERT 架构的 BERT 风格模型，专门在希伯来语文本上进行训练。相较于同等规模的模型，这些模型在多数希伯来语基准上取得了最先进的表现，并在多语言检索任务中表现出色。文章阐述了训练流程并公开模型以供社区使用。<br /><strong>Keywords:</strong> NeoDictaBERT, Hebrew NLP, BERT, NeoBERT, multilingual retrieval, transformer architecture<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shaltiel Shmidman, Avi Shmidman, Moshe Koppel</div>
Since their initial release, BERT models have demonstrated exceptional performance on a variety of tasks, despite their relatively small size (BERT-base has ~100M parameters). Nevertheless, the architectural choices used in these models are outdated compared to newer transformer-based models such as Llama3 and Qwen3. In recent months, several architectures have been proposed to close this gap. ModernBERT and NeoBERT both show strong improvements on English benchmarks and significantly extend the supported context window. Following their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual: BERT-style models trained using the same architecture as NeoBERT, with a dedicated focus on Hebrew texts. These models outperform existing ones on almost all Hebrew benchmarks and provide a strong foundation for downstream tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on retrieval tasks, outperforming other multilingual models of similar size. In this paper, we describe the training process and report results across various benchmarks. We release the models to the community as part of our goal to advance research and development in Hebrew NLP.
<div><strong>Authors:</strong> Shaltiel Shmidman, Avi Shmidman, Moshe Koppel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces NeoDictaBERT and NeoDictaBERT-bilingual, BERT-style models based on the NeoBERT architecture that are specifically trained on Hebrew text. These models achieve state-of-the-art results on most Hebrew benchmarks and demonstrate strong performance on multilingual retrieval tasks compared to similarly sized models. The authors detail the training methodology and release the models for community use.", "summary_cn": "本文提出了 NeoDictaBERT 和 NeoDictaBERT‑bilingual 两种基于 NeoBERT 架构的 BERT 风格模型，专门在希伯来语文本上进行训练。相较于同等规模的模型，这些模型在多数希伯来语基准上取得了最先进的表现，并在多语言检索任务中表现出色。文章阐述了训练流程并公开模型以供社区使用。", "keywords": "NeoDictaBERT, Hebrew NLP, BERT, NeoBERT, multilingual retrieval, transformer architecture", "scoring": {"interpretability": 2, "understanding": 5, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shaltiel Shmidman", "Avi Shmidman", "Moshe Koppel"]}
]]></acme>

<pubDate>2025-10-23T09:34:53+00:00</pubDate>
</item>
<item>
<title>VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation</title>
<link>https://papers.cool/arxiv/2510.20381</link>
<guid>https://papers.cool/arxiv/2510.20381</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the VLSP 2025 MLQA-TSR shared task, which provides a benchmark dataset for Vietnamese multimodal legal question answering focused on traffic sign regulations. The task comprises two subtasks—multimodal legal retrieval and multimodal question answering—with the best reported results being an F2 score of 64.55% for retrieval and an accuracy of 86.30% for QA.<br /><strong>Summary (CN):</strong> 本文提出了 VLSP 2025 MLQA-TSR 共享任务，提供了一个针对越南交通标志法规的多模态法律问答基准数据集。该任务包括多模态法律检索和多模态问答两个子任务，最佳成绩分别为检索的 F2 分数 64.55% 和问答的准确率 86.30%。<br /><strong>Keywords:</strong> multimodal question answering, legal AI, Vietnamese, traffic sign regulation, benchmark dataset, retrieval, QA, VLSP 2025<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Son T. Luu, Trung Vo, Hiep Nguyen, Khanh Quoc Tran, Kiet Van Nguyen, Vu Tran, Ngan Luu-Thuy Nguyen, Le-Minh Nguyen</div>
This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025 MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal question answering. The goal is to advance research on Vietnamese multimodal legal text processing and to provide a benchmark dataset for building and evaluating intelligent systems in multimodal legal domains, with a focus on traffic sign regulation in Vietnam. The best-reported results on VLSP 2025 MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an accuracy of 86.30% for multimodal question answering.
<div><strong>Authors:</strong> Son T. Luu, Trung Vo, Hiep Nguyen, Khanh Quoc Tran, Kiet Van Nguyen, Vu Tran, Ngan Luu-Thuy Nguyen, Le-Minh Nguyen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the VLSP 2025 MLQA-TSR shared task, which provides a benchmark dataset for Vietnamese multimodal legal question answering focused on traffic sign regulations. The task comprises two subtasks—multimodal legal retrieval and multimodal question answering—with the best reported results being an F2 score of 64.55% for retrieval and an accuracy of 86.30% for QA.", "summary_cn": "本文提出了 VLSP 2025 MLQA-TSR 共享任务，提供了一个针对越南交通标志法规的多模态法律问答基准数据集。该任务包括多模态法律检索和多模态问答两个子任务，最佳成绩分别为检索的 F2 分数 64.55% 和问答的准确率 86.30%。", "keywords": "multimodal question answering, legal AI, Vietnamese, traffic sign regulation, benchmark dataset, retrieval, QA, VLSP 2025", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Son T. Luu", "Trung Vo", "Hiep Nguyen", "Khanh Quoc Tran", "Kiet Van Nguyen", "Vu Tran", "Ngan Luu-Thuy Nguyen", "Le-Minh Nguyen"]}
]]></acme>

<pubDate>2025-10-23T09:24:43+00:00</pubDate>
</item>
<item>
<title>The Impact of Negated Text on Hallucination with Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20375</link>
<guid>https://papers.cool/arxiv/2510.20375</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how negated text affects hallucination detection in large language models, introducing the NegHalu dataset that rewrites existing hallucination benchmarks with negated expressions. Experiments reveal that LLMs often fail to identify hallucinations in negated contexts, producing inconsistent judgments, and token-level tracing shows internal processing challenges. The study highlights a previously overlooked weakness in model faithfulness and suggests avenues for mitigation.<br /><strong>Summary (CN):</strong> 本文研究了否定文本对大语言模型中幻觉检测的影响，构建了 NegHalu 数据集，将已有的幻觉检测数据集改写为否定表述。实验表明，模型在否定语境下往往难以正确识别幻觉，导致逻辑不一致或不忠实的判断，并通过逐词追踪展示了内部处理的困难。该工作揭示了模型可信度的一项被忽视的弱点，并提出了潜在的缓解方向。<br /><strong>Keywords:</strong> hallucination detection, negation, large language models, NegHalu dataset, token-level analysis, model interpretability, AI safety<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 7, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Jaehyung Seo, Hyeonseok Moon, Heuiseok Lim</div>
Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects.
<div><strong>Authors:</strong> Jaehyung Seo, Hyeonseok Moon, Heuiseok Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how negated text affects hallucination detection in large language models, introducing the NegHalu dataset that rewrites existing hallucination benchmarks with negated expressions. Experiments reveal that LLMs often fail to identify hallucinations in negated contexts, producing inconsistent judgments, and token-level tracing shows internal processing challenges. The study highlights a previously overlooked weakness in model faithfulness and suggests avenues for mitigation.", "summary_cn": "本文研究了否定文本对大语言模型中幻觉检测的影响，构建了 NegHalu 数据集，将已有的幻觉检测数据集改写为否定表述。实验表明，模型在否定语境下往往难以正确识别幻觉，导致逻辑不一致或不忠实的判断，并通过逐词追踪展示了内部处理的困难。该工作揭示了模型可信度的一项被忽视的弱点，并提出了潜在的缓解方向。", "keywords": "hallucination detection, negation, large language models, NegHalu dataset, token-level analysis, model interpretability, AI safety", "scoring": {"interpretability": 6, "understanding": 7, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Jaehyung Seo", "Hyeonseok Moon", "Heuiseok Lim"]}
]]></acme>

<pubDate>2025-10-23T09:20:15+00:00</pubDate>
</item>
<item>
<title>Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)</title>
<link>https://papers.cool/arxiv/2510.20358</link>
<guid>https://papers.cool/arxiv/2510.20358</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether pre‑training solely on dialogue data yields small language models that are both formally and functionally suitable for the BabyLM benchmark. Using a dialogue‑only pre‑trained Llamalogue model, various fine‑tuning strategies (including PPO and DPO) are applied to encourage more communicative text generation; the models underperform on standard BabyLM tasks but excel at a custom dialogue continuation minimal‑pair benchmark, with DPO improving performance where PPO shows mixed or adverse effects.<br /><strong>Summary (CN):</strong> 本文研究仅使用对话数据进行预训练是否能够产出在 BabyLM 基准上形式和功能上都合适的小型语言模型。作者基于预训练的 Llamalogue 模型，采用包括 PPO 与 DPO 在内的多种微调策略，以促使模型生成更具交互性的文本；尽管这些在多数标准 BabyLM 评测上表现不佳，却在自定义的对话续写最小对比基准上表现突出，其中 DPO 能进一步提升性能，而 PPO 则呈现出混合甚至对抗性的效果。<br /><strong>Keywords:</strong> dialogue pretraining, BabyLM, language models, PPO fine-tuning, DPO fine-tuning, communicative generation, minimal pair evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Francesca Padovani, Bastian Bunzeck, Manar Ali, Omar Momen, Arianna Bisazza, Hendrik Buschmeier, Sina Zarrieß</div>
We investigate whether pre-training exclusively on dialogue data results in formally and functionally apt small language models. Based on this pre-trained llamalogue model, we employ a variety of fine-tuning strategies to enforce "more communicative" text generations by our models. Although our models underperform on most standard BabyLM benchmarks, they excel at dialogue continuation prediction in a minimal pair setting. While PPO fine-tuning has mixed to adversarial effects on our models, DPO fine-tuning further improves their performance on our custom dialogue benchmark.
<div><strong>Authors:</strong> Francesca Padovani, Bastian Bunzeck, Manar Ali, Omar Momen, Arianna Bisazza, Hendrik Buschmeier, Sina Zarrieß</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether pre‑training solely on dialogue data yields small language models that are both formally and functionally suitable for the BabyLM benchmark. Using a dialogue‑only pre‑trained Llamalogue model, various fine‑tuning strategies (including PPO and DPO) are applied to encourage more communicative text generation; the models underperform on standard BabyLM tasks but excel at a custom dialogue continuation minimal‑pair benchmark, with DPO improving performance where PPO shows mixed or adverse effects.", "summary_cn": "本文研究仅使用对话数据进行预训练是否能够产出在 BabyLM 基准上形式和功能上都合适的小型语言模型。作者基于预训练的 Llamalogue 模型，采用包括 PPO 与 DPO 在内的多种微调策略，以促使模型生成更具交互性的文本；尽管这些在多数标准 BabyLM 评测上表现不佳，却在自定义的对话续写最小对比基准上表现突出，其中 DPO 能进一步提升性能，而 PPO 则呈现出混合甚至对抗性的效果。", "keywords": "dialogue pretraining, BabyLM, language models, PPO fine-tuning, DPO fine-tuning, communicative generation, minimal pair evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Francesca Padovani", "Bastian Bunzeck", "Manar Ali", "Omar Momen", "Arianna Bisazza", "Hendrik Buschmeier", "Sina Zarrieß"]}
]]></acme>

<pubDate>2025-10-23T08:57:56+00:00</pubDate>
</item>
<item>
<title>FreeChunker: A Cross-Granularity Chunking Framework</title>
<link>https://papers.cool/arxiv/2510.20356</link>
<guid>https://papers.cool/arxiv/2510.20356</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> FreeChunker introduces a cross‑granularity encoding framework that treats sentences as atomic units and enables flexible retrieval of arbitrary sentence combinations, replacing static chunk segmentation in Retrieval‑Augmented Generation (RAG) pipelines. This approach reduces the computational cost of semantic boundary detection while improving retrieval performance, as demonstrated on the LongBench V2 benchmark. The paper shows that FreeChunker outperforms traditional chunking methods both in accuracy and efficiency.<br /><strong>Summary (CN):</strong> FreeChunker 提出了一种跨粒度编码框架，将句子视为原子单元，支持任意句子组合的灵活检索，取代了检索增强生成（RAG）系统中的固定块划分。该方法显著降低了语义边界检测的计算开销，并在 LongBench V2 基准上提升了检索性能。实验表明 FreeChunker 在准确率和效率上均优于传统块划分方法。<br /><strong>Keywords:</strong> chunking, retrieval-augmented generation, cross-granularity, flexible retrieval, LongBench, computational efficiency, semantic boundary detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wenxuan Zhang, Yuan-Hao Jiang, Yonghe Wu</div>
Chunking strategies significantly impact the effectiveness of Retrieval-Augmented Generation (RAG) systems. Existing methods operate within fixed-granularity paradigms that rely on static boundary identification, limiting their adaptability to diverse query requirements. This paper presents FreeChunker, a Cross-Granularity Encoding Framework that fundamentally transforms the traditional chunking paradigm: the framework treats sentences as atomic units and shifts from static chunk segmentation to flexible retrieval supporting arbitrary sentence combinations. This paradigm shift not only significantly reduces the computational overhead required for semantic boundary detection but also enhances adaptability to complex queries. Experimental evaluation on LongBench V2 demonstrates that FreeChunker achieves superior retrieval performance compared to traditional chunking methods, while significantly outperforming existing approaches in computational efficiency.
<div><strong>Authors:</strong> Wenxuan Zhang, Yuan-Hao Jiang, Yonghe Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "FreeChunker introduces a cross‑granularity encoding framework that treats sentences as atomic units and enables flexible retrieval of arbitrary sentence combinations, replacing static chunk segmentation in Retrieval‑Augmented Generation (RAG) pipelines. This approach reduces the computational cost of semantic boundary detection while improving retrieval performance, as demonstrated on the LongBench V2 benchmark. The paper shows that FreeChunker outperforms traditional chunking methods both in accuracy and efficiency.", "summary_cn": "FreeChunker 提出了一种跨粒度编码框架，将句子视为原子单元，支持任意句子组合的灵活检索，取代了检索增强生成（RAG）系统中的固定块划分。该方法显著降低了语义边界检测的计算开销，并在 LongBench V2 基准上提升了检索性能。实验表明 FreeChunker 在准确率和效率上均优于传统块划分方法。", "keywords": "chunking, retrieval-augmented generation, cross-granularity, flexible retrieval, LongBench, computational efficiency, semantic boundary detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Zhang", "Yuan-Hao Jiang", "Yonghe Wu"]}
]]></acme>

<pubDate>2025-10-23T08:57:00+00:00</pubDate>
</item>
<item>
<title>Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20351</link>
<guid>https://papers.cool/arxiv/2510.20351</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper investigates whether large language models (LLMs) possess prior knowledge of widely used public tabular benchmarks such as Adult Income and Titanic. Through controlled probing experiments, the authors find that contamination effects arise only for datasets with strong semantic cues (e.g., informative column names), while removing or randomizing these cues drops performance to near-random levels, indicating that apparent competence may stem from memorization rather than genuine reasoning. The study discusses evaluation protocol implications and proposes methods to separate semantic leakage from authentic reasoning ability.<br /><strong>Summary (CN):</strong> 本文研究了大语言模型（LLM）是否已经记住了常用的公开表格数据集（如 Adult Income、Titanic）。通过一系列受控探测实验，作者发现只有包含明显语义线索（如有意义的列名或可解释的取值）的数据集会出现污染效应；当这些线索被移除或随机化时，模型性能急剧下降至接近随机水平，表明表面上的表格推理能力可能源自对公开数据的记忆而非真正的泛化。文中进一步讨论了评估协议的影响，并提出了在未来 LLM 评估中区分语义泄漏和真实推理能力的策略。<br /><strong>Keywords:</strong> latent knowledge, tabular data, dataset contamination, LLM evaluation, structured reasoning, memorization, probing, semantic cues<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Matteo Silvestri, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei</div>
Large Language Models (LLMs) are increasingly evaluated on their ability to reason over structured data, yet such assessments often overlook a crucial confound: dataset contamination. In this work, we investigate whether LLMs exhibit prior knowledge of widely used tabular benchmarks such as Adult Income, Titanic, and others. Through a series of controlled probing experiments, we reveal that contamination effects emerge exclusively for datasets containing strong semantic cues-for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels. These findings suggest that LLMs' apparent competence on tabular reasoning tasks may, in part, reflect memorization of publicly available datasets rather than genuine generalization. We discuss implications for evaluation protocols and propose strategies to disentangle semantic leakage from authentic reasoning ability in future LLM assessments.
<div><strong>Authors:</strong> Matteo Silvestri, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper investigates whether large language models (LLMs) possess prior knowledge of widely used public tabular benchmarks such as Adult Income and Titanic. Through controlled probing experiments, the authors find that contamination effects arise only for datasets with strong semantic cues (e.g., informative column names), while removing or randomizing these cues drops performance to near-random levels, indicating that apparent competence may stem from memorization rather than genuine reasoning. The study discusses evaluation protocol implications and proposes methods to separate semantic leakage from authentic reasoning ability.", "summary_cn": "本文研究了大语言模型（LLM）是否已经记住了常用的公开表格数据集（如 Adult Income、Titanic）。通过一系列受控探测实验，作者发现只有包含明显语义线索（如有意义的列名或可解释的取值）的数据集会出现污染效应；当这些线索被移除或随机化时，模型性能急剧下降至接近随机水平，表明表面上的表格推理能力可能源自对公开数据的记忆而非真正的泛化。文中进一步讨论了评估协议的影响，并提出了在未来 LLM 评估中区分语义泄漏和真实推理能力的策略。", "keywords": "latent knowledge, tabular data, dataset contamination, LLM evaluation, structured reasoning, memorization, probing, semantic cues", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Matteo Silvestri", "Flavio Giorgi", "Fabrizio Silvestri", "Gabriele Tolomei"]}
]]></acme>

<pubDate>2025-10-23T08:51:14+00:00</pubDate>
</item>
<item>
<title>Teaching Language Models to Reason with Tools</title>
<link>https://papers.cool/arxiv/2510.20342</link>
<guid>https://papers.cool/arxiv/2510.20342</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CoRT (Code-Optimized Reasoning Training), a post‑training framework that teaches large reasoning models to effectively integrate code interpreters via Hint‑Engineering data synthesis, rejection sampling and reinforcement learning. Experiments on 1.5B‑32B models show notable accuracy gains (4%‑8%) on challenging math reasoning benchmarks and substantial token‑efficiency improvements compared to pure language‑only reasoning.<br /><strong>Summary (CN):</strong> 本文提出 CoRT（代码优化推理训练）框架，通过 Hint‑Engineering 数据合成、拒绝采样与强化学习，使大规模推理模型能够高效地与代码解释器交互。实验在 1.5B‑32B 参数模型上展示了在复杂数学推理基准上准确率提升 4%‑8% 并显著降低 token 使用量，相较于纯语言推理有约 30%‑50% 的效率提升。<br /><strong>Keywords:</strong> large reasoning models, code interpreter, tool use, CoRT, hint engineering, supervised fine-tuning, reinforcement learning, mathematical reasoning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</div>
Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\% for the 32B model and 50\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: https://github.com/ChengpengLi1003/CoRT.
<div><strong>Authors:</strong> Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CoRT (Code-Optimized Reasoning Training), a post‑training framework that teaches large reasoning models to effectively integrate code interpreters via Hint‑Engineering data synthesis, rejection sampling and reinforcement learning. Experiments on 1.5B‑32B models show notable accuracy gains (4%‑8%) on challenging math reasoning benchmarks and substantial token‑efficiency improvements compared to pure language‑only reasoning.", "summary_cn": "本文提出 CoRT（代码优化推理训练）框架，通过 Hint‑Engineering 数据合成、拒绝采样与强化学习，使大规模推理模型能够高效地与代码解释器交互。实验在 1.5B‑32B 参数模型上展示了在复杂数学推理基准上准确率提升 4%‑8% 并显著降低 token 使用量，相较于纯语言推理有约 30%‑50% 的效率提升。", "keywords": "large reasoning models, code interpreter, tool use, CoRT, hint engineering, supervised fine-tuning, reinforcement learning, mathematical reasoning", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"]}
]]></acme>

<pubDate>2025-10-23T08:41:44+00:00</pubDate>
</item>
<item>
<title>Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering</title>
<link>https://papers.cool/arxiv/2510.20304</link>
<guid>https://papers.cool/arxiv/2510.20304</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper conducts the first systematic evaluation of generative process reward models (PRMs) on table question answering (TQA), examining how step‑by‑step grading and combined text‑code verification affect answer selection. Experiments reveal that PRMs can improve solution selection but struggle with out‑of‑domain tables and show weak correlation between step verification performance and final accuracy due to loose causal links. The analysis highlights current limitations of PRMs for semi‑structured data and suggests directions for more robust, process‑aware verifiers.<br /><strong>Summary (CN):</strong> 本文首次系统性地评估生成式过程奖励模型（PRM）在表格问答（TQA）任务中的表现，研究逐步打分及文本‑代码联合验证对答案选择的影响。实验表明，PRM 能在一定程度上提升解答选择，但在跨域表格上表现不佳，且步骤验证性能与最终准确率相关性弱，可能源于步骤之间因果联系松散。分析指出当前 PRM 在半结构化数据上的局限，并提供构建更稳健、过程感知验证器的思路。<br /><strong>Keywords:</strong> process reward modeling, table question answering, semi-structured data, step verification, generative PRM, LLM reasoning, code verification, out-of-domain generalization, AI safety, robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Lei Tang, Wei Zhou, Mohsen Mesgar</div>
Process reward models (PRMs) improve complex reasoning in large language models (LLMs) by grading candidate solutions step-by-step and selecting answers via aggregated step scores. While effective in domains such as mathematics, their applicability to tasks involving semi-structured data, like table question answering (TQA) remains unexplored. TQA poses unique challenges for PRMs, including abundant irrelevant information, loosely connected reasoning steps, and domain-specific reasoning. This work presents the first systematic study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from both answer and step perspectives. Results show that PRMs that combine textual and code verification can aid solution selection but struggle to generalize to out-of-domain data. Analysis reveals a weak correlation between performance in step-level verification and answer accuracy, possibly stemming from weak step dependencies and loose causal links. Our findings highlight limitations of current PRMs on TQA and offer valuable insights for building more robust, process-aware verifiers.
<div><strong>Authors:</strong> Lei Tang, Wei Zhou, Mohsen Mesgar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper conducts the first systematic evaluation of generative process reward models (PRMs) on table question answering (TQA), examining how step‑by‑step grading and combined text‑code verification affect answer selection. Experiments reveal that PRMs can improve solution selection but struggle with out‑of‑domain tables and show weak correlation between step verification performance and final accuracy due to loose causal links. The analysis highlights current limitations of PRMs for semi‑structured data and suggests directions for more robust, process‑aware verifiers.", "summary_cn": "本文首次系统性地评估生成式过程奖励模型（PRM）在表格问答（TQA）任务中的表现，研究逐步打分及文本‑代码联合验证对答案选择的影响。实验表明，PRM 能在一定程度上提升解答选择，但在跨域表格上表现不佳，且步骤验证性能与最终准确率相关性弱，可能源于步骤之间因果联系松散。分析指出当前 PRM 在半结构化数据上的局限，并提供构建更稳健、过程感知验证器的思路。", "keywords": "process reward modeling, table question answering, semi-structured data, step verification, generative PRM, LLM reasoning, code verification, out-of-domain generalization, AI safety, robustness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Lei Tang", "Wei Zhou", "Mohsen Mesgar"]}
]]></acme>

<pubDate>2025-10-23T07:49:39+00:00</pubDate>
</item>
<item>
<title>Citation Failure: Definition, Analysis and Efficient Mitigation</title>
<link>https://papers.cool/arxiv/2510.20303</link>
<guid>https://papers.cool/arxiv/2510.20303</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines citation failure as the inability of LLM-based retrieval‑augmented generation systems to provide complete evidence for a helpful response, distinguishing it from response failure. It introduces the CITECONTROL benchmark to study how relational complexity between response and evidence affects citation quality, and proposes the CITENTION framework that combines generative, attention‑based, and retrieval‑based methods to efficiently mitigate citation failure, showing strong improvements both on the benchmark and in transfer settings.<br /><strong>Summary (CN):</strong> 本文 "citation failure" 定义为大型语言模型检索增强生成系统在给出有用回复时未能提供完整证据的现象，并将其与响应失败区分开来。作者构建了 CITECONTROL 基准，系统研究响应与证据之间关系的复杂性如何影响引用质量；随后提出 CITENTION 框架，融合生成式、注意力驱动和检索式方法，以高效缓解引用失效，并在基准及迁移实验中实现了显著改进。<br /><strong>Keywords:</strong> citation failure, LLM, RAG, benchmark, CITECONTROL, CITENTION, evidence completeness, retrieval-augmented generation, mitigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Jan Buchmann, Iryna Gurevych</div>
Citations from LLM-based RAG systems are supposed to simplify response verification. However, this does not hold for citation failure, when a model generates a helpful response, but fails to cite complete evidence. In contrast to previous work, we propose to disentangle this from response failure, where the response itself is flawed, and citing complete evidence is impossible. To address citation failure, this work follows a two-step approach: (1) We study when citation failure occurs and (2) how it can be mitigated. For step 1, we extend prior work by investigating how the relation between response and evidence affects citation quality. We introduce CITECONTROL, a benchmark that systematically varies this relation to analyze failure modes. Experiments show that failures increase with relational complexity and suggest that combining citation methods could improve performance, motivating step 2. To improve LLM citation efficiently, we propose CITENTION, a framework integrating generative, attention-based, and retrieval-based methods. Results demonstrate substantial citation improvements on CITECONTROL and in transfer settings. We make our data and code publicly available.
<div><strong>Authors:</strong> Jan Buchmann, Iryna Gurevych</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines citation failure as the inability of LLM-based retrieval‑augmented generation systems to provide complete evidence for a helpful response, distinguishing it from response failure. It introduces the CITECONTROL benchmark to study how relational complexity between response and evidence affects citation quality, and proposes the CITENTION framework that combines generative, attention‑based, and retrieval‑based methods to efficiently mitigate citation failure, showing strong improvements both on the benchmark and in transfer settings.", "summary_cn": "本文 \"citation failure\" 定义为大型语言模型检索增强生成系统在给出有用回复时未能提供完整证据的现象，并将其与响应失败区分开来。作者构建了 CITECONTROL 基准，系统研究响应与证据之间关系的复杂性如何影响引用质量；随后提出 CITENTION 框架，融合生成式、注意力驱动和检索式方法，以高效缓解引用失效，并在基准及迁移实验中实现了显著改进。", "keywords": "citation failure, LLM, RAG, benchmark, CITECONTROL, CITENTION, evidence completeness, retrieval-augmented generation, mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Jan Buchmann", "Iryna Gurevych"]}
]]></acme>

<pubDate>2025-10-23T07:47:22+00:00</pubDate>
</item>
<item>
<title>Context-level Language Modeling by Learning Predictive Context Embeddings</title>
<link>https://papers.cool/arxiv/2510.20280</link>
<guid>https://papers.cool/arxiv/2510.20280</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes ContextLM, a framework that adds a next‑context prediction objective to standard next‑token language model pretraining, enabling models to learn predictive embeddings of multi‑token contexts while remaining compatible with autoregressive evaluation. Experiments with GPT‑2 and Pythia models up to 1.5 B parameters show consistent reductions in perplexity and gains on downstream tasks, attributed to better long‑range coherence and more efficient attention allocation. The authors analyze the impact of the objective and demonstrate its scalability with minimal computational overhead.<br /><strong>Summary (CN):</strong> 本文提出 ContextLM 框架，在标准的下一个词预测预训练中加入了“下一上下文预测”目标，使模型能够学习多词上下文的预测嵌入，同时保持对自回归评估（如困惑度）的兼容性。对 GPT‑2 和 Pythia 系列（最高 1.5 B 参数）进行的大规模实验表明，该目标能够持续降低困惑度并提升下游任务表现，原因在于更好的长程连贯性和更高效的注意力分配。作者进一步分析了该目标的作用机制，并展示了其在计算开销极小的情况下可扩展的优势。<br /><strong>Keywords:</strong> next-context prediction, predictive context embeddings, language modeling, GPT-2, Pythia, long-range coherence, attention efficiency, pretraining objective<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Beiya Dai, Yuliang Liu, Daozheng Xue, Qipeng Guo, Kai Chen, Xinbing Wang</div>
Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model's capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \textbf{ContextLM}, a framework that augments standard pretraining with an inherent \textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.
<div><strong>Authors:</strong> Beiya Dai, Yuliang Liu, Daozheng Xue, Qipeng Guo, Kai Chen, Xinbing Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes ContextLM, a framework that adds a next‑context prediction objective to standard next‑token language model pretraining, enabling models to learn predictive embeddings of multi‑token contexts while remaining compatible with autoregressive evaluation. Experiments with GPT‑2 and Pythia models up to 1.5 B parameters show consistent reductions in perplexity and gains on downstream tasks, attributed to better long‑range coherence and more efficient attention allocation. The authors analyze the impact of the objective and demonstrate its scalability with minimal computational overhead.", "summary_cn": "本文提出 ContextLM 框架，在标准的下一个词预测预训练中加入了“下一上下文预测”目标，使模型能够学习多词上下文的预测嵌入，同时保持对自回归评估（如困惑度）的兼容性。对 GPT‑2 和 Pythia 系列（最高 1.5 B 参数）进行的大规模实验表明，该目标能够持续降低困惑度并提升下游任务表现，原因在于更好的长程连贯性和更高效的注意力分配。作者进一步分析了该目标的作用机制，并展示了其在计算开销极小的情况下可扩展的优势。", "keywords": "next-context prediction, predictive context embeddings, language modeling, GPT-2, Pythia, long-range coherence, attention efficiency, pretraining objective", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Beiya Dai", "Yuliang Liu", "Daozheng Xue", "Qipeng Guo", "Kai Chen", "Xinbing Wang"]}
]]></acme>

<pubDate>2025-10-23T07:09:45+00:00</pubDate>
</item>
<item>
<title>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders</title>
<link>https://papers.cool/arxiv/2510.20239</link>
<guid>https://papers.cool/arxiv/2510.20239</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a unified tri‑modal framework that fuses interview text, audio, and facial signals to predict graded severity levels for depression (PHQ‑8) and PTSD. Standardized sentence‑level transformer embeddings, log‑Mel audio features, and facial action‑unit descriptors are combined via a calibrated late‑fusion classifier, achieving superior weighted F1 and robustness compared to unimodal baselines. The approach also provides feature‑level attributions to support clinician decision‑making.<br /><strong>Summary (CN):</strong> 本文提出一种统一的三模态框架，融合访谈文本、音频和面部信号，以预测抑郁（PHQ‑8）和 PTSD 的分级严重程度。通过句子级 Transformer 嵌入、对数梅尔音频特征和面部动作单元描述，采用校准的后期融合分类器，实现了相较于单模态基线的更高加权 F1 与鲁棒性。该方法还能提供特征层面的归因，辅助临床决策。<br /><strong>Keywords:</strong> multimodal fusion, depression diagnosis, PTSD severity, affective analysis, transformer embeddings, log-Mel audio features, facial action units, late fusion classifier, clinical decision support<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao</div>
Depression and post traumatic stress disorder (PTSD) often co-occur with connected symptoms, complicating automated assessment, which is often binary and disorder specific. Clinically useful diagnosis needs severity aware cross disorder estimates and decision support explanations. Our unified tri modal affective severity framework synchronizes and fuses interview text with sentence level transformer embeddings, audio with log Mel statistics with deltas, and facial signals with action units, gaze, head and pose descriptors to output graded severities for diagnosing both depression (PHQ-8; 5 classes) and PTSD (3 classes). Standardized features are fused via a calibrated late fusion classifier, yielding per disorder probabilities and feature-level attributions. This severity aware tri-modal affective fusion approach is demoed on multi disorder concurrent depression and PTSD assessment. Stratified cross validation on DAIC derived corpora outperforms unimodal/ablation baselines. The fused model matches the strongest unimodal baseline on accuracy and weighted F1, while improving decision curve utility and robustness under noisy or missing modalities. For PTSD specifically, fusion reduces regression error and improves class concordance. Errors cluster between adjacent severities; extreme classes are identified reliably. Ablations show text contributes most to depression severity, audio and facial cues are critical for PTSD, whereas attributions align with linguistic and behavioral markers. Our approach offers reproducible evaluation and clinician in the loop support for affective clinical decision making.
<div><strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a unified tri‑modal framework that fuses interview text, audio, and facial signals to predict graded severity levels for depression (PHQ‑8) and PTSD. Standardized sentence‑level transformer embeddings, log‑Mel audio features, and facial action‑unit descriptors are combined via a calibrated late‑fusion classifier, achieving superior weighted F1 and robustness compared to unimodal baselines. The approach also provides feature‑level attributions to support clinician decision‑making.", "summary_cn": "本文提出一种统一的三模态框架，融合访谈文本、音频和面部信号，以预测抑郁（PHQ‑8）和 PTSD 的分级严重程度。通过句子级 Transformer 嵌入、对数梅尔音频特征和面部动作单元描述，采用校准的后期融合分类器，实现了相较于单模态基线的更高加权 F1 与鲁棒性。该方法还能提供特征层面的归因，辅助临床决策。", "keywords": "multimodal fusion, depression diagnosis, PTSD severity, affective analysis, transformer embeddings, log-Mel audio features, facial action units, late fusion classifier, clinical decision support", "scoring": {"interpretability": 4, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Filippo Cenacchi", "Deborah Richards", "Longbing Cao"]}
]]></acme>

<pubDate>2025-10-23T05:46:38+00:00</pubDate>
</item>
<item>
<title>Decoding-Free Sampling Strategies for LLM Marginalization</title>
<link>https://papers.cool/arxiv/2510.20208</link>
<guid>https://papers.cool/arxiv/2510.20208</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes decoding‑free sampling strategies for estimating the marginal probability of a text under large language models, avoiding costly generation by relying on cheap, model‑agnostic tokenization sampling. Experiments on open models show that these methods achieve accurate marginal estimates with a fraction of the runtime, and the authors demonstrate their utility on downstream inference tasks.<br /><strong>Summary (CN):</strong> 本文提出了无需解码的抽样策略，用于在大语言模型下估计文本的边际概率，避免了昂贵的生成步骤，完全基于轻量、与模型和分词器无关的抽样方法。实验表明，这些方法在开放模型上能够以极低的运行时间成本获得足够精确的边际估计，并在若干下游推理任务中展示了其应用价值。<br /><strong>Keywords:</strong> marginalization, tokenization, decoding-free sampling, language model evaluation, subword tokenization, probability estimation, inference efficiency, LLM evaluation, sampling strategies<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> David Pohl, Marco Cognetta, Junyoung Lee, Naoaki Okazaki</div>
Modern language models operate on subword-tokenized text in order to make a trade-off between model size, inference speed, and vocabulary coverage. A side effect of this is that, during inference, models are evaluated by measuring the probability of only the specific tokenization produced as the output, despite there being many possible ways to represent the same text with a subword vocabulary. Recent studies have argued instead for evaluating LLMs by marginalization - the probability mass of all tokenizations of a given text. Marginalization is difficult due to the number of possible tokenizations of a text, so often approximate marginalization is done via sampling. However, a downside of sampling is that an expensive generation step must be performed by the LLM for each sample, which limits the number of samples that can be acquired given a runtime budget, and therefore also the accuracy of the approximation. Since computing the probability of a sequence given the tokenization is relatively cheap compared to actually generating it, we investigate sampling strategies that are decoding-free - they require no generation from the LLM, instead relying entirely on extremely cheap sampling strategies that are model and tokenizer agnostic. We investigate the approximation quality and speed of decoding-free sampling strategies for a number of open models to find that they provide sufficiently accurate marginal estimates at a small fraction of the runtime cost and demonstrate its use on a set of downstream inference tasks.
<div><strong>Authors:</strong> David Pohl, Marco Cognetta, Junyoung Lee, Naoaki Okazaki</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes decoding‑free sampling strategies for estimating the marginal probability of a text under large language models, avoiding costly generation by relying on cheap, model‑agnostic tokenization sampling. Experiments on open models show that these methods achieve accurate marginal estimates with a fraction of the runtime, and the authors demonstrate their utility on downstream inference tasks.", "summary_cn": "本文提出了无需解码的抽样策略，用于在大语言模型下估计文本的边际概率，避免了昂贵的生成步骤，完全基于轻量、与模型和分词器无关的抽样方法。实验表明，这些方法在开放模型上能够以极低的运行时间成本获得足够精确的边际估计，并在若干下游推理任务中展示了其应用价值。", "keywords": "marginalization, tokenization, decoding-free sampling, language model evaluation, subword tokenization, probability estimation, inference efficiency, LLM evaluation, sampling strategies", "scoring": {"interpretability": 4, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["David Pohl", "Marco Cognetta", "Junyoung Lee", "Naoaki Okazaki"]}
]]></acme>

<pubDate>2025-10-23T04:50:14+00:00</pubDate>
</item>
<item>
<title>Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.20198</link>
<guid>https://papers.cool/arxiv/2510.20198</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a suite of five grid‑based tasks to probe large language models' ability to perform spatial reasoning from textual descriptions, covering quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Results show that while models achieve moderate accuracy on small grids, performance degrades sharply as grid size and task complexity increase, with average accuracy drops of around 43% and up to 84% loss. The authors argue that this scaling failure reveals a lack of robust spatial representations in current LLM architectures and propose the benchmark as a tool for future research at the intersection of language and geometry.<br /><strong>Summary (CN):</strong> 本文提出了五个基于网格的任务，用于评估大语言模型在文本描述下的空间推理能力，任务包括象限识别、几何变换、距离评估、单词搜索和滑块拼图。实验发现，模型在小规模网格上能够取得中等准确率，但随着网格尺寸和任务复杂度的提升，性能急剧下降，平均准确率下降约 43%，最高下降达 84%。作者认为，这种尺度退化表明现有 LLM 缺乏稳健的空间表征，并将该基准视为语言与几何交叉研究的未来方向。<br /><strong>Keywords:</strong> spatial reasoning, large language models, grid-based tasks, quadrant identification, geometric transformations, distance evaluation, word search, tile sliding, benchmark<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 2, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
This paper explores the spatial reasoning capability of large language models (LLMs) over textual input through a suite of five tasks aimed at probing their spatial understanding and computational abilities. The models were tested on both fundamental spatial reasoning and multi-step problem-solving within structured grid-based environments using tasks such as quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Each task was scaled in complexity through increasing grid dimensions, requiring models to extend beyond simple pattern recognition into abstract spatial reasoning. Our results reveal that while LLMs demonstrate moderate success in all tasks with small complexity and size, performance drops off rapidly as scale increases, with an average loss in accuracy of 42.7%, and reaching as high as 84%. Every test that began with over 50% accuracy showed a loss of at least 48%, illustrating the consistent nature of the deterioration. Furthermore, their struggles with scaling complexity hint at a lack of robust spatial representations in their underlying architectures. This paper underscores the gap between linguistic and spatial reasoning in LLMs, offering insights into their current limitations, and laying the groundwork for future integrative benchmarks at the intersection of language and geometry.
<div><strong>Authors:</strong> Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a suite of five grid‑based tasks to probe large language models' ability to perform spatial reasoning from textual descriptions, covering quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Results show that while models achieve moderate accuracy on small grids, performance degrades sharply as grid size and task complexity increase, with average accuracy drops of around 43% and up to 84% loss. The authors argue that this scaling failure reveals a lack of robust spatial representations in current LLM architectures and propose the benchmark as a tool for future research at the intersection of language and geometry.", "summary_cn": "本文提出了五个基于网格的任务，用于评估大语言模型在文本描述下的空间推理能力，任务包括象限识别、几何变换、距离评估、单词搜索和滑块拼图。实验发现，模型在小规模网格上能够取得中等准确率，但随着网格尺寸和任务复杂度的提升，性能急剧下降，平均准确率下降约 43%，最高下降达 84%。作者认为，这种尺度退化表明现有 LLM 缺乏稳健的空间表征，并将该基准视为语言与几何交叉研究的未来方向。", "keywords": "spatial reasoning, large language models, grid-based tasks, quadrant identification, geometric transformations, distance evaluation, word search, tile sliding, benchmark", "scoring": {"interpretability": 3, "understanding": 7, "safety": 2, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"]}
]]></acme>

<pubDate>2025-10-23T04:32:46+00:00</pubDate>
</item>
<item>
<title>Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</title>
<link>https://papers.cool/arxiv/2510.20176</link>
<guid>https://papers.cool/arxiv/2510.20176</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Mixture-of-Minds introduces a multi-agent framework that splits table reasoning into three specialized roles—planning, coding, and answering—combined with code execution for precise manipulation. The authors train the agents using a self-improvement loop that generates pseudo‑gold trajectories via Monte Carlo Tree Search rollouts and applies reinforcement learning, achieving a 62.13% accuracy on TableBench and outperforming strong baselines.<br /><strong>Summary (CN):</strong> Mixture-of-Minds 提出了一种多代理框架，将表格推理分解为规划、编码和回答三个专门角色，并利用代码执行实现精准的表格操作。作者通过蒙特卡罗树搜索产生伪金轨迹并使用强化学习进行自我提升训练，使模型在 TableBench 上达到 62.13% 的准确率，超了强基线模型。<br /><strong>Keywords:</strong> table understanding, multi-agent reinforcement learning, Mixture-of-Minds, Monte Carlo Tree Search, code generation, table reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei wang, Jiayi Liu, Fei Liu, Serena Li, Weiwi Li, Mingze Gao, Abhishek Kumar, Xiangjun Fan, Zhuokai Zhao, Lizhu Zhang</div>
Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.
<div><strong>Authors:</strong> Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei wang, Jiayi Liu, Fei Liu, Serena Li, Weiwi Li, Mingze Gao, Abhishek Kumar, Xiangjun Fan, Zhuokai Zhao, Lizhu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Mixture-of-Minds introduces a multi-agent framework that splits table reasoning into three specialized roles—planning, coding, and answering—combined with code execution for precise manipulation. The authors train the agents using a self-improvement loop that generates pseudo‑gold trajectories via Monte Carlo Tree Search rollouts and applies reinforcement learning, achieving a 62.13% accuracy on TableBench and outperforming strong baselines.", "summary_cn": "Mixture-of-Minds 提出了一种多代理框架，将表格推理分解为规划、编码和回答三个专门角色，并利用代码执行实现精准的表格操作。作者通过蒙特卡罗树搜索产生伪金轨迹并使用强化学习进行自我提升训练，使模型在 TableBench 上达到 62.13% 的准确率，超了强基线模型。", "keywords": "table understanding, multi-agent reinforcement learning, Mixture-of-Minds, Monte Carlo Tree Search, code generation, table reasoning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhang Zhou", "Mingrui Zhang", "Ke Li", "Mingyi Wang", "Qiao Liu", "Qifei wang", "Jiayi Liu", "Fei Liu", "Serena Li", "Weiwi Li", "Mingze Gao", "Abhishek Kumar", "Xiangjun Fan", "Zhuokai Zhao", "Lizhu Zhang"]}
]]></acme>

<pubDate>2025-10-23T03:51:17+00:00</pubDate>
</item>
<item>
<title>DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking</title>
<link>https://papers.cool/arxiv/2510.20168</link>
<guid>https://papers.cool/arxiv/2510.20168</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeepWideSearch, a new benchmark that tests agents’ ability to combine deep multi‑hop reasoning with wide‑scale information collection. comprises 220 questions across 15 domains and shows that current state‑of‑the‑art agents succeed on only about 2.4% of them, revealing four primary failure modes (lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow). The authors release the dataset to spur research on more capable and robust information‑seeking agents.<br /><strong>Summary (CN):</strong> 本文推出了 DeepWideSearch 基准，用于评估智能体在进行深度多跳推理的同时进行大规模信息收集的能力。数据集包含 15 个领域的 220 条问题，实验表明当前最先进的智能体仅在约 2.4% 的问题上成功，揭示了四类主要失败模式：缺乏自我反思、过度依赖内部知识、检索不足以及上下文溢出。作者公开发布该数据集，以促进对更强大、更稳健的信息检索智能体的研究。<br /><strong>Keywords:</strong> information seeking, depth-width benchmark, retrieval agents, multi-hop reasoning, agentic AI, evaluation dataset, market analysis, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang</div>
Current search agents fundamentally lack the ability to simultaneously perform \textit{deep} reasoning over multi-hop retrieval and \textit{wide}-scale information collection-a critical deficiency for real-world applications like comprehensive market analysis and business development. To bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly designed to evaluate agents to integrate depth and width in information seeking. In DeepWideSearch, agents must process a large volume of data, each requiring deep reasoning over multi-hop retrieval paths. Specifically, we propose two methods to converse established datasets, resulting in a curated collection of 220 questions spanning 15 diverse domains. Extensive experiments demonstrate that even state-of-the-art agents achieve only 2.39% average success rate on DeepWideSearch, highlighting the substantial challenge of integrating depth and width search in information-seeking tasks. Furthermore, our error analysis reveals four failure modes: lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow-exposing key limitations in current agent architectures. We publicly release DeepWideSearch to catalyze future research on more capable and robust information-seeking agents.
<div><strong>Authors:</strong> Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeepWideSearch, a new benchmark that tests agents’ ability to combine deep multi‑hop reasoning with wide‑scale information collection. comprises 220 questions across 15 domains and shows that current state‑of‑the‑art agents succeed on only about 2.4% of them, revealing four primary failure modes (lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow). The authors release the dataset to spur research on more capable and robust information‑seeking agents.", "summary_cn": "本文推出了 DeepWideSearch 基准，用于评估智能体在进行深度多跳推理的同时进行大规模信息收集的能力。数据集包含 15 个领域的 220 条问题，实验表明当前最先进的智能体仅在约 2.4% 的问题上成功，揭示了四类主要失败模式：缺乏自我反思、过度依赖内部知识、检索不足以及上下文溢出。作者公开发布该数据集，以促进对更强大、更稳健的信息检索智能体的研究。", "keywords": "information seeking, depth-width benchmark, retrieval agents, multi-hop reasoning, agentic AI, evaluation dataset, market analysis, robustness", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Tian Lan", "Bin Zhu", "Qianghuai Jia", "Junyang Ren", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"]}
]]></acme>

<pubDate>2025-10-23T03:28:45+00:00</pubDate>
</item>
<item>
<title>Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?</title>
<link>https://papers.cool/arxiv/2510.20154</link>
<guid>https://papers.cool/arxiv/2510.20154</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how Large Language Models inherit stereotypes that affect zero-shot stance detection, focusing on attributes such as dialect and text complexity. By automatically annotating existing stance detection datasets, the authors reveal systematic biases, e.g., associating pro‑marijuana stances with low readability and African American dialect with opposition to Donald Trump. These findings highlight overlooked bias risks in politically sensitive NLP tasks.<br /><strong>Summary (CN):</strong> 本文研究了大型语言模型在零样本立场检测中继承的刻板印象，重点考察方言和文本复杂度等属性。通过对已有立场检测数据集进行自动标注，作者发现系统性偏见，例如将支持大麻的观点与低可读性文本关联，以及将非洲裔美国人方言与反对唐纳德·特朗普联系起来。研究揭示了政治敏感 NLP 任务中被忽视的偏见风险。<br /><strong>Keywords:</strong> large language models, bias, stereotypes, zero-shot stance detection, dialect, text complexity, political bias, fairness, NLP, bias evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Anthony Dubreuil, Antoine Gourru, Christine Largeron, Amine Trabelsi</div>
Large Language Models inherit stereotypes from their pretraining data, leading to biased behavior toward certain social groups in many Natural Language Processing tasks, such as hateful speech detection or sentiment analysis. Surprisingly, the evaluation of this kind of bias in stance detection methods has been largely overlooked by the community. Stance Detection involves labeling a statement as being against, in favor, or neutral towards a specific target and is among the most sensitive NLP tasks, as it often relates to political leanings. In this paper, we focus on the bias of Large Language Models when performing stance detection in a zero-shot setting. We automatically annotate posts in pre-existing stance detection datasets with two attributes: dialect or vernacular of a specific group and text complexity/readability, to investigate whether these attributes influence the model's stance detection decisions. Our results show that LLMs exhibit significant stereotypes in stance detection tasks, such as incorrectly associating pro-marijuana views with low text complexity and African American dialect with opposition to Donald Trump.
<div><strong>Authors:</strong> Anthony Dubreuil, Antoine Gourru, Christine Largeron, Amine Trabelsi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how Large Language Models inherit stereotypes that affect zero-shot stance detection, focusing on attributes such as dialect and text complexity. By automatically annotating existing stance detection datasets, the authors reveal systematic biases, e.g., associating pro‑marijuana stances with low readability and African American dialect with opposition to Donald Trump. These findings highlight overlooked bias risks in politically sensitive NLP tasks.", "summary_cn": "本文研究了大型语言模型在零样本立场检测中继承的刻板印象，重点考察方言和文本复杂度等属性。通过对已有立场检测数据集进行自动标注，作者发现系统性偏见，例如将支持大麻的观点与低可读性文本关联，以及将非洲裔美国人方言与反对唐纳德·特朗普联系起来。研究揭示了政治敏感 NLP 任务中被忽视的偏见风险。", "keywords": "large language models, bias, stereotypes, zero-shot stance detection, dialect, text complexity, political bias, fairness, NLP, bias evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Anthony Dubreuil", "Antoine Gourru", "Christine Largeron", "Amine Trabelsi"]}
]]></acme>

<pubDate>2025-10-23T03:05:25+00:00</pubDate>
</item>
<item>
<title>BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation</title>
<link>https://papers.cool/arxiv/2510.20151</link>
<guid>https://papers.cool/arxiv/2510.20151</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> BoundRL proposes a reinforcement‑learning based method that generates only the starting tokens of segments in long, structured texts and reconstructs full segment contents from the original document, dramatically reducing inference cost and hallucination. The approach employs a verifiable reward (RLVR) that balances reconstruction fidelity and semantic alignment, and mitigates entropy collapse by creating intermediate candidate sequences through systematic perturbations. Experiments on complex LLM prompts show that small models (1.7B) using BoundRL outperform few‑shot prompting of much larger models, with RLVR and intermediate candidates further boosting performance and generalisation.<br /><strong>Summary (CN):</strong> BoundRL 提出一种基于强化学习的文本分段方法，仅生成结构化长文本中每个段落的起始 token，并通过在原文中定位这些 token 来重构完整内容，从而大幅降低推理成本并减少幻觉。该方法使用可验证的奖励函数（RLVR）同时优化文档重建精度和语义对齐，并通过系统性扰动生成中间候选序列以缓解熵坍塌。实验在用于 LLM 应用的复杂提示上表明，使用 BoundRL 的小模型（1.7B 参数）能够超越更大模型的 few‑shot 提示，RLVR 和中间候选的加入进一步提升了性能和泛化能力。<br /><strong>Keywords:</strong> structured text segmentation, reinforcement learning, boundary generation, token-level segmentation, RLVR reward, intermediate candidate perturbation, LLM prompts, small language models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoyuan Li, Zhengyuan Shen, Sullam Jeoung, Yueyan Chen, Jiayu Li, Qi Zhu, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala</div>
As structured texts become increasingly complex across diverse domains -- from technical reports to generative AI prompts -- the need for text segmentation into semantically meaningful components becomes critical. Such texts often contain elements beyond plain language, including tables, code snippets, and placeholders, which conventional sentence- or paragraph-level segmentation methods cannot handle effectively. To address this challenge, we propose BoundRL, a novel and efficient approach that jointly performs token-level text segmentation and label prediction for long structured texts. Instead of generating complete contents for each segment, it generates only a sequence of starting tokens and reconstructs the complete contents by locating these tokens within the original texts, thereby reducing inference costs by orders of magnitude and minimizing hallucination. To adapt the model for the output format, BoundRL~performs reinforcement learning with verifiable rewards (RLVR) with a specifically designed reward that jointly optimizes document reconstruction fidelity and semantic alignment. To mitigate entropy collapse, it further constructs intermediate candidates by systematically perturbing a fraction of generated sequences of segments to create stepping stones toward higher-quality solutions. To demonstrate BoundRL's effectiveness on particularly challenging structured texts, we focus evaluation on complex prompts used for LLM applications. Experiments show that BoundRL enables small language models (1.7B parameters) to outperform few-shot prompting of much larger models. Moreover, RLVR with our designed reward yields significant improvements over supervised fine-tuning, and incorporating intermediate candidates further improves both performance and generalization.
<div><strong>Authors:</strong> Haoyuan Li, Zhengyuan Shen, Sullam Jeoung, Yueyan Chen, Jiayu Li, Qi Zhu, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "BoundRL proposes a reinforcement‑learning based method that generates only the starting tokens of segments in long, structured texts and reconstructs full segment contents from the original document, dramatically reducing inference cost and hallucination. The approach employs a verifiable reward (RLVR) that balances reconstruction fidelity and semantic alignment, and mitigates entropy collapse by creating intermediate candidate sequences through systematic perturbations. Experiments on complex LLM prompts show that small models (1.7B) using BoundRL outperform few‑shot prompting of much larger models, with RLVR and intermediate candidates further boosting performance and generalisation.", "summary_cn": "BoundRL 提出一种基于强化学习的文本分段方法，仅生成结构化长文本中每个段落的起始 token，并通过在原文中定位这些 token 来重构完整内容，从而大幅降低推理成本并减少幻觉。该方法使用可验证的奖励函数（RLVR）同时优化文档重建精度和语义对齐，并通过系统性扰动生成中间候选序列以缓解熵坍塌。实验在用于 LLM 应用的复杂提示上表明，使用 BoundRL 的小模型（1.7B 参数）能够超越更大模型的 few‑shot 提示，RLVR 和中间候选的加入进一步提升了性能和泛化能力。", "keywords": "structured text segmentation, reinforcement learning, boundary generation, token-level segmentation, RLVR reward, intermediate candidate perturbation, LLM prompts, small language models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyuan Li", "Zhengyuan Shen", "Sullam Jeoung", "Yueyan Chen", "Jiayu Li", "Qi Zhu", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"]}
]]></acme>

<pubDate>2025-10-23T02:56:10+00:00</pubDate>
</item>
<item>
<title>Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning</title>
<link>https://papers.cool/arxiv/2510.20098</link>
<guid>https://papers.cool/arxiv/2510.20098</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ARTER, an Adaptive Routing and Targeted Entity Reasoning pipeline that combines classic candidate generation with selective large language model (LLM) reasoning to improve entity linking. By classifying mentions into easy and hard cases using lightweight signals, ARTER routes easy cases to a fast linker (e.g., ReFinED) and hard cases to an LLM, achieving comparable accuracy to full‑LLM pipelines while reducing token usage by about half.<br /><strong>Summary (CN):</strong> 本文提出 ARTER（自适应路由与目标实体推理）流水线，将传统候选生成与选择性的 LLM 推理相结合，以提升实体链接效果。通过利用轻量信号将提及划分为易/难两类，易例交由快速链接器（如 ReFinED）处理，难例则使用 LLM 进行推理，从而在保持与全 LLM 流水线相当的准确率的同时，将 LLM 令牌消耗降低约一半。<br /><strong>Keywords:</strong> entity linking, large language models, adaptive routing, targeted reasoning, few-shot prompting, efficiency, ReFinED, candidate generation, contextual scoring<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</div>
Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.
<div><strong>Authors:</strong> Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ARTER, an Adaptive Routing and Targeted Entity Reasoning pipeline that combines classic candidate generation with selective large language model (LLM) reasoning to improve entity linking. By classifying mentions into easy and hard cases using lightweight signals, ARTER routes easy cases to a fast linker (e.g., ReFinED) and hard cases to an LLM, achieving comparable accuracy to full‑LLM pipelines while reducing token usage by about half.", "summary_cn": "本文提出 ARTER（自适应路由与目标实体推理）流水线，将传统候选生成与选择性的 LLM 推理相结合，以提升实体链接效果。通过利用轻量信号将提及划分为易/难两类，易例交由快速链接器（如 ReFinED）处理，难例则使用 LLM 进行推理，从而在保持与全 LLM 流水线相当的准确率的同时，将 LLM 令牌消耗降低约一半。", "keywords": "entity linking, large language models, adaptive routing, targeted reasoning, few-shot prompting, efficiency, ReFinED, candidate generation, contextual scoring", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yajie Li", "Albert Galimov", "Mitra Datta Ganapaneni", "Pujitha Thejaswi", "De Meng", "Priyanshu Kumar", "Saloni Potdar"]}
]]></acme>

<pubDate>2025-10-23T00:50:14+00:00</pubDate>
</item>
<item>
<title>CreativityPrism: A Holistic Benchmark for Large Language Model Creativity</title>
<link>https://papers.cool/arxiv/2510.20091</link>
<guid>https://papers.cool/arxiv/2510.20091</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> CreativityPrism is a comprehensive benchmark that assesses large language model creativity along three dimensions—quality, novelty, and diversity—across nine tasks spanning divergent thinking, creative writing, and logical reasoning. The authors evaluate 17 state-of-the-art proprietary and open-source LLMs, revealing performance gaps and varying correlations between metrics and domains, highlighting that strong results in one creativity dimension do not guarantee generalization to others.<br /><strong>Summary (CN):</strong> CreativityPrism 是一个全面的基准，用于在质量、创新性和多样性三个维度上评估大型语言模型的创造力，涵盖发散思维、创意写作和逻辑推理等九个任务。作者对 17 个最先进的专有和开源模型进行评估，揭示了性能差距以及不同指标和领域之间的相关性差异，强调在某一创造力维度上表现出色并不一定能推广到其他维度。<br /><strong>Keywords:</strong> creativity benchmark, large language models, quality novelty diversity, evaluation metrics, divergent thinking, creative writing, logical reasoning, model comparison, holistic evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhaoyi Joey Hou, Bowei Alvin Zhang, Yining Lu, Bhiman Kumar Baghel, Anneliese Brei, Ximing Lu, Meng Jiang, Faeze Brahman, Snigdha Chaturvedi, Haw-Shiuan Chang, Daniel Khashabi, Xiang Lorraine Li</div>
Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as producing creative text, there is still no holistic framework to evaluate their creativity across diverse scenarios. Existing evaluation methods remain fragmented, with dramatic variation across domains and tasks, largely due to differing definitions and measurements of creativity. Inspired by the hypothesis that creativity is not one fixed idea, we propose CreativityPrism, an evaluation analysis framework that decomposes creativity into three dimensions: quality, novelty, and diversity. CreativityPrism incorporates nine tasks, three domains, i.e., divergent thinking, creative writing, and logical reasoning, and twenty evaluation metrics, which measure each dimension in task-specific, unique ways. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CreativityPrism and analyze the performance correlations among different metrics and task domains. Our results reveal a notable gap between proprietary and open-source models. Overall, model performance tends to be highly correlated across tasks within the same domain and less so across different domains. Among evaluation dimensions, diversity and quality metrics show strong correlations - models that perform well on one often excel on the other - whereas novelty exhibits much weaker correlation with either. These findings support our hypothesis that strong performance in one creativity task or dimension does not necessarily generalize to others, underscoring the need for a holistic evaluation of LLM creativity.
<div><strong>Authors:</strong> Zhaoyi Joey Hou, Bowei Alvin Zhang, Yining Lu, Bhiman Kumar Baghel, Anneliese Brei, Ximing Lu, Meng Jiang, Faeze Brahman, Snigdha Chaturvedi, Haw-Shiuan Chang, Daniel Khashabi, Xiang Lorraine Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "CreativityPrism is a comprehensive benchmark that assesses large language model creativity along three dimensions—quality, novelty, and diversity—across nine tasks spanning divergent thinking, creative writing, and logical reasoning. The authors evaluate 17 state-of-the-art proprietary and open-source LLMs, revealing performance gaps and varying correlations between metrics and domains, highlighting that strong results in one creativity dimension do not guarantee generalization to others.", "summary_cn": "CreativityPrism 是一个全面的基准，用于在质量、创新性和多样性三个维度上评估大型语言模型的创造力，涵盖发散思维、创意写作和逻辑推理等九个任务。作者对 17 个最先进的专有和开源模型进行评估，揭示了性能差距以及不同指标和领域之间的相关性差异，强调在某一创造力维度上表现出色并不一定能推广到其他维度。", "keywords": "creativity benchmark, large language models, quality novelty diversity, evaluation metrics, divergent thinking, creative writing, logical reasoning, model comparison, holistic evaluation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhaoyi Joey Hou", "Bowei Alvin Zhang", "Yining Lu", "Bhiman Kumar Baghel", "Anneliese Brei", "Ximing Lu", "Meng Jiang", "Faeze Brahman", "Snigdha Chaturvedi", "Haw-Shiuan Chang", "Daniel Khashabi", "Xiang Lorraine Li"]}
]]></acme>

<pubDate>2025-10-23T00:22:10+00:00</pubDate>
</item>
<item>
<title>Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training</title>
<link>https://papers.cool/arxiv/2510.20059</link>
<guid>https://papers.cool/arxiv/2510.20059</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method to improve reasoning in a small Persian medical language model by using Reinforcement Learning with AI Feedback (RLAIF) and Direct Preference Optimization (DPO) on a translated multiple‑choice medical QA dataset, generating preferred and rejected chain‑of‑thought answer pairs. Despite training on a modest dataset, the resulting model surpasses a larger predecessor trained on 57 million tokens, demonstrating that reasoning‑focused fine‑tuning can yield strong domain‑specific performance.<br /><strong>Summary (CN):</strong> 本文提出利用强化学习（RLAIF）和直接偏好优化（DPO）在翻译后的波斯语医学问答数据集上生成首选与被拒的思考链（CoT）答案，对小型波斯语医学语言模型进行推理能力提升。尽管数据规模有限，训练后模型在波斯语医学问答上超过了使用约5700万标记进行训练的前代模型，展示了针对推理的微调在资源受限场景下的高效性。<br /><strong>Keywords:</strong> Persian language model, medical QA, RLAIF, DPO, chain-of-thought, reasoning, low-resource, fine-tuning, preference learning<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Mehrdad Ghassabi, Sadra Hakim, Hamidreza Baradaran Kashani, Pedram Rostami</div>
Enhancing reasoning capabilities in small language models is critical for specialized applications such as medical question answering, particularly in underrepresented languages like Persian. In this study, we employ Reinforcement Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to improve the reasoning skills of a general-purpose Persian language model. To achieve this, we translated a multiple-choice medical question-answering dataset into Persian and used RLAIF to generate rejected-preferred answer pairs, which are essential for DPO training. By prompting both teacher and student models to produce Chain-of-Thought (CoT) reasoning responses, we compiled a dataset containing correct and incorrect reasoning trajectories. This dataset, comprising 2 million tokens in preferred answers and 2.5 million tokens in rejected ones, was used to train a baseline model, significantly enhancing its medical reasoning capabilities in Persian. Remarkably, the resulting model outperformed its predecessor, gaokerena-V, which was trained on approximately 57 million tokens, despite leveraging a much smaller dataset. These results highlight the efficiency and effectiveness of reasoning-focused training approaches in developing domain-specific language models with limited data availability.
<div><strong>Authors:</strong> Mehrdad Ghassabi, Sadra Hakim, Hamidreza Baradaran Kashani, Pedram Rostami</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method to improve reasoning in a small Persian medical language model by using Reinforcement Learning with AI Feedback (RLAIF) and Direct Preference Optimization (DPO) on a translated multiple‑choice medical QA dataset, generating preferred and rejected chain‑of‑thought answer pairs. Despite training on a modest dataset, the resulting model surpasses a larger predecessor trained on 57 million tokens, demonstrating that reasoning‑focused fine‑tuning can yield strong domain‑specific performance.", "summary_cn": "本文提出利用强化学习（RLAIF）和直接偏好优化（DPO）在翻译后的波斯语医学问答数据集上生成首选与被拒的思考链（CoT）答案，对小型波斯语医学语言模型进行推理能力提升。尽管数据规模有限，训练后模型在波斯语医学问答上超过了使用约5700万标记进行训练的前代模型，展示了针对推理的微调在资源受限场景下的高效性。", "keywords": "Persian language model, medical QA, RLAIF, DPO, chain-of-thought, reasoning, low-resource, fine-tuning, preference learning", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Mehrdad Ghassabi", "Sadra Hakim", "Hamidreza Baradaran Kashani", "Pedram Rostami"]}
]]></acme>

<pubDate>2025-10-22T22:22:59+00:00</pubDate>
</item>
<item>
<title>From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge</title>
<link>https://papers.cool/arxiv/2510.20043</link>
<guid>https://papers.cool/arxiv/2510.20043</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Bengali Language Cultural Knowledge (BLanCK) dataset covering folk traditions, culinary arts, and regional dialects, and uses it to evaluate several multilingual large language models. Experiments reveal that while models perform well on non‑cultural categories, they struggle considerably with cultural knowledge, and providing contextual information dramatically improves results, highlighting the need for context-aware architectures and culturally curated training data.<br /><strong>Summary (CN):</strong> 本文提出了Bengali Language Cultural Knowledge（BLanCK）数据集，涵盖民俗、烹饪和方言等文化内容，并用于评估多语言大模型的表现。实验显示模型在非文化任务上表现良好，但在文化知识上显著不足，提供上下文信息可显著提升性能，强调了上下文感知结构和文化专属训练数据的重要性。<br /><strong>Keywords:</strong> Bengali cultural knowledge, low-resource languages, LLM evaluation, multilingual benchmarks, cultural knowledge dataset, context-aware models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nafis Chowdhury, Moinul Haque, Anika Ahmed, Nazia Tasnim, Md. Istiak Hossain Shihab, Sajjadur Rahman, Farig Sadeque</div>
Recent progress in NLP research has demonstrated remarkable capabilities of large language models (LLMs) across a wide range of tasks. While recent multilingual benchmarks have advanced cultural evaluation for LLMs, critical gaps remain in capturing the nuances of low-resource cultures. Our work addresses these limitations through a Bengali Language Cultural Knowledge (BLanCK) dataset including folk traditions, culinary arts, and regional dialects. Our investigation of several multilingual language models shows that while these models perform well in non-cultural categories, they struggle significantly with cultural knowledge and performance improves substantially across all models when context is provided, emphasizing context-aware architectures and culturally curated training data.
<div><strong>Authors:</strong> Nafis Chowdhury, Moinul Haque, Anika Ahmed, Nazia Tasnim, Md. Istiak Hossain Shihab, Sajjadur Rahman, Farig Sadeque</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Bengali Language Cultural Knowledge (BLanCK) dataset covering folk traditions, culinary arts, and regional dialects, and uses it to evaluate several multilingual large language models. Experiments reveal that while models perform well on non‑cultural categories, they struggle considerably with cultural knowledge, and providing contextual information dramatically improves results, highlighting the need for context-aware architectures and culturally curated training data.", "summary_cn": "本文提出了Bengali Language Cultural Knowledge（BLanCK）数据集，涵盖民俗、烹饪和方言等文化内容，并用于评估多语言大模型的表现。实验显示模型在非文化任务上表现良好，但在文化知识上显著不足，提供上下文信息可显著提升性能，强调了上下文感知结构和文化专属训练数据的重要性。", "keywords": "Bengali cultural knowledge, low-resource languages, LLM evaluation, multilingual benchmarks, cultural knowledge dataset, context-aware models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nafis Chowdhury", "Moinul Haque", "Anika Ahmed", "Nazia Tasnim", "Md. Istiak Hossain Shihab", "Sajjadur Rahman", "Farig Sadeque"]}
]]></acme>

<pubDate>2025-10-22T21:42:59+00:00</pubDate>
</item>
<item>
<title>ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering</title>
<link>https://papers.cool/arxiv/2510.20036</link>
<guid>https://papers.cool/arxiv/2510.20036</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ToolScope enhances LLM agent tool use by automatically merging redundant tools and applying a context‑aware retriever that selects the most relevant tools for a given query, allowing larger toolsets to fit within LLM context windows. Experiments on three state‑of‑the‑art LLMs and three open‑source tool‑use benchmarks show improvements of 8.38% to 38.6% in tool selection accuracy.<br /><strong>Summary (CN):</strong> ToolScope 通过自动合并冗余工具并使用上下文感知检索器，为每个查询挑选最相关的工具，从而在不超出 LLM 上下文限制的情况下提升工具使用效果。对三种主流 LLM 和三套开源工具使用基准的实验显示，工具选择准确率提升了 8.38% 到 .6%。<br /><strong>Keywords:</strong> LLM agents, tool merging, tool selection, context-aware retrieval, redundancy reduction, agent tool use<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 3, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Marianne Menglin Liu, Daniel Garcia, Fjona Parllaku, Vikas Upadhyay, Syed Fahad Allam Shah, Dan Roth</div>
Large language model (LLM) agents rely on external tools to solve complex tasks, but real-world toolsets often contain redundant tools with overlapping names and descriptions, introducing ambiguity and reducing selection accuracy. LLMs also face strict input context limits, preventing efficient consideration of large toolsets. To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.
<div><strong>Authors:</strong> Marianne Menglin Liu, Daniel Garcia, Fjona Parllaku, Vikas Upadhyay, Syed Fahad Allam Shah, Dan Roth</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ToolScope enhances LLM agent tool use by automatically merging redundant tools and applying a context‑aware retriever that selects the most relevant tools for a given query, allowing larger toolsets to fit within LLM context windows. Experiments on three state‑of‑the‑art LLMs and three open‑source tool‑use benchmarks show improvements of 8.38% to 38.6% in tool selection accuracy.", "summary_cn": "ToolScope 通过自动合并冗余工具并使用上下文感知检索器，为每个查询挑选最相关的工具，从而在不超出 LLM 上下文限制的情况下提升工具使用效果。对三种主流 LLM 和三套开源工具使用基准的实验显示，工具选择准确率提升了 8.38% 到 .6%。", "keywords": "LLM agents, tool merging, tool selection, context-aware retrieval, redundancy reduction, agent tool use", "scoring": {"interpretability": 2, "understanding": 4, "safety": 3, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"]}
]]></acme>

<pubDate>2025-10-22T21:29:27+00:00</pubDate>
</item>
<item>
<title>Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.20033</link>
<guid>https://papers.cool/arxiv/2510.20033</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The thesis proposes three methods to improve transfer learning for sequence labeling tasks using pre‑trained neural language models: (1) a multi‑task model that incorporates an extra domain‑independent signal for event trigger detection, (2) architectural modifications that enable bidirectional information flow across layers of autoregressive LLMs, and (3) a generative supervised in‑context fine‑tuning framework combined with response‑oriented adaptation. Experiments on domain transfer and autoregressive LLMs show that these targeted adaptations yield state‑of‑the‑art performance on sequence labeling benchmarks.<br /><strong>Summary (CN):</strong> 本文提出三种提升预训练神经语言模型在序列标注任务上迁移学习效果的方法：（1）在事件触发检测的领域转移中，引入来自域无关文本处理系统的额外信号，构建多任务模型；（2）对自回归大型模型的架构进行修改，使层间信息流可以双向传播；（3）采用生成式监督式上下文微调结合面向响应的适配策略，将自回归模型用于序列标注。实验表明，这些针对性的适配显著提升了序列标注任务的表现。<br /><strong>Keywords:</strong> transfer learning, sequence labeling, pre-trained language models, multi-task learning, domain adaptation, autoregressive LLM, in-context fine-tuning, response-oriented adaptation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> David Dukić</div>
This doctoral thesis improves the transfer learning for sequence labeling tasks by adapting pre-trained neural language models. The proposed improvements in transfer learning involve introducing a multi-task model that incorporates an additional signal, a method based on architectural modifications in autoregressive large language models, and a sequence labeling framework for autoregressive large language models utilizing supervised in-context fine-tuning combined with response-oriented adaptation strategies. The first improvement is given in the context of domain transfer for the event trigger detection task. The domain transfer of the event trigger detection task can be improved by incorporating an additional signal obtained from a domain-independent text processing system into a multi-task model. The second improvement involves modifying the model's architecture. For that purpose, a method is proposed to enable bidirectional information flow across layers of autoregressive large language models. The third improvement utilizes autoregressive large language models as text generators through a generative supervised in-context fine-tuning framework. The proposed model, method, and framework demonstrate that pre-trained neural language models achieve their best performance on sequence labeling tasks when adapted through targeted transfer learning paradigms.
<div><strong>Authors:</strong> David Dukić</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The thesis proposes three methods to improve transfer learning for sequence labeling tasks using pre‑trained neural language models: (1) a multi‑task model that incorporates an extra domain‑independent signal for event trigger detection, (2) architectural modifications that enable bidirectional information flow across layers of autoregressive LLMs, and (3) a generative supervised in‑context fine‑tuning framework combined with response‑oriented adaptation. Experiments on domain transfer and autoregressive LLMs show that these targeted adaptations yield state‑of‑the‑art performance on sequence labeling benchmarks.", "summary_cn": "本文提出三种提升预训练神经语言模型在序列标注任务上迁移学习效果的方法：（1）在事件触发检测的领域转移中，引入来自域无关文本处理系统的额外信号，构建多任务模型；（2）对自回归大型模型的架构进行修改，使层间信息流可以双向传播；（3）采用生成式监督式上下文微调结合面向响应的适配策略，将自回归模型用于序列标注。实验表明，这些针对性的适配显著提升了序列标注任务的表现。", "keywords": "transfer learning, sequence labeling, pre-trained language models, multi-task learning, domain adaptation, autoregressive LLM, in-context fine-tuning, response-oriented adaptation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["David Dukić"]}
]]></acme>

<pubDate>2025-10-22T21:23:53+00:00</pubDate>
</item>
<item>
<title>Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training</title>
<link>https://papers.cool/arxiv/2510.20002</link>
<guid>https://papers.cool/arxiv/2510.20002</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a family of Greek Embedding Models (GEMs) that are pre‑trained on large, quality‑filtered corpora covering both general‑domain and legal texts, and evaluates several modern transformer architectures such as ELECTRA, ConvBERT and ModernBERT for Greek. It also presents the first bilingual Greek‑English embedding models for the legal domain and shows that GEM‑RoBERTa and GEM‑ConvBERT outperform existing baselines on downstream tasks.<br /><strong>Summary (CN):</strong> 本文提出了一套希腊语嵌入模型（GEM），通过对通用和法律领域的大规模高质量语料进行严格过滤和预处理后进行预训练，并评估了包括 ELECTRA、ConvBERT 与 ModernBERT 在内的多种现代 Transformer 架构。文中还首次推出针对法律领域的双语希腊‑英语嵌入模型，并展示 GEM‑RoBERTa 与 GEM‑ConvBERT 在下游任务上显著优于现有基线。<br /><strong>Keywords:</strong> Greek NLP, corpus curation, transformer pretraining, ELECTRA, ConvBERT, ModernBERT, legal domain, bilingual Greek-English models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexandra Apostolopoulou, Konstantinos Kanaris, Athanasios Koursaris, Dimitris Tsakalidis, George Domalis, Ioannis E. Livieris</div>
The advancement of natural language processing for morphologically rich, moderately-resourced languages like Modern Greek is often hindered by a fragmented research landscape, a lack of architectural diversity and reliance on limited context-length models. This is particularly true in specialized, high-value domains such as law, where existing models are frequently confined to early transformer architectures with a restrictive 512-token window, insufficient for analyzing long legal documents. To address these challenges, this paper presents Greek Embedding Models, a new family of transformer models for Greek language built upon a foundation of extensive, quality-driven data curation. We detail the construction of several large-scale Greek corpora, emphasizing a rigorous, quality-based filtering and preprocessing methodology to create high-value training datasets from both general-domain and specialized legal sources. On this carefully curated foundation, we pre-train and systematically evaluate a diverse suite of modern architectures, which has not previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT. Furthermore, we propose the first bilingual Greek-English Embedding Models tailored for the legal domain. The extensive experiments on downstream tasks demonstrate that the new class of models establish the effectiveness of the proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models significantly outperform existing baselines.
<div><strong>Authors:</strong> Alexandra Apostolopoulou, Konstantinos Kanaris, Athanasios Koursaris, Dimitris Tsakalidis, George Domalis, Ioannis E. Livieris</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a family of Greek Embedding Models (GEMs) that are pre‑trained on large, quality‑filtered corpora covering both general‑domain and legal texts, and evaluates several modern transformer architectures such as ELECTRA, ConvBERT and ModernBERT for Greek. It also presents the first bilingual Greek‑English embedding models for the legal domain and shows that GEM‑RoBERTa and GEM‑ConvBERT outperform existing baselines on downstream tasks.", "summary_cn": "本文提出了一套希腊语嵌入模型（GEM），通过对通用和法律领域的大规模高质量语料进行严格过滤和预处理后进行预训练，并评估了包括 ELECTRA、ConvBERT 与 ModernBERT 在内的多种现代 Transformer 架构。文中还首次推出针对法律领域的双语希腊‑英语嵌入模型，并展示 GEM‑RoBERTa 与 GEM‑ConvBERT 在下游任务上显著优于现有基线。", "keywords": "Greek NLP, corpus curation, transformer pretraining, ELECTRA, ConvBERT, ModernBERT, legal domain, bilingual Greek-English models", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexandra Apostolopoulou", "Konstantinos Kanaris", "Athanasios Koursaris", "Dimitris Tsakalidis", "George Domalis", "Ioannis E. Livieris"]}
]]></acme>

<pubDate>2025-10-22T20:06:48+00:00</pubDate>
</item>
<item>
<title>Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs</title>
<link>https://papers.cool/arxiv/2510.20001</link>
<guid>https://papers.cool/arxiv/2510.20001</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unifying paradigm that characterizes clinical decision‑making tasks along two axes—clinical backgrounds and clinical questions—to better reflect real‑world complexity beyond simplified QA datasets like MedQA. It surveys existing medical benchmarks, reviews training‑time and test‑time methods for LLMs, expands evaluation metrics to include efficiency and explainability, and outlines open challenges for building clinically useful LLM systems.<br /><strong>Summary (CN):</strong> 本文提出一种统一范式，通过“临床背景”和“临床问题”两维度对临床决策任务进行表征，以弥补 MedQA 等简化问答数据在真实临床环境中的不足。作者梳理了现有医学数据集与基准，回顾了训练时与测试时的 LLM 方法，并将评估扩展至效率和可解释性等指标，最后指出了实现临床可用 LLM 的挑战。<br /><strong>Keywords:</strong> large language models, clinical decision making, medical evaluation, explainability, efficiency, dataset paradigm, LLM safety<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 5, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Yunpeng Xiao, Carl Yang, Mark Mai, Xiao Hu, Kai Shu</div>
Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.
<div><strong>Authors:</strong> Yunpeng Xiao, Carl Yang, Mark Mai, Xiao Hu, Kai Shu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unifying paradigm that characterizes clinical decision‑making tasks along two axes—clinical backgrounds and clinical questions—to better reflect real‑world complexity beyond simplified QA datasets like MedQA. It surveys existing medical benchmarks, reviews training‑time and test‑time methods for LLMs, expands evaluation metrics to include efficiency and explainability, and outlines open challenges for building clinically useful LLM systems.", "summary_cn": "本文提出一种统一范式，通过“临床背景”和“临床问题”两维度对临床决策任务进行表征，以弥补 MedQA 等简化问答数据在真实临床环境中的不足。作者梳理了现有医学数据集与基准，回顾了训练时与测试时的 LLM 方法，并将评估扩展至效率和可解释性等指标，最后指出了实现临床可用 LLM 的挑战。", "keywords": "large language models, clinical decision making, medical evaluation, explainability, efficiency, dataset paradigm, LLM safety", "scoring": {"interpretability": 4, "understanding": 5, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yunpeng Xiao", "Carl Yang", "Mark Mai", "Xiao Hu", "Kai Shu"]}
]]></acme>

<pubDate>2025-10-22T20:06:10+00:00</pubDate>
</item>
<item>
<title>A Fundamental Algorithm for Dependency Parsing (With Corrections)</title>
<link>https://papers.cool/arxiv/2510.19996</link>
<guid>https://papers.cool/arxiv/2510.19996</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fundamental algorithm for dependency parsing that processes sentences word by word, attaching each word as soon as possible, mirroring hypothesized properties of human brain parsing. The algorithm achieves a worst‑case time complexity of O(n^3), but the worst case is rare in typical human language sentences. Experimental or theoretical analysis suggests the method is efficient for realistic language inputs.<br /><strong>Summary (CN):</strong> 本文提出了一种基础的依存句法分析算法，该算法逐词处理句子，在可能时立即附加每个词，模拟人脑句法解析的假设特性。算法的最坏情况时间复杂度为 O(n^3)，但在实际的人类语言中，这种最坏情况极少出现，因而在常规语言输入下表现高效。<br /><strong>Keywords:</strong> dependency parsing, transition-based parsing, O(n^3) algorithm, natural language processing, parsing efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 6, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Michael A. Covington</div>
This paper presents a fundamental algorithm for parsing natural language sentences into dependency trees. Unlike phrase-structure (constituency) parsers, this algorithm operates one word at a time, attaching each word as soon as it can be attached, corresponding to properties claimed for the parser in the human brain. Like phrase-structure parsing, its worst-case complexity is $O(n^3)$, but in human language, the worst case occurs only for small $n$.
<div><strong>Authors:</strong> Michael A. Covington</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fundamental algorithm for dependency parsing that processes sentences word by word, attaching each word as soon as possible, mirroring hypothesized properties of human brain parsing. The algorithm achieves a worst‑case time complexity of O(n^3), but the worst case is rare in typical human language sentences. Experimental or theoretical analysis suggests the method is efficient for realistic language inputs.", "summary_cn": "本文提出了一种基础的依存句法分析算法，该算法逐词处理句子，在可能时立即附加每个词，模拟人脑句法解析的假设特性。算法的最坏情况时间复杂度为 O(n^3)，但在实际的人类语言中，这种最坏情况极少出现，因而在常规语言输入下表现高效。", "keywords": "dependency parsing, transition-based parsing, O(n^3) algorithm, natural language processing, parsing efficiency", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 6, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Michael A. Covington"]}
]]></acme>

<pubDate>2025-10-22T19:48:38+00:00</pubDate>
</item>
<item>
<title>LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation</title>
<link>https://papers.cool/arxiv/2510.19988</link>
<guid>https://papers.cool/arxiv/2510.19988</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a hybrid architecture that combines large language models (LLMs) for text simplification and gap‑filling with a symbolic natural‑language‑understanding (NLU) pipeline that generates structured relational representations of quantities and causal laws. Experiments on commonsense science texts show that the hybrid system substantially outperforms a symbolic‑only baseline in extracting and interpreting continuous causal statements. The approach aims to retain the interpretability and reasoning benefits of symbolic NLU while leveraging the broad coverage of LLMs.<br /><strong>Summary (CN):</strong> 本文提出一种混合架构，将大型语言模型（LLM）用于文本重述和填补知识空白，并结合符号化自然语言理解（NLU）管线生成量化和因果律的结构化关系表示。 在常识科学文本上的实验表明，该混合系统在提取和解释连续因果陈述方面显著优于仅符号化的基线。 该方法旨在保持符号化 NLU 的可解释性和推理优势，同时利用 LLM 的广泛覆盖能力。<br /><strong>Keywords:</strong> LLM, symbolic NLU, hybrid system, causal statement interpretation, relational representation, commonsense science, knowledge extraction<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 6, Safety: 4, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Xin Lian, Kenneth D. Forbus</div>
Despite the broad applicability of large language models (LLMs), their reliance on probabilistic inference makes them vulnerable to errors such as hallucination in generated facts and inconsistent output structure in natural language understanding (NLU) tasks. By contrast, symbolic NLU systems provide interpretable understanding grounded in curated lexicons, semantic resources, and syntactic & semantic interpretation rules. They produce relational representations that can be used for accurate reasoning and planning, as well as incremental debuggable learning. However, symbolic NLU systems tend to be more limited in coverage than LLMs and require scarce knowledge representation and linguistics skills to extend and maintain. This paper explores a hybrid approach that integrates the broad-coverage language processing of LLMs with the symbolic NLU capabilities of producing structured relational representations to hopefully get the best of both approaches. We use LLMs for rephrasing and text simplification, to provide broad coverage, and as a source of information to fill in knowledge gaps more automatically. We use symbolic NLU to produce representations that can be used for reasoning and for incremental learning. We evaluate this approach on the task of extracting and interpreting quantities and causal laws from commonsense science texts, along with symbolic- and LLM-only pipelines. Our results suggest that our hybrid method works significantly better than the symbolic-only pipeline.
<div><strong>Authors:</strong> Xin Lian, Kenneth D. Forbus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a hybrid architecture that combines large language models (LLMs) for text simplification and gap‑filling with a symbolic natural‑language‑understanding (NLU) pipeline that generates structured relational representations of quantities and causal laws. Experiments on commonsense science texts show that the hybrid system substantially outperforms a symbolic‑only baseline in extracting and interpreting continuous causal statements. The approach aims to retain the interpretability and reasoning benefits of symbolic NLU while leveraging the broad coverage of LLMs.", "summary_cn": "本文提出一种混合架构，将大型语言模型（LLM）用于文本重述和填补知识空白，并结合符号化自然语言理解（NLU）管线生成量化和因果律的结构化关系表示。 在常识科学文本上的实验表明，该混合系统在提取和解释连续因果陈述方面显著优于仅符号化的基线。 该方法旨在保持符号化 NLU 的可解释性和推理优势，同时利用 LLM 的广泛覆盖能力。", "keywords": "LLM, symbolic NLU, hybrid system, causal statement interpretation, relational representation, commonsense science, knowledge extraction", "scoring": {"interpretability": 6, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Xin Lian", "Kenneth D. Forbus"]}
]]></acme>

<pubDate>2025-10-22T19:38:20+00:00</pubDate>
</item>
<item>
<title>LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation</title>
<link>https://papers.cool/arxiv/2510.19967</link>
<guid>https://papers.cool/arxiv/2510.19967</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> LyriCAR is a fully unsupervised framework that uses difficulty-aware curriculum reinforcement learning to enable controllable lyric translation, addressing cross-line coherence and global rhyme. The adaptive curriculum strategy allocates training resources efficiently, speeding up convergence and achieving state-of-the-art performance on English‑Chinese lyric translation across standard and multi-dimensional reward metrics. Experiments show that LyriCAR reduces training steps by about 40% while maintaining superior translation quality.<br /><strong>Summary (CN):</strong> LyriCAR 是一个完全无监督的框架，利用难度感知的 curriculum 强化学习实现可控的歌词翻译，解决跨行连贯性和全局押韵等问题。自适应课程策略有效分配训练资源，加速收敛，并在英‑中歌词翻译任务上在标准翻译指标和多维奖励分数上达到最新水平。实验表明，LyriCAR 将训练步数减少约 40%，同时保持更优的翻译质量。<br /><strong>Keywords:</strong> lyric translation, curriculum reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, multi-dimensional rewards<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.
<div><strong>Authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "LyriCAR is a fully unsupervised framework that uses difficulty-aware curriculum reinforcement learning to enable controllable lyric translation, addressing cross-line coherence and global rhyme. The adaptive curriculum strategy allocates training resources efficiently, speeding up convergence and achieving state-of-the-art performance on English‑Chinese lyric translation across standard and multi-dimensional reward metrics. Experiments show that LyriCAR reduces training steps by about 40% while maintaining superior translation quality.", "summary_cn": "LyriCAR 是一个完全无监督的框架，利用难度感知的 curriculum 强化学习实现可控的歌词翻译，解决跨行连贯性和全局押韵等问题。自适应课程策略有效分配训练资源，加速收敛，并在英‑中歌词翻译任务上在标准翻译指标和多维奖励分数上达到最新水平。实验表明，LyriCAR 将训练步数减少约 40%，同时保持更优的翻译质量。", "keywords": "lyric translation, curriculum reinforcement learning, difficulty-aware curriculum, controllable generation, unsupervised translation, multi-dimensional rewards", "scoring": {"interpretability": 3, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"]}
]]></acme>

<pubDate>2025-10-22T18:57:20+00:00</pubDate>
</item>
<item>
<title>Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</title>
<link>https://papers.cool/arxiv/2510.19897</link>
<guid>https://papers.cool/arxiv/2510.19897</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a memory‑augmented framework for large‑language‑model agents that learns classification tasks from labeled examples and model‑generated critiques without updating model parameters. By storing instance‑level critiques in episodic memory and distilling them into task‑level guidance in semantic memory, the approach achieves up to 24.8 % higher accuracy than retrieval‑only baselines, and the authors propose a metric called suggestibility to interpret how different memory strategies affect learning dynamics. Experiments reveal distinct behaviors between OpenAI and open‑source models on factual versus preference data, highlighting the potential of reflective, memory‑driven learning for more adaptable and interpretable agents.<br /><strong>Summary (CN):</strong> 本文提出一种记忆增强框架，使基于预训练大语言模型的代理能够在不更新参数的情况下，仅通过带标签示例和模型生成的批评来学习分类任务。框架将实例级批评存入情景记忆（episodic memory），并在语义记忆（semantic memory）中提炼为可复用的任务指导，从而在准确率上相较仅使用检索的基线提升最高 24.8%，作者还引入了“suggestibility”度量以解释不同记忆策略对学习动态的影响。实验展示了 OpenAI 与开源模型在处理事实型与偏好型数据时的行为差异，凸显记忆驱动的反思学习在构建更适应且可解释的 LLM 代理方面的前景。<br /><strong>Keywords:</strong> memory-augmented LLM, episodic memory, semantic memory, reflective learning, few-shot classification, suggestibility metric, interpretability, LLM critiques<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.
<div><strong>Authors:</strong> Jackson Hassell, Dan Zhang, Hannah Kim, Tom Mitchell, Estevam Hruschka</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a memory‑augmented framework for large‑language‑model agents that learns classification tasks from labeled examples and model‑generated critiques without updating model parameters. By storing instance‑level critiques in episodic memory and distilling them into task‑level guidance in semantic memory, the approach achieves up to 24.8 % higher accuracy than retrieval‑only baselines, and the authors propose a metric called suggestibility to interpret how different memory strategies affect learning dynamics. Experiments reveal distinct behaviors between OpenAI and open‑source models on factual versus preference data, highlighting the potential of reflective, memory‑driven learning for more adaptable and interpretable agents.", "summary_cn": "本文提出一种记忆增强框架，使基于预训练大语言模型的代理能够在不更新参数的情况下，仅通过带标签示例和模型生成的批评来学习分类任务。框架将实例级批评存入情景记忆（episodic memory），并在语义记忆（semantic memory）中提炼为可复用的任务指导，从而在准确率上相较仅使用检索的基线提升最高 24.8%，作者还引入了“suggestibility”度量以解释不同记忆策略对学习动态的影响。实验展示了 OpenAI 与开源模型在处理事实型与偏好型数据时的行为差异，凸显记忆驱动的反思学习在构建更适应且可解释的 LLM 代理方面的前景。", "keywords": "memory-augmented LLM, episodic memory, semantic memory, reflective learning, few-shot classification, suggestibility metric, interpretability, LLM critiques", "scoring": {"interpretability": 6, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"]}
]]></acme>

<pubDate>2025-10-22T17:58:03+00:00</pubDate>
</item>
<item>
<title>Large Language Model enabled Mathematical Modeling</title>
<link>https://papers.cool/arxiv/2510.19895</link>
<guid>https://papers.cool/arxiv/2510.19895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates the DeepSeek-R1 large language model for automatically generating mathematical optimization formulations across several operations‑research benchmarks, proposing a hallucination taxonomy and mitigation techniques such as LLM‑as‑a‑Judge, few‑shot prompting, tool calling, and a multi‑agent framework. Experiments on NL4OPT, IndustryOR, EasyLP and ComplexOR show that these strategies can reduce hallucinations and improve formulation accuracy, offering a cost‑effective alternative to more expensive models like GPT‑4.<br /><strong>Summary (CN):</strong> 本文评估了 DeepSeek‑R1 大语言模型在自动生成运筹学优化模型方面的表现，针对四个基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）提出了幻觉分类法并采用 LLM‑as‑a‑Judge、少样本提示、工具调用和多代理框架等减轻幻觉的策略。实验表明这些方法可以降低幻觉率并提升模型对实际需求的对齐度，提供了相较 GPT‑4 更具成本效益的方案。<br /><strong>Keywords:</strong> large language models, mathematical optimization, operations research, hallucination mitigation, DeepSeek-R1, NL4OPT, tool calling, few-shot learning, multi-agent framework<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Guoyun Zhang</div>
The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.
<div><strong>Authors:</strong> Guoyun Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates the DeepSeek-R1 large language model for automatically generating mathematical optimization formulations across several operations‑research benchmarks, proposing a hallucination taxonomy and mitigation techniques such as LLM‑as‑a‑Judge, few‑shot prompting, tool calling, and a multi‑agent framework. Experiments on NL4OPT, IndustryOR, EasyLP and ComplexOR show that these strategies can reduce hallucinations and improve formulation accuracy, offering a cost‑effective alternative to more expensive models like GPT‑4.", "summary_cn": "本文评估了 DeepSeek‑R1 大语言模型在自动生成运筹学优化模型方面的表现，针对四个基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）提出了幻觉分类法并采用 LLM‑as‑a‑Judge、少样本提示、工具调用和多代理框架等减轻幻觉的策略。实验表明这些方法可以降低幻觉率并提升模型对实际需求的对齐度，提供了相较 GPT‑4 更具成本效益的方案。", "keywords": "large language models, mathematical optimization, operations research, hallucination mitigation, DeepSeek-R1, NL4OPT, tool calling, few-shot learning, multi-agent framework", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Guoyun Zhang"]}
]]></acme>

<pubDate>2025-10-22T17:41:42+00:00</pubDate>
</item>
<item>
<title>Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</title>
<link>https://papers.cool/arxiv/2510.19892</link>
<guid>https://papers.cool/arxiv/2510.19892</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes using the fantasy card game Dixit as a game‑based evaluation framework for multimodal language models (MLMs), arguing that games require multiple abilities and provide objective, competitive assessment. Quantitative experiments with five MLMs show that Dixit win‑rate rankings correlate with standard benchmark rankings, and human‑MLM game play reveals strategic differences and areas for improvement in MLM reasoning.<br /><strong>Summary (CN):</strong> 本文提出将幻想卡牌游戏 Dixit 用作多模态语言模型（MLM）的游戏化评估框架，认为游戏需要多种能力且具备客观竞争的评估规则。对五种 MLM 的量化实验表明，Dixit 的胜率排序与常用基准高度相关，同时人机对战揭示了模型策略的差异并指出了 MLM 推理的改进方向。<br /><strong>Keywords:</strong> multimodal language models, game-based evaluation, Dixit, benchmark, quantitative experiments, human-MLM interaction, reasoning assessment, competitive games<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nishant Balepur, Dang Nguyen, Dayeon Ki</div>
Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.
<div><strong>Authors:</strong> Nishant Balepur, Dang Nguyen, Dayeon Ki</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes using the fantasy card game Dixit as a game‑based evaluation framework for multimodal language models (MLMs), arguing that games require multiple abilities and provide objective, competitive assessment. Quantitative experiments with five MLMs show that Dixit win‑rate rankings correlate with standard benchmark rankings, and human‑MLM game play reveals strategic differences and areas for improvement in MLM reasoning.", "summary_cn": "本文提出将幻想卡牌游戏 Dixit 用作多模态语言模型（MLM）的游戏化评估框架，认为游戏需要多种能力且具备客观竞争的评估规则。对五种 MLM 的量化实验表明，Dixit 的胜率排序与常用基准高度相关，同时人机对战揭示了模型策略的差异并指出了 MLM 推理的改进方向。", "keywords": "multimodal language models, game-based evaluation, Dixit, benchmark, quantitative experiments, human-MLM interaction, reasoning assessment, competitive games", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"]}
]]></acme>

<pubDate>2025-10-22T17:21:16+00:00</pubDate>
</item>
<item>
<title>An Expert-grounded benchmark of General Purpose LLMs in LCA</title>
<link>https://papers.cool/arxiv/2510.19886</link>
<guid>https://papers.cool/arxiv/2510.19886</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first expert‑grounded benchmark assessing general‑purpose large language models (LLMs) on 22 life cycle assessment (LCA) tasks, with 17 practitioners reviewing 168 model outputs for scientific accuracy, explanation quality, robustness, verifiability, and instruction adherence. Results show that 37% of responses contain inaccurate or misleading information, hallucination rates reach up to 40% for citations, and while open‑weight models can match or exceed closed‑weight models on some criteria, overall reliability varies considerably. The study highlights risks of naïvely using LLMs as oracles in LCA and underscores the need for grounding mechanisms despite some benefits in explanation quality and labor reduction.<br /><strong>Summary (CN):</strong> 本文首次提供了基于专家评审的通用大语言模型（LLM）在生命周期评估（LCA）中的基准评测，涵盖 22 项 LCA 任务，邀请 17 位从业者对 168 条模型输出进行科学准确性、解释质量、鲁棒性、可验证性和指令遵循度的评估。结果显示，37% 的回答存在不准确或误导信息，引用的幻觉率最高可达 40%，且开放权重模型在部分指标上能够与闭源模型相媲美，但整体可靠性差异显著。研究强调了在 LCA 中对 LLM 盲目信任的风险，并指出尽管在解释质量和降低劳动强度方面有一定优势，但仍需采用接地机制以提升安全性。<br /><strong>Keywords:</strong> life cycle assessment, LLM benchmark, expert evaluation, hallucination, accuracy, explanation quality, AI safety, model reliability, open-source LLMs, closed-source LLMs<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 6, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Artur Donaldson, Bharathan Balaji, Cajetan Oriekezie, Manish Kumar, Laure Patouillard</div>
Purpose: Artificial intelligence (AI), and in particular large language models (LLMs), are increasingly being explored as tools to support life cycle assessment (LCA). While demonstrations exist across environmental and social domains, systematic evidence on their reliability, robustness, and usability remains limited. This study provides the first expert-grounded benchmark of LLMs in LCA, addressing the absence of standardized evaluation frameworks in a field where no clear ground truth or consensus protocols exist. Methods: We evaluated eleven general-purpose LLMs, spanning both commercial and open-source families, across 22 LCA-related tasks. Seventeen experienced practitioners reviewed model outputs against criteria directly relevant to LCA practice, including scientific accuracy, explanation quality, robustness, verifiability, and adherence to instructions. We collected 168 expert reviews. Results: Experts judged 37% of responses to contain inaccurate or misleading information. Ratings of accuracy and quality of explanation were generally rated average or good on many models even smaller models, and format adherence was generally rated favourably. Hallucination rates varied significantly, with some models producing hallucinated citations at rates of up to 40%. There was no clear-cut distinction between ratings on open-weight versus closed-weight LLMs, with open-weight models outperforming or competing on par with closed-weight models on criteria such as accuracy and quality of explanation. Conclusion: These findings highlight the risks of applying LLMs naïvely in LCA, such as when LLMs are treated as free-form oracles, while also showing benefits especially around quality of explanation and alleviating labour intensiveness of simple tasks. The use of general-purpose LLMs without grounding mechanisms presents ...
<div><strong>Authors:</strong> Artur Donaldson, Bharathan Balaji, Cajetan Oriekezie, Manish Kumar, Laure Patouillard</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first expert‑grounded benchmark assessing general‑purpose large language models (LLMs) on 22 life cycle assessment (LCA) tasks, with 17 practitioners reviewing 168 model outputs for scientific accuracy, explanation quality, robustness, verifiability, and instruction adherence. Results show that 37% of responses contain inaccurate or misleading information, hallucination rates reach up to 40% for citations, and while open‑weight models can match or exceed closed‑weight models on some criteria, overall reliability varies considerably. The study highlights risks of naïvely using LLMs as oracles in LCA and underscores the need for grounding mechanisms despite some benefits in explanation quality and labor reduction.", "summary_cn": "本文首次提供了基于专家评审的通用大语言模型（LLM）在生命周期评估（LCA）中的基准评测，涵盖 22 项 LCA 任务，邀请 17 位从业者对 168 条模型输出进行科学准确性、解释质量、鲁棒性、可验证性和指令遵循度的评估。结果显示，37% 的回答存在不准确或误导信息，引用的幻觉率最高可达 40%，且开放权重模型在部分指标上能够与闭源模型相媲美，但整体可靠性差异显著。研究强调了在 LCA 中对 LLM 盲目信任的风险，并指出尽管在解释质量和降低劳动强度方面有一定优势，但仍需采用接地机制以提升安全性。", "keywords": "life cycle assessment, LLM benchmark, expert evaluation, hallucination, accuracy, explanation quality, AI safety, model reliability, open-source LLMs, closed-source LLMs", "scoring": {"interpretability": 3, "understanding": 6, "safety": 6, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Artur Donaldson", "Bharathan Balaji", "Cajetan Oriekezie", "Manish Kumar", "Laure Patouillard"]}
]]></acme>

<pubDate>2025-10-22T15:56:54+00:00</pubDate>
</item>
<item>
<title>Automated HIV Screening on Dutch EHR with Large Language Models</title>
<link>https://papers.cool/arxiv/2510.19879</link>
<guid>https://papers.cool/arxiv/2510.19879</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pipeline that uses a large language model to analyze unstructured clinical notes in electronic health records for automated HIV screening, showing high accuracy and low false‑negative rates on data from Erasmus University Medical Center Rotterdam.<br /><strong>Summary (CN):</strong> 本文提出一种利用大型语言模型（LLM）分析电子健康记录中非结构化临床笔记，以实现自动 HIV 筛查的流程，并在鹿特丹伊拉斯姆斯大学医学中心的数据上展示了高准确率和低漏报率。<br /><strong>Keywords:</strong> large language model, electronic health records, HIV screening, clinical notes, medical AI, disease detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lang Zhou, Amrish Jhingoer, Yinghao Luo, Klaske Vliegenthart--Jongbloed, Carlijn Jordans, Ben Werkhoven, Tom Seinen, Erik van Mulligen, Casper Rokx, Yunlei Li</div>
Efficient screening and early diagnosis of HIV are critical for reducing onward transmission. Although large scale laboratory testing is not feasible, the widespread adoption of Electronic Health Records (EHRs) offers new opportunities to address this challenge. Existing research primarily focuses on applying machine learning methods to structured data, such as patient demographics, for improving HIV diagnosis. However, these approaches often overlook unstructured text data such as clinical notes, which potentially contain valuable information relevant to HIV risk. In this study, we propose a novel pipeline that leverages a Large Language Model (LLM) to analyze unstructured EHR text and determine a patient's eligibility for further HIV testing. Experimental results on clinical data from Erasmus University Medical Center Rotterdam demonstrate that our pipeline achieved high accuracy while maintaining a low false negative rate.
<div><strong>Authors:</strong> Lang Zhou, Amrish Jhingoer, Yinghao Luo, Klaske Vliegenthart--Jongbloed, Carlijn Jordans, Ben Werkhoven, Tom Seinen, Erik van Mulligen, Casper Rokx, Yunlei Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pipeline that uses a large language model to analyze unstructured clinical notes in electronic health records for automated HIV screening, showing high accuracy and low false‑negative rates on data from Erasmus University Medical Center Rotterdam.", "summary_cn": "本文提出一种利用大型语言模型（LLM）分析电子健康记录中非结构化临床笔记，以实现自动 HIV 筛查的流程，并在鹿特丹伊拉斯姆斯大学医学中心的数据上展示了高准确率和低漏报率。", "keywords": "large language model, electronic health records, HIV screening, clinical notes, medical AI, disease detection", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lang Zhou", "Amrish Jhingoer", "Yinghao Luo", "Klaske Vliegenthart--Jongbloed", "Carlijn Jordans", "Ben Werkhoven", "Tom Seinen", "Erik van Mulligen", "Casper Rokx", "Yunlei Li"]}
]]></acme>

<pubDate>2025-10-22T11:53:14+00:00</pubDate>
</item>
<item>
<title>Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention</title>
<link>https://papers.cool/arxiv/2510.19875</link>
<guid>https://papers.cool/arxiv/2510.19875</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Sparse Tracing and the Stream algorithm, which use dynamic sparse attention to enable near‑linear‑time, linear‑space analysis of attention patterns in million‑token contexts. By hierarchically pruning attention masks and retaining only the top‑k key blocks per query, the method preserves next‑token behavior while discarding the vast majority of token interactions, allowing interpretable tracing of long‑chain‑of‑thought reasoning on consumer GPUs.<br /><strong>Summary (CN):</strong> 本文提出了稀疏追踪（Sparse Tracing）和 Stream 算法，利用动态稀疏注意力在近线性时间和线性空间内分析百万级上下文的注意力模式。通过层次化剪枝，仅保留每个查询的前 k 个关键块，保持模型的下一个 token 行为，同时剔除绝大多数交互，使得在普通 GPU 上也能对长链式思考进行可解释的追踪。<br /><strong>Keywords:</strong> mechanistic interpretability, sparse attention, long-context LLM, hierarchical pruning, chain-of-thought monitoring, attention tracing<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> J Rosser, José Luis Redondo García, Gustavo Penha, Konstantina Palla, Hugues Bouchard</div>
As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.
<div><strong>Authors:</strong> J Rosser, José Luis Redondo García, Gustavo Penha, Konstantina Palla, Hugues Bouchard</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Sparse Tracing and the Stream algorithm, which use dynamic sparse attention to enable near‑linear‑time, linear‑space analysis of attention patterns in million‑token contexts. By hierarchically pruning attention masks and retaining only the top‑k key blocks per query, the method preserves next‑token behavior while discarding the vast majority of token interactions, allowing interpretable tracing of long‑chain‑of‑thought reasoning on consumer GPUs.", "summary_cn": "本文提出了稀疏追踪（Sparse Tracing）和 Stream 算法，利用动态稀疏注意力在近线性时间和线性空间内分析百万级上下文的注意力模式。通过层次化剪枝，仅保留每个查询的前 k 个关键块，保持模型的下一个 token 行为，同时剔除绝大多数交互，使得在普通 GPU 上也能对长链式思考进行可解释的追踪。", "keywords": "mechanistic interpretability, sparse attention, long-context LLM, hierarchical pruning, chain-of-thought monitoring, attention tracing", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["J Rosser", "José Luis Redondo García", "Gustavo Penha", "Konstantina Palla", "Hugues Bouchard"]}
]]></acme>

<pubDate>2025-10-22T09:42:29+00:00</pubDate>
</item>
<item>
<title>From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</title>
<link>https://papers.cool/arxiv/2510.19871</link>
<guid>https://papers.cool/arxiv/2510.19871</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces ReDiff, a corrective framework for vision‑language diffusion models that shifts generation from passive denoising to active refining. By training a model to detect and fix its own synthetic errors in a two‑stage process, it breaks error cascades that cause syntactic mistakes and semantic hallucinations, enabling stable parallel generation with higher coherence and factual accuracy.<br /><strong>Summary (CN):</strong> 本文提出 ReDiff，一种面向视觉‑语言扩散模型的纠错框架，将生成过程从被动去噪转换为主动精炼。通过两阶段训练让模型学会识别并纠正自身的合成错误，打破导致语法错误和语义幻觉的错误级联，实现稳定的并行生成并提升内容连贯性与事实准确性。<br /><strong>Keywords:</strong> vision-language diffusion, error cascade, self-correction, refinement, parallel decoding, hallucination mitigation, ReDiff, generative models, synthetic error revision, factual accuracy<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Yatai Ji, Teng Wang, Yuying Ge, Zhiheng Liu, Sidi Yang, Ying Shan, Ping Luo</div>
Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.
<div><strong>Authors:</strong> Yatai Ji, Teng Wang, Yuying Ge, Zhiheng Liu, Sidi Yang, Ying Shan, Ping Luo</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces ReDiff, a corrective framework for vision‑language diffusion models that shifts generation from passive denoising to active refining. By training a model to detect and fix its own synthetic errors in a two‑stage process, it breaks error cascades that cause syntactic mistakes and semantic hallucinations, enabling stable parallel generation with higher coherence and factual accuracy.", "summary_cn": "本文提出 ReDiff，一种面向视觉‑语言扩散模型的纠错框架，将生成过程从被动去噪转换为主动精炼。通过两阶段训练让模型学会识别并纠正自身的合成错误，打破导致语法错误和语义幻觉的错误级联，实现稳定的并行生成并提升内容连贯性与事实准确性。", "keywords": "vision-language diffusion, error cascade, self-correction, refinement, parallel decoding, hallucination mitigation, ReDiff, generative models, synthetic error revision, factual accuracy", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Yatai Ji", "Teng Wang", "Yuying Ge", "Zhiheng Liu", "Sidi Yang", "Ying Shan", "Ping Luo"]}
]]></acme>

<pubDate>2025-10-22T06:58:55+00:00</pubDate>
</item>
<item>
<title>An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</title>
<link>https://papers.cool/arxiv/2510.19866</link>
<guid>https://papers.cool/arxiv/2510.19866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper evaluates the pedagogical soundness and usability of AI‑generated high‑school physics lesson plans produced by five leading large language models and three structured prompt frameworks. Using automated metrics for readability, factual accuracy, curriculum alignment, and cognitive demand, the study finds that model choice primarily affects linguistic accessibility while prompt structure most influences factual reliability and curricular alignment. The best configuration combines a readability‑optimized model with the RACE prompt framework and explicit instructional checklists.<br /><strong>Summary (CN):</strong> 本文评估了五种主流大型语言模型和三种结构化提示框架生成的高中物理教学计划在教学合理性和可用性方面的表现。通过可读性、事实准确性、课程对齐度和学习目标认知需求四项自动化指标分析，结果显示模型选择对语言可读性影响最大，而提示框架结构对事实可靠性和课程对齐度影响更显著。最佳配置为使用可读性优化的模型搭配 RACE 提示框架并加入明确的物理概念及课程标准检查清单。<br /><strong>Keywords:</strong> AI-generated lesson plans, large language models, prompt engineering, pedagogical evaluation, readability, factual accuracy, curriculum alignment, Bloom's taxonomy<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xincheng Liu</div>
This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice, three structured prompt frameworks were tested: TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format). Fifteen lesson plans were generated for a single high-school physics topic, The Electromagnetic Spectrum. The lesson plans were analyzed through four automated computational metrics: (1) readability and linguistic complexity, (2) factual accuracy and hallucination detection, (3) standards and curriculum alignment, and (4) cognitive demand of learning objectives. Results indicate that model selection exerted the strongest influence on linguistic accessibility, with DeepSeek producing the most readable teaching plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89). The prompt framework structure most strongly affected the factual accuracy and pedagogical completeness, with the RACE framework yielding the lowest hallucination index and the highest incidental alignment with NGSS curriculum standards. Across all models, the learning objectives in the fifteen lesson plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There were limited higher-order verbs in the learning objectives extracted. Overall, the findings suggest that readability is significantly governed by model design, while instructional reliability and curricular alignment depend more on the prompt framework. The most effective configuration for lesson plans identified in the results was to combine a readability-optimized model with the RACE framework and an explicit checklist of physics concepts, curriculum standards, and higher-order objectives.
<div><strong>Authors:</strong> Xincheng Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper evaluates the pedagogical soundness and usability of AI‑generated high‑school physics lesson plans produced by five leading large language models and three structured prompt frameworks. Using automated metrics for readability, factual accuracy, curriculum alignment, and cognitive demand, the study finds that model choice primarily affects linguistic accessibility while prompt structure most influences factual reliability and curricular alignment. The best configuration combines a readability‑optimized model with the RACE prompt framework and explicit instructional checklists.", "summary_cn": "本文评估了五种主流大型语言模型和三种结构化提示框架生成的高中物理教学计划在教学合理性和可用性方面的表现。通过可读性、事实准确性、课程对齐度和学习目标认知需求四项自动化指标分析，结果显示模型选择对语言可读性影响最大，而提示框架结构对事实可靠性和课程对齐度影响更显著。最佳配置为使用可读性优化的模型搭配 RACE 提示框架并加入明确的物理概念及课程标准检查清单。", "keywords": "AI-generated lesson plans, large language models, prompt engineering, pedagogical evaluation, readability, factual accuracy, curriculum alignment, Bloom's taxonomy", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xincheng Liu"]}
]]></acme>

<pubDate>2025-10-22T02:53:06+00:00</pubDate>
</item>
<item>
<title>DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse</title>
<link>https://papers.cool/arxiv/2510.19858</link>
<guid>https://papers.cool/arxiv/2510.19858</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeBERTa-KC, a transformer‑based classifier that automatically identifies four levels of knowledge construction (nonKC, Share, Explore, Negotiate) in comments from popular YouTube science channels. By extending DeBERTa‑v3 with focal loss, label smoothing, and R‑Drop, the model achieves a macro‑F1 of 0.836 on a manually annotated corpus of 20,000 samples, outperforming baseline methods. The work demonstrates that large language models can capture nuanced epistemic engagement in informal digital learning environments, providing a scalable tool for discourse analysis.<br /><strong>Summary (CN):</strong> 本文提出 DeBERTa‑KC，一种基于 Transformer 的分类模型，用于自动识别 YouTube 科普频道评论中的四种知识建构层级（非建构、共享、探索、协商）。通过在 DeBERTa‑v3 上加入焦点损失、标签平滑和 R‑Drop 正则化，模型在 20,000 条人工标注样本上实现了 0.836 的宏观 F1 分数，显著优于基线方法。该研究表明，大语言模型能够捕捉非正式数字学习环境中的细微认知参与，为话语分析提供了可扩展的自动化工具。<br /><strong>Keywords:</strong> DeBERTa, knowledge construction, epistemic engagement, transformer classifier, online learning discourse, focal loss, label smoothing, R-Drop<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jindi Wang, Yidi Zhang, Zhaoxing Li</div>
This study presents DeBERTa-KC, a transformer-based model for automatic classification of knowledge construction (KC) levels in online science learning discourse. Using comments collected from four popular YouTube science channels (2022--2024), a balanced corpus of 20,000 manually annotated samples was created across four KC categories: \textit{nonKC}, \textit{Share}, \textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3 with Focal Loss, Label Smoothing, and R-Drop regularization to address class imbalance and enhance generalization. A reproducible end-to-end pipeline was implemented, encompassing data extraction, annotation, preprocessing, training, and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical and transformer baselines ($p<0.01$). Per-category results indicate strong sensitivity to higher-order epistemic engagement, particularly in \textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate that large language models can effectively capture nuanced indicators of knowledge construction in informal digital learning environments, offering scalable, theory-informed approaches to discourse analysis and the development of automated tools for assessing epistemic engagement.
<div><strong>Authors:</strong> Jindi Wang, Yidi Zhang, Zhaoxing Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeBERTa-KC, a transformer‑based classifier that automatically identifies four levels of knowledge construction (nonKC, Share, Explore, Negotiate) in comments from popular YouTube science channels. By extending DeBERTa‑v3 with focal loss, label smoothing, and R‑Drop, the model achieves a macro‑F1 of 0.836 on a manually annotated corpus of 20,000 samples, outperforming baseline methods. The work demonstrates that large language models can capture nuanced epistemic engagement in informal digital learning environments, providing a scalable tool for discourse analysis.", "summary_cn": "本文提出 DeBERTa‑KC，一种基于 Transformer 的分类模型，用于自动识别 YouTube 科普频道评论中的四种知识建构层级（非建构、共享、探索、协商）。通过在 DeBERTa‑v3 上加入焦点损失、标签平滑和 R‑Drop 正则化，模型在 20,000 条人工标注样本上实现了 0.836 的宏观 F1 分数，显著优于基线方法。该研究表明，大语言模型能够捕捉非正式数字学习环境中的细微认知参与，为话语分析提供了可扩展的自动化工具。", "keywords": "DeBERTa, knowledge construction, epistemic engagement, transformer classifier, online learning discourse, focal loss, label smoothing, R-Drop", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jindi Wang", "Yidi Zhang", "Zhaoxing Li"]}
]]></acme>

<pubDate>2025-10-21T20:26:18+00:00</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://papers.cool/arxiv/2510.20812</link>
<guid>https://papers.cool/arxiv/2510.20812</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Speculative Verdict (SV), a training‑free framework that leverages multiple lightweight draft VLMs to generate diverse reasoning paths and a strong verdict VLM to synthesize the final answer for information‑intensive visual question answering. A consensus expert selection mechanism forwards only high‑agreement drafts to the verdict, improving both accuracy and computational efficiency on benchmarks such as InfographicVQA and ChartQAPro.<br /><strong>Summary (CN):</strong> 本文提出了“Speculative Verdict”(SV) 框架，该框架在无训练的情况下利用多个轻量级草稿 VLM 生成多样化推理路径，并由强大的 verdict VLM 进行答案合成，以应对信息密集型视觉问答任务。通过共识专家选择机制，只将高一致性的草稿路径转交给 verdict，从而在 InfographicVQA、ChartQAPro 等基准上实现更高的准确率和计算效率。<br /><strong>Keywords:</strong> visual language model, speculative decoding, information-intensive visual reasoning, draft expert, consensus selection, multimodal QA, training-free inference<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict
<div><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Speculative Verdict (SV), a training‑free framework that leverages multiple lightweight draft VLMs to generate diverse reasoning paths and a strong verdict VLM to synthesize the final answer for information‑intensive visual question answering. A consensus expert selection mechanism forwards only high‑agreement drafts to the verdict, improving both accuracy and computational efficiency on benchmarks such as InfographicVQA and ChartQAPro.", "summary_cn": "本文提出了“Speculative Verdict”(SV) 框架，该框架在无训练的情况下利用多个轻量级草稿 VLM 生成多样化推理路径，并由强大的 verdict VLM 进行答案合成，以应对信息密集型视觉问答任务。通过共识专家选择机制，只将高一致性的草稿路径转交给 verdict，从而在 InfographicVQA、ChartQAPro 等基准上实现更高的准确率和计算效率。", "keywords": "visual language model, speculative decoding, information-intensive visual reasoning, draft expert, consensus selection, multimodal QA, training-free inference", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:21+00:00</pubDate>
</item>
<item>
<title>Real Deep Research for AI, Robotics and Beyond</title>
<link>https://papers.cool/arxiv/2510.20809</link>
<guid>https://papers.cool/arxiv/2510.20809</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area by identifying emerging trends, uncovering cross‑domain opportunities, and suggesting concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across multiple topics.<br /><strong>Summary (CN):</strong> 本文提出 Real Deep Research（RDR）框架，一种可通用的管线用于系统性分析任意研究领域，识别新兴趋势、发掘跨领域机会并提供具体的研究起点。作者在 AI 与机器人领域（尤其是基础模型和机器人技术）进行案例展示，并在附录中提供了大量跨主题分析结果。<br /><strong>Keywords:</strong> meta-research, trend analysis, interdisciplinary, AI foundations, robotics, research pipelines, emerging topics<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.
<div><strong>Authors:</strong> Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Real Deep Research (RDR), a generalizable pipeline for systematically analyzing any research area by identifying emerging trends, uncovering cross‑domain opportunities, and suggesting concrete starting points for new investigations. It demonstrates the framework on AI and robotics, focusing on foundation models and robotic advancements, and provides extensive appendix results across multiple topics.", "summary_cn": "本文提出 Real Deep Research（RDR）框架，一种可通用的管线用于系统性分析任意研究领域，识别新兴趋势、发掘跨领域机会并提供具体的研究起点。作者在 AI 与机器人领域（尤其是基础模型和机器人技术）进行案例展示，并在附录中提供了大量跨主题分析结果。", "keywords": "meta-research, trend analysis, interdisciplinary, AI foundations, robotics, research pipelines, emerging topics", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xueyan Zou", "Jianglong Ye", "Hao Zhang", "Xiaoyu Xiang", "Mingyu Ding", "Zhaojing Yang", "Yong Jae Lee", "Zhuowen Tu", "Sifei Liu", "Xiaolong Wang"]}
]]></acme>

<pubDate>2025-10-23T17:59:05+00:00</pubDate>
</item>
<item>
<title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title>
<link>https://papers.cool/arxiv/2510.20800</link>
<guid>https://papers.cool/arxiv/2510.20800</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a fast LLM adaptation method that uses a single gradient step on only 100 examples combined with a targeted scan of top candidate layers and factorization techniques, eliminating the need for full fine‑tuning. By computing gradients of singular values to select matrices for low‑rank reduction and clustering matrix rows into multiple subspaces, the approach improves downstream accuracy by up to 24.6 points while dramatically reducing search time.<br /><strong>Summary (CN):</strong> 本文提出一种快速适配大型语言模型的方法，仅使用 100 条样本进行一次梯度更新，并结合对关键层的快速扫描和因式分解技术，从而无需完整微调。通过计算矩阵奇异值的梯度来挑选需要降秩的矩阵，并将矩阵行聚类到多个子空间进行分解，该方法在显著降低搜索时间的同时，使下游任务的准确率提升最高可达 24.6%。<br /><strong>Keywords:</strong> LLM adaptation, LASER, low-rank pruning, singular value gradient, matrix factorization, efficient fine-tuning, single gradient step, weight matrix selection, model compression, downstream accuracy<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.
<div><strong>Authors:</strong> Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a fast LLM adaptation method that uses a single gradient step on only 100 examples combined with a targeted scan of top candidate layers and factorization techniques, eliminating the need for full fine‑tuning. By computing gradients of singular values to select matrices for low‑rank reduction and clustering matrix rows into multiple subspaces, the approach improves downstream accuracy by up to 24.6 points while dramatically reducing search time.", "summary_cn": "本文提出一种快速适配大型语言模型的方法，仅使用 100 条样本进行一次梯度更新，并结合对关键层的快速扫描和因式分解技术，从而无需完整微调。通过计算矩阵奇异值的梯度来挑选需要降秩的矩阵，并将矩阵行聚类到多个子空间进行分解，该方法在显著降低搜索时间的同时，使下游任务的准确率提升最高可达 24.6%。", "keywords": "LLM adaptation, LASER, low-rank pruning, singular value gradient, matrix factorization, efficient fine-tuning, single gradient step, weight matrix selection, model compression, downstream accuracy", "scoring": {"interpretability": 5, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"]}
]]></acme>

<pubDate>2025-10-23T17:58:01+00:00</pubDate>
</item>
<item>
<title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title>
<link>https://papers.cool/arxiv/2510.20792</link>
<guid>https://papers.cool/arxiv/2510.20792</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BadGraph, a backdoor attack against latent diffusion models used for text‑guided graph generation. By poisoning a small fraction of the training data with textual triggers, the attacker can cause the model to insert attacker‑specified subgraphs at inference time while maintaining normal performance on clean inputs. Experiments on four benchmark datasets show high attack success rates with low poisoning ratios and highlight the need for defenses in applications such as drug discovery.<br /><strong>Summary (CN):</strong> 本文提出 BadGraph，一种针对文本引导图生成的潜在扩散模型的后门攻击方法。通过在训练数据中注入文本触发词进行少量中毒，攻击者可以在推理时强制模型生成指定子图，同时对干净输入的性能几乎不受影响。四个基准数据集的实验表明，以低于 10% 的中毒率即可实现 50% 以上的攻击成功率，凸显了在药物发现等应用中防御此类后门的紧迫性。<br /><strong>Keywords:</strong> backdoor attack, latent diffusion model, text-guided graph generation, graph generation, security, VAE, poisoning, robustness, drug discovery<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 8, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Liang Ye, Shengqin Chen, Jiazhu Dai</div>
The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method targeting latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.
<div><strong>Authors:</strong> Liang Ye, Shengqin Chen, Jiazhu Dai</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BadGraph, a backdoor attack against latent diffusion models used for text‑guided graph generation. By poisoning a small fraction of the training data with textual triggers, the attacker can cause the model to insert attacker‑specified subgraphs at inference time while maintaining normal performance on clean inputs. Experiments on four benchmark datasets show high attack success rates with low poisoning ratios and highlight the need for defenses in applications such as drug discovery.", "summary_cn": "本文提出 BadGraph，一种针对文本引导图生成的潜在扩散模型的后门攻击方法。通过在训练数据中注入文本触发词进行少量中毒，攻击者可以在推理时强制模型生成指定子图，同时对干净输入的性能几乎不受影响。四个基准数据集的实验表明，以低于 10% 的中毒率即可实现 50% 以上的攻击成功率，凸显了在药物发现等应用中防御此类后门的紧迫性。", "keywords": "backdoor attack, latent diffusion model, text-guided graph generation, graph generation, security, VAE, poisoning, robustness, drug discovery", "scoring": {"interpretability": 2, "understanding": 7, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Liang Ye", "Shengqin Chen", "Jiazhu Dai"]}
]]></acme>

<pubDate>2025-10-23T17:54:17+00:00</pubDate>
</item>
<item>
<title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</title>
<link>https://papers.cool/arxiv/2510.20743</link>
<guid>https://papers.cool/arxiv/2510.20743</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Empathic Prompting, a framework that augments Large Language Model conversations with implicit non‑verbal cues by integrating a facial expression recognition service and embedding affective signals into prompts. The modular architecture, demonstrated with a locally deployed DeepSeek instance, shows that unobtrusive emotional context improves conversational fluidity in a small usability study (N=5), suggesting applications in domains such as healthcare and education.<br /><strong>Summary (CN):</strong> 本文提出“共情提示”（Empathic Prompting）框架，通过整合面部表情识别服务，将用户情感线索嵌入 Large Language Model 的提示中，实现对话的非语言上下文增强。基于本地部署的 DeepSeek 示例，实验（N=5）显示该方式在不需要用户显式操作的情况下提升了对话流畅度，显示出在医疗、教育等需要情感感知的场景中的潜在应用价值。<br /><strong>Keywords:</strong> empathic prompting, multimodal LLM, facial expression recognition, affective AI, conversational alignment, human-AI interaction, non-verbal context, DeepSeek, usability evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</div>
We present Empathic Prompting, a novel framework for multimodal human-AI interaction that enriches Large Language Model (LLM) conversations with implicit non-verbal context. The system integrates a commercial facial expression recognition service to capture users' emotional cues and embeds them as contextual signals during prompting. Unlike traditional multimodal interfaces, empathic prompting requires no explicit user control; instead, it unobtrusively augments textual input with affective information for conversational and smoothness alignment. The architecture is modular and scalable, allowing integration of additional non-verbal modules. We describe the system design, implemented through a locally deployed DeepSeek instance, and report a preliminary service and usability evaluation (N=5). Results show consistent integration of non-verbal input into coherent LLM outputs, with participants highlighting conversational fluidity. Beyond this proof of concept, empathic prompting points to applications in chatbot-mediated communication, particularly in domains like healthcare or education, where users' emotional signals are critical yet often opaque in verbal exchanges.
<div><strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Empathic Prompting, a framework that augments Large Language Model conversations with implicit non‑verbal cues by integrating a facial expression recognition service and embedding affective signals into prompts. The modular architecture, demonstrated with a locally deployed DeepSeek instance, shows that unobtrusive emotional context improves conversational fluidity in a small usability study (N=5), suggesting applications in domains such as healthcare and education.", "summary_cn": "本文提出“共情提示”（Empathic Prompting）框架，通过整合面部表情识别服务，将用户情感线索嵌入 Large Language Model 的提示中，实现对话的非语言上下文增强。基于本地部署的 DeepSeek 示例，实验（N=5）显示该方式在不需要用户显式操作的情况下提升了对话流畅度，显示出在医疗、教育等需要情感感知的场景中的潜在应用价值。", "keywords": "empathic prompting, multimodal LLM, facial expression recognition, affective AI, conversational alignment, human-AI interaction, non-verbal context, DeepSeek, usability evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Lorenzo Stacchio", "Andrea Ubaldi", "Alessandro Galdelli", "Maurizio Mauri", "Emanuele Frontoni", "Andrea Gaggioli"]}
]]></acme>

<pubDate>2025-10-23T17:08:03+00:00</pubDate>
</item>
<item>
<title>Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</title>
<link>https://papers.cool/arxiv/2510.20728</link>
<guid>https://papers.cool/arxiv/2510.20728</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a human-in-the-loop, multi-agent workflow that co-designs quantum error-correcting codes with specific transversal diagonal gates. Using the Subset-Sum Linear Programming framework and agents powered by GPT-5 within a LaTeX-Python environment, the system automatically enumerates small codes, verifies Knill-Laflamme conditions, and abstracts families of codes analytically. Experiments for distance d=2 and up to six qubits produce new codes and a catalog of attainable logical groups.<br /><strong>Summary (CN):</strong> 本文提出一种人机协同的多智能体工作流，用于共同设计具备特定横向对角门的量子纠错码。该工作流基于子集求和线性规划框架，并在 LaTeX‑Python 环境中使用 GPT‑5 驱动的合成、搜索和审计智能体，实现自动枚举、验证 Knill‑Laflamme 条件并归纳出一般化的代码族。实验在距离 d=2、最多六个量子比特的情况下生成了若干新码并列出可实现的逻辑群组。<br /><strong>Keywords:</strong> quantum error correction, transversal diagonal gates, multi-agent workflow, GPT-5, Subset-Sum Linear Programming, Knill-Laflamme conditions, code co-design, automated enumeration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Xi He, Sirui Lu, Bei Zeng</div>
We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (https://texra.ai)-a multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git/Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps/screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d=2$ with nondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$ at $n=6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.
<div><strong>Authors:</strong> Xi He, Sirui Lu, Bei Zeng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a human-in-the-loop, multi-agent workflow that co-designs quantum error-correcting codes with specific transversal diagonal gates. Using the Subset-Sum Linear Programming framework and agents powered by GPT-5 within a LaTeX-Python environment, the system automatically enumerates small codes, verifies Knill-Laflamme conditions, and abstracts families of codes analytically. Experiments for distance d=2 and up to six qubits produce new codes and a catalog of attainable logical groups.", "summary_cn": "本文提出一种人机协同的多智能体工作流，用于共同设计具备特定横向对角门的量子纠错码。该工作流基于子集求和线性规划框架，并在 LaTeX‑Python 环境中使用 GPT‑5 驱动的合成、搜索和审计智能体，实现自动枚举、验证 Knill‑Laflamme 条件并归纳出一般化的代码族。实验在距离 d=2、最多六个量子比特的情况下生成了若干新码并列出可实现的逻辑群组。", "keywords": "quantum error correction, transversal diagonal gates, multi-agent workflow, GPT-5, Subset-Sum Linear Programming, Knill-Laflamme conditions, code co-design, automated enumeration", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Xi He", "Sirui Lu", "Bei Zeng"]}
]]></acme>

<pubDate>2025-10-23T16:45:39+00:00</pubDate>
</item>
<item>
<title>Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE</title>
<link>https://papers.cool/arxiv/2510.20674</link>
<guid>https://papers.cool/arxiv/2510.20674</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper describes a multilingual e-commerce product search system built by the Tredence_AICOE team for a competition featuring Query-Category and Query-Item relevance tasks. They augment data by translating existing sets into missing languages andune Gemma-3 12B and Qwen-2.5 14B models using various strategies, achieving a 4th‑place finish with an average F1 of 0.8857. The work emphasizes practical engineering solutions for multilingual relevance rather than theoretical AI safety or interpretability contributions.<br /><strong>Summary (CN):</strong> 本文介绍了 Tredence_AICOE 队伍为多语言电子商务产品竞赛构建的系统，涵盖查询‑类目（QC）和查询‑商品（QI）相关性任务。团队通过将现有数据翻译至缺失语言进行数据增强，并针对两大模型 Gemma-3 12B 与 Qwen-2.5 14B 进行多策略微调，最终在私有测试集上获得 0.8857 的平均 F1 并排名第四。该研究侧重于多语言检索的工程实现，而非 AI 安全或可解释性理论。<br /><strong>Keywords:</strong> multilingual search, e-commerce retrieval, relevance ranking, data augmentation, large language models, Gemma-3, Qwen-2.5, multilingual relevance, fine-tuning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 1, Technicality: 8, Surprisal: 4<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rakshith R, Shubham Sharma, Mohammed Sameer Khan, Ankush Chopra</div>
This study presents the multilingual e-commerce search system developed by the Tredence_AICOE team. The competition features two multilingual relevance tasks: Query-Category (QC) Relevance, which evaluates how well a user's search query aligns with a product category, and Query-Item (QI) Relevance, which measures the match between a multilingual search query and an individual product listing. To ensure full language coverage, we performed data augmentation by translating existing datasets into languages missing from the development set, enabling training across all target languages. We fine-tuned Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies. The Gemma-3 12B (4-bit) model achieved the best QC performance using original and translated data, and the best QI performance using original, translated, and minority class data creation. These approaches secured 4th place on the final leaderboard, with an average F1-score of 0.8857 on the private test set.
<div><strong>Authors:</strong> Rakshith R, Shubham Sharma, Mohammed Sameer Khan, Ankush Chopra</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper describes a multilingual e-commerce product search system built by the Tredence_AICOE team for a competition featuring Query-Category and Query-Item relevance tasks. They augment data by translating existing sets into missing languages andune Gemma-3 12B and Qwen-2.5 14B models using various strategies, achieving a 4th‑place finish with an average F1 of 0.8857. The work emphasizes practical engineering solutions for multilingual relevance rather than theoretical AI safety or interpretability contributions.", "summary_cn": "本文介绍了 Tredence_AICOE 队伍为多语言电子商务产品竞赛构建的系统，涵盖查询‑类目（QC）和查询‑商品（QI）相关性任务。团队通过将现有数据翻译至缺失语言进行数据增强，并针对两大模型 Gemma-3 12B 与 Qwen-2.5 14B 进行多策略微调，最终在私有测试集上获得 0.8857 的平均 F1 并排名第四。该研究侧重于多语言检索的工程实现，而非 AI 安全或可解释性理论。", "keywords": "multilingual search, e-commerce retrieval, relevance ranking, data augmentation, large language models, Gemma-3, Qwen-2.5, multilingual relevance, fine-tuning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 1, "technicality": 8, "surprisal": 4}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rakshith R", "Shubham Sharma", "Mohammed Sameer Khan", "Ankush Chopra"]}
]]></acme>

<pubDate>2025-10-23T15:49:20+00:00</pubDate>
</item>
<item>
<title>What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation</title>
<link>https://papers.cool/arxiv/2510.20603</link>
<guid>https://papers.cool/arxiv/2510.20603</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Causal Stepwise Evaluation (CaSE), a framework that scores each reasoning step of large language models on relevance to the problem and logical coherence with preceding steps, avoiding hindsight bias. It validates CaSE against human judgments on new expert‑annotated benchmarks (MRa‑GSM8K and MRa‑MATH) and shows that using CaSE‑filtered data for training improves final‑answer performance. The work offers a scalable method for analyzing, debugging, and enhancing LLM reasoning beyond simple correctness checks.<br /><strong>Summary (CN):</strong> 本文提出因果分步评估（CaSE）框架，对大语言模型的每一步推理分别评估其与问题的相关性以及与前一步的逻辑连贯性，从而避免后视偏差。作者在新的人类专家标注基准（MRa‑GSM8K 与 MRa‑MATH）上验证了 CaSE 与人工判断的一致性，并展示了使用 CaSE 评估后筛选的训练数据可直接提升模型的最终答案表现。该工作提供了一种可规模化的方式用于分析、调试和改进 LLM 推理，超越了仅检验答案正确性的传统评估。<br /><strong>Keywords:</strong> reasoning evaluation, relevance, coherence, causal stepwise evaluation, LLM reasoning, interpretability, benchmark, data curation<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Heejin Do, Jaehui Hwang, Dongyoon Han, Seong Joon Oh, Sangdoo Yun</div>
Evaluating large language models (LLMs) on final-answer correctness is the dominant paradigm. This approach, however, provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process. We argue that a more granular evaluation of reasoning offers a more effective path to building robust models. We decompose reasoning quality into two dimensions: relevance and coherence. Relevance measures if a step is grounded in the problem; coherence measures if it follows logically from prior steps. To measure these aspects reliably, we introduce causal stepwise evaluation (CaSE). This method assesses each reasoning step using only its preceding context, which avoids hindsight bias. We validate CaSE against human judgments on our new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we show that curating training data with CaSE-evaluated relevance and coherence directly improves final task performance. Our work provides a scalable framework for analyzing, debugging, and improving LLM reasoning, demonstrating the practical value of moving beyond validity checks.
<div><strong>Authors:</strong> Heejin Do, Jaehui Hwang, Dongyoon Han, Seong Joon Oh, Sangdoo Yun</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Causal Stepwise Evaluation (CaSE), a framework that scores each reasoning step of large language models on relevance to the problem and logical coherence with preceding steps, avoiding hindsight bias. It validates CaSE against human judgments on new expert‑annotated benchmarks (MRa‑GSM8K and MRa‑MATH) and shows that using CaSE‑filtered data for training improves final‑answer performance. The work offers a scalable method for analyzing, debugging, and enhancing LLM reasoning beyond simple correctness checks.", "summary_cn": "本文提出因果分步评估（CaSE）框架，对大语言模型的每一步推理分别评估其与问题的相关性以及与前一步的逻辑连贯性，从而避免后视偏差。作者在新的人类专家标注基准（MRa‑GSM8K 与 MRa‑MATH）上验证了 CaSE 与人工判断的一致性，并展示了使用 CaSE 评估后筛选的训练数据可直接提升模型的最终答案表现。该工作提供了一种可规模化的方式用于分析、调试和改进 LLM 推理，超越了仅检验答案正确性的传统评估。", "keywords": "reasoning evaluation, relevance, coherence, causal stepwise evaluation, LLM reasoning, interpretability, benchmark, data curation", "scoring": {"interpretability": 7, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Heejin Do", "Jaehui Hwang", "Dongyoon Han", "Seong Joon Oh", "Sangdoo Yun"]}
]]></acme>

<pubDate>2025-10-23T14:30:37+00:00</pubDate>
</item>
<item>
<title>Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment</title>
<link>https://papers.cool/arxiv/2510.20513</link>
<guid>https://papers.cool/arxiv/2510.20513</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DeEAR, a framework that converts human preference for speech expressiveness into an objective score by evaluating emotion, prosody, and spontaneity. It achieves strong alignment with human perception (SRCC = 0.86) using fewer than 500 annotated samples and uses the metric for fair benchmarking and to select 14K expressive utterances (ExpressiveSpeech), which substantially improve expressiveness scores of S2S models.<br /><strong>Summary (CN):</strong> 本文提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分，评估维度包括情感、韵律和自发性。该方法在不足 500 条标注样本下实现了与人类感知的高度一致（Spearman 秩相关系数 SRCC = 0.86），并用于公平基准测试以及筛选 14 K 条表达性语料（ExpressiveSpeech），显著提升了 S2S 模型的表达性评分。<br /><strong>Keywords:</strong> expressive speech, speech-to-speech, evaluation metric, human preference, emotion prosody spontaneity, SRCC, data curation, ExpressiveSpeech<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang</div>
Recent speech-to-speech (S2S) models generate intelligible speech but still lack natural expressiveness, largely due to the absence of a reliable evaluation metric. Existing approaches, such as subjective MOS ratings, low-level acoustic features, and emotion recognition are costly, limited, or incomplete. To address this, we present DeEAR (Decoding the Expressive Preference of eAR), a framework that converts human preference for speech expressiveness into an objective score. Grounded in phonetics and psychology, DeEAR evaluates speech across three dimensions: Emotion, Prosody, and Spontaneity, achieving strong alignment with human perception (Spearman's Rank Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples. Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data curation. It not only distinguishes expressiveness gaps across S2S models but also selects 14K expressive utterances to form ExpressiveSpeech, which improves the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models. Demos and codes are available at https://github.com/FreedomIntelligence/ExpressiveSpeech
<div><strong>Authors:</strong> Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DeEAR, a framework that converts human preference for speech expressiveness into an objective score by evaluating emotion, prosody, and spontaneity. It achieves strong alignment with human perception (SRCC = 0.86) using fewer than 500 annotated samples and uses the metric for fair benchmarking and to select 14K expressive utterances (ExpressiveSpeech), which substantially improve expressiveness scores of S2S models.", "summary_cn": "本文提出 DeEAR 框架，将人类对语音表达性的偏好转化为客观评分，评估维度包括情感、韵律和自发性。该方法在不足 500 条标注样本下实现了与人类感知的高度一致（Spearman 秩相关系数 SRCC = 0.86），并用于公平基准测试以及筛选 14 K 条表达性语料（ExpressiveSpeech），显著提升了 S2S 模型的表达性评分。", "keywords": "expressive speech, speech-to-speech, evaluation metric, human preference, emotion prosody spontaneity, SRCC, data curation, ExpressiveSpeech", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Zhiyu Lin", "Jingwen Yang", "Jiale Zhao", "Meng Liu", "Sunzhu Li", "Benyou Wang"]}
]]></acme>

<pubDate>2025-10-23T12:57:46+00:00</pubDate>
</item>
<item>
<title>Relative-Based Scaling Law for Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.20387</link>
<guid>https://papers.cool/arxiv/2510.20387</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces the Relative-Based Probability (RBP) metric, which measures how often the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Extensive experiments across multiple datasets and model families show that this scaling law accurately predicts performance and offers new insights into emergence phenomena and theoretical scaling principles. By complementing traditional cross‑entropy scaling analyses, the work provides a more complete picture of large language model behavior.<br /><strong>Summary (CN):</strong> 本文提出相对概率（RBP）度量，用于衡量正确 token 在预测排名中的位置，并基于此建立了相对基准的尺度律，描述 RBP 随模型规模的提升而改进的规律。通过在四个数据集和四类模型上进行大规模实验，验证了该尺度律的稳健性和预测准确性，并展示了其在解释模型涌现现象及寻找尺度律基本理论方面的应用。该工作补充了传统交叉熵尺度分析，为大语言模型行为提供了更完整的理解。<br /><strong>Keywords:</strong> scaling laws, relative-based probability, language models, emergence, model performance prediction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
Scaling laws aim to accurately predict model performance across different scales. Existing scaling-law studies almost exclusively rely on cross-entropy as the evaluation metric. However, cross-entropy provides only a partial view of performance: it measures the absolute probability assigned to the correct token, but ignores the relative ordering between correct and incorrect tokens. Yet, relative ordering is crucial for language models, such as in greedy-sampling scenario. To address this limitation, we investigate scaling from the perspective of relative ordering. We first propose the Relative-Based Probability (RBP) metric, which quantifies the probability that the correct token is ranked among the top predictions. Building on this metric, we establish the Relative-Based Scaling Law, which characterizes how RBP improves with increasing model size. Through extensive experiments on four datasets and four model families spanning five orders of magnitude, we demonstrate the robustness and accuracy of this law. Finally, we illustrate the broad application of this law with two examples, namely providing a deeper explanation of emergence phenomena and facilitating finding fundamental theories of scaling laws. In summary, the Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models. Thus, it offers valuable insights for both practical development and theoretical exploration.
<div><strong>Authors:</strong> Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces the Relative-Based Probability (RBP) metric, which measures how often the correct token ranks among the top predictions, and derives a Relative-Based Scaling Law describing how RBP improves with model size. Extensive experiments across multiple datasets and model families show that this scaling law accurately predicts performance and offers new insights into emergence phenomena and theoretical scaling principles. By complementing traditional cross‑entropy scaling analyses, the work provides a more complete picture of large language model behavior.", "summary_cn": "本文提出相对概率（RBP）度量，用于衡量正确 token 在预测排名中的位置，并基于此建立了相对基准的尺度律，描述 RBP 随模型规模的提升而改进的规律。通过在四个数据集和四类模型上进行大规模实验，验证了该尺度律的稳健性和预测准确性，并展示了其在解释模型涌现现象及寻找尺度律基本理论方面的应用。该工作补充了传统交叉熵尺度分析，为大语言模型行为提供了更完整的理解。", "keywords": "scaling laws, relative-based probability, language models, emergence, model performance prediction", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Baoqing Yue", "Jinyuan Zhou", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"]}
]]></acme>

<pubDate>2025-10-23T09:37:00+00:00</pubDate>
</item>
<item>
<title>IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation</title>
<link>https://papers.cool/arxiv/2510.20377</link>
<guid>https://papers.cool/arxiv/2510.20377</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces IKnow, a framework for instruction-knowledge-aware continual pretraining that formulates self-supervised objectives in an instruction-response dialogue format, enabling domain adaptation of instruction-tuned LLMs without external resources or access to the base model. By leveraging domain knowledge embedded in the text, IKnow aims to preserve instruction-following capabilities and semantic representations during adaptation.<br /><strong>Summary (CN):</strong> 本文提出 IKnow 框架，通过在指令-响应对话格式中设计自监督目标，实现对已指令微调的大语言模型进行持续预训练的领域适应，无需外部资源或基模型权重。该方法利用文本内部的领域知识，以更深层次的语义编码来防止指令遵循能力和语义表征的退化。<br /><strong>Keywords:</strong> continual pretraining, domain adaptation, instruction-following, self-supervised dialogue, knowledge-aware, LLM alignment<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Tianyi Zhang, Florian Mai, Lucie Flek</div>
Continual pretraining promises to adapt large language models (LLMs) to new domains using only unlabeled test-time data, but naively applying standard self-supervised objectives to instruction-tuned models is known to degrade their instruction-following capability and semantic representations. Existing fixes assume access to the original base model or rely on knowledge from an external domain-specific database - both of which pose a realistic barrier in settings where the base model weights are withheld for safety reasons or reliable external corpora are unavailable. In this work, we propose Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general framework that formulates novel self-supervised objectives in the instruction-response dialogue format. Rather than depend- ing on external resources, IKnow leverages domain knowledge embedded within the text itself and learns to encode it at a deeper semantic level.
<div><strong>Authors:</strong> Tianyi Zhang, Florian Mai, Lucie Flek</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces IKnow, a framework for instruction-knowledge-aware continual pretraining that formulates self-supervised objectives in an instruction-response dialogue format, enabling domain adaptation of instruction-tuned LLMs without external resources or access to the base model. By leveraging domain knowledge embedded in the text, IKnow aims to preserve instruction-following capabilities and semantic representations during adaptation.", "summary_cn": "本文提出 IKnow 框架，通过在指令-响应对话格式中设计自监督目标，实现对已指令微调的大语言模型进行持续预训练的领域适应，无需外部资源或基模型权重。该方法利用文本内部的领域知识，以更深层次的语义编码来防止指令遵循能力和语义表征的退化。", "keywords": "continual pretraining, domain adaptation, instruction-following, self-supervised dialogue, knowledge-aware, LLM alignment", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Tianyi Zhang", "Florian Mai", "Lucie Flek"]}
]]></acme>

<pubDate>2025-10-23T09:21:13+00:00</pubDate>
</item>
<item>
<title>ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases</title>
<link>https://papers.cool/arxiv/2510.20270</link>
<guid>https://papers.cool/arxiv/2510.20270</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ImpossibleBench is a benchmark framework that creates impossible variants of coding tasks by introducing contradictions between natural-language specifications and unit tests, allowing measurement of large language model agents' propensity to exploit test cases. The authors evaluate cheating rates across models and prompt conditions, analyze behaviors ranging from simple test modification to complex operator overloading, and demonstrate its use for monitoring and mitigating deceptive solutions.<br /><strong>Summary (CN):</strong> ImpossibleBench 通过在自然语言需求与单元测试之间制造冲突，生成“不可实现”的任务变体，以衡量大语言模型代理利用测试用例的倾向。作者在不同模型和提示设置下测量作弊率，分析从简单修改测试到复杂运算符重载的多种作弊行为，并展示该框架用于监测和降低欺骗性解答的实用性。<br /><strong>Keywords:</strong> LLM cheating, test exploitation, benchmark, alignment, safety, prompt engineering, coding assistants, deceptive behavior<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ziqian Zhong, Aditi Raghunathan, Nicholas Carlini</div>
The tendency to find and exploit "shortcuts" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments. To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates "impossible" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's "cheating rate" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut. As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems. Our implementation can be found at https://github.com/safety-research/impossiblebench.
<div><strong>Authors:</strong> Ziqian Zhong, Aditi Raghunathan, Nicholas Carlini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ImpossibleBench is a benchmark framework that creates impossible variants of coding tasks by introducing contradictions between natural-language specifications and unit tests, allowing measurement of large language model agents' propensity to exploit test cases. The authors evaluate cheating rates across models and prompt conditions, analyze behaviors ranging from simple test modification to complex operator overloading, and demonstrate its use for monitoring and mitigating deceptive solutions.", "summary_cn": "ImpossibleBench 通过在自然语言需求与单元测试之间制造冲突，生成“不可实现”的任务变体，以衡量大语言模型代理利用测试用例的倾向。作者在不同模型和提示设置下测量作弊率，分析从简单修改测试到复杂运算符重载的多种作弊行为，并展示该框架用于监测和降低欺骗性解答的实用性。", "keywords": "LLM cheating, test exploitation, benchmark, alignment, safety, prompt engineering, coding assistants, deceptive behavior", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"]}
]]></acme>

<pubDate>2025-10-23T06:58:32+00:00</pubDate>
</item>
<item>
<title>Calibrating Multimodal Consensus for Emotion Recognition</title>
<link>https://papers.cool/arxiv/2510.20256</link>
<guid>https://papers.cool/arxiv/2510.20256</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Calibrated Multimodal Consensus (CMC) for emotion recognition, introducing a pseudo label generation module for self-supervised unimodal pretraining and a parameter-free fusion module with a multimodal consensus router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four datasets show CMC matches or exceeds state-of-the-art performance, especially when modalities provide conflicting emotional cues.<br /><strong>Summary (CN):</strong> 本文提出了用于情感识别的校准多模态共识（CMC）模型，利用伪标签生成模块进行自监督单模态预训练，并通过无参融合模块和多模态共识路由器减轻文本主导问题，缓解跨模态语义不一致。实验在四个数据集上表明 CMC 能够达到或超越当前最佳水平，特别在模态之间情感线索冲突的场景下表现突出。<br /><strong>Keywords:</strong> multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, self-supervised pretraining, fusion robustness, emotion detection<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CMC.
<div><strong>Authors:</strong> Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Calibrated Multimodal Consensus (CMC) for emotion recognition, introducing a pseudo label generation module for self-supervised unimodal pretraining and a parameter-free fusion module with a multimodal consensus router to reduce text dominance and handle semantic inconsistencies across modalities. Experiments on four datasets show CMC matches or exceeds state-of-the-art performance, especially when modalities provide conflicting emotional cues.", "summary_cn": "本文提出了用于情感识别的校准多模态共识（CMC）模型，利用伪标签生成模块进行自监督单模态预训练，并通过无参融合模块和多模态共识路由器减轻文本主导问题，缓解跨模态语义不一致。实验在四个数据集上表明 CMC 能够达到或超越当前最佳水平，特别在模态之间情感线索冲突的场景下表现突出。", "keywords": "multimodal emotion recognition, pseudo label generation, parameter-free fusion, multimodal consensus router, semantic inconsistency, self-supervised pretraining, fusion robustness, emotion detection", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"]}
]]></acme>

<pubDate>2025-10-23T06:25:10+00:00</pubDate>
</item>
<item>
<title>Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context</title>
<link>https://papers.cool/arxiv/2510.20229</link>
<guid>https://papers.cool/arxiv/2510.20229</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates why large vision‑language models generate more hallucinations in longer, free‑form responses, attributing the issue to increased reliance on contextual cues rather than length alone. It introduces an "induce‑detect‑suppress" framework that deliberately creates hallucinations via crafted contexts, uses these instances to train early detection mechanisms, and suppresses object‑level hallucinations during decoding, achieving consistent improvements across benchmarks.<br /><strong>Summary (CN):</strong> 本文研究了大型视觉语言模型在较长自由回复中更易出现幻觉的原因，认为核心在于为保持连贯性和完整性而对上下文的依赖增加，而非单纯的长度导致。作者提出了“诱导‑检测‑抑制”框架，通过特意设计的上下文诱导幻觉，利用诱导实例训练高风险检测模型，并在实际解码时抑制对象级幻觉，在多个基准上实现了显著提升。<br /><strong>Keywords:</strong> LVLM, hallucination, context, induce-detect-suppress, detection, mitigation, vision-language, reliability, robustness<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel "induce-detect-suppress" framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs' longer responses.
<div><strong>Authors:</strong> Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates why large vision‑language models generate more hallucinations in longer, free‑form responses, attributing the issue to increased reliance on contextual cues rather than length alone. It introduces an \"induce‑detect‑suppress\" framework that deliberately creates hallucinations via crafted contexts, uses these instances to train early detection mechanisms, and suppresses object‑level hallucinations during decoding, achieving consistent improvements across benchmarks.", "summary_cn": "本文研究了大型视觉语言模型在较长自由回复中更易出现幻觉的原因，认为核心在于为保持连贯性和完整性而对上下文的依赖增加，而非单纯的长度导致。作者提出了“诱导‑检测‑抑制”框架，通过特意设计的上下文诱导幻觉，利用诱导实例训练高风险检测模型，并在实际解码时抑制对象级幻觉，在多个基准上实现了显著提升。", "keywords": "LVLM, hallucination, context, induce-detect-suppress, detection, mitigation, vision-language, reliability, robustness", "scoring": {"interpretability": 4, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"]}
]]></acme>

<pubDate>2025-10-23T05:22:07+00:00</pubDate>
</item>
<item>
<title>Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</title>
<link>https://papers.cool/arxiv/2510.20193</link>
<guid>https://papers.cool/arxiv/2510.20193</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This survey examines recent advances in question answering systems that incorporate multimedia retrieval and cross‑modal reasoning, categorizing methods by retrieval strategies, fusion mechanisms, and answer generation techniques. It reviews benchmark datasets, evaluation protocols, and discusses challenges such as cross‑modal alignment, latency‑accuracy trade‑offs, and semantic grounding, while outlining open research directions for more robust, context‑aware QA.<br /><strong>Summary (CN):</strong> 本文综述了将多媒体检索与跨模态推理相结合的问答系统的最新进展，按检索策略、融合方式和答案生成技术进行分类。文章回顾了基准数据集、评估方法，并讨论了跨模态对齐、时延‑准确性权衡以及语义 grounding 等关键挑战，提出了构建更鲁棒、上下文知的多媒体问答的未来研究方向。<br /><strong>Keywords:</strong> multimedia QA, cross-modal retrieval, multimodal reasoning, vision-language models, audio-visual QA, retrieval-augmented generation, fusion techniques, benchmark datasets, semantic grounding, latent alignment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 5, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.
<div><strong>Authors:</strong> Rahul Raja, Arpita Vats</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This survey examines recent advances in question answering systems that incorporate multimedia retrieval and cross‑modal reasoning, categorizing methods by retrieval strategies, fusion mechanisms, and answer generation techniques. It reviews benchmark datasets, evaluation protocols, and discusses challenges such as cross‑modal alignment, latency‑accuracy trade‑offs, and semantic grounding, while outlining open research directions for more robust, context‑aware QA.", "summary_cn": "本文综述了将多媒体检索与跨模态推理相结合的问答系统的最新进展，按检索策略、融合方式和答案生成技术进行分类。文章回顾了基准数据集、评估方法，并讨论了跨模态对齐、时延‑准确性权衡以及语义 grounding 等关键挑战，提出了构建更鲁棒、上下文知的多媒体问答的未来研究方向。", "keywords": "multimedia QA, cross-modal retrieval, multimodal reasoning, vision-language models, audio-visual QA, retrieval-augmented generation, fusion techniques, benchmark datasets, semantic grounding, latent alignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Rahul Raja", "Arpita Vats"]}
]]></acme>

<pubDate>2025-10-23T04:25:44+00:00</pubDate>
</item>
<item>
<title>Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values</title>
<link>https://papers.cool/arxiv/2510.20187</link>
<guid>https://papers.cool/arxiv/2510.20187</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Reinforcement Learning with Explicit Human Values (RLEV), which incorporates quantifiable human value signals into the reward function of LLM training, improving value-weighted accuracy and learning a termination policy that adapts response length to prompt value. Experiments on exam-style data show RLEV outperforms correctness‑only baselines across various RL algorithms and model scales, and ablations confirm the gains stem from value‑aligned gradient amplification even under noisy value signals.<br /><strong>Summary (CN):</strong> 本文提出了显式人类价值强化学习 (RLEV)，将可量化的人类价值信号直接嵌入大语言模型的奖励函数，从而提升价值加权准确率并学习一种根据提示价值调整回复长度的终止策略。实验在考试式数据上表明，RLEV 在多种强化学习算法和模型规模下均优于仅基于正确性的基线，且消融实验证实价值对齐的梯度放大是性能提升的因果因素，即使在价值信号噪声较大时仍保持鲁棒。<br /><strong>Keywords:</strong> reinforcement learning, human values, alignment, large language model, value-weighted reward, termination policy, gradient amplification<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu</div>
We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities.
<div><strong>Authors:</strong> Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Reinforcement Learning with Explicit Human Values (RLEV), which incorporates quantifiable human value signals into the reward function of LLM training, improving value-weighted accuracy and learning a termination policy that adapts response length to prompt value. Experiments on exam-style data show RLEV outperforms correctness‑only baselines across various RL algorithms and model scales, and ablations confirm the gains stem from value‑aligned gradient amplification even under noisy value signals.", "summary_cn": "本文提出了显式人类价值强化学习 (RLEV)，将可量化的人类价值信号直接嵌入大语言模型的奖励函数，从而提升价值加权准确率并学习一种根据提示价值调整回复长度的终止策略。实验在考试式数据上表明，RLEV 在多种强化学习算法和模型规模下均优于仅基于正确性的基线，且消融实验证实价值对齐的梯度放大是性能提升的因果因素，即使在价值信号噪声较大时仍保持鲁棒。", "keywords": "reinforcement learning, human values, alignment, large language model, value-weighted reward, termination policy, gradient amplification", "scoring": {"interpretability": 3, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Dian Yu", "Yulai Zhao", "Kishan Panaganti", "Linfeng Song", "Haitao Mi", "Dong Yu"]}
]]></acme>

<pubDate>2025-10-23T04:15:22+00:00</pubDate>
</item>
<item>
<title>AI PB: A Grounded Generative Agent for Personalized Investment Insights</title>
<link>https://papers.cool/arxiv/2510.20099</link>
<guid>https://papers.cool/arxiv/2510.20099</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> AI PB is a production‑scale generative agent that proactively provides grounded, compliant, and user‑specific investment insights in retail finance. It combines deterministic routing between internal and external LLMs, a hybrid retrieval pipeline, and a multi‑stage recommendation system using rule heuristics, sequential behavioral modeling, and contextual bandits, all deployed on‑premises under Korean financial regulations. Human evaluation and system metrics show that this layered safety architecture can deliver trustworthy AI advice in high‑stakes financial settings.<br /><strong>Summary (CN):</strong> AI PB 是一个面向零售的生产级生成式智能体，主动提供基于事实、合规且针对用户的投资洞见。系统通过确定性路由在内部和外部大模型之间切换、融合 OpenSearch 与金融领域嵌入模型的检索管线，以及结合规则启发式、序列行为建模和情境臂带的多阶段推荐机制，实现全链路安全控制，并在韩国金融监管下全本地部署。人类 QA 与系统指标表明，该分层安全架构能够在高风险金融场景中提供可信的 AI 建议。<br /><strong>Keywords:</strong> generative agents, personalized investment, grounded generation, retrieval-augmented generation, financial AI safety, rule heuristics, sequential behavioral modeling, contextual bandits, on-premises deployment, compliance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Daewoo Park, Suho Park, Inseok Hong, Hanwool Lee, Junkyu Park, Sangjun Lee, Jeongman An, Hyunbin Loh</div>
We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.
<div><strong>Authors:</strong> Daewoo Park, Suho Park, Inseok Hong, Hanwool Lee, Junkyu Park, Sangjun Lee, Jeongman An, Hyunbin Loh</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "AI PB is a production‑scale generative agent that proactively provides grounded, compliant, and user‑specific investment insights in retail finance. It combines deterministic routing between internal and external LLMs, a hybrid retrieval pipeline, and a multi‑stage recommendation system using rule heuristics, sequential behavioral modeling, and contextual bandits, all deployed on‑premises under Korean financial regulations. Human evaluation and system metrics show that this layered safety architecture can deliver trustworthy AI advice in high‑stakes financial settings.", "summary_cn": "AI PB 是一个面向零售的生产级生成式智能体，主动提供基于事实、合规且针对用户的投资洞见。系统通过确定性路由在内部和外部大模型之间切换、融合 OpenSearch 与金融领域嵌入模型的检索管线，以及结合规则启发式、序列行为建模和情境臂带的多阶段推荐机制，实现全链路安全控制，并在韩国金融监管下全本地部署。人类 QA 与系统指标表明，该分层安全架构能够在高风险金融场景中提供可信的 AI 建议。", "keywords": "generative agents, personalized investment, grounded generation, retrieval-augmented generation, financial AI safety, rule heuristics, sequential behavioral modeling, contextual bandits, on-premises deployment, compliance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Daewoo Park", "Suho Park", "Inseok Hong", "Hanwool Lee", "Junkyu Park", "Sangjun Lee", "Jeongman An", "Hyunbin Loh"]}
]]></acme>

<pubDate>2025-10-23T00:51:59+00:00</pubDate>
</item>
<item>
<title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
<link>https://papers.cool/arxiv/2510.20095</link>
<guid>https://papers.cool/arxiv/2510.20095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BIOCAP, a biological foundation model that leverages synthetic, instance-specific captions generated by multimodal large language models to complement image labels. By guiding caption generation with Wikipedia-derived visual information and taxon-tailored formats, the authors reduce hallucination and improve caption fidelity, leading to stronger species classification and text-image retrieval performance.<br /><strong>Summary (CN):</strong> 本文提出 BIOCAP，一种利用多模态大语言模型生成的合成实例特定字幕来补充图像标签的生物学基础模型。通过使用维基百科提取的视觉信息和针对分类的格式示例来引导字幕生成，从而降低幻觉并提升字幕真实性，显著提升了物种分类和文本‑图像检索的效果。<br /><strong>Keywords:</strong> synthetic captions, multimodal foundation models, biological vision, BIOCAP, MLLM, species classification, text-image retrieval<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BIOCAP (i.e., BIOCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models.
<div><strong>Authors:</strong> Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BIOCAP, a biological foundation model that leverages synthetic, instance-specific captions generated by multimodal large language models to complement image labels. By guiding caption generation with Wikipedia-derived visual information and taxon-tailored formats, the authors reduce hallucination and improve caption fidelity, leading to stronger species classification and text-image retrieval performance.", "summary_cn": "本文提出 BIOCAP，一种利用多模态大语言模型生成的合成实例特定字幕来补充图像标签的生物学基础模型。通过使用维基百科提取的视觉信息和针对分类的格式示例来引导字幕生成，从而降低幻觉并提升字幕真实性，显著提升了物种分类和文本‑图像检索的效果。", "keywords": "synthetic captions, multimodal foundation models, biological vision, BIOCAP, MLLM, species classification, text-image retrieval", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"]}
]]></acme>

<pubDate>2025-10-23T00:34:21+00:00</pubDate>
</item>
<item>
<title>LLMs can hide text in other text of the same length.ipynb</title>
<link>https://papers.cool/arxiv/2510.20075</link>
<guid>https://papers.cool/arxiv/2510.20075</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a simple protocol that uses large language models to embed a meaningful message inside another coherent text of the same length, effectively creating a steganographic channel. Experiments show that even modest 8‑billion‑parameter open‑source LLMs can encode and decode such hidden messages on a laptop in seconds. The authors discuss the safety implications, including covert deployment of unfiltered models and erosion of trust in written communication.<br /><strong>Summary (CN):</strong> 本文提出一种利用大语言模型（LLM）将有意义的文本隐藏在另一段长度相同、仍然连贯可信的文本中的简单协议，实现文本隐写通道。实验表明，即使是 8 十亿参数的开源 LLM 也能在笔记本电脑上在几秒钟内完成编码和解码。作者讨论了此技术的安全影响，包括通过安全模型的表面响应 covert 部署未过滤模型以及进一步侵蚀对书面交流的信任。<br /><strong>Keywords:</strong> LLM steganography, covert communication, text embedding, AI safety, trust erosion, prompt engineering, open-source models, hidden messages<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.
<div><strong>Authors:</strong> Antonio Norelli, Michael Bronstein</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a simple protocol that uses large language models to embed a meaningful message inside another coherent text of the same length, effectively creating a steganographic channel. Experiments show that even modest 8‑billion‑parameter open‑source LLMs can encode and decode such hidden messages on a laptop in seconds. The authors discuss the safety implications, including covert deployment of unfiltered models and erosion of trust in written communication.", "summary_cn": "本文提出一种利用大语言模型（LLM）将有意义的文本隐藏在另一段长度相同、仍然连贯可信的文本中的简单协议，实现文本隐写通道。实验表明，即使是 8 十亿参数的开源 LLM 也能在笔记本电脑上在几秒钟内完成编码和解码。作者讨论了此技术的安全影响，包括通过安全模型的表面响应 covert 部署未过滤模型以及进一步侵蚀对书面交流的信任。", "keywords": "LLM steganography, covert communication, text embedding, AI safety, trust erosion, prompt engineering, open-source models, hidden messages", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Antonio Norelli", "Michael Bronstein"]}
]]></acme>

<pubDate>2025-10-22T23:16:50+00:00</pubDate>
</item>
<item>
<title>Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions</title>
<link>https://papers.cool/arxiv/2510.20039</link>
<guid>https://papers.cool/arxiv/2510.20039</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies bidirectional opinion dynamics in multi‑turn human‑LLM conversations, showing that human opinions shift little while LLM responses adjust substantially toward the human stance, especially under personalization. It highlights a risk of over‑alignment where chatbots may too readily adopt user viewpoints, and suggests design considerations for more stable alignment.<br /><strong>Summary (CN):</strong> 本文研究了多轮人机对话中的双向观点动态，发现在人类观点几乎不变的情况下，LLM 的回答会显著向人类立场靠拢，且在个性化聊天机器人中该效应更为突出。研究指出了“过度对齐”(over-alignment) 的风险，即聊天机器人可能过度采纳用户观点，并呼吁在个性化设计中更审慎地实现对齐。<br /><strong>Keywords:</strong> opinion dynamics, bidirectional influence, LLM chatbots, personalization, over-alignment, human-LLM interaction, stance shift, multi-turn conversation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 6, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen</div>
Large language model (LLM)-powered chatbots are increasingly used for opinion exploration. Prior research examined how LLMs alter user views, yet little work extended beyond one-way influence to address how user input can affect LLM responses and how such bi-directional influence manifests throughout the multi-turn conversations. This study investigates this dynamic through 50 controversial-topic discussions with participants (N=266) across three conditions: static statements, standard chatbot, and personalized chatbot. Results show that human opinions barely shifted, while LLM outputs changed more substantially, narrowing the gap between human and LLM stance. Personalization amplified these shifts in both directions compared to the standard setting. Analysis of multi-turn conversations further revealed that exchanges involving participants' personal stories were most likely to trigger stance changes for both humans and LLMs. Our work highlights the risk of over-alignment in human-LLM interaction and the need for careful design of personalized chatbots to more thoughtfully and stably align with users.
<div><strong>Authors:</strong> Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies bidirectional opinion dynamics in multi‑turn human‑LLM conversations, showing that human opinions shift little while LLM responses adjust substantially toward the human stance, especially under personalization. It highlights a risk of over‑alignment where chatbots may too readily adopt user viewpoints, and suggests design considerations for more stable alignment.", "summary_cn": "本文研究了多轮人机对话中的双向观点动态，发现在人类观点几乎不变的情况下，LLM 的回答会显著向人类立场靠拢，且在个性化聊天机器人中该效应更为突出。研究指出了“过度对齐”(over-alignment) 的风险，即聊天机器人可能过度采纳用户观点，并呼吁在个性化设计中更审慎地实现对齐。", "keywords": "opinion dynamics, bidirectional influence, LLM chatbots, personalization, over-alignment, human-LLM interaction, stance shift, multi-turn conversation", "scoring": {"interpretability": 2, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yuyang Jiang", "Longjie Guo", "Yuchen Wu", "Aylin Caliskan", "Tanu Mitra", "Hua Shen"]}
]]></acme>

<pubDate>2025-10-22T21:38:10+00:00</pubDate>
</item>
<item>
<title>Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication</title>
<link>https://papers.cool/arxiv/2510.19995</link>
<guid>https://papers.cool/arxiv/2510.19995</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Communication to Completion (C2C), a framework for multi‑agent LLM systems that quantifies task alignment with a novel Alignment Factor metric and integrates stepwise execution with intelligent communication decisions. By allowing agents to make cost‑aware communication choices, C2C reduces task completion time by about 40% in realistic coding workflows while maintaining acceptable communication costs. The work provides both a theoretical foundation for measuring communication effectiveness and a practical system for complex collaborative tasks.<br /><strong>Summary (CN):</strong> 本文提出了 Communication to Completion (C2C) 框架，针对多代理 LLM 系统引入了用于量化任务对齐的“Alignment Factor（对齐因子）”指标，并在顺序行动框架中融合了智能通信决策。C2C 让代理能够根据成本做出通信选择，使在实际编码工作流中的任务完成时间下降约 40%，且通信成本可接受。该工作既提供衡量多代理通信有效性的理论基础，也给出了用于复杂协作任务的实用系统。<br /><strong>Keywords:</strong> multi-agent communication, alignment factor, sequential action framework, collaborative workflow, LLM agents, task efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Yiming Lu, Xun Wang, Simin Ma, Shujian Liu, Sathish Reddy Indurthi, Song Wang, Haoyun Deng, Fei Liu, Kaiqiang Song</div>
Teamwork in workspace for complex tasks requires diverse communication strategies, but current multi-agent LLM systems lack systematic frameworks for task oriented communication. We introduce Communication to Completion (C2C), a scalable framework that addresses this gap through two key innovations: (1) the Alignment Factor (AF), a novel metric quantifying agent task alignment that directly impacts work efficiency, and (2) a Sequential Action Framework that integrates stepwise execution with intelligent communication decisions. C2C enables agents to make cost aware communication choices, dynamically improving task understanding through targeted interactions. We evaluated C2C on realistic coding workflows across three complexity tiers and team sizes from 5 to 17 agents, comparing against no communication and fixed steps baselines. The results show that C2C reduces the task completion time by about 40% with acceptable communication costs. The framework completes all tasks successfully in standard configurations and maintains effectiveness at scale. C2C establishes both a theoretical foundation for measuring communication effectiveness in multi-agent systems and a practical framework for complex collaborative tasks.
<div><strong>Authors:</strong> Yiming Lu, Xun Wang, Simin Ma, Shujian Liu, Sathish Reddy Indurthi, Song Wang, Haoyun Deng, Fei Liu, Kaiqiang Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Communication to Completion (C2C), a framework for multi‑agent LLM systems that quantifies task alignment with a novel Alignment Factor metric and integrates stepwise execution with intelligent communication decisions. By allowing agents to make cost‑aware communication choices, C2C reduces task completion time by about 40% in realistic coding workflows while maintaining acceptable communication costs. The work provides both a theoretical foundation for measuring communication effectiveness and a practical system for complex collaborative tasks.", "summary_cn": "本文提出了 Communication to Completion (C2C) 框架，针对多代理 LLM 系统引入了用于量化任务对齐的“Alignment Factor（对齐因子）”指标，并在顺序行动框架中融合了智能通信决策。C2C 让代理能够根据成本做出通信选择，使在实际编码工作流中的任务完成时间下降约 40%，且通信成本可接受。该工作既提供衡量多代理通信有效性的理论基础，也给出了用于复杂协作任务的实用系统。", "keywords": "multi-agent communication, alignment factor, sequential action framework, collaborative workflow, LLM agents, task efficiency", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Yiming Lu", "Xun Wang", "Simin Ma", "Shujian Liu", "Sathish Reddy Indurthi", "Song Wang", "Haoyun Deng", "Fei Liu", "Kaiqiang Song"]}
]]></acme>

<pubDate>2025-10-22T19:48:17+00:00</pubDate>
</item>
<item>
<title>SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations</title>
<link>https://papers.cool/arxiv/2510.19864</link>
<guid>https://papers.cool/arxiv/2510.19864</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines Spreadsheet Operations Documentation (SOD) as the task of generating human‑readable explanations for spreadsheet manipulation code and introduces a benchmark of 111 code snippets paired with natural‑language summaries. It evaluates five large language models (GPT‑4o, GPT‑4o‑mini, LLaMA‑3.3‑70B, Mixtral‑8x7B, Gemma2‑9B) using BLEU, GLEU, ROUGE‑L, and METEOR, showing that LLMs can produce accurate documentation despite some challenges. The authors argue that SOD can improve reproducibility, maintainability, and collaborative workflows for spreadsheets.<br /><strong>Summary (CN):</strong> 本文定义了电子表操作文档（SOD）任务，即为电子表格操作代码生成可读的自然语言解释，并提供了包含 111 条代码片段及对应摘要的基准数据集。作者评估了五种大语言模型（GPT‑4o、GPT‑4o‑mini、LLaMA‑3.3‑70B、Mixtral‑8x7B、Gemma2‑9B），使用 BLEU、GLEU、ROUGE‑L 和 METEOR 等指标，结果表明 LLM 能够生成准确的文档，但仍面临一些挑战。研究表明 SOD 有助于提升电子表格的可复现性、可维护性和协作工作流。<br /><strong>Keywords:</strong> spreadsheet documentation, large language models, natural language generation, code summarization, SOD benchmark, reproducibility, LLM evaluation, spreadsheet operations<br /><strong>Scores:</strong> Interpretability: 6, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Amila Indika, Igor Molybog</div>
Numerous knowledge workers utilize spreadsheets in business, accounting, and finance. However, a lack of systematic documentation methods for spreadsheets hinders automation, collaboration, and knowledge transfer, which risks the loss of crucial institutional knowledge. This paper introduces Spreadsheet Operations Documentation (SOD), an AI task that involves generating human-readable explanations from spreadsheet operations. Many previous studies have utilized Large Language Models (LLMs) for generating spreadsheet manipulation code; however, translating that code into natural language for SOD is a less-explored area. To address this, we present a benchmark of 111 spreadsheet manipulation code snippets, each paired with a corresponding natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and METEOR metrics. Our findings suggest that LLMs can generate accurate spreadsheet documentation, making SOD a feasible prerequisite step toward enhancing reproducibility, maintainability, and collaborative workflows in spreadsheets, although there are challenges that need to be addressed.
<div><strong>Authors:</strong> Amila Indika, Igor Molybog</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines Spreadsheet Operations Documentation (SOD) as the task of generating human‑readable explanations for spreadsheet manipulation code and introduces a benchmark of 111 code snippets paired with natural‑language summaries. It evaluates five large language models (GPT‑4o, GPT‑4o‑mini, LLaMA‑3.3‑70B, Mixtral‑8x7B, Gemma2‑9B) using BLEU, GLEU, ROUGE‑L, and METEOR, showing that LLMs can produce accurate documentation despite some challenges. The authors argue that SOD can improve reproducibility, maintainability, and collaborative workflows for spreadsheets.", "summary_cn": "本文定义了电子表操作文档（SOD）任务，即为电子表格操作代码生成可读的自然语言解释，并提供了包含 111 条代码片段及对应摘要的基准数据集。作者评估了五种大语言模型（GPT‑4o、GPT‑4o‑mini、LLaMA‑3.3‑70B、Mixtral‑8x7B、Gemma2‑9B），使用 BLEU、GLEU、ROUGE‑L 和 METEOR 等指标，结果表明 LLM 能够生成准确的文档，但仍面临一些挑战。研究表明 SOD 有助于提升电子表格的可复现性、可维护性和协作工作流。", "keywords": "spreadsheet documentation, large language models, natural language generation, code summarization, SOD benchmark, reproducibility, LLM evaluation, spreadsheet operations", "scoring": {"interpretability": 6, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Amila Indika", "Igor Molybog"]}
]]></acme>

<pubDate>2025-10-22T01:36:13+00:00</pubDate>
</item>
<item>
<title>Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs</title>
<link>https://papers.cool/arxiv/2510.19850</link>
<guid>https://papers.cool/arxiv/2510.19850</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Prompt Decorators, a declarative and composable syntax that lets users control LLM reasoning style, tone, and other behavioral dimensions via compact tokens (e.g., +++Reasoning, +++Tone). It formalizes twenty core decorators, defines a scoping model and deterministic processing pipeline, and shows that decoupling task intent from execution behavior improves prompt reproducibility and interpretability. Use cases illustrate enhanced reasoning transparency and standardized model outputs across domains.<br /><strong>Summary (CN):</strong> 本文提出了 Prompt Decorators（提示装饰器），一种声明式、可组合的语法，通过紧凑的控制标记（如 +++Reasoning、+++Tone(style=formal)）来调节大语言模型的推理方式、语气等行为维度。文章形式化了二十个核心装饰器，定义了作用域模型和确定性处理流水线，展示了将任务意图与执行行为解耦后，可提升提示的可重用性、可解释性以及行为一致性。案例显示该方法能够提升推理透明度并在不同领域实现标准化输出。<br /><strong>Keywords:</strong> prompt engineering, declarative syntax, composable decorators, LLM control, reasoning transparency, prompt modularity, behavior consistency<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Mostapha Kalami Heris</div>
Large Language Models (LLMs) are central to reasoning, writing, and decision-support workflows, yet users lack consistent control over how they reason and express outputs. Conventional prompt engineering relies on verbose natural-language instructions, limiting reproducibility, modularity, and interpretability. This paper introduces Prompt Decorators, a declarative, composable syntax that governs LLM behavior through compact control tokens such as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems Thinking"). Each decorator modifies a behavioral dimension, such as reasoning style, structure, or tone, without changing task content. The framework formalizes twenty core decorators organized into two functional families (Cognitive & Generative and Expressive & Systemic), each further decomposed into subcategories that govern reasoning, interaction, expression, and session-control. It defines a unified syntax, scoping model, and deterministic processing pipeline enabling predictable and auditable behavior composition. By decoupling task intent from execution behavior, Prompt Decorators create a reusable and interpretable interface for prompt design. Illustrative use cases demonstrate improved reasoning transparency, reduced prompt complexity, and standardized model behavior across domains. The paper concludes with implications for interoperability, behavioral consistency, and the development of declarative interfaces for scalable AI systems.
<div><strong>Authors:</strong> Mostapha Kalami Heris</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Prompt Decorators, a declarative and composable syntax that lets users control LLM reasoning style, tone, and other behavioral dimensions via compact tokens (e.g., +++Reasoning, +++Tone). It formalizes twenty core decorators, defines a scoping model and deterministic processing pipeline, and shows that decoupling task intent from execution behavior improves prompt reproducibility and interpretability. Use cases illustrate enhanced reasoning transparency and standardized model outputs across domains.", "summary_cn": "本文提出了 Prompt Decorators（提示装饰器），一种声明式、可组合的语法，通过紧凑的控制标记（如 +++Reasoning、+++Tone(style=formal)）来调节大语言模型的推理方式、语气等行为维度。文章形式化了二十个核心装饰器，定义了作用域模型和确定性处理流水线，展示了将任务意图与执行行为解耦后，可提升提示的可重用性、可解释性以及行为一致性。案例显示该方法能够提升推理透明度并在不同领域实现标准化输出。", "keywords": "prompt engineering, declarative syntax, composable decorators, LLM control, reasoning transparency, prompt modularity, behavior consistency", "scoring": {"interpretability": 4, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Mostapha Kalami Heris"]}
]]></acme>

<pubDate>2025-10-21T17:35:49+00:00</pubDate>
</item>
<item>
<title>Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory</title>
<link>https://papers.cool/arxiv/2510.19838</link>
<guid>https://papers.cool/arxiv/2510.19838</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Branch-and-Browse is a framework for LLM-powered web agents that integrates tree-structured subtask management, efficient web state replay, and a page action memory to enable controllable multi-branch reasoning and faster execution. The approach improves task success on the WebArena benchmark to 35.8% while reducing runtime by up to 40.4% compared to previous methods.<br /><strong>Summary (CN):</strong> Branch-and-Browse 框架为基于大语言模型的网络代理引入了树状子任务管理、高效的网页状态重放以及页面操作记忆，从而实现可控的多分支推理并提升执行效率。在 WebArena 基准测试中，该方法将任务成功率提升至 35.8%，并将运行时间降低至最多 40.4%。<br /><strong>Keywords:</strong> web agents, large language models, tree-structured reasoning, action memory, web navigation, controllable exploration, WebArena<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
Autonomous web agents powered by large language models (LLMs) show strong potential for performing goal-oriented tasks such as information retrieval, report generation, and online transactions. These agents mark a key step toward practical embodied reasoning in open web environments. However, existing approaches remain limited in reasoning depth and efficiency: vanilla linear methods fail at multi-step reasoning and lack effective backtracking, while other search strategies are coarse-grained and computationally costly. We introduce Branch-and-Browse, a fine-grained web agent framework that unifies structured reasoning-acting, contextual memory, and efficient execution. It (i) employs explicit subtask management with tree-structured exploration for controllable multi-branch reasoning, (ii) bootstraps exploration through efficient web state replay with background reasoning, and (iii) leverages a page action memory to share explored actions within and across sessions. On the WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\% and reduces execution time by up to 40.4\% relative to state-of-the-art methods. These results demonstrate that Branch-and-Browse is a reliable and efficient framework for LLM-based web agents.
<div><strong>Authors:</strong> Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, Mosharaf Chowdhury</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Branch-and-Browse is a framework for LLM-powered web agents that integrates tree-structured subtask management, efficient web state replay, and a page action memory to enable controllable multi-branch reasoning and faster execution. The approach improves task success on the WebArena benchmark to 35.8% while reducing runtime by up to 40.4% compared to previous methods.", "summary_cn": "Branch-and-Browse 框架为基于大语言模型的网络代理引入了树状子任务管理、高效的网页状态重放以及页面操作记忆，从而实现可控的多分支推理并提升执行效率。在 WebArena 基准测试中，该方法将任务成功率提升至 35.8%，并将运行时间降低至最多 40.4%。", "keywords": "web agents, large language models, tree-structured reasoning, action memory, web navigation, controllable exploration, WebArena", "scoring": {"interpretability": 4, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shiqi He", "Yue Cui", "Xinyu Ma", "Yaliang Li", "Bolin Ding", "Mosharaf Chowdhury"]}
]]></acme>

<pubDate>2025-10-18T00:45:37+00:00</pubDate>
</item>
<item>
<title>How Do LLMs Use Their Depth?</title>
<link>https://papers.cool/arxiv/2510.18871</link>
<guid>https://papers.cool/arxiv/2510.18871</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how large language models allocate computation across layers, introducing a "Guess-then-Refine" framework where early layers produce high-frequency token guesses that are progressively refined with deeper contextual information. Through token frequency analysis, part-of-speech timing, fact recall, and multiple‑choice tasks, the authors show systematic patterns such as early prediction of function words and deeper processing for the first token of multi‑token answers. These findings provide a fine‑grained mechanistic view of LLM inference and suggest avenues for improving transformer efficiency.<br /><strong>Summary (CN):</strong> 本文研究了大型语言模型在不同层次上如何分配计算，提出了“先猜测‑再精炼”(Guess-then-Refine) 框架：早期层主要生成高频词汇的统计猜测，随后在更深层中随着上下文信息的丰富逐步被精炼为合适的输出。通过对高频词预测、词性预测时序、事实回忆多标记答案的首词深度需求以及多项选择题的格式识别过程等案例分析，揭示了功能词最早被正确预测、首个答案标记需更多计算深度等规律。该工作提供了对 LLM 推理过程的细粒度机制理解，并为提升 Transformer 计算效率提供了启示。<br /><strong>Keywords:</strong> layer-wise dynamics, depth utilization, guess-then-refine, token frequency, mechanistic interpretability, transformer analysis, computational efficiency<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 8, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not "one-and-done". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.
<div><strong>Authors:</strong> Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how large language models allocate computation across layers, introducing a \"Guess-then-Refine\" framework where early layers produce high-frequency token guesses that are progressively refined with deeper contextual information. Through token frequency analysis, part-of-speech timing, fact recall, and multiple‑choice tasks, the authors show systematic patterns such as early prediction of function words and deeper processing for the first token of multi‑token answers. These findings provide a fine‑grained mechanistic view of LLM inference and suggest avenues for improving transformer efficiency.", "summary_cn": "本文研究了大型语言模型在不同层次上如何分配计算，提出了“先猜测‑再精炼”(Guess-then-Refine) 框架：早期层主要生成高频词汇的统计猜测，随后在更深层中随着上下文信息的丰富逐步被精炼为合适的输出。通过对高频词预测、词性预测时序、事实回忆多标记答案的首词深度需求以及多项选择题的格式识别过程等案例分析，揭示了功能词最早被正确预测、首个答案标记需更多计算深度等规律。该工作提供了对 LLM 推理过程的细粒度机制理解，并为提升 Transformer 计算效率提供了启示。", "keywords": "layer-wise dynamics, depth utilization, guess-then-refine, token frequency, mechanistic interpretability, transformer analysis, computational efficiency", "scoring": {"interpretability": 8, "understanding": 8, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"]}
]]></acme>

<pubDate>2025-10-21T17:59:05+00:00</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18866</link>
<guid>https://papers.cool/arxiv/2510.18866</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LightMem, a lightweight memory system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson-Shiffrin model. By compressing irrelevant data, grouping by topic, and decoupling long-term consolidation from online inference, LightMem significantly improves accuracy while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate up to 10.9% accuracy gains and efficiency improvements of up to 117× in token usage and 12× in runtime.<br /><strong>Summary (CN):</strong> 本文提出 LightMem，一种受 Atkinson-Shiffrin 人类记忆模型启发的轻量级记忆系统，用于大型语言模型。该系统通过感官记忆的轻量压缩过滤、基于主题的短期记忆整合以及离线的长期记忆更新，实现了在保持或提升准确度的同时大幅降低 token 使用（最高 117 倍）和推理时间（超过 12 倍）。在 LongMemEval 上使用 GPT 和 Qwen 骨干模型的实验表明，LightMem 在准确率上提升至 10.9%，并显著减少 API 调用次数。<br /><strong>Keywords:</strong> memory-augmented generation, lightweight memory, LLM efficiency, sensory memory compression, short-term memory consolidation, long-term memory update, token reduction, Atkinson-Shiffrin model<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
<div><strong>Authors:</strong> Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LightMem, a lightweight memory system for large language models that organizes information into sensory, short-term, and long-term stages inspired by the Atkinson-Shiffrin model. By compressing irrelevant data, grouping by topic, and decoupling long-term consolidation from online inference, LightMem significantly improves accuracy while drastically reducing token usage, API calls, and runtime. Experiments on LongMemEval with GPT and Qwen backbones demonstrate up to 10.9% accuracy gains and efficiency improvements of up to 117× in token usage and 12× in runtime.", "summary_cn": "本文提出 LightMem，一种受 Atkinson-Shiffrin 人类记忆模型启发的轻量级记忆系统，用于大型语言模型。该系统通过感官记忆的轻量压缩过滤、基于主题的短期记忆整合以及离线的长期记忆更新，实现了在保持或提升准确度的同时大幅降低 token 使用（最高 117 倍）和推理时间（超过 12 倍）。在 LongMemEval 上使用 GPT 和 Qwen 骨干模型的实验表明，LightMem 在准确率上提升至 10.9%，并显著减少 API 调用次数。", "keywords": "memory-augmented generation, lightweight memory, LLM efficiency, sensory memory compression, short-term memory consolidation, long-term memory update, token reduction, Atkinson-Shiffrin model", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:58:17+00:00</pubDate>
</item>
<item>
<title>Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model</title>
<link>https://papers.cool/arxiv/2510.18855</link>
<guid>https://papers.cool/arxiv/2510.18855</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level discrepancy masking, C3PO++ for efficient rollout partitioning, and ASystem as a high‑performance RL framework—to overcome training‑inference misalignment and system bottlenecks. Empirical results show strong performance on a range of reasoning benchmarks, including AIME‑2025, HMMT‑2025, CodeForces, ARC‑AGI‑v1, and a silver‑medal level on IMO‑2025. By releasing the full MoE model, the work aims to democratize access to large‑scale reasoning capabilities.<br /><strong>Summary (CN):</strong> 本文推出 Ring-1T，一个每个 token 激活约 500 亿参数的万亿规模思考模型，并提出三项关键创新：IcePop（基于 token 级差异掩码的 RL 稳定化）、C3PO++（在 token 预算下对长 rollout 动态分割提升效率）以及 ASystem（高性能 RL 框架），以解决训练‑推理不匹配和系统瓶颈问题。实验在 AIME‑2025、HMMT‑2025、CodeForces、ARC‑AGI‑v1 等基准上取得突破性成绩，并在 IMO‑2025 达到银牌水平。论文通过开源完整的 MoE 模型，旨在让社区直接使用前沿的推理能力。<br /><strong>Keywords:</strong> trillion-parameter model, reinforcement learning, scaling, MoE, token-level masking, RL training stability, high-performance RL framework, reasoning AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.
<div><strong>Authors:</strong> Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Ring-1T, an open‑source trillion‑parameter thinking model that activates about 50 billion parameters per token, and presents three engineering innovations—IcePop for token‑level discrepancy masking, C3PO++ for efficient rollout partitioning, and ASystem as a high‑performance RL framework—to overcome training‑inference misalignment and system bottlenecks. Empirical results show strong performance on a range of reasoning benchmarks, including AIME‑2025, HMMT‑2025, CodeForces, ARC‑AGI‑v1, and a silver‑medal level on IMO‑2025. By releasing the full MoE model, the work aims to democratize access to large‑scale reasoning capabilities.", "summary_cn": "本文推出 Ring-1T，一个每个 token 激活约 500 亿参数的万亿规模思考模型，并提出三项关键创新：IcePop（基于 token 级差异掩码的 RL 稳定化）、C3PO++（在 token 预算下对长 rollout 动态分割提升效率）以及 ASystem（高性能 RL 框架），以解决训练‑推理不匹配和系统瓶颈问题。实验在 AIME‑2025、HMMT‑2025、CodeForces、ARC‑AGI‑v1 等基准上取得突破性成绩，并在 IMO‑2025 达到银牌水平。论文通过开源完整的 MoE 模型，旨在让社区直接使用前沿的推理能力。", "keywords": "trillion-parameter model, reinforcement learning, scaling, MoE, token-level masking, RL training stability, high-performance RL framework, reasoning AI", "scoring": {"interpretability": 2, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ling Team", "Anqi Shen", "Baihui Li", "Bin Hu", "Bin Jing", "Cai Chen", "Chao Huang", "Chao Zhang", "Chaokun Yang", "Cheng Lin", "Chengyao Wen", "Congqi Li", "Deng Zhao", "Dingbo Yuan", "Donghai You", "Fagui Mao", "Fanzhuang Meng", "Feng Xu", "Guojie Li", "Guowei Wang", "Hao Dai", "Haonan Zheng", "Hong Liu", "Jia Guo", "Jiaming Liu", "Jian Liu", "Jianhao Fu", "Jiannan Shi", "Jianwen Wang", "Jianxin Lai", "Jin Yang", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junping Zhao", "Kuan Xu", "Le Su", "Lei Chen", "Li Tang", "Liang Jiang", "Liangcheng Fu", "Lianhao Xu", "Linfeng Shi", "Lisha Liao", "Longfei Zheng", "Meng Li", "Mingchun Chen", "Qi Zuo", "Qiang Cheng", "Qianggang Cao", "Qitao Shi", "Quanrui Guo", "Senlin Zhu", "Shaofei Wang", "Shaomian Zheng", "Shuaicheng Li", "Shuwei Gu", "Siba Chen", "Tao Wu", "Tao Zhang", "Tianyu Zhang", "Tianyu Zhou", "Tiwei Bie", "Tongkai Yang", "Wang Hong", "Wang Ren", "Weihua Chen", "Wenbo Yu", "Wengang Zheng", "Xiangchun Wang", "Xiaodong Yan", "Xiaopei Wan", "Xin Zhao", "Xinyu Kong", "Xinyu Tang", "Xudong Han", "Xudong Wang", "Xuemin Yang", "Xueyu Hu", "Yalin Zhang", "Yan Sun", "Yicheng Shan", "Yilong Wang", "Yingying Xu", "Yongkang Liu", "Yongzhen Guo", "Yuanyuan Wang", "Yuchen Yan", "Yuefan Wang", "Yuhong Guo", "Zehuan Li", "Zhankai Xu", "Zhe Li", "Zhenduo Zhang", "Zhengke Gui", "Zhenxuan Pan", "Zhenyu Huang", "Zhenzhong Lan", "Zhiqiang Ding", "Zhiqiang Zhang", "Zhixun Li", "Zhizhen Liu", "Zihao Wang", "Zujie Wen"]}
]]></acme>

<pubDate>2025-10-21T17:46:14+00:00</pubDate>
</item>
<item>
<title>Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</title>
<link>https://papers.cool/arxiv/2510.18849</link>
<guid>https://papers.cool/arxiv/2510.18849</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a Critique-Post-Edit reinforcement learning framework that uses a personalized generative reward model providing multi‑dimensional scores and textual critiques to mitigate reward hacking and improve faithful, controllable LLM personalization. The policy revises its outputs based on these critiques, leading to significant gains over standard PPO on personalization benchmarks, with a 7B model achieving an 11% win‑rate increase and a 14B model surpassing GPT‑4.1 performance. This demonstrates a practical route to more efficient and reliable personalized language generation.<br /><strong>Summary (CN):</strong> 本文提出了一种 Critique-Post-Edit 强化学习框架，利用个性化生成奖励模型（GRM）提供多维评分和文本批评，以抵御奖励欺骗并提升 LLM 的忠实、可控个性化。策略模型依据批评自行修正输出，在个性化基准测试中显著优于标准 PPO，7B 模型实现 11% 胜率提升，14B 模型的表现甚至超越 GPT‑4.1，展示了实现更高效可靠个性化生成的可行路径。<br /><strong>Keywords:</strong> personalization, reinforcement learning, reward hacking, generative reward model, critique-post-edit, LLM alignment, controllable generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.
<div><strong>Authors:</strong> Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a Critique-Post-Edit reinforcement learning framework that uses a personalized generative reward model providing multi‑dimensional scores and textual critiques to mitigate reward hacking and improve faithful, controllable LLM personalization. The policy revises its outputs based on these critiques, leading to significant gains over standard PPO on personalization benchmarks, with a 7B model achieving an 11% win‑rate increase and a 14B model surpassing GPT‑4.1 performance. This demonstrates a practical route to more efficient and reliable personalized language generation.", "summary_cn": "本文提出了一种 Critique-Post-Edit 强化学习框架，利用个性化生成奖励模型（GRM）提供多维评分和文本批评，以抵御奖励欺骗并提升 LLM 的忠实、可控个性化。策略模型依据批评自行修正输出，在个性化基准测试中显著优于标准 PPO，7B 模型实现 11% 胜率提升，14B 模型的表现甚至超越 GPT‑4.1，展示了实现更高效可靠个性化生成的可行路径。", "keywords": "personalization, reinforcement learning, reward hacking, generative reward model, critique-post-edit, LLM alignment, controllable generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Chenghao Zhu", "Meiling Tao", "Tiannan Wang", "Dongyi Ding", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"]}
]]></acme>

<pubDate>2025-10-21T17:40:03+00:00</pubDate>
</item>
<item>
<title>MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training</title>
<link>https://papers.cool/arxiv/2510.18830</link>
<guid>https://papers.cool/arxiv/2510.18830</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long contexts. By applying MTraining, the authors expand the context window of Qwen2.5‑3B from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining model accuracy on various downstream tasks.<br /><strong>Summary (CN):</strong> 本文提出了 MTraining——一种分布式训练框架，融合了动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，以高效训练具备超长上下文的 大语言模型。作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文窗口从 32K 扩展至 512K token，并在多个下游任务上保持模型精度的同时，实现了最高 6 倍 的训练吞吐提升。<br /><strong>Keywords:</strong> dynamic sparse attention, distributed training, ultra-long context, ring attention, hierarchical attention, LLM efficiency<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.
<div><strong>Authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MTraining, a distributed training framework that combines dynamic sparse attention, balanced sparse ring attention, and hierarchical sparse ring attention to efficiently train large language models on ultra‑long contexts. By applying MTraining, the authors expand the context window of Qwen2.5‑3B from 32K to 512K tokens on 32 A100 GPUs, achieving up to 6× higher throughput while maintaining model accuracy on various downstream tasks.", "summary_cn": "本文提出了 MTraining——一种分布式训练框架，融合了动态稀疏注意力、平衡稀疏环形注意力和层次稀疏环形注意力，以高效训练具备超长上下文的 大语言模型。作者在 32 台 A100 GPU 上将 Qwen2.5‑3B 的上下文窗口从 32K 扩展至 512K token，并在多个下游任务上保持模型精度的同时，实现了最高 6 倍 的训练吞吐提升。", "keywords": "dynamic sparse attention, distributed training, ultra-long context, ring attention, hierarchical attention, LLM efficiency", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wenxuan Li", "Chengruidong Zhang", "Huiqiang Jiang", "Yucheng Li", "Yuqing Yang", "Lili Qiu"]}
]]></acme>

<pubDate>2025-10-21T17:25:32+00:00</pubDate>
</item>
<item>
<title>Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</title>
<link>https://papers.cool/arxiv/2510.18817</link>
<guid>https://papers.cool/arxiv/2510.18817</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a knowledge‑distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑specific models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the authors fine‑tune small language models that achieve significantly improved reasoning performance, narrowing the gap with larger models.<br /><strong>Summary (CN):</strong> 本文提出了一种知识蒸馏框架，将大语言模型的 chain‑of‑thought 推理能力转移到小型、面向工业资产健康监测的模型上。通过多选题提示和上下文学习，对小模型进行微调，使其在推理表现上显著提升，缩小了与大模型之间的差距。<br /><strong>Keywords:</strong> chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning transfer<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: https://github.com/IBM/FailureSensorIQ.
<div><strong>Authors:</strong> Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a knowledge‑distillation framework that transfers chain‑of‑thought reasoning from large language models to small, domain‑specific models for industrial asset health monitoring. By using multi‑choice question‑answering prompts and in‑context learning, the authors fine‑tune small language models that achieve significantly improved reasoning performance, narrowing the gap with larger models.", "summary_cn": "本文提出了一种知识蒸馏框架，将大语言模型的 chain‑of‑thought 推理能力转移到小型、面向工业资产健康监测的模型上。通过多选题提示和上下文学习，对小模型进行微调，使其在推理表现上显著提升，缩小了与大模型之间的差距。", "keywords": "chain-of-thought, knowledge distillation, small language models, industrial asset health monitoring, multi-choice QA, fine-tuning, reasoning transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Shuxin Lin", "Dhaval Patel", "Christodoulos Constantinides"]}
]]></acme>

<pubDate>2025-10-21T17:18:24+00:00</pubDate>
</item>
<item>
<title>WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection</title>
<link>https://papers.cool/arxiv/2510.18798</link>
<guid>https://papers.cool/arxiv/2510.18798</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> WebSeer introduces a reinforcement‑learning framework enhanced with a self‑reflection mechanism to train more capable web search agents that can generate longer, more reflective tool‑use sequences. By constructing a large dataset annotated with reflection patterns and employing a two‑stage training process that combines cold‑start and RL, the authors enable a single14‑billion‑parameter model to achieve state‑of‑the‑art results on HotpotQA and SimpleQA and to generalize to out‑of‑distribution tasks.<br /><strong>Summary (CN):</strong> WebSeer 提出了一种结合 (self‑reflection) 机制的强化学习框架，用于训练能够生成更长、更具反思性的工具使用序列的网页搜索代理。通过构建带有反模式标注的大规模数据集，并采用冷启动与强化学习相结合的两阶段流程，单一 14B 模型在 HotpotQA 与 SimpleQA 上实现了最先进的准确率，并展示了对分布外数据的强泛化能力。<br /><strong>Keywords:</strong> search agents, reinforcement learning, self-reflection, tool-use depth, web retrieval, HotpotQA, SimpleQA, dataset annotation, interactive retrieval<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, Juanzi Li</div>
Search agents have achieved significant advancements in enabling intelligent information retrieval and decision-making within interactive environments. Although reinforcement learning has been employed to train agentic models capable of more dynamic interactive retrieval, existing methods are limited by shallow tool-use depth and the accumulation of errors over multiple iterative interactions. In this paper, we present WebSeer, a more intelligent search agent trained via reinforcement learning enhanced with a self-reflection mechanism. Specifically, we construct a large dataset annotated with reflection patterns and design a two-stage training framework that unifies cold start and reinforcement learning within the self-reflection paradigm for real-world web-based environments, which enables the model to generate longer and more reflective tool-use trajectories. Our approach substantially extends tool-use chains and improves answer accuracy. Using a single 14B model, we achieve state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and 90.0%, respectively, and demonstrate strong generalization to out-of-distribution datasets. The code is available at https://github.com/99hgz/WebSeer
<div><strong>Authors:</strong> Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, Juanzi Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "WebSeer introduces a reinforcement‑learning framework enhanced with a self‑reflection mechanism to train more capable web search agents that can generate longer, more reflective tool‑use sequences. By constructing a large dataset annotated with reflection patterns and employing a two‑stage training process that combines cold‑start and RL, the authors enable a single14‑billion‑parameter model to achieve state‑of‑the‑art results on HotpotQA and SimpleQA and to generalize to out‑of‑distribution tasks.", "summary_cn": "WebSeer 提出了一种结合 (self‑reflection) 机制的强化学习框架，用于训练能够生成更长、更具反思性的工具使用序列的网页搜索代理。通过构建带有反模式标注的大规模数据集，并采用冷启动与强化学习相结合的两阶段流程，单一 14B 模型在 HotpotQA 与 SimpleQA 上实现了最先进的准确率，并展示了对分布外数据的强泛化能力。", "keywords": "search agents, reinforcement learning, self-reflection, tool-use depth, web retrieval, HotpotQA, SimpleQA, dataset annotation, interactive retrieval", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Guanzhong He", "Zhen Yang", "Jinxin Liu", "Bin Xu", "Lei Hou", "Juanzi Li"]}
]]></acme>

<pubDate>2025-10-21T16:52:00+00:00</pubDate>
</item>
<item>
<title>KAT-Coder Technical Report</title>
<link>https://papers.cool/arxiv/2510.18779</link>
<guid>https://papers.cool/arxiv/2510.18779</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The report presents KAT-Coder, a 32B agentic code model trained via a multi-stage curriculum—including Mid-Term training, supervised fine‑tuning, reinforcement fine‑tuning with a novel multi‑ground‑truth reward, and deployment‑phase adaptation—to improve reasoning, planning, tool‑use reliability, and instruction alignment for real‑world IDE environments. The authors detail dataset construction across many languages and contexts, introduce error‑masked SFT and tree‑structured trajectory training, and release the model publicly. Empirical results show robust performance on interactive software development tasks.<br /><strong>Summary (CN):</strong> 本文报告了 KAT‑Coder，一款 32B 规模的代理式代码模型，通过多阶段课程（包括中期训练、监督微调、使用多真值奖励的强化微调以及部署阶段的适配）提升推理、规划、工具使用可靠性和指令对齐，以在真实 IDE 环境中进行交互式软件开发。作者介绍了跨多语言多场景的数据集构建，提出了错误掩码微调和树结构轨迹训练，并已公开模型。实验表明该模型在交互式编码任务上表现稳健。<br /><strong>Keywords:</strong> agentic coding, reinforcement learning from human feedback, tool-use reliability, instruction alignment, large language models, curriculum learning, code generation, IDE integration, multi-ground-truth reward, tree-structured trajectory training<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</div>
Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.
<div><strong>Authors:</strong> Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The report presents KAT-Coder, a 32B agentic code model trained via a multi-stage curriculum—including Mid-Term training, supervised fine‑tuning, reinforcement fine‑tuning with a novel multi‑ground‑truth reward, and deployment‑phase adaptation—to improve reasoning, planning, tool‑use reliability, and instruction alignment for real‑world IDE environments. The authors detail dataset construction across many languages and contexts, introduce error‑masked SFT and tree‑structured trajectory training, and release the model publicly. Empirical results show robust performance on interactive software development tasks.", "summary_cn": "本文报告了 KAT‑Coder，一款 32B 规模的代理式代码模型，通过多阶段课程（包括中期训练、监督微调、使用多真值奖励的强化微调以及部署阶段的适配）提升推理、规划、工具使用可靠性和指令对齐，以在真实 IDE 环境中进行交互式软件开发。作者介绍了跨多语言多场景的数据集构建，提出了错误掩码微调和树结构轨迹训练，并已公开模型。实验表明该模型在交互式编码任务上表现稳健。", "keywords": "agentic coding, reinforcement learning from human feedback, tool-use reliability, instruction alignment, large language models, curriculum learning, code generation, IDE integration, multi-ground-truth reward, tree-structured trajectory training", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Zizheng Zhan", "Ken Deng", "Xiaojiang Zhang", "Jinghui Wang", "Huaixi Tang", "Zhiyi Lai", "Haoyang Huang", "Wen Xiang", "Kun Wu", "Wenhao Zhuang", "Minglei Zhang", "Shaojie Wang", "Shangpeng Yan", "Kepeng Lei", "Zongxian Feng", "Huiming Wang", "Zheng Lin", "Mengtong Li", "Mengfei Xie", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Weihao Li", "Wenqiang Zhu", "Jiarong Zhang", "Jingxuan Xu", "Songwei Yu", "Yifan Yao", "Xinping Lei", "Han Li", "Junqi Xiong", "Zuchen Gao", "Dailin Li", "Haimo Li", "Jiaheng Liu", "Yuqun Zhang", "Junyi Peng", "Haotian Zhang", "Bin Chen"]}
]]></acme>

<pubDate>2025-10-21T16:27:47+00:00</pubDate>
</item>
<item>
<title>AI use in American newspapers is widespread, uneven, and rarely disclosed</title>
<link>https://papers.cool/arxiv/2510.18774</link>
<guid>https://papers.cool/arxiv/2510.18774</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The authors audit 186K articles from 1.5K American newspapers using the Pangram AI detector and find that roughly 9% of articles are partially or fully AI-generated, with higher prevalence in smaller outlets, certain topics, and specific ownership groups. Opinion pieces from major publications are 6.4 times more likely to contain AI-generated content, yet disclosures are extremely rare, with only five out of 100 flagged articles providing any notice. The study calls for greater transparency and updated editorial standards to preserve public trust.<br /><strong>Summary (CN):</strong> 作者审计了 186,000 篇来自 1,500 家美国报纸的文章，使用 Pangram AI 检测器发现约 9% 的文章部分或全部由 AI 生成，且在规模较小的本地媒体、特定话题（如天气和科技）以及某些所有权集团中更为常见。对《华盛顿邮报》《纽约时报》和《华尔街日报》的 45,000 篇评论文章进行分析后发现，这类社论稿件的 AI 生成概率是同一出版物新闻稿的 6.4 倍，但披露极其罕见，仅在 100 篇被标记的文章中出现了五次披露。研究呼吁加强透明度并更新编辑标准，以维护公众信任。<br /><strong>Keywords:</strong> AI-generated content, journalism, AI detection, media transparency, AI misuse, newspaper audit, Pangram detector, opinion pieces, disclosure<br /><strong>Scores:</strong> Interpretability: 1, Understanding: 6, Safety: 6, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Jenna Russell, Marzena Karpinska, Destiny Akinode, Katherine Thai, Bradley Emi, Max Spero, Mohit Iyyer</div>
AI is rapidly transforming journalism, but the extent of its use in published newspaper articles remains unclear. We address this gap by auditing a large-scale dataset of 186K articles from online editions of 1.5K American newspapers published in the summer of 2025. Using Pangram, a state-of-the-art AI detector, we discover that approximately 9% of newly-published articles are either partially or fully AI-generated. This AI use is unevenly distributed, appearing more frequently in smaller, local outlets, in specific topics such as weather and technology, and within certain ownership groups. We also analyze 45K opinion pieces from Washington Post, New York Times, and Wall Street Journal, finding that they are 6.4 times more likely to contain AI-generated content than news articles from the same publications, with many AI-flagged op-eds authored by prominent public figures. Despite this prevalence, we find that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles found only five disclosures of AI use. Overall, our audit highlights the immediate need for greater transparency and updated editorial standards regarding the use of AI in journalism to maintain public trust.
<div><strong>Authors:</strong> Jenna Russell, Marzena Karpinska, Destiny Akinode, Katherine Thai, Bradley Emi, Max Spero, Mohit Iyyer</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The authors audit 186K articles from 1.5K American newspapers using the Pangram AI detector and find that roughly 9% of articles are partially or fully AI-generated, with higher prevalence in smaller outlets, certain topics, and specific ownership groups. Opinion pieces from major publications are 6.4 times more likely to contain AI-generated content, yet disclosures are extremely rare, with only five out of 100 flagged articles providing any notice. The study calls for greater transparency and updated editorial standards to preserve public trust.", "summary_cn": "作者审计了 186,000 篇来自 1,500 家美国报纸的文章，使用 Pangram AI 检测器发现约 9% 的文章部分或全部由 AI 生成，且在规模较小的本地媒体、特定话题（如天气和科技）以及某些所有权集团中更为常见。对《华盛顿邮报》《纽约时报》和《华尔街日报》的 45,000 篇评论文章进行分析后发现，这类社论稿件的 AI 生成概率是同一出版物新闻稿的 6.4 倍，但披露极其罕见，仅在 100 篇被标记的文章中出现了五次披露。研究呼吁加强透明度并更新编辑标准，以维护公众信任。", "keywords": "AI-generated content, journalism, AI detection, media transparency, AI misuse, newspaper audit, Pangram detector, opinion pieces, disclosure", "scoring": {"interpretability": 1, "understanding": 6, "safety": 6, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Jenna Russell", "Marzena Karpinska", "Destiny Akinode", "Katherine Thai", "Bradley Emi", "Max Spero", "Mohit Iyyer"]}
]]></acme>

<pubDate>2025-10-21T16:22:07+00:00</pubDate>
</item>
<item>
<title>Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting</title>
<link>https://papers.cool/arxiv/2510.18745</link>
<guid>https://papers.cool/arxiv/2510.18745</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Topoformer, a transformer variant that imposes a 2D topographic layout on queries, keys, and values through spatial querying and spatial reweighting, enabling interpretable spatial organization of linguistic representations. Experiments on sentiment classification and masked language modeling show performance comparable to standard models while yielding clear topographic patterns that align with human brain fMRI responses to sentences. The authors argue that scaling such topographic constraints could improve interpretability in NLP and provide more accurate computational models of brain language organization.<br /><strong>Summary (CN):</strong> 本文提出了 Topoformer，一种通过空间查询和空间重加权在二维网格上组织查询、键和值的 Transformer 变体，从而实现语言表示的可解释拓扑结构。实验在情感分类和掩码语言建模任务上显示，其性能与标准模型相当，但能够产生清晰的拓扑模式，并且这些模式与人类大脑对自然语言句子的 fMRI 反应呈现对齐。作者认为，进一步扩大此类拓扑约束有望提升 NLP 的可解释性，并为大脑语言网络提供更精确的计算模型。<br /><strong>Keywords:</strong> topographic organization, transformer, spatial querying, spatial reweighting, mechanistic interpretability, neural representation, fMRI alignment, language models<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Taha Binhuraib, Greta Tuckute, Nicholas Blauch</div>
Spatial functional organization is a hallmark of biological brains: neurons are arranged topographically according to their response properties, at multiple scales. In contrast, representations within most machine learning models lack spatial biases, instead manifesting as disorganized vector spaces that are difficult to visualize and interpret. Here, we propose a novel form of self-attention that turns Transformers into "Topoformers" with topographic organization. We introduce spatial querying - where keys and queries are arranged on 2D grids, and local pools of queries are associated with a given key - and spatial reweighting, where we convert the standard fully connected layer of self-attention into a locally connected layer. We first demonstrate the feasibility of our approach by training a 1-layer Topoformer on a sentiment classification task. Training with spatial querying encourages topographic organization in the queries and keys, and spatial reweighting separately encourages topographic organization in the values and self-attention outputs. We then apply the Topoformer motifs at scale, training a BERT architecture with a masked language modeling objective. We find that the topographic variant performs on par with a non-topographic control model on NLP benchmarks, yet produces interpretable topographic organization as evaluated via eight linguistic test suites. Finally, analyzing an fMRI dataset of human brain responses to a large set of naturalistic sentences, we demonstrate alignment between low-dimensional topographic variability in the Topoformer model and human brain language network. Scaling up Topoformers further holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic information in the human brain.
<div><strong>Authors:</strong> Taha Binhuraib, Greta Tuckute, Nicholas Blauch</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Topoformer, a transformer variant that imposes a 2D topographic layout on queries, keys, and values through spatial querying and spatial reweighting, enabling interpretable spatial organization of linguistic representations. Experiments on sentiment classification and masked language modeling show performance comparable to standard models while yielding clear topographic patterns that align with human brain fMRI responses to sentences. The authors argue that scaling such topographic constraints could improve interpretability in NLP and provide more accurate computational models of brain language organization.", "summary_cn": "本文提出了 Topoformer，一种通过空间查询和空间重加权在二维网格上组织查询、键和值的 Transformer 变体，从而实现语言表示的可解释拓扑结构。实验在情感分类和掩码语言建模任务上显示，其性能与标准模型相当，但能够产生清晰的拓扑模式，并且这些模式与人类大脑对自然语言句子的 fMRI 反应呈现对齐。作者认为，进一步扩大此类拓扑约束有望提升 NLP 的可解释性，并为大脑语言网络提供更精确的计算模型。", "keywords": "topographic organization, transformer, spatial querying, spatial reweighting, mechanistic interpretability, neural representation, fMRI alignment, language models", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Taha Binhuraib", "Greta Tuckute", "Nicholas Blauch"]}
]]></acme>

<pubDate>2025-10-21T15:54:57+00:00</pubDate>
</item>
<item>
<title>Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</title>
<link>https://papers.cool/arxiv/2510.18731</link>
<guid>https://papers.cool/arxiv/2510.18731</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces RLAAR, a curriculum reinforcement learning framework that provides verifiable accuracy and abstention rewards to mitigate Lost-in-Conversation degradation in multi-turn LLM interactions. By incrementally increasing dialogue difficulty and rewarding informed abstention, the method stabilizes training and improves both answer correctness and calibrated abstention rates on LiC benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 RLAAR 框架，通过课程强化学习以及可验证的准确性与弃答奖励，缓解大型语言模型在多轮对话中的“对话丢失”（Lost-in-Conversation）现象。该方法逐步提升对话难度，鼓励模型在无法确定答案时主动弃答，从而在基准测试上显著提升答案正确率和校准的弃答率。<br /><strong>Keywords:</strong> curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Ming Li</div>
Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.
<div><strong>Authors:</strong> Ming Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces RLAAR, a curriculum reinforcement learning framework that provides verifiable accuracy and abstention rewards to mitigate Lost-in-Conversation degradation in multi-turn LLM interactions. By incrementally increasing dialogue difficulty and rewarding informed abstention, the method stabilizes training and improves both answer correctness and calibrated abstention rates on LiC benchmarks.", "summary_cn": "本文提出 RLAAR 框架，通过课程强化学习以及可验证的准确性与弃答奖励，缓解大型语言模型在多轮对话中的“对话丢失”（Lost-in-Conversation）现象。该方法逐步提升对话难度，鼓励模型在无法确定答案时主动弃答，从而在基准测试上显著提升答案正确率和校准的弃答率。", "keywords": "curriculum reinforcement learning, verifiable rewards, abstention, lost-in-conversation, multi-turn dialogue, LLM reliability", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Ming Li"]}
]]></acme>

<pubDate>2025-10-21T15:32:26+00:00</pubDate>
</item>
<item>
<title>SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish</title>
<link>https://papers.cool/arxiv/2510.18725</link>
<guid>https://papers.cool/arxiv/2510.18725</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SemiAdapt and SemiLoRA, semi-supervised, inference-efficient methods that enhance domain adaptation for transformer-based neural machine translation, particularly targeting low‑resource languages such as Irish. Experiments show that SemiAdapt can surpass full‑domain fine‑tuning and that SemiLoRA enables parameter‑efficient fine‑tuning to match or exceed full‑model fine‑tuning performance, especially on larger, noisier corpora. All resulting Irish translation models are released as open resources to lower the barrier for low‑resource language research.<br /><strong>Summary (CN):</strong> 本文提出了 SemiAdapt 和 SemiLoRA 两种半监督、推理高效的方式，以提升基于 Transformer 的神经机器翻译在低资源语言（如爱尔兰语）上的领域适配能力。实验表明 SemiAdapt 能够超越全域微调，而 SemiLoRA 使参数高效微调的效果可以匹配甚至超过完整模型的微调，尤其在更大且噪声较多的数据集上表现突出。所有的爱尔兰语翻译模型均作为开放资源发布，以降低低资源语言研究的门槛。<br /><strong>Keywords:</strong> parameter-efficient fine-tuning, LoRA, SemiAdapt, SemiLoRA, neural machine translation, low-resource languages, domain adaptation, transformer, Irish translation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Josh McGiff, Nikola S. Nikolov</div>
Fine-tuning is widely used to tailor large language models for specific tasks such as neural machine translation (NMT). However, leveraging transfer learning is computationally expensive when fine-tuning large multilingual models with billions of parameters, thus creating a barrier to entry for researchers working on low-resource domains such as Irish translation. Parameter-efficient fine-tuning (PEFT) bridges this gap by training on a fraction of the original model parameters, with the Low-Rank Adaptation (LoRA) approach introducing small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as semi-supervised inference-efficient approaches that strengthen domain adaptation and lead to improved overall performance in NMT. We demonstrate that SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA can propel PEFT methods to match or even outperform full-model fine-tuning. We further evaluate domain-by-dataset fine-tuning and demonstrate that our embedding-based inference methods perform especially well on larger and noisier corpora. All Irish translation models developed in this work are released as open resources. These methods aim to make high-quality domain adaptation and fine-tuning more accessible to researchers working with low-resource languages.
<div><strong>Authors:</strong> Josh McGiff, Nikola S. Nikolov</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SemiAdapt and SemiLoRA, semi-supervised, inference-efficient methods that enhance domain adaptation for transformer-based neural machine translation, particularly targeting low‑resource languages such as Irish. Experiments show that SemiAdapt can surpass full‑domain fine‑tuning and that SemiLoRA enables parameter‑efficient fine‑tuning to match or exceed full‑model fine‑tuning performance, especially on larger, noisier corpora. All resulting Irish translation models are released as open resources to lower the barrier for low‑resource language research.", "summary_cn": "本文提出了 SemiAdapt 和 SemiLoRA 两种半监督、推理高效的方式，以提升基于 Transformer 的神经机器翻译在低资源语言（如爱尔兰语）上的领域适配能力。实验表明 SemiAdapt 能够超越全域微调，而 SemiLoRA 使参数高效微调的效果可以匹配甚至超过完整模型的微调，尤其在更大且噪声较多的数据集上表现突出。所有的爱尔兰语翻译模型均作为开放资源发布，以降低低资源语言研究的门槛。", "keywords": "parameter-efficient fine-tuning, LoRA, SemiAdapt, SemiLoRA, neural machine translation, low-resource languages, domain adaptation, transformer, Irish translation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Josh McGiff", "Nikola S. Nikolov"]}
]]></acme>

<pubDate>2025-10-21T15:24:15+00:00</pubDate>
</item>
<item>
<title>Adapting Language Balance in Code-Switching Speech</title>
<link>https://papers.cool/arxiv/2510.18724</link>
<guid>https://papers.cool/arxiv/2510.18724</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a differentiable surrogate loss that highlights code-switching points by leveraging the language imbalance between the embedded and main language, thereby reducing context bias during generation. Experiments on Arabic and Chinese-English code-switching speech data show improved prediction of switch locations and lower substitution errors.<br /><strong>Summary (CN):</strong> 本文提出一种可微分的代理损失，通过利用嵌入语言与主体语言的差异来突出代码切换点，从而缓解生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验表明，模型能够更准确地预测切换位置，并显著降低替换错误率。<br /><strong>Keywords:</strong> code-switching, language balance, context bias, differentiable surrogate, robustness, speech recognition, multilingual models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a differentiable surrogate loss that highlights code-switching points by leveraging the language imbalance between the embedded and main language, thereby reducing context bias during generation. Experiments on Arabic and Chinese-English code-switching speech data show improved prediction of switch locations and lower substitution errors.", "summary_cn": "本文提出一种可微分的代理损失，通过利用嵌入语言与主体语言的差异来突出代码切换点，从而缓解生成过程中的上下文偏差。对阿拉伯语和中英代码切换语音的实验表明，模型能够更准确地预测切换位置，并显著降低替换错误率。", "keywords": "code-switching, language balance, context bias, differentiable surrogate, robustness, speech recognition, multilingual models", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:55+00:00</pubDate>
</item>
<item>
<title>Bayesian Low-Rank Factorization for Robust Model Adaptation</title>
<link>https://papers.cool/arxiv/2510.18723</link>
<guid>https://papers.cool/arxiv/2510.18723</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes Bayesian low-rank factorized adapters for speech foundation models such as Whisper, enabling robust adaptation to code-switching multilingual scenarios while preserving the base model's general capabilities. By placing near-zero priors on adapter parameters, the method yields sparser adaptation matrices that reduce catastrophic forgetting, achieving a 54% backward gain with only a 4% drop on the new domain compared to LoRA.<br /><strong>Summary (CN):</strong> 该论文提出使用贝叶斯低秩因子化适配器对 Whisper 等语音基础模型进行鲁棒的领域适配，能够在代码切换等多语言场景下实现有效的微调，同时通过在参数上施加接近零的先验实现稀疏适配矩阵，显著降低对原始模型能力的遗忘。实验表明，与 LoRA 相比，本文方法在新领域性能下降仅 4%，但在原始任务上实现了 54% 的后向收益。<br /><strong>Keywords:</strong> Bayesian adaptation, low-rank factorization, adapters, Whisper, code-switching, catastrophic forgetting, LoRA, speech foundation models<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.
<div><strong>Authors:</strong> Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes Bayesian low-rank factorized adapters for speech foundation models such as Whisper, enabling robust adaptation to code-switching multilingual scenarios while preserving the base model's general capabilities. By placing near-zero priors on adapter parameters, the method yields sparser adaptation matrices that reduce catastrophic forgetting, achieving a 54% backward gain with only a 4% drop on the new domain compared to LoRA.", "summary_cn": "该论文提出使用贝叶斯低秩因子化适配器对 Whisper 等语音基础模型进行鲁棒的领域适配，能够在代码切换等多语言场景下实现有效的微调，同时通过在参数上施加接近零的先验实现稀疏适配矩阵，显著降低对原始模型能力的遗忘。实验表明，与 LoRA 相比，本文方法在新领域性能下降仅 4%，但在原始任务上实现了 54% 的后向收益。", "keywords": "Bayesian adaptation, low-rank factorization, adapters, Whisper, code-switching, catastrophic forgetting, LoRA, speech foundation models", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"]}
]]></acme>

<pubDate>2025-10-21T15:23:30+00:00</pubDate>
</item>
<item>
<title>Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering</title>
<link>https://papers.cool/arxiv/2510.18691</link>
<guid>https://papers.cool/arxiv/2510.18691</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first systematic study of large language model (LLM) performance on long-context medical question answering, evaluating multiple models, relevance-based content inclusion settings, and datasets across various task formulations. It investigates how Retrieval-Augmented Generation (RAG) affects long-context comprehension, identifies optimal single- versus multi-document reasoning setups, and provides qualitative and error analyses that highlight when RAG is beneficial and common failure modes. The findings reveal effects of model size, memorization issues, and the advantages of reasoning-oriented models for clinical relevance.<br /><strong>Summary (CN):</strong> 本文首次系统性地研究了大型语言模型在长上下文医学问答任务中的表现，评估了不同模型、基于相关性的内容包含设置以及跨任务形式的多个数据集。研究分析了检索增强生成（RAG）对长上下文理解的影响，找出了单文档与多文档推理的最佳配置，并通过定性和错误分析阐明了 RAG 何时有益以及常见的失败案例。结果揭示了模型规模、记忆问题以及推理模型在临床相关任务中的优势。<br /><strong>Keywords:</strong> long-context language models, medical question answering, Retrieval-Augmented Generation, RAG, context comprehension, model scaling, memorization, multi-document reasoning, evaluation metrics, error analysis<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Feras AlMannaa, Talia Tseriotou, Jenny Chim, Maria Liakata</div>
This study is the first to investigate LLM comprehension capabilities over long-context (LC) medical QA of clinical relevance. Our comprehensive assessment spans a range of content-inclusion settings based on their relevance, LLM models of varying capabilities and datasets across task formulations, revealing insights on model size effects, limitations, underlying memorization issues and the benefits of reasoning models. Importantly, we examine the effect of RAG on medical LC comprehension, uncover best settings in single versus multi-document reasoning datasets and showcase RAG strategies for improvements over LC. We shed light into some of the evaluation aspects using a multi-faceted approach. Our qualitative and error analyses address open questions on when RAG is beneficial over LC, revealing common failure cases.
<div><strong>Authors:</strong> Feras AlMannaa, Talia Tseriotou, Jenny Chim, Maria Liakata</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first systematic study of large language model (LLM) performance on long-context medical question answering, evaluating multiple models, relevance-based content inclusion settings, and datasets across various task formulations. It investigates how Retrieval-Augmented Generation (RAG) affects long-context comprehension, identifies optimal single- versus multi-document reasoning setups, and provides qualitative and error analyses that highlight when RAG is beneficial and common failure modes. The findings reveal effects of model size, memorization issues, and the advantages of reasoning-oriented models for clinical relevance.", "summary_cn": "本文首次系统性地研究了大型语言模型在长上下文医学问答任务中的表现，评估了不同模型、基于相关性的内容包含设置以及跨任务形式的多个数据集。研究分析了检索增强生成（RAG）对长上下文理解的影响，找出了单文档与多文档推理的最佳配置，并通过定性和错误分析阐明了 RAG 何时有益以及常见的失败案例。结果揭示了模型规模、记忆问题以及推理模型在临床相关任务中的优势。", "keywords": "long-context language models, medical question answering, Retrieval-Augmented Generation, RAG, context comprehension, model scaling, memorization, multi-document reasoning, evaluation metrics, error analysis", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Feras AlMannaa", "Talia Tseriotou", "Jenny Chim", "Maria Liakata"]}
]]></acme>

<pubDate>2025-10-21T14:50:24+00:00</pubDate>
</item>
<item>
<title>MLMA: Towards Multilingual with Mamba Based Architectures</title>
<link>https://papers.cool/arxiv/2510.18684</link>
<guid>https://papers.cool/arxiv/2510.18684</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes MLMA, a multilingual automatic speech recognition system that adopts the Mamba state-space architecture to handle long-context sequences efficiently. By embedding language-aware conditioning within a shared representation, MLMA attains performance comparable to Transformer‑based models on standard multilingual benchmarks, demonstrating Mamba’s suitability as a scalable backbone for speech tasks.<br /><strong>Summary (CN):</strong> 本文提出了 MLMA，一种采用 Mamba 状态空间模型进行长序列处理的多语言自动语音识别系统。通过在共享表征中隐式加入语言感知条件，MLMA 在标准多语言基准上实现了与 Transformer 模型相当的性能，展示了 Mamba 作为可扩展、高效语音识别骨干的潜力。<br /><strong>Keywords:</strong> multilingual ASR, Mamba, state-space model, speech recognition, efficient architecture, language-aware conditioning, long-context modeling<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</div>
Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture--an efficient state-space model optimized for long-context sequence processing--for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition.
<div><strong>Authors:</strong> Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes MLMA, a multilingual automatic speech recognition system that adopts the Mamba state-space architecture to handle long-context sequences efficiently. By embedding language-aware conditioning within a shared representation, MLMA attains performance comparable to Transformer‑based models on standard multilingual benchmarks, demonstrating Mamba’s suitability as a scalable backbone for speech tasks.", "summary_cn": "本文提出了 MLMA，一种采用 Mamba 状态空间模型进行长序列处理的多语言自动语音识别系统。通过在共享表征中隐式加入语言感知条件，MLMA 在标准多语言基准上实现了与 Transformer 模型相当的性能，展示了 Mamba 作为可扩展、高效语音识别骨干的潜力。", "keywords": "multilingual ASR, Mamba, state-space model, speech recognition, efficient architecture, language-aware conditioning, long-context modeling", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohamed Nabih Ali", "Daniele Falavigna", "Alessio Brutti"]}
]]></acme>

<pubDate>2025-10-21T14:44:16+00:00</pubDate>
</item>
<item>
<title>Dynamical model parameters from ultrasound tongue kinematics</title>
<link>https://papers.cool/arxiv/2510.18629</link>
<guid>https://papers.cool/arxiv/2510.18629</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether parameters of a linear harmonic oscillator model of speech articulation can be reliably estimated from ultrasound tongue kinematics and compares these estimates with those obtained from simultaneously recorded electromagnetic articulography (EMA) data. Results show that ultrasound provides comparable dynamical parameters to EMA, and mandibular short‑tendon tracking also captures jaw motion effectively, supporting ultrasound as a viable method for evaluating dynamical articulatory models.<br /><strong>Summary (CN):</strong> 本文研究了能否从超声舌部运动中可靠地估计线性简谐振子模型的参数，并将其与同步记录的电磁测轨（EMA）数据估计的参数进行比较。结果表明，超声获取的动力学参数与 EMA 相当，颚短肌腱追踪同样能够有效捕获下颌运动，支持将超声用于评估动力学发音模型。<br /><strong>Keywords:</strong> ultrasound tongue imaging, speech dynamics, linear harmonic oscillator, articulatory modeling, electromagnetic articulography, mandibular tracking, dynamical parameter estimation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 2, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sam Kirkham, Patrycja Strycharczuk</div>
The control of speech can be modelled as a dynamical system in which articulators are driven toward target positions. These models are typically evaluated using fleshpoint data, such as electromagnetic articulography (EMA), but recent methodological advances make ultrasound imaging a promising alternative. We evaluate whether the parameters of a linear harmonic oscillator can be reliably estimated from ultrasound tongue kinematics and compare these with parameters estimated from simultaneously-recorded EMA data. We find that ultrasound and EMA yield comparable dynamical parameters, while mandibular short tendon tracking also adequately captures jaw motion. This supports using ultrasound kinematics to evaluate dynamical articulatory models.
<div><strong>Authors:</strong> Sam Kirkham, Patrycja Strycharczuk</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether parameters of a linear harmonic oscillator model of speech articulation can be reliably estimated from ultrasound tongue kinematics and compares these estimates with those obtained from simultaneously recorded electromagnetic articulography (EMA) data. Results show that ultrasound provides comparable dynamical parameters to EMA, and mandibular short‑tendon tracking also captures jaw motion effectively, supporting ultrasound as a viable method for evaluating dynamical articulatory models.", "summary_cn": "本文研究了能否从超声舌部运动中可靠地估计线性简谐振子模型的参数，并将其与同步记录的电磁测轨（EMA）数据估计的参数进行比较。结果表明，超声获取的动力学参数与 EMA 相当，颚短肌腱追踪同样能够有效捕获下颌运动，支持将超声用于评估动力学发音模型。", "keywords": "ultrasound tongue imaging, speech dynamics, linear harmonic oscillator, articulatory modeling, electromagnetic articulography, mandibular tracking, dynamical parameter estimation", "scoring": {"interpretability": 2, "understanding": 2, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sam Kirkham", "Patrycja Strycharczuk"]}
]]></acme>

<pubDate>2025-10-21T13:34:13+00:00</pubDate>
</item>
<item>
<title>Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media</title>
<link>https://papers.cool/arxiv/2510.18582</link>
<guid>https://papers.cool/arxiv/2510.18582</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a theory‑informed bilingual dataset of 16,000 social‑media instances annotated for explicit and subtle forms of dehumanization at both document and span levels. It demonstrates that fine‑tuned language models trained on this resource outperform state‑of‑the‑art baselines in zero‑shot and few‑shot‑context settings, highlighting the dataset’s utility for broader dehumanization detection. The work aims to fill a gap in NLP research by covering non‑overtly offensive yet harmful language that reinforces negative stereotypes.<br /><strong>Summary (CN):</strong> 本文构建了一个理论驱动的双语数据集，包含 16,000 条来自 Twitter 和 Reddit 的社交媒体文本，针对显性与隐性去人性化行为进行文档级和跨度级标注。实验表明，在该数据集上微调的语言模型在零样本和少样本场景下的表现超过了现有最先进模型，展示了数据集在更广泛去人性化检测中的价值。此工作旨在弥补 NLP 研究中对并非显性冒犯但仍会加深负面刻板印象的有害语言的关注不足。<br /><strong>Keywords:</strong> dehumanization detection, bilingual dataset, social media, bias mitigation, NLP, -shot, few-shot, document-level annotation, span-level annotation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Dennis Assenmacher, Paloma Piot, Katarina Laken, David Jurgens, Claudia Wagner</div>
Digital dehumanization, although a critical issue, remains largely overlooked within the field of computational linguistics and Natural Language Processing. The prevailing approach in current research concentrating primarily on a single aspect of dehumanization that identifies overtly negative statements as its core marker. This focus, while crucial for understanding harmful online communications, inadequately addresses the broader spectrum of dehumanization. Specifically, it overlooks the subtler forms of dehumanization that, despite not being overtly offensive, still perpetuate harmful biases against marginalized groups in online interactions. These subtler forms can insidiously reinforce negative stereotypes and biases without explicit offensiveness, making them harder to detect yet equally damaging. Recognizing this gap, we use different sampling methods to collect a theory-informed bilingual dataset from Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances on a document- and span-level, we show that our dataset covers the different dimensions of dehumanization. This dataset serves as both a training resource for machine learning models and a benchmark for evaluating future dehumanization detection techniques. To demonstrate its effectiveness, we fine-tune ML models on this dataset, achieving performance that surpasses state-of-the-art models in zero and few-shot in-context settings.
<div><strong>Authors:</strong> Dennis Assenmacher, Paloma Piot, Katarina Laken, David Jurgens, Claudia Wagner</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a theory‑informed bilingual dataset of 16,000 social‑media instances annotated for explicit and subtle forms of dehumanization at both document and span levels. It demonstrates that fine‑tuned language models trained on this resource outperform state‑of‑the‑art baselines in zero‑shot and few‑shot‑context settings, highlighting the dataset’s utility for broader dehumanization detection. The work aims to fill a gap in NLP research by covering non‑overtly offensive yet harmful language that reinforces negative stereotypes.", "summary_cn": "本文构建了一个理论驱动的双语数据集，包含 16,000 条来自 Twitter 和 Reddit 的社交媒体文本，针对显性与隐性去人性化行为进行文档级和跨度级标注。实验表明，在该数据集上微调的语言模型在零样本和少样本场景下的表现超过了现有最先进模型，展示了数据集在更广泛去人性化检测中的价值。此工作旨在弥补 NLP 研究中对并非显性冒犯但仍会加深负面刻板印象的有害语言的关注不足。", "keywords": "dehumanization detection, bilingual dataset, social media, bias mitigation, NLP,-shot, few-shot, document-level annotation, span-level annotation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Dennis Assenmacher", "Paloma Piot", "Katarina Laken", "David Jurgens", "Claudia Wagner"]}
]]></acme>

<pubDate>2025-10-21T12:35:30+00:00</pubDate>
</item>
<item>
<title>Large language models for folktale type automation based on motifs: Cinderella case study</title>
<link>https://papers.cool/arxiv/2510.18561</link>
<guid>https://papers.cool/arxiv/2510.18561</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents a methodology that uses large language models to automatically identify motifs in a large corpus of Cinderella variants and analyzes similarities with clustering and dimensionality reduction techniques. Results demonstrate that LLMs can capture complex interactions within folk narratives, enabling large‑scale computational analysis and cross‑lingual comparisons. This showcases a novel application of AI to digital humanities and folkloristics.<br /><strong>Summary (CN):</strong> 本文提出了一种方法论，利用大型语言模型自动检测大量《灰姑娘》变体中的叙事母题，并使用聚类和降维技术分析它们的相似性与差异。结果显示，LLM 能捕捉故事中复杂的交互关系，从而实现大规模文本分析和跨语言比较。此工作展示了人工智能在数字人文和民俗学中的新颖应用。<br /><strong>Keywords:</strong> large language models, folktale motifs, Cinderella, computational folkloristics, motif detection, clustering, dimensionality reduction, cross-lingual analysis<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 1, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.
<div><strong>Authors:</strong> Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents a methodology that uses large language models to automatically identify motifs in a large corpus of Cinderella variants and analyzes similarities with clustering and dimensionality reduction techniques. Results demonstrate that LLMs can capture complex interactions within folk narratives, enabling large‑scale computational analysis and cross‑lingual comparisons. This showcases a novel application of AI to digital humanities and folkloristics.", "summary_cn": "本文提出了一种方法论，利用大型语言模型自动检测大量《灰姑娘》变体中的叙事母题，并使用聚类和降维技术分析它们的相似性与差异。结果显示，LLM 能捕捉故事中复杂的交互关系，从而实现大规模文本分析和跨语言比较。此工作展示了人工智能在数字人文和民俗学中的新颖应用。", "keywords": "large language models, folktale motifs, Cinderella, computational folkloristics, motif detection, clustering, dimensionality reduction, cross-lingual analysis", "scoring": {"interpretability": 3, "understanding": 5, "safety": 1, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Tjaša Arčon", "Marko Robnik-Šikonja", "Polona Tratnik"]}
]]></acme>

<pubDate>2025-10-21T12:18:20+00:00</pubDate>
</item>
<item>
<title>Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency</title>
<link>https://papers.cool/arxiv/2510.18556</link>
<guid>https://papers.cool/arxiv/2510.18556</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a detailed bias analysis of clinical large language models, focusing on differential opioid prescription patterns across ethnicity, gender, and age, and introduces the HC4 (Healthcare Comprehensive Commons Corpus), a 89‑billion‑token pre‑training dataset. It evaluates models using standard benchmarks and a novel healthcare‑specific methodology to assess fairness and safety, highlighting the importance of dataset transparency for trustworthy clinical AI.<br /><strong>Summary (CN):</strong> 本文对临床大语言模型进行深入的偏差分析，重点关注不同族裔、性别和年龄群体在阿片类药物处方上的差异，并推出 HC4（Healthcare Comprehensive Commons Corpus），一个超过 890 亿标记的预训练数据集。通过通用基准和新提出的面向医疗的评估方法，评估模型的公平性和安全性，强调数据集透明度对于建立可信临床 AI 的重要性。<br /><strong>Keywords:</strong> bias analysis, clinical LLMs, dataset transparency, HC4, fairness, opioid prescription, healthcare AI, large language models<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Svetlana Maslenkova, Clement Christophe, Marco AF Pimentel, Tathagata Raha, Muhammad Umar Salman, Ahmed Al Mahrooqi, Avani Gupta, Shadab Khan, Ronnie Rajan, Praveenkumar Kanithi</div>
Large language models offer transformative potential for healthcare, yet their responsible and equitable development depends critically on a deeper understanding of how training data characteristics influence model behavior, including the potential for bias. Current practices in dataset curation and bias assessment often lack the necessary transparency, creating an urgent need for comprehensive evaluation frameworks to foster trust and guide improvements. In this study, we present an in-depth analysis of potential downstream biases in clinical language models, with a focus on differential opioid prescription tendencies across diverse demographic groups, such as ethnicity, gender, and age. As part of this investigation, we introduce HC4: Healthcare Comprehensive Commons Corpus, a novel and extensively curated pretraining dataset exceeding 89 billion tokens. Our evaluation leverages both established general benchmarks and a novel, healthcare-specific methodology, offering crucial insights to support fairness and safety in clinical AI applications.
<div><strong>Authors:</strong> Svetlana Maslenkova, Clement Christophe, Marco AF Pimentel, Tathagata Raha, Muhammad Umar Salman, Ahmed Al Mahrooqi, Avani Gupta, Shadab Khan, Ronnie Rajan, Praveenkumar Kanithi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a detailed bias analysis of clinical large language models, focusing on differential opioid prescription patterns across ethnicity, gender, and age, and introduces the HC4 (Healthcare Comprehensive Commons Corpus), a 89‑billion‑token pre‑training dataset. It evaluates models using standard benchmarks and a novel healthcare‑specific methodology to assess fairness and safety, highlighting the importance of dataset transparency for trustworthy clinical AI.", "summary_cn": "本文对临床大语言模型进行深入的偏差分析，重点关注不同族裔、性别和年龄群体在阿片类药物处方上的差异，并推出 HC4（Healthcare Comprehensive Commons Corpus），一个超过 890 亿标记的预训练数据集。通过通用基准和新提出的面向医疗的评估方法，评估模型的公平性和安全性，强调数据集透明度对于建立可信临床 AI 的重要性。", "keywords": "bias analysis, clinical LLMs, dataset transparency, HC4, fairness, opioid prescription, healthcare AI, large language models", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Svetlana Maslenkova", "Clement Christophe", "Marco AF Pimentel", "Tathagata Raha", "Muhammad Umar Salman", "Ahmed Al Mahrooqi", "Avani Gupta", "Shadab Khan", "Ronnie Rajan", "Praveenkumar Kanithi"]}
]]></acme>

<pubDate>2025-10-21T12:08:39+00:00</pubDate>
</item>
<item>
<title>Identity-Aware Large Language Models require Cultural Reasoning</title>
<link>https://papers.cool/arxiv/2510.18510</link>
<guid>https://papers.cool/arxiv/2510.18510</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper defines cultural reasoning as the ability of large language models to recognize culture‑specific knowledge, values, and social norms and to adjust outputs to match individual user expectations. It argues that current models default to Western norms, leading to stereotypes and mistrust, and proposes treating cultural reasoning as a foundational capability alongside factual accuracy, outlining initial assessment directions.<br /><strong>Summary (CN):</strong> 本文将“文化推理”定义为大型语言模型识别特定文化知识、价值观和社会规范并相应调整输出以符合用户期望的能力。文章指出，现有模型倾向于西方视角，易产生刻板印象和不信任，并主张将文化推理视为与事实准确性并列的基础能力，提出了初步评估方向。<br /><strong>Keywords:</strong> cultural reasoning, identity-aware AI, bias mitigation, multicultural alignment, LLM evaluation, stereotype reduction, cultural competence<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 6, Technicality: 5, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Alistair Plum, Anne-Marie Lutgen, Christoph Purschke, Achim Rettinger</div>
Large language models have become the latest trend in natural language processing, heavily featuring in the digital tools we use every day. However, their replies often reflect a narrow cultural viewpoint that overlooks the diversity of global users. This missing capability could be referred to as cultural reasoning, which we define here as the capacity of a model to recognise culture-specific knowledge values and social norms, and to adjust its output so that it aligns with the expectations of individual users. Because culture shapes interpretation, emotional resonance, and acceptable behaviour, cultural reasoning is essential for identity-aware AI. When this capacity is limited or absent, models can sustain stereotypes, ignore minority perspectives, erode trust, and perpetuate hate. Recent empirical studies strongly suggest that current models default to Western norms when judging moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning on survey data only partly reduces this tendency. The present evaluation methods mainly report static accuracy scores and thus fail to capture adaptive reasoning in context. Although broader datasets can help, they cannot alone ensure genuine cultural competence. Therefore, we argue that cultural reasoning must be treated as a foundational capability alongside factual accuracy and linguistic coherence. By clarifying the concept and outlining initial directions for its assessment, a foundation is laid for future systems to be able to respond with greater sensitivity to the complex fabric of human culture.
<div><strong>Authors:</strong> Alistair Plum, Anne-Marie Lutgen, Christoph Purschke, Achim Rettinger</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper defines cultural reasoning as the ability of large language models to recognize culture‑specific knowledge, values, and social norms and to adjust outputs to match individual user expectations. It argues that current models default to Western norms, leading to stereotypes and mistrust, and proposes treating cultural reasoning as a foundational capability alongside factual accuracy, outlining initial assessment directions.", "summary_cn": "本文将“文化推理”定义为大型语言模型识别特定文化知识、价值观和社会规范并相应调整输出以符合用户期望的能力。文章指出，现有模型倾向于西方视角，易产生刻板印象和不信任，并主张将文化推理视为与事实准确性并列的基础能力，提出了初步评估方向。", "keywords": "cultural reasoning, identity-aware AI, bias mitigation, multicultural alignment, LLM evaluation, stereotype reduction, cultural competence", "scoring": {"interpretability": 2, "understanding": 6, "safety": 6, "technicality": 5, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Alistair Plum", "Anne-Marie Lutgen", "Christoph Purschke", "Achim Rettinger"]}
]]></acme>

<pubDate>2025-10-21T10:50:51+00:00</pubDate>
</item>
<item>
<title>How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</title>
<link>https://papers.cool/arxiv/2510.18480</link>
<guid>https://papers.cool/arxiv/2510.18480</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper provides a systematic benchmark of diffusion language models (DLMs) versus autoregressive models, showing that DLMs generally lag in throughput despite their parallel decoding capability. It identifies shortcomings in prior efficiency evaluation practices and demonstrates that acceleration techniques like dual cache only help at small batch sizes, with diminishing returns at scale. The work calls for more robust evaluation methods and better acceleration strategies to improve DLM practicality.<br /><strong>Summary (CN):</strong> 本文系统性地对比了扩散语言模型（DLM）与自回归模型的吞吐量，发现尽管 DLM 具备并行解码的优势，但在实践中其效率普遍低于自回归模型。文章指出了以往效率评估方法的缺陷，并展示了双缓存等加速技术仅在小批量时有效，批量放大后收益减弱。作者呼吁采用更严格的评估方法并发展更佳的加速策略，以提升 DLM 的实际可用性。<br /><strong>Keywords:</strong> diffusion language model, efficiency evaluation, throughput, parallel decoding, roofline analysis, benchmarking<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</div>
Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.
<div><strong>Authors:</strong> Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper provides a systematic benchmark of diffusion language models (DLMs) versus autoregressive models, showing that DLMs generally lag in throughput despite their parallel decoding capability. It identifies shortcomings in prior efficiency evaluation practices and demonstrates that acceleration techniques like dual cache only help at small batch sizes, with diminishing returns at scale. The work calls for more robust evaluation methods and better acceleration strategies to improve DLM practicality.", "summary_cn": "本文系统性地对比了扩散语言模型（DLM）与自回归模型的吞吐量，发现尽管 DLM 具备并行解码的优势，但在实践中其效率普遍低于自回归模型。文章指出了以往效率评估方法的缺陷，并展示了双缓存等加速技术仅在小批量时有效，批量放大后收益减弱。作者呼吁采用更严格的评估方法并发展更佳的加速策略，以提升 DLM 的实际可用性。", "keywords": "diffusion language model, efficiency evaluation, throughput, parallel decoding, roofline analysis, benchmarking", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Han Peng", "Peiyu Liu", "Zican Dong", "Daixuan Cheng", "Junyi Li", "Yiru Tang", "Shuo Wang", "Wayne Xin Zhao"]}
]]></acme>

<pubDate>2025-10-21T10:00:32+00:00</pubDate>
</item>
<item>
<title>DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP</title>
<link>https://papers.cool/arxiv/2510.18475</link>
<guid>https://papers.cool/arxiv/2510.18475</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces DART, the first structured corpus of Italian Summaries of Product Characteristics from the Italian Medicines Agency, annotated for indications, adverse drug reactions, and drug‑drug interactions using a reproducible pipeline of web retrieval, semantic segmentation, and few‑shot LLM summarization. It also presents an LLM‑based drug interaction checker that leverages DART to infer clinically meaningful interactions, demonstrating accurate performance. The dataset and code are publicly released.<br /><strong>Summary (CN):</strong> 本文提出 DART 数据集——首个来源于意大利药品监管机构（AIFA）官方药品说明书的意大利语结构化语料库，涵盖适应症、不良反应和药物相互作用等关键药理学领域，并通过网页检索、语义分块以及少样本微调的大语言模型（LLM）低温解码实现自动化标注。作者进一步展示了基于 DART 的 LLM 药物相互作用检查器，能够准确推断临床相关的相互作用及其意义。该数据集及代码已在 GitHub 上公开发布。<br /><strong>Keywords:</strong> Italian clinical NLP, drug regulatory documents, DART dataset, drug-drug interaction, LLM, few-shot summarization, pharmacological knowledge extraction<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</div>
The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, with applications ranging from adverse event monitoring to AI-assisted clinical decision support. However, research in this field has predominantly relied on English-language corpora such as DrugBank, leaving a significant gap in resources tailored to other healthcare systems. To address this limitation, we introduce DART (Drug Annotation from Regulatory Texts), the first structured corpus of Italian Summaries of Product Characteristics derived from the official repository of the Italian Medicines Agency (AIFA). The dataset was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding. DART provides structured information on key pharmacological domains such as indications, adverse drug reactions, and drug-drug interactions. To validate its utility, we implemented an LLM-based drug interaction checker that leverages the dataset to infer clinically meaningful interactions. Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART. We publicly release our code on GitHub: https://github.com/PRAISELab-PicusLab/DART.
<div><strong>Authors:</strong> Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces DART, the first structured corpus of Italian Summaries of Product Characteristics from the Italian Medicines Agency, annotated for indications, adverse drug reactions, and drug‑drug interactions using a reproducible pipeline of web retrieval, semantic segmentation, and few‑shot LLM summarization. It also presents an LLM‑based drug interaction checker that leverages DART to infer clinically meaningful interactions, demonstrating accurate performance. The dataset and code are publicly released.", "summary_cn": "本文提出 DART 数据集——首个来源于意大利药品监管机构（AIFA）官方药品说明书的意大利语结构化语料库，涵盖适应症、不良反应和药物相互作用等关键药理学领域，并通过网页检索、语义分块以及少样本微调的大语言模型（LLM）低温解码实现自动化标注。作者进一步展示了基于 DART 的 LLM 药物相互作用检查器，能够准确推断临床相关的相互作用及其意义。该数据集及代码已在 GitHub 上公开发布。", "keywords": "Italian clinical NLP, drug regulatory documents, DART dataset, drug-drug interaction, LLM, few-shot summarization, pharmacological knowledge extraction", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mariano Barone", "Antonio Laudante", "Giuseppe Riccio", "Antonio Romano", "Marco Postiglione", "Vincenzo Moscato"]}
]]></acme>

<pubDate>2025-10-21T09:53:17+00:00</pubDate>
</item>
<item>
<title>IMB: An Italian Medical Benchmark for Question Answering</title>
<link>https://papers.cool/arxiv/2510.18468</link>
<guid>https://papers.cool/arxiv/2510.18468</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces two large Italian medical QA benchmarks, IMB-QA with 782,644 patient‑doctor dialogues and IMB-MCQA with 25,862 multiple‑choice questions, and evaluates various LLM architectures, Retrieval‑Augmented Generation and domain‑specific fine‑tuning on open‑ended and multiple‑choice tasks, showing that specialized adaptation can outperform larger general models. It also demonstrates how LLMs can be used to clean and standardize forum data while preserving conversational style. The datasets and evaluation code are released publicly to facilitate multilingual medical QA research.<br /><strong>Summary (CN):</strong> 本文推出了两个大规模意大利语医学问答基准：含 782,644 条患者‑医生对话的 IMB‑QA 与含 25,862 道医学专业考试选择题的 IMB‑MCQA，并评估了多种大型语言模型（LLM）架构、检索增强生成（RAG）以及领域特定微调在开放式和选择题任务上的表现，证明专业化适配策略可胜过更大规模的通用模型。研究还展示了利用 LLM 对医学论坛数据进行澄清与一致性提升的方式，保留原有对话风格。数据集与评估框架已在 GitHub 开源，以推动多语言医学问答研究。<br /><strong>Keywords:</strong> medical question answering, Italian language, benchmark, large language models, retrieval-augmented generation, domain fine-tuning, multilingual QA, healthcare dataset<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Vincenzo Moscato</div>
Online medical forums have long served as vital platforms where patients seek professional healthcare advice, generating vast amounts of valuable knowledge. However, the informal nature and linguistic complexity of forum interactions pose significant challenges for automated question answering systems, especially when dealing with non-English languages. We present two comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644 patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA}, comprising 25,862 multiple-choice questions from medical specialty examinations. We demonstrate how Large Language Models (LLMs) can be leveraged to improve the clarity and consistency of medical forum data while retaining their original meaning and conversational style, and compare a variety of LLM architectures on both open and multiple-choice question answering tasks. Our experiments with Retrieval Augmented Generation (RAG) and domain-specific fine-tuning reveal that specialized adaptation strategies can outperform larger, general-purpose models in medical question answering tasks. These findings suggest that effective medical AI systems may benefit more from domain expertise and efficient information retrieval than from increased model scale. We release both datasets and evaluation frameworks in our GitHub repository to support further research on multilingual medical question answering: https://github.com/PRAISELab-PicusLab/IMB.
<div><strong>Authors:</strong> Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Vincenzo Moscato</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces two large Italian medical QA benchmarks, IMB-QA with 782,644 patient‑doctor dialogues and IMB-MCQA with 25,862 multiple‑choice questions, and evaluates various LLM architectures, Retrieval‑Augmented Generation and domain‑specific fine‑tuning on open‑ended and multiple‑choice tasks, showing that specialized adaptation can outperform larger general models. It also demonstrates how LLMs can be used to clean and standardize forum data while preserving conversational style. The datasets and evaluation code are released publicly to facilitate multilingual medical QA research.", "summary_cn": "本文推出了两个大规模意大利语医学问答基准：含 782,644 条患者‑医生对话的 IMB‑QA 与含 25,862 道医学专业考试选择题的 IMB‑MCQA，并评估了多种大型语言模型（LLM）架构、检索增强生成（RAG）以及领域特定微调在开放式和选择题任务上的表现，证明专业化适配策略可胜过更大规模的通用模型。研究还展示了利用 LLM 对医学论坛数据进行澄清与一致性提升的方式，保留原有对话风格。数据集与评估框架已在 GitHub 开源，以推动多语言医学问答研究。", "keywords": "medical question answering, Italian language, benchmark, large language models, retrieval-augmented generation, domain fine-tuning, multilingual QA, healthcare dataset", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Antonio Romano", "Giuseppe Riccio", "Mariano Barone", "Marco Postiglione", "Vincenzo Moscato"]}
]]></acme>

<pubDate>2025-10-21T09:45:59+00:00</pubDate>
</item>
<item>
<title>CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning</title>
<link>https://papers.cool/arxiv/2510.18466</link>
<guid>https://papers.cool/arxiv/2510.18466</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces CEFR-Annotated WordNet, a version of WordNet enriched with Common European Framework of Reference for Languages proficiency levels. Using a large language model, the authors automatically align WordNet sense definitions with CEFR levels from the English Vocabulary Profile, creating a large corpus for training contextual lexical classifiers that achieve high macro-F1 scores. The resources are released to bridge NLP and language education.<br /><strong>Summary (CN):</strong> 本文提出了 CEFR 注释的 WordNet，即在 WordNet 语义网络中加入欧洲语言共同参考框架（CEFR）水平的版本。利用大语言模型将 WordNet 释义与英语词汇档案（English Vocabulary Profile）中的 CEFR 等级进行自动匹配，构建了大规模语料库并用于训练上下文词汇分类器，实现了 0.81 的宏观 F1 分数。作者公开了注释的 WordNet、语料库和分类器，以促进 NLP 与语言教学的结合。<br /><strong>Keywords:</strong> CEFR, WordNet, language learning, LLM annotation, semantic similarity, lexical classifier, English Vocabulary Profile, proficiency-guided dataset, NLP education<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono</div>
Although WordNet is a valuable resource owing to its structured semantic networks and extensive vocabulary, its fine-grained sense distinctions can be challenging for second-language learners. To address this, we developed a WordNet annotated with the Common European Framework of Reference for Languages (CEFR), integrating its semantic networks with language-proficiency levels. We automated this process using a large language model to measure the semantic similarity between sense definitions in WordNet and entries in the English Vocabulary Profile Online. To validate our method, we constructed a large-scale corpus containing both sense and CEFR-level information from our annotated WordNet and used it to develop contextual lexical classifiers. Our experiments demonstrate that models fine-tuned on our corpus perform comparably to those trained on gold-standard annotations. Furthermore, by combining our corpus with the gold-standard data, we developed a practical classifier that achieves a Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our annotated WordNet, corpus, and classifiers are publicly available to help bridge the gap between natural language processing and language education, thereby facilitating more effective and efficient language learning.
<div><strong>Authors:</strong> Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces CEFR-Annotated WordNet, a version of WordNet enriched with Common European Framework of Reference for Languages proficiency levels. Using a large language model, the authors automatically align WordNet sense definitions with CEFR levels from the English Vocabulary Profile, creating a large corpus for training contextual lexical classifiers that achieve high macro-F1 scores. The resources are released to bridge NLP and language education.", "summary_cn": "本文提出了 CEFR 注释的 WordNet，即在 WordNet 语义网络中加入欧洲语言共同参考框架（CEFR）水平的版本。利用大语言模型将 WordNet 释义与英语词汇档案（English Vocabulary Profile）中的 CEFR 等级进行自动匹配，构建了大规模语料库并用于训练上下文词汇分类器，实现了 0.81 的宏观 F1 分数。作者公开了注释的 WordNet、语料库和分类器，以促进 NLP 与语言教学的结合。", "keywords": "CEFR, WordNet, language learning, LLM annotation, semantic similarity, lexical classifier, English Vocabulary Profile, proficiency-guided dataset, NLP education", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Masato Kikuchi", "Masatsugu Ono", "Toshioki Soga", "Tetsu Tanabe", "Tadachika Ozono"]}
]]></acme>

<pubDate>2025-10-21T09:42:48+00:00</pubDate>
</item>
<item>
<title>DePass: Unified Feature Attributing by Simple Decomposed Forward Pass</title>
<link>https://papers.cool/arxiv/2510.18462</link>
<guid>https://papers.cool/arxiv/2510.18462</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DePass is a unified framework for attributing Transformer behavior by decomposing hidden states into additive components and propagating them through fixed attention and MLP activations in a single forward pass. The method provides fine-grained, faithful attribution at token, component, and subspace levels without auxiliary training, and is validated across multiple attribution tasks. The authors present DePass as a foundational tool for broader mechanistic interpretability applications.<br /><strong>Summary (CN):</strong> DePass 是一种统一的特征归因框架，通过将 Transformer 隐藏状态分解为可定制的加性成分，并在注意力分数和 MLP 激活保持不变的情况下进行一次前向传播，实现对模型行为的细粒度归因。该方法在 token 级、模型组件级和子空间级归因任务中展示了高忠实度和细致性，无需额外训练。作者将 DePass 视为机制可解释性研究的基础工具。<br /><strong>Keywords:</strong> feature attribution, transformer, mechanistic interpretability, decomposed forward pass, additive decomposition, token-level attribution, model component attribution, subspace attribution<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 3, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou</div>
Attributing the behavior of Transformer models to internal computations is a central challenge in mechanistic interpretability. We introduce DePass, a unified framework for feature attribution based on a single decomposed forward pass. DePass decomposes hidden states into customized additive components, then propagates them with attention scores and MLP's activations fixed. It achieves faithful, fine-grained attribution without requiring auxiliary training. We validate DePass across token-level, model component-level, and subspace-level attribution tasks, demonstrating its effectiveness and fidelity. Our experiments highlight its potential to attribute information flow between arbitrary components of a Transformer model. We hope DePass serves as a foundational tool for broader applications in interpretability.
<div><strong>Authors:</strong> Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DePass is a unified framework for attributing Transformer behavior by decomposing hidden states into additive components and propagating them through fixed attention and MLP activations in a single forward pass. The method provides fine-grained, faithful attribution at token, component, and subspace levels without auxiliary training, and is validated across multiple attribution tasks. The authors present DePass as a foundational tool for broader mechanistic interpretability applications.", "summary_cn": "DePass 是一种统一的特征归因框架，通过将 Transformer 隐藏状态分解为可定制的加性成分，并在注意力分数和 MLP 激活保持不变的情况下进行一次前向传播，实现对模型行为的细粒度归因。该方法在 token 级、模型组件级和子空间级归因任务中展示了高忠实度和细致性，无需额外训练。作者将 DePass 视为机制可解释性研究的基础工具。", "keywords": "feature attribution, transformer, mechanistic interpretability, decomposed forward pass, additive decomposition, token-level attribution, model component attribution, subspace attribution", "scoring": {"interpretability": 8, "understanding": 7, "safety": 3, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Xiangyu Hong", "Che Jiang", "Kai Tian", "Biqing Qi", "Youbang Sun", "Ning Ding", "Bowen Zhou"]}
]]></acme>

<pubDate>2025-10-21T09:36:12+00:00</pubDate>
</item>
<item>
<title>ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks</title>
<link>https://papers.cool/arxiv/2510.18455</link>
<guid>https://papers.cool/arxiv/2510.18455</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> ChronoPlay is a framework that automatically creates and continuously updates retrieval‑augmented generation (RAG) benchmarks for video games by modelling two intertwined dynamics: game content changes and evolving player‑community interests. It combines official game sources with community‑generated queries to ensure factual correctness and authentic question patterns, and demonstrates its approach on three games, providing the first dynamic RAG benchmark for the gaming domain.<br /><strong>Summary (CN):</strong> ChronoPlay 框架通过建模游戏内容更新和玩家社区兴趣的双重动态，自动生成并持续更新针对视频游戏的检索增强生成（RAG）基准。它融合官方游戏信息和社区生成的查询，以确保答案的事实正确性和问题的真实感，并在三个游戏上实现，构建了首个面向游戏领域的动态 RAG 基准。<br /><strong>Keywords:</strong> RAG, benchmark, gaming, dual dynamics, authenticity, retrieval-augmented generation, continuous evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Liyang He, Yuren Zhang, Ziwei Zhu, Zhenghui Li, Shiwei Tong</div>
Retrieval Augmented Generation (RAG) systems are increasingly vital in dynamic domains like online gaming, yet the lack of a dedicated benchmark has impeded standardized evaluation in this area. The core difficulty lies in Dual Dynamics: the constant interplay between game content updates and the shifting focus of the player community. Furthermore, the necessity of automating such a benchmark introduces a critical requirement for player-centric authenticity to ensure generated questions are realistic. To address this integrated challenge, we introduce ChronoPlay, a novel framework for the automated and continuous generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update mechanism to track both forms of change, and a dual-source synthesis engine that draws from official sources and player community to ensure both factual correctness and authentic query patterns. We instantiate our framework on three distinct games to create the first dynamic RAG benchmark for the gaming domain, offering new insights into model performance under these complex and realistic conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.
<div><strong>Authors:</strong> Liyang He, Yuren Zhang, Ziwei Zhu, Zhenghui Li, Shiwei Tong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "ChronoPlay is a framework that automatically creates and continuously updates retrieval‑augmented generation (RAG) benchmarks for video games by modelling two intertwined dynamics: game content changes and evolving player‑community interests. It combines official game sources with community‑generated queries to ensure factual correctness and authentic question patterns, and demonstrates its approach on three games, providing the first dynamic RAG benchmark for the gaming domain.", "summary_cn": "ChronoPlay 框架通过建模游戏内容更新和玩家社区兴趣的双重动态，自动生成并持续更新针对视频游戏的检索增强生成（RAG）基准。它融合官方游戏信息和社区生成的查询，以确保答案的事实正确性和问题的真实感，并在三个游戏上实现，构建了首个面向游戏领域的动态 RAG 基准。", "keywords": "RAG, benchmark, gaming, dual dynamics, authenticity, retrieval-augmented generation, continuous evaluation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Liyang He", "Yuren Zhang", "Ziwei Zhu", "Zhenghui Li", "Shiwei Tong"]}
]]></acme>

<pubDate>2025-10-21T09:28:13+00:00</pubDate>
</item>
<item>
<title>Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models</title>
<link>https://papers.cool/arxiv/2510.18454</link>
<guid>https://papers.cool/arxiv/2510.18454</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how optimizing for funniness in modern LLM pipelines leads to higher rates of stereotypical and toxic humor, revealing a bias amplification loop between generators and evaluators. Using information‑theoretic incongruity metrics and human judgments on satire, it shows that harmful jokes are scored as funnier and can even become more statistically expected for certain models. These results highlight safety risks in deploying LLMs for creative and engagement‑focused content.<br /><strong>Summary (CN):</strong> 本文研究了在现代大型语言模型中优化幽默度如何导致刻板印象和有害内容的增加，揭示了生成器与评估器之间的偏见放大循环。通过信息论不一致性度量以及对讽刺文本的人类评分，发现有害笑话往往获得更高的幽默分数，且对某些模型而言，这些有害笑点甚至变得更可预测。研究结果凸显了在创意和互动内容中使用 LLM 所面临的安全风险。<br /><strong>Keywords:</strong> humor generation, large language models, toxicity, stereotypes, bias amplification, information-theoretic analysis, safety, alignment, role-based prompting, satire<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Atharvan Dogra, Soumya Suvra Ghosal, Ameet Deshpande, Ashwin Kalyan, Dinesh Manocha</div>
Large language models are increasingly used for creative writing and engagement content, raising safety concerns about the outputs. Therefore, casting humor generation as a testbed, this work evaluates how funniness optimization in modern LLM pipelines couples with harmful content by jointly measuring humor, stereotypicality, and toxicity. This is further supplemented by analyzing incongruity signals through information-theoretic metrics. Across six models, we observe that harmful outputs receive higher humor scores which further increase under role-based prompting, indicating a bias amplification loop between generators and evaluators. Information-theoretic analyses show harmful cues widen predictive uncertainty and surprisingly, can even make harmful punchlines more expected for some models, suggesting structural embedding in learned humor distributions. External validation on an additional satire-generation task with human perceived funniness judgments shows that LLM satire increases stereotypicality and typically toxicity, including for closed models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes marked funny by LLM-based metric and up to $10\%$ more often in generations perceived as funny by humans.
<div><strong>Authors:</strong> Atharvan Dogra, Soumya Suvra Ghosal, Ameet Deshpande, Ashwin Kalyan, Dinesh Manocha</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how optimizing for funniness in modern LLM pipelines leads to higher rates of stereotypical and toxic humor, revealing a bias amplification loop between generators and evaluators. Using information‑theoretic incongruity metrics and human judgments on satire, it shows that harmful jokes are scored as funnier and can even become more statistically expected for certain models. These results highlight safety risks in deploying LLMs for creative and engagement‑focused content.", "summary_cn": "本文研究了在现代大型语言模型中优化幽默度如何导致刻板印象和有害内容的增加，揭示了生成器与评估器之间的偏见放大循环。通过信息论不一致性度量以及对讽刺文本的人类评分，发现有害笑话往往获得更高的幽默分数，且对某些模型而言，这些有害笑点甚至变得更可预测。研究结果凸显了在创意和互动内容中使用 LLM 所面临的安全风险。", "keywords": "humor generation, large language models, toxicity, stereotypes, bias amplification, information-theoretic analysis, safety, alignment, role-based prompting, satire", "scoring": {"interpretability": 4, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Atharvan Dogra", "Soumya Suvra Ghosal", "Ameet Deshpande", "Ashwin Kalyan", "Dinesh Manocha"]}
]]></acme>

<pubDate>2025-10-21T09:28:09+00:00</pubDate>
</item>
<item>
<title>Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation</title>
<link>https://papers.cool/arxiv/2510.18439</link>
<guid>https://papers.cool/arxiv/2510.18439</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a token-level reliability measure that quantifies how much a sign-language translation model relies on visual input versus language priors, using feature-based sensitivity and counterfactual probability differences. Experiments on PHOENIX-2014T and CSL-Daily show that the reliability score predicts hallucination rates across gloss-based and gloss-free models, generalizes to visual degradations, and improves risk estimation when combined with text-based confidence signals.<br /><strong>Summary (CN):</strong> 本文提出一种基于特征敏感度和反事实概率差异的 token 级可靠性指标，用于衡量手语翻译模型在生成文本对视觉输入的依赖程度。实验在 PHOENIX-2014T 与 CSL-Daily 数据集的 gloss-based 与 gloss-free 模型上表明，该可靠性分数能够预测幻觉率、在视觉降级下仍具泛化性，并与文本置信度等信号结合提升幻觉风险估计。<br /><strong>Keywords:</strong> hallucination detection, sign language translation, visual grounding, token-level reliability, counterfactual sensitivity, multimodal generation, gloss-free models, model evaluation<br /><strong>Scores:</strong> Interpretability: 5, Understanding: 7, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</div>
Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on language priors rather than visual input. To capture this, we propose a token-level reliability measure that quantifies how much the decoder uses visual information. Our method combines feature-based sensitivity, which measures internal changes when video is masked, with counterfactual signals, which capture probability differences between clean and altered video inputs. These signals are aggregated into a sentence-level reliability score, providing a compact and interpretable measure of visual grounding. We evaluate the proposed measure on two SLT benchmarks (PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our results show that reliability predicts hallucination rates, generalizes across datasets and architectures, and decreases under visual degradations. Beyond these quantitative trends, we also find that reliability distinguishes grounded tokens from guessed ones, allowing risk estimation without references; when combined with text-based signals (confidence, perplexity, or entropy), it further improves hallucination risk estimation. Qualitative analysis highlights why gloss-free models are more susceptible to hallucinations. Taken together, our findings establish reliability as a practical and reusable tool for diagnosing hallucinations in SLT, and lay the groundwork for more robust hallucination detection in multimodal generation.
<div><strong>Authors:</strong> Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a token-level reliability measure that quantifies how much a sign-language translation model relies on visual input versus language priors, using feature-based sensitivity and counterfactual probability differences. Experiments on PHOENIX-2014T and CSL-Daily show that the reliability score predicts hallucination rates across gloss-based and gloss-free models, generalizes to visual degradations, and improves risk estimation when combined with text-based confidence signals.", "summary_cn": "本文提出一种基于特征敏感度和反事实概率差异的 token 级可靠性指标，用于衡量手语翻译模型在生成文本对视觉输入的依赖程度。实验在 PHOENIX-2014T 与 CSL-Daily 数据集的 gloss-based 与 gloss-free 模型上表明，该可靠性分数能够预测幻觉率、在视觉降级下仍具泛化性，并与文本置信度等信号结合提升幻觉风险估计。", "keywords": "hallucination detection, sign language translation, visual grounding, token-level reliability, counterfactual sensitivity, multimodal generation, gloss-free models, model evaluation", "scoring": {"interpretability": 5, "understanding": 7, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Yasser Hamidullah", "Koel Dutta Chowdury", "Yusser Al-Ghussin", "Shakib Yazdani", "Cennet Oguz", "Josef van Genabith", "Cristina España-Bonet"]}
]]></acme>

<pubDate>2025-10-21T09:13:46+00:00</pubDate>
</item>
<item>
<title>Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response</title>
<link>https://papers.cool/arxiv/2510.18434</link>
<guid>https://papers.cool/arxiv/2510.18434</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Chain of Conceptual Thought (CoCT), a prompt-based paradigm where a language model first tags a concept and then expands on it, forming a chain of concepts within its response. Experiments on daily and emotional‑support dialogues show that CoCT outperforms baselines such as Self‑Refine, ECoT, ToT, SoT and RAG, suggesting that concept‑driven prompting can foster deeper, more strategic thinking in open‑domain tasks.<br /><strong>Summary (CN):</strong> 本文提出“概念链思考”(Chain of Conceptual Thought, CoCT) 的提示范式：让语言模型先标记概念，再围绕该概念展开详细内容，在回复中形成概念链。针对日常对话和情感支持场景的实验表明，CoCT 在自动、人工和模型评估上均优于 Self‑Refine、ECoT、ToT、SoT 与 RAG 等基线，显示概念驱动的提示能够促使模型在开放域任务中进行更深层次、策略性的思考。<br /><strong>Keywords:</strong> Chain-of-Conceptual-Thought, prompting, large language models, open-domain reasoning, emotional support conversation, concept tagging, Chain-of-Thought, Self-Refine, ECoT, ToT<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Qingqing Gu, Dan Wang, Yue Zhao, Xiaoyu Wang, Zhonglin Jiang, Yong Chen, Hongyan Li, Luo Ji</div>
Chain-of-Thought (CoT) is widely applied to improve the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks since there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose another prompt-based paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a concept, then generates the detailed content. The chain of concepts is allowed within the utterance, encouraging the LLM's deep and strategic thinking. We experiment with this paradigm in daily and emotional support conversations where the concept is comprised of emotions, strategies and topics. Automatic, human and model evaluations suggest that CoCT surpasses baselines such as Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective prompt-based paradigm of LLM for a wider scope of tasks.
<div><strong>Authors:</strong> Qingqing Gu, Dan Wang, Yue Zhao, Xiaoyu Wang, Zhonglin Jiang, Yong Chen, Hongyan Li, Luo Ji</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Chain of Conceptual Thought (CoCT), a prompt-based paradigm where a language model first tags a concept and then expands on it, forming a chain of concepts within its response. Experiments on daily and emotional‑support dialogues show that CoCT outperforms baselines such as Self‑Refine, ECoT, ToT, SoT and RAG, suggesting that concept‑driven prompting can foster deeper, more strategic thinking in open‑domain tasks.", "summary_cn": "本文提出“概念链思考”(Chain of Conceptual Thought, CoCT) 的提示范式：让语言模型先标记概念，再围绕该概念展开详细内容，在回复中形成概念链。针对日常对话和情感支持场景的实验表明，CoCT 在自动、人工和模型评估上均优于 Self‑Refine、ECoT、ToT、SoT 与 RAG 等基线，显示概念驱动的提示能够促使模型在开放域任务中进行更深层次、策略性的思考。", "keywords": "Chain-of-Conceptual-Thought, prompting, large language models, open-domain reasoning, emotional support conversation, concept tagging, Chain-of-Thought, Self-Refine, ECoT, ToT", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Qingqing Gu", "Dan Wang", "Yue Zhao", "Xiaoyu Wang", "Zhonglin Jiang", "Yong Chen", "Hongyan Li", "Luo Ji"]}
]]></acme>

<pubDate>2025-10-21T09:08:21+00:00</pubDate>
</item>
<item>
<title>Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference</title>
<link>https://papers.cool/arxiv/2510.18413</link>
<guid>https://papers.cool/arxiv/2510.18413</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Adamas introduces a lightweight sparse attention mechanism for long-context inference that leverages the Hadamard transform, bucketization, and 2-bit compression to create compact representations and uses Manhattan-distance estimation for efficient top‑k selection. Experiments show that it matches full‑attention accuracy with a 64‑token budget and achieves up to 8× higher sparsity than prior methods, delivering up to 4.4× self‑attention and 1.5× end‑to‑end speedups on 32K‑length sequences.<br /><strong>Summary (CN):</strong> Adamas 提出了一种用于长上下文推理的轻量稀疏注意力机制，利用 Hadamard 变换、分桶和 2 位压缩生成紧凑表示，并使用曼哈顿距离估计进行高效的 top‑k 选择。实验表明，在仅使用 64 个 token 预算的情况下，其准确性与完整注意力相当，并实现了比现有方法高达 8 倍的稀疏度以及在 32K 长度序列上 4.4 倍的自注意力加速和 1.5 倍的端到端加速。<br /><strong>Keywords:</strong> sparse attention, Hadamard transform, long-context inference, efficient transformer, top-k selection, compression<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu</div>
Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce Adamas, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity.
<div><strong>Authors:</strong> Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Adamas introduces a lightweight sparse attention mechanism for long-context inference that leverages the Hadamard transform, bucketization, and 2-bit compression to create compact representations and uses Manhattan-distance estimation for efficient top‑k selection. Experiments show that it matches full‑attention accuracy with a 64‑token budget and achieves up to 8× higher sparsity than prior methods, delivering up to 4.4× self‑attention and 1.5× end‑to‑end speedups on 32K‑length sequences.", "summary_cn": "Adamas 提出了一种用于长上下文推理的轻量稀疏注意力机制，利用 Hadamard 变换、分桶和 2 位压缩生成紧凑表示，并使用曼哈顿距离估计进行高效的 top‑k 选择。实验表明，在仅使用 64 个 token 预算的情况下，其准确性与完整注意力相当，并实现了比现有方法高达 8 倍的稀疏度以及在 32K 长度序列上 4.4 倍的自注意力加速和 1.5 倍的端到端加速。", "keywords": "sparse attention, Hadamard transform, long-context inference, efficient transformer, top-k selection, compression", "scoring": {"interpretability": 2, "understanding": 7, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Siyuan Yan", "Guo-Qing Jiang", "Yuchen Zhang", "Xiaoxing Ma", "Ran Zhu", "Chun Cao", "Jingwei Xu"]}
]]></acme>

<pubDate>2025-10-21T08:44:47+00:00</pubDate>
</item>
<item>
<title>MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models</title>
<link>https://papers.cool/arxiv/2510.18383</link>
<guid>https://papers.cool/arxiv/2510.18383</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MENTOR, a reinforcement‑learning framework that distills the tool‑using abilities of large language models into smaller language models by using a dense, teacher‑guided reward constructed from reference trajectories. By combining exploration‑driven RL with fine‑grained teacher feedback, MENTOR achieves better cross‑domain generalization and strategic competence than supervised fine‑tuning or standard sparse‑reward RL baselines.<br /><strong>Summary (CN):</strong> 本文提出 MENTOR 框架，利用强化学习将大语言模型的工具使用能力蒸馏到小语言模型中，方法是通过教师提供的参考轨迹构造密集的教师引导奖励。该框架将探索性的 RL 与细粒度的教师反馈相结合，使小模型在跨领域泛化和策略能力上显著于传统的监督微调和稀疏奖励 RL 基线。<br /><strong>Keywords:</strong> reinforcement learning, teacher-guided rewards, model distillation, small language models, cross-domain generalization, RL-based distillation, tool use, policy learning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.
<div><strong>Authors:</strong> ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MENTOR, a reinforcement‑learning framework that distills the tool‑using abilities of large language models into smaller language models by using a dense, teacher‑guided reward constructed from reference trajectories. By combining exploration‑driven RL with fine‑grained teacher feedback, MENTOR achieves better cross‑domain generalization and strategic competence than supervised fine‑tuning or standard sparse‑reward RL baselines.", "summary_cn": "本文提出 MENTOR 框架，利用强化学习将大语言模型的工具使用能力蒸馏到小语言模型中，方法是通过教师提供的参考轨迹构造密集的教师引导奖励。该框架将探索性的 RL 与细粒度的教师反馈相结合，使小模型在跨领域泛化和策略能力上显著于传统的监督微调和稀疏奖励 RL 基线。", "keywords": "reinforcement learning, teacher-guided rewards, model distillation, small language models, cross-domain generalization, RL-based distillation, tool use, policy learning", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"]}
]]></acme>

<pubDate>2025-10-21T08:03:14+00:00</pubDate>
</item>
<item>
<title>Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning</title>
<link>https://papers.cool/arxiv/2510.18374</link>
<guid>https://papers.cool/arxiv/2510.18374</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates fairness gaps in popular English automatic speech recognition models (Whisper and Seamless‑M4T) across 26 accent groups of second‑language speakers, finding large WER variance. It proposes a fairness‑prompted finetuning approach using lightweight adapters combined with Spectral Decoupling, Group‑DRO, and Invariant Risk Minimization to jointly optimize accuracy and accent fairness. Experiments show macro‑averaged WER improvements of about 58 % relative to the pretrained models while preserving overall recognition performance.<br /><strong>Summary (CN):</strong> 本文研究了主流英文自动语音识别模型（Whisper 和 Seamless‑M4T）在 26 种第二语言口音组之间的公平性差距，发现词错误率（WER）波动显著。作者提出使用轻量级适配器并融合光谱解耦（Spectral Decoupling）、组分布鲁棒优化（Group‑DRO）和不变风险最小化（IRM）的公平提示微调方法，以同时提升准确率和口音公平性。实验表明，在宏观平均 WER 上相较于原始模型提升约 58%，且整体识别性能保持。<br /><strong>Keywords:</strong> fairness, automatic speech recognition, accent bias, Whisper, adapters, spectral decoupling, group DRO, invariant risk minimization, multilingual ASR, bias mitigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - robustness<br /><strong>Authors:</strong> Monorama Swain, Bubai Maji, Jagabandhu Mishra, Markus Schedl, Anders Søgaard, Jesper Rindom Jensen</div>
In this work, we address the challenge of building fair English ASR systems for second-language speakers. Our analysis of widely used ASR models, Whisper and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26 accent groups, indicating significant fairness gaps. To mitigate this, we propose fairness-prompted finetuning with lightweight adapters, incorporating Spectral Decoupling (SD), Group Distributionally Robust Optimization (Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of traditional empirical risk minimization (ERM) with cross-entropy and fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across accent groups while maintaining overall recognition accuracy. In terms of macro-averaged word error rate, our approach achieves a relative improvement of 58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and 7.8% over them, finetuning with standard empirical risk minimization with cross-entropy loss.
<div><strong>Authors:</strong> Monorama Swain, Bubai Maji, Jagabandhu Mishra, Markus Schedl, Anders Søgaard, Jesper Rindom Jensen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates fairness gaps in popular English automatic speech recognition models (Whisper and Seamless‑M4T) across 26 accent groups of second‑language speakers, finding large WER variance. It proposes a fairness‑prompted finetuning approach using lightweight adapters combined with Spectral Decoupling, Group‑DRO, and Invariant Risk Minimization to jointly optimize accuracy and accent fairness. Experiments show macro‑averaged WER improvements of about 58 % relative to the pretrained models while preserving overall recognition performance.", "summary_cn": "本文研究了主流英文自动语音识别模型（Whisper 和 Seamless‑M4T）在 26 种第二语言口音组之间的公平性差距，发现词错误率（WER）波动显著。作者提出使用轻量级适配器并融合光谱解耦（Spectral Decoupling）、组分布鲁棒优化（Group‑DRO）和不变风险最小化（IRM）的公平提示微调方法，以同时提升准确率和口音公平性。实验表明，在宏观平均 WER 上相较于原始模型提升约 58%，且整体识别性能保持。", "keywords": "fairness, automatic speech recognition, accent bias, Whisper, adapters, spectral decoupling, group DRO, invariant risk minimization, multilingual ASR, bias mitigation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "robustness"}, "authors": ["Monorama Swain", "Bubai Maji", "Jagabandhu Mishra", "Markus Schedl", "Anders Søgaard", "Jesper Rindom Jensen"]}
]]></acme>

<pubDate>2025-10-21T07:45:21+00:00</pubDate>
</item>
<item>
<title>KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs</title>
<link>https://papers.cool/arxiv/2510.18368</link>
<guid>https://papers.cool/arxiv/2510.18368</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces KoSimpleQA, a Korean factuality benchmark consisting of 1,000 short, unambiguous fact-seeking questions aimed at evaluating large language models' knowledge of Korean cultural information. Experiments on various open-source Korean-capable LLMs show low accuracy (33.7% for the strongest model) and reveal performance differences compared to English SimpleQA, while analysis of reasoning-augmented LLMs indicates that reasoning improves knowledge extraction and uncertainty abstention.<br /><strong>Summary (CN):</strong> 本文推出 KoSimpleQA（Korean SimpleQA），一个包含 1,000 条简短且答案明确的韩语事实问答题目的基准，用于评估大语言模型在韩国文化知识方面的事实性。实验表明，即使是性能最强的开源模型也仅有约 33.7% 的正确率，并且在该基准上的排序与英文 SimpleQA 差异显著；进一步分析显示，启用推理能力的模型能够更好地挖掘潜在知识并在不确定时选择弃答。<br /><strong>Keywords:</strong> Korean QA, factuality benchmark, LLM evaluation, reasoning, cultural knowledge, KoSimpleQA<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Donghyeon Ko, Yeguk Jin, Kyubyung Chae, Byungwook Lee, Chansong Jo, Sookyo In, Jaehong Lee, Taesup Kim, Donghyun Kwak</div>
We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for evaluating factuality in large language models (LLMs) with a focus on Korean cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade, consisting of 1,000 short, fact-seeking questions with unambiguous answers. We conduct a comprehensive evaluation across a diverse set of open-source LLMs of varying sizes that support Korean, and find that even the strongest model generates correct answer only 33.7% of the time, underscoring the challenging nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ substantially from those on the English SimpleQA, highlighting the unique value of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging reasoning capabilities in the factual QA task can both help models better elicit their latent knowledge and improve their ability to abstain when uncertain. KoSimpleQA can be found at https://anonymous.4open.science/r/KoSimpleQA-62EB.
<div><strong>Authors:</strong> Donghyeon Ko, Yeguk Jin, Kyubyung Chae, Byungwook Lee, Chansong Jo, Sookyo In, Jaehong Lee, Taesup Kim, Donghyun Kwak</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces KoSimpleQA, a Korean factuality benchmark consisting of 1,000 short, unambiguous fact-seeking questions aimed at evaluating large language models' knowledge of Korean cultural information. Experiments on various open-source Korean-capable LLMs show low accuracy (33.7% for the strongest model) and reveal performance differences compared to English SimpleQA, while analysis of reasoning-augmented LLMs indicates that reasoning improves knowledge extraction and uncertainty abstention.", "summary_cn": "本文推出 KoSimpleQA（Korean SimpleQA），一个包含 1,000 条简短且答案明确的韩语事实问答题目的基准，用于评估大语言模型在韩国文化知识方面的事实性。实验表明，即使是性能最强的开源模型也仅有约 33.7% 的正确率，并且在该基准上的排序与英文 SimpleQA 差异显著；进一步分析显示，启用推理能力的模型能够更好地挖掘潜在知识并在不确定时选择弃答。", "keywords": "Korean QA, factuality benchmark, LLM evaluation, reasoning, cultural knowledge, KoSimpleQA", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Donghyeon Ko", "Yeguk Jin", "Kyubyung Chae", "Byungwook Lee", "Chansong Jo", "Sookyo In", "Jaehong Lee", "Taesup Kim", "Donghyun Kwak"]}
]]></acme>

<pubDate>2025-10-21T07:37:51+00:00</pubDate>
</item>
<item>
<title>KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers</title>
<link>https://papers.cool/arxiv/2510.18355</link>
<guid>https://papers.cool/arxiv/2510.18355</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces KrishokBondhu, a voice‑based call‑center system for Bengali‑speaking farmers that uses a Retrieval‑Augmented Generation (RAG) pipeline to provide real‑time agricultural advice. It combines OCR digitization of extension manuals, a vector‑database retrieval layer, speech‑to‑text, a 3‑4B Gemma language model, and text‑to‑speech to generate context‑grounded answers, achieving a 44.7% improvement over the KisanQRS benchmark in pilot tests.<br /><strong>Summary (CN):</strong> 本文提出 KrishokBondhu，一个面向孟加拉语农民的语音呼叫中心系统，采用检索增强生成（Retrieval‑Augmented Generation，RAG）流水线提供实时农业咨询。系统通过 OCR 将推广手册数字化，使用向量数据库检索、语音转文字、Gemma 3‑4B 大语言模型生成答案，再转为自然语音，实验显示相较 KisanQRS 基准提升 44.7%。<br /><strong>Keywords:</strong> retrieval-augmented generation, voice interface, Bengali, agricultural advisory, call center, OCR, large language model, semantic retrieval, multilingual AI<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</div>
In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.
<div><strong>Authors:</strong> Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces KrishokBondhu, a voice‑based call‑center system for Bengali‑speaking farmers that uses a Retrieval‑Augmented Generation (RAG) pipeline to provide real‑time agricultural advice. It combines OCR digitization of extension manuals, a vector‑database retrieval layer, speech‑to‑text, a 3‑4B Gemma language model, and text‑to‑speech to generate context‑grounded answers, achieving a 44.7% improvement over the KisanQRS benchmark in pilot tests.", "summary_cn": "本文提出 KrishokBondhu，一个面向孟加拉语农民的语音呼叫中心系统，采用检索增强生成（Retrieval‑Augmented Generation，RAG）流水线提供实时农业咨询。系统通过 OCR 将推广手册数字化，使用向量数据库检索、语音转文字、Gemma 3‑4B 大语言模型生成答案，再转为自然语音，实验显示相较 KisanQRS 基准提升 44.7%。", "keywords": "retrieval-augmented generation, voice interface, Bengali, agricultural advisory, call center, OCR, large language model, semantic retrieval, multilingual AI", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Farjana Aktar", "M. Saifuzzaman Rafat"]}
]]></acme>

<pubDate>2025-10-21T07:24:55+00:00</pubDate>
</item>
<item>
<title>Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction</title>
<link>https://papers.cool/arxiv/2510.18344</link>
<guid>https://papers.cool/arxiv/2510.18344</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces HYDRE, a hybrid framework that first uses a trained distant supervision relation extraction (DSRE) model to generate top‑k candidate relations and then retrieves reliable sentence‑level exemplars to prompt a large language model for final relation prediction. It demonstrates substantial F1 improvements on English and on a newly created benchmark for four low‑resource Indic languages, highlighting the benefit of combining DSRE with in‑context learning and a dynamic exemplar retrieval strategy.<br /><strong>Summary (CN):</strong> 本文提出 HYDRE（Hybrid Distantly Supervised Relation Extraction）框架，先利用已训练的远程监督关系抽取（DSRE）模型获取候选关系，再通过动态示例检索从训练数据中挑选可靠的句子级示例，填入大语言模型（LLM）提示以输出最终关系。实验显示，在英语以及四种低资源印地语（Oriya、Santali、Manipuri、Tulu）新基准上，模型的 F1 提升显著，验证了将 DSRE 与上下文学习相结合以及动态示例检索的有效性。<br /><strong>Keywords:</strong> distant supervision, relation extraction, in-context learning, large language models, cross-lingual, exemplar retrieval, HYDRE<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</div>
Distantly Supervised Relation Extraction (DSRE) remains a long-standing challenge in NLP, where models must learn from noisy bag-level annotations while making sentence-level predictions. While existing state-of-the-art (SoTA) DSRE models rely on task-specific training, their integration with in-context learning (ICL) using large language models (LLMs) remains underexplored. A key challenge is that the LLM may not learn relation semantics correctly, due to noisy annotation. In response, we propose HYDRE -- HYbrid Distantly Supervised Relation Extraction framework. It first uses a trained DSRE model to identify the top-k candidate relations for a given test sentence, then uses a novel dynamic exemplar retrieval strategy that extracts reliable, sentence-level exemplars from training data, which are then provided in LLM prompt for outputting the final relation(s). We further extend HYDRE to cross-lingual settings for RE in low-resource languages. Using available English DSRE training data, we evaluate all methods on English as well as a newly curated benchmark covering four diverse low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's efficacy compared to other prompting strategies.
<div><strong>Authors:</strong> Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces HYDRE, a hybrid framework that first uses a trained distant supervision relation extraction (DSRE) model to generate top‑k candidate relations and then retrieves reliable sentence‑level exemplars to prompt a large language model for final relation prediction. It demonstrates substantial F1 improvements on English and on a newly created benchmark for four low‑resource Indic languages, highlighting the benefit of combining DSRE with in‑context learning and a dynamic exemplar retrieval strategy.", "summary_cn": "本文提出 HYDRE（Hybrid Distantly Supervised Relation Extraction）框架，先利用已训练的远程监督关系抽取（DSRE）模型获取候选关系，再通过动态示例检索从训练数据中挑选可靠的句子级示例，填入大语言模型（LLM）提示以输出最终关系。实验显示，在英语以及四种低资源印地语（Oriya、Santali、Manipuri、Tulu）新基准上，模型的 F1 提升显著，验证了将 DSRE 与上下文学习相结合以及动态示例检索的有效性。", "keywords": "distant supervision, relation extraction, in-context learning, large language models, cross-lingual, exemplar retrieval, HYDRE", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Vipul Rathore", "Malik Hammad Faisal", "Parag Singla", "Mausam"]}
]]></acme>

<pubDate>2025-10-21T06:55:19+00:00</pubDate>
</item>
<item>
<title>ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</title>
<link>https://papers.cool/arxiv/2510.18339</link>
<guid>https://papers.cool/arxiv/2510.18339</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates domain adaptation of open-weight large language models for electrocardiography by fine‑tuning Llama 3.1 70B on specialized literature and comparing it to retrieval‑augmented generation (RAG) and Claude Sonnet 3.7 across multiple evaluation methods. Results show that the fine‑tuned model outperforms its base version and achieves competitive performance with proprietary models, though human experts favour Claude 3.7 and RAG for complex queries, highlighting evaluation heterogeneity.<br /><strong>Summary (CN):</strong> 本文研究了在心电图（electrocardiography）领域对开源大语言模型进行域适配的效果，通过在专门文献上微调 Llama 3.1 70B，并与检索增强生成（RAG）以及 Claude Sonnet 3.7 在多种评估方式下进行比较。结果表明，微调模型在多数评估中优于基模型，并在与商业模型的竞争中表现出色，但在人类专家对复杂查询的评估中更倾向于 Claude 3.7 和 RAG，凸显了评估方法的多样性与复杂性。<br /><strong>Keywords:</strong> electrocardiography, domain adaptation, large language model, Llama 3.1, retrieval-augmented generation, medical AI, fine-tuning, evaluation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.
<div><strong>Authors:</strong> Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates domain adaptation of open-weight large language models for electrocardiography by fine‑tuning Llama 3.1 70B on specialized literature and comparing it to retrieval‑augmented generation (RAG) and Claude Sonnet 3.7 across multiple evaluation methods. Results show that the fine‑tuned model outperforms its base version and achieves competitive performance with proprietary models, though human experts favour Claude 3.7 and RAG for complex queries, highlighting evaluation heterogeneity.", "summary_cn": "本文研究了在心电图（electrocardiography）领域对开源大语言模型进行域适配的效果，通过在专门文献上微调 Llama 3.1 70B，并与检索增强生成（RAG）以及 Claude Sonnet 3.7 在多种评估方式下进行比较。结果表明，微调模型在多数评估中优于基模型，并在与商业模型的竞争中表现出色，但在人类专家对复杂查询的评估中更倾向于 Claude 3.7 和 RAG，凸显了评估方法的多样性与复杂性。", "keywords": "electrocardiography, domain adaptation, large language model, Llama 3.1, retrieval-augmented generation, medical AI, fine-tuning, evaluation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lara Ahrens", "Wilhelm Haverkamp", "Nils Strodthoff"]}
]]></acme>

<pubDate>2025-10-21T06:45:38+00:00</pubDate>
</item>
<item>
<title>From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</title>
<link>https://papers.cool/arxiv/2510.18297</link>
<guid>https://papers.cool/arxiv/2510.18297</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces MedRGAG, a framework that unifies external retrieval and parametric generation to improve medical question answering by jointly leveraging retrieved evidence and generator‑produced background documents. It proposes two modules—Knowledge‑Guided Context Completion to fill gaps in retrieved knowledge, and Knowledge‑Aware Document Selection to compose concise evidence—resulting in notable performance gains on several benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 MedRGAG 框架，统一外部检索与模型内部生成，以提升医学问答系统的准确性。通过“知识引导的上下文补全”模块补足检索缺失的知识，并使用“知识感知文档选择”模块构建简洁且完整的证据集合，在多个医学 QA 基准上实现显著提升。<br /><strong>Keywords:</strong> medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge-guided context completion, knowledge-aware document selection, MedRGAG, external knowledge, parametric knowledge, hallucination mitigation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG
<div><strong>Authors:</strong> Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces MedRGAG, a framework that unifies external retrieval and parametric generation to improve medical question answering by jointly leveraging retrieved evidence and generator‑produced background documents. It proposes two modules—Knowledge‑Guided Context Completion to fill gaps in retrieved knowledge, and Knowledge‑Aware Document Selection to compose concise evidence—resulting in notable performance gains on several benchmarks.", "summary_cn": "本文提出 MedRGAG 框架，统一外部检索与模型内部生成，以提升医学问答系统的准确性。通过“知识引导的上下文补全”模块补足检索缺失的知识，并使用“知识感知文档选择”模块构建简洁且完整的证据集合，在多个医学 QA 基准上实现显著提升。", "keywords": "medical question answering, retrieval-augmented generation, generation-augmented generation, knowledge-guided context completion, knowledge-aware document selection, MedRGAG, external knowledge, parametric knowledge, hallucination mitigation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Lei Li", "Xiao Zhou", "Yingying Zhang", "Xian Wu"]}
]]></acme>

<pubDate>2025-10-21T04:58:29+00:00</pubDate>
</item>
<item>
<title>Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata</title>
<link>https://papers.cool/arxiv/2510.18289</link>
<guid>https://papers.cool/arxiv/2510.18289</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> Food4All introduces a multi‑agent framework that aggregates heterogeneous data sources and uses a lightweight reinforcement‑learning algorithm to provide real‑time, geographically‑aware recommendations of free food resources with nutritional annotations. The system continuously updates its resource pool from official databases, community platforms and social media, and incorporates a feedback loop to adapt to user needs, aiming to improve access for food‑insecure populations.<br /><strong>Summary (CN):</strong> Food4All 提出一个多代理框架，通过聚合官方数据库、社区平台和社交媒体等异构数据，并使用轻量级强化学习算法，实时提供地理可达且带有营养标注的免费食物资源推荐。系统通过在线反馈循环持续更新资源池并适应用户需求，旨在提升食物不安全人群的获取渠道。<br /><strong>Keywords:</strong> multi-agent system, reinforcement learning, food insecurity, real-time retrieval, nutritional metadata, data aggregation, resource recommendation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Zhengqing Yuan, Yiyang Li, Weixiang Sun, Zheyuan Zhang, Kaiwen Shi, Keerthiram Murugesan, Yanfang Ye</div>
Food insecurity remains a persistent public health emergency in the United States, tightly interwoven with chronic disease, mental illness, and opioid misuse. Yet despite the existence of thousands of food banks and pantries, access remains fragmented: 1) current retrieval systems depend on static directories or generic search engines, which provide incomplete and geographically irrelevant results; 2) LLM-based chatbots offer only vague nutritional suggestions and fail to adapt to real-world constraints such as time, mobility, and transportation; and 3) existing food recommendation systems optimize for culinary diversity but overlook survival-critical needs of food-insecure populations, including immediate proximity, verified availability, and contextual barriers. These limitations risk leaving the most vulnerable individuals, those experiencing homelessness, addiction, or digital illiteracy, unable to access urgently needed resources. To address this, we introduce Food4All, the first multi-agent framework explicitly designed for real-time, context-aware free food retrieval. Food4All unifies three innovations: 1) heterogeneous data aggregation across official databases, community platforms, and social media to provide a continuously updated pool of food resources; 2) a lightweight reinforcement learning algorithm trained on curated cases to optimize for both geographic accessibility and nutritional correctness; and 3) an online feedback loop that dynamically adapts retrieval policies to evolving user needs. By bridging information acquisition, semantic analysis, and decision support, Food4All delivers nutritionally annotated and guidance at the point of need. This framework establishes an urgent step toward scalable, equitable, and intelligent systems that directly support populations facing food insecurity and its compounding health risks.
<div><strong>Authors:</strong> Zhengqing Yuan, Yiyang Li, Weixiang Sun, Zheyuan Zhang, Kaiwen Shi, Keerthiram Murugesan, Yanfang Ye</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "Food4All introduces a multi‑agent framework that aggregates heterogeneous data sources and uses a lightweight reinforcement‑learning algorithm to provide real‑time, geographically‑aware recommendations of free food resources with nutritional annotations. The system continuously updates its resource pool from official databases, community platforms and social media, and incorporates a feedback loop to adapt to user needs, aiming to improve access for food‑insecure populations.", "summary_cn": "Food4All 提出一个多代理框架，通过聚合官方数据库、社区平台和社交媒体等异构数据，并使用轻量级强化学习算法，实时提供地理可达且带有营养标注的免费食物资源推荐。系统通过在线反馈循环持续更新资源池并适应用户需求，旨在提升食物不安全人群的获取渠道。", "keywords": "multi-agent system, reinforcement learning, food insecurity, real-time retrieval, nutritional metadata, data aggregation, resource recommendation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Zhengqing Yuan", "Yiyang Li", "Weixiang Sun", "Zheyuan Zhang", "Kaiwen Shi", "Keerthiram Murugesan", "Yanfang Ye"]}
]]></acme>

<pubDate>2025-10-21T04:35:02+00:00</pubDate>
</item>
<item>
<title>BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks</title>
<link>https://papers.cool/arxiv/2510.18288</link>
<guid>https://papers.cool/arxiv/2510.18288</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BrailleLLM, an instruction‑tuned large language model designed for Braille‑related tasks such as Braille translation, formula‑to‑Braille conversion, and mixed‑text translation. To overcome data scarcity, the authors release English and Chinese Braille mixed datasets with mathematical formulas and propose a syntax‑tree‑based augmentation technique, as well as Braille Knowledge‑Based Fine‑Tuning (BKFT) to improve contextual learning. Experiments show that BKFT yields substantial gains over standard fine‑tuning, providing a foundation for low‑resource multilingual Braille research.<br /><strong>Summary (CN):</strong> 本文提出 BrailleLLM，一种通过指令微调的 大语言模型（LLM），用于盲文翻译、公式转盲文以及混合文本翻译等盲文相关任务。为缓解数据稀缺，作者发布了英中文盲文混合数据集（EBMD/CBMD），并提出基于语法树的增强方法和盲文知识驱动的微调（BKFT），显著提升了模型在盲文翻译场景中的表现，为低资源多语言盲文研究奠定基础。<br /><strong>Keywords:</strong> Braille, instruction tuning, low-resource language modeling, data augmentation, mixed-text translation, knowledge-based fine-tuning, multilingual, accessibility<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu</div>
Braille plays a vital role in education and information accessibility for visually impaired individuals. However, Braille information processing faces challenges such as data scarcity and ambiguities in mixed-text contexts. We construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with mathematical formulas to support diverse Braille domain research, and propose a syntax tree-based augmentation method tailored for Braille data. To address the underperformance of traditional fine-tuning methods in Braille-related tasks, we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the learning difficulty of Braille contextual features. BrailleLLM employs BKFT via instruction tuning to achieve unified Braille translation, formula-to-Braille conversion, and mixed-text translation. Experiments demonstrate that BKFT achieves significant performance improvements over conventional fine-tuning in Braille translation scenarios. Our open-sourced datasets and methodologies establish a foundation for low-resource multilingual Braille research.
<div><strong>Authors:</strong> Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BrailleLLM, an instruction‑tuned large language model designed for Braille‑related tasks such as Braille translation, formula‑to‑Braille conversion, and mixed‑text translation. To overcome data scarcity, the authors release English and Chinese Braille mixed datasets with mathematical formulas and propose a syntax‑tree‑based augmentation technique, as well as Braille Knowledge‑Based Fine‑Tuning (BKFT) to improve contextual learning. Experiments show that BKFT yields substantial gains over standard fine‑tuning, providing a foundation for low‑resource multilingual Braille research.", "summary_cn": "本文提出 BrailleLLM，一种通过指令微调的 大语言模型（LLM），用于盲文翻译、公式转盲文以及混合文本翻译等盲文相关任务。为缓解数据稀缺，作者发布了英中文盲文混合数据集（EBMD/CBMD），并提出基于语法树的增强方法和盲文知识驱动的微调（BKFT），显著提升了模型在盲文翻译场景中的表现，为低资源多语言盲文研究奠定基础。", "keywords": "Braille, instruction tuning, low-resource language modeling, data augmentation, mixed-text translation, knowledge-based fine-tuning, multilingual, accessibility", "scoring": {"interpretability": 2, "understanding": 5, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Tianyuan Huang", "Zepeng Zhu", "Hangdi Xing", "Zirui Shao", "Zhi Yu", "Chaoxiong Yang", "Jiaxian He", "Xiaozhong Liu", "Jiajun Bu"]}
]]></acme>

<pubDate>2025-10-21T04:33:05+00:00</pubDate>
</item>
<item>
<title>Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18279</link>
<guid>https://papers.cool/arxiv/2510.18279</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes rendering long textual inputs as a single image and feeding it to multimodal decoder LLMs, achieving roughly a 50% reduction in required decoder tokens while maintaining performance on tasks such as long‑context retrieval (RULER) and document summarization (CNN/DailyMail). Experiments show that this visual text representation acts as an effective form of input compression without degrading downstream results.<br /><strong>Summary (CN):</strong> 本文提出将冗长的文本渲染为单幅图像，并直接输入多模态解码器 LLM，从而在保持性能的前提下将解码器 token 使用率降低约一半。通过在 RULER（长上下文检索）和 CNN/DailyMail（文档摘要）等基准上的实验，验证了视觉文本作为输入压缩方法的实用性和有效性。<br /><strong>Keywords:</strong> token efficiency, multimodal LLM, visual text, input compression, decoder tokens, long-context retrieval, summarization, image rendering<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.
<div><strong>Authors:</strong> Yanhong Li, Zixuan Lan, Jiawei Zhou</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes rendering long textual inputs as a single image and feeding it to multimodal decoder LLMs, achieving roughly a 50% reduction in required decoder tokens while maintaining performance on tasks such as long‑context retrieval (RULER) and document summarization (CNN/DailyMail). Experiments show that this visual text representation acts as an effective form of input compression without degrading downstream results.", "summary_cn": "本文提出将冗长的文本渲染为单幅图像，并直接输入多模态解码器 LLM，从而在保持性能的前提下将解码器 token 使用率降低约一半。通过在 RULER（长上下文检索）和 CNN/DailyMail（文档摘要）等基准上的实验，验证了视觉文本作为输入压缩方法的实用性和有效性。", "keywords": "token efficiency, multimodal LLM, visual text, input compression, decoder tokens, long-context retrieval, summarization, image rendering", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yanhong Li", "Zixuan Lan", "Jiawei Zhou"]}
]]></acme>

<pubDate>2025-10-21T04:07:20+00:00</pubDate>
</item>
<item>
<title>DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization</title>
<link>https://papers.cool/arxiv/2510.18257</link>
<guid>https://papers.cool/arxiv/2510.18257</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> DelvePO introduces a direction‑guided, self‑evolving framework for flexible prompt optimization that decouples prompts into components and employs a working memory to mitigate LLM uncertainties. By iteratively generating and refining prompts, the method achieves stable, transferable improvements across diverse tasks and models, outperform prior state‑of‑the‑art techniques in extensive experiments.<br /><strong>Summary (CN):</strong> DelvePO 提出了一种方向引导的自我演化提示优化框架，通过将提示拆分为不同组件并引入工作记忆，帮助大型语言模型克服自身不确定性并获取关键洞见，以生成新提示。该方法在多个任务和不同模型上实现了稳定、可迁移的性能提升，实验结果显示其在相同设置下持续优于之前的最方法。<br /><strong>Keywords:</strong> prompt optimization, large language models, self-evolving framework, direction-guided, working memory, task-agnostic, transferability, prompt engineering, L steering, experimental evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - alignment<br /><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\textbf{DelvePO}$ ($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.
<div><strong>Authors:</strong> Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "DelvePO introduces a direction‑guided, self‑evolving framework for flexible prompt optimization that decouples prompts into components and employs a working memory to mitigate LLM uncertainties. By iteratively generating and refining prompts, the method achieves stable, transferable improvements across diverse tasks and models, outperform prior state‑of‑the‑art techniques in extensive experiments.", "summary_cn": "DelvePO 提出了一种方向引导的自我演化提示优化框架，通过将提示拆分为不同组件并引入工作记忆，帮助大型语言模型克服自身不确定性并获取关键洞见，以生成新提示。该方法在多个任务和不同模型上实现了稳定、可迁移的性能提升，实验结果显示其在相同设置下持续优于之前的最方法。", "keywords": "prompt optimization, large language models, self-evolving framework, direction-guided, working memory, task-agnostic, transferability, prompt engineering, L steering, experimental evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "alignment"}, "authors": ["Tao Tao", "Guanghui Zhu", "Lang Guo", "Hongyi Chen", "Chunfeng Yuan", "Yihua Huang"]}
]]></acme>

<pubDate>2025-10-21T03:28:53+00:00</pubDate>
</item>
<item>
<title>MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives</title>
<link>https://papers.cool/arxiv/2510.18201</link>
<guid>https://papers.cool/arxiv/2510.18201</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents MARCUS, an event‑centric NLP pipeline that extracts events, participant characters, implied emotions, and sentiment from narratives, then aggregates inter‑character relations to generate quantitative character arcs illustrated on the Harry Potter and Lord of the Rings series. The authors evaluate the approach, discuss challenges, and outline potential applications of the generated arcs.<br /><strong>Summary (CN):</strong> 本文提出 MARCUS（Modelling Arcs for Understanding Stories），一种以事件为中心的 NLP 流水线，用于从叙事文本中抽取事件、角色、情感与情绪，并汇总角色间的关系以生成可量化的角色弧线，案例包括《哈利·波特》和《指环王》。文中对该方法进行评估，阐述现存挑战，并探讨其后续应用前景。<br /><strong>Keywords:</strong> character arcs, narrative analysis, event extraction, sentiment analysis, relational modeling, NLP pipeline, story understanding, fantasy literature<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sriharsh Bhyravajjula, Ujwal Narayan, Manish Shrivastava</div>
Character arcs are important theoretical devices employed in literary studies to understand character journeys, identify tropes across literary genres, and establish similarities between narratives. This work addresses the novel task of computationally generating event-centric, relation-based character arcs from narratives. Providing a quantitative representation for arcs brings tangibility to a theoretical concept and paves the way for subsequent applications. We present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that extracts events, participant characters, implied emotion, and sentiment to model inter-character relations. MARCUS tracks and aggregates these relations across the narrative to generate character arcs as graphical plots. We generate character arcs from two extended fantasy series, Harry Potter and Lord of the Rings. We evaluate our approach before outlining existing challenges, suggesting applications of our pipeline, and discussing future work.
<div><strong>Authors:</strong> Sriharsh Bhyravajjula, Ujwal Narayan, Manish Shrivastava</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents MARCUS, an event‑centric NLP pipeline that extracts events, participant characters, implied emotions, and sentiment from narratives, then aggregates inter‑character relations to generate quantitative character arcs illustrated on the Harry Potter and Lord of the Rings series. The authors evaluate the approach, discuss challenges, and outline potential applications of the generated arcs.", "summary_cn": "本文提出 MARCUS（Modelling Arcs for Understanding Stories），一种以事件为中心的 NLP 流水线，用于从叙事文本中抽取事件、角色、情感与情绪，并汇总角色间的关系以生成可量化的角色弧线，案例包括《哈利·波特》和《指环王》。文中对该方法进行评估，阐述现存挑战，并探讨其后续应用前景。", "keywords": "character arcs, narrative analysis, event extraction, sentiment analysis, relational modeling, NLP pipeline, story understanding, fantasy literature", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sriharsh Bhyravajjula", "Ujwal Narayan", "Manish Shrivastava"]}
]]></acme>

<pubDate>2025-10-21T01:03:48+00:00</pubDate>
</item>
<item>
<title>Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge</title>
<link>https://papers.cool/arxiv/2510.18196</link>
<guid>https://papers.cool/arxiv/2510.18196</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper identifies a score range bias in using large language models (LLMs) as judges for direct assessment, where the model outputs are highly sensitive to the predefined scoring interval. To mitigate this bias, the authors introduce a contrastive decoding technique, achieving up to a 11.3% relative improvement in Spearman correlation with human judgments across various score ranges.<br /><strong>Summary (CN):</strong> 本文指出在使用大型语言模型（LLM）作为评审者进行直接打分时存在分数区间偏差，即模型输出对预设评分范围高度敏感。作者提出采用对比解码方法来缓解此偏差，在不同分数区间下相较于人类评判的 Spearman 相关性提升最高 11.3%。<br /><strong>Keywords:</strong> contrastive decoding, LLM as judge, score range bias, evaluation reliability, Spearman correlation, human judgment alignment, language model evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yoshinari Fujinuma</div>
Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.
<div><strong>Authors:</strong> Yoshinari Fujinuma</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper identifies a score range bias in using large language models (LLMs) as judges for direct assessment, where the model outputs are highly sensitive to the predefined scoring interval. To mitigate this bias, the authors introduce a contrastive decoding technique, achieving up to a 11.3% relative improvement in Spearman correlation with human judgments across various score ranges.", "summary_cn": "本文指出在使用大型语言模型（LLM）作为评审者进行直接打分时存在分数区间偏差，即模型输出对预设评分范围高度敏感。作者提出采用对比解码方法来缓解此偏差，在不同分数区间下相较于人类评判的 Spearman 相关性提升最高 11.3%。", "keywords": "contrastive decoding, LLM as judge, score range bias, evaluation reliability, Spearman correlation, human judgment alignment, language model evaluation", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yoshinari Fujinuma"]}
]]></acme>

<pubDate>2025-10-21T00:47:11+00:00</pubDate>
</item>
<item>
<title>CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18173</link>
<guid>https://papers.cool/arxiv/2510.18173</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires large language models to generate dynamic tables under evolving schemas, probing robustness through extractive-cue ablation, temporal prefixing, and entity-form perturbations. Experiments on state-of-the-art LLMs reveal substantial performance drops when extractive shortcuts are removed, with longer contexts and altered entity forms further degrading accuracy, indicating brittle reasoning rather than random noise. The authors argue that robustness-first evaluation is essential for developing efficient and scalable text-to-table systems.<br /><strong>Summary (CN):</strong> 本文提出 CMT-Bench 基准，利用实时板球解说数据要求大语言模型在不断演变的模式下生成动态表格，并通过抽取线索消融、时间前缀以及实体形式扰动三类语义保持的改动来检验模型的鲁棒性。对最前沿 LLM 的实验显示，去除抽取摘要会导致显著性能下降，且输入长度增加和实体形式变化进一步削弱准确率，表明模型推理易出现漂移而非仅仅噪声。作者主张在文本到表格任务中应首先评估鲁棒性，以推动高效可扩展的方法开发。<br /><strong>Keywords:</strong> text-to-table, dynamic table generation, robustness, long-context, state tracking, benchmark, cricket commentary, LLM evaluation<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Ritam Upadhyay, Naman Ahuja, Rishabh Baral, Aparna Garimella, Vivek Gupta</div>
LLM Driven text-to-table (T2T) systems often rely on extensive prompt-engineering or iterative event extraction in code-parsable formats, which boosts scores but are computationally expensive and obscure how models actually reason over temporal evolving narratives to summarise key information. We present CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires dynamic table generation across two evolving schemas under a dense, rule-governed policy. CMT-Bench is designed to probe robustness via three semantics-preserving dimensions: (i) extractive-cue ablation to separate extractive shortcuts from state tracking, (ii) temporal prefixing to test long-context stability, and (iii) entity-form perturbations (anonymization, outof-distribution substitutions, role-entangling paraphrases) to assess sensitivity to surface variation. Across diverse long-context stateof-the-art LLMs, we find large drops without extractive summaries, monotonic degradation with input length, and consistent accuracy drop under entity-form changes. Complementary distributional tests confirm significant shifts in numeric error patterns, indicating drift in reasoning rather than mere noise. Our results show that current LLMs are brittle in dynamic Textto-table generation, motivating robustness-first evaluation as a prerequisite for developing efficient and scalable approaches for this task.
<div><strong>Authors:</strong> Ritam Upadhyay, Naman Ahuja, Rishabh Baral, Aparna Garimella, Vivek Gupta</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires large language models to generate dynamic tables under evolving schemas, probing robustness through extractive-cue ablation, temporal prefixing, and entity-form perturbations. Experiments on state-of-the-art LLMs reveal substantial performance drops when extractive shortcuts are removed, with longer contexts and altered entity forms further degrading accuracy, indicating brittle reasoning rather than random noise. The authors argue that robustness-first evaluation is essential for developing efficient and scalable text-to-table systems.", "summary_cn": "本文提出 CMT-Bench 基准，利用实时板球解说数据要求大语言模型在不断演变的模式下生成动态表格，并通过抽取线索消融、时间前缀以及实体形式扰动三类语义保持的改动来检验模型的鲁棒性。对最前沿 LLM 的实验显示，去除抽取摘要会导致显著性能下降，且输入长度增加和实体形式变化进一步削弱准确率，表明模型推理易出现漂移而非仅仅噪声。作者主张在文本到表格任务中应首先评估鲁棒性，以推动高效可扩展的方法开发。", "keywords": "text-to-table, dynamic table generation, robustness, long-context, state tracking, benchmark, cricket commentary, LLM evaluation", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Ritam Upadhyay", "Naman Ahuja", "Rishabh Baral", "Aparna Garimella", "Vivek Gupta"]}
]]></acme>

<pubDate>2025-10-20T23:51:28+00:00</pubDate>
</item>
<item>
<title>Automatic Prompt Generation via Adaptive Selection of Prompting Techniques</title>
<link>https://papers.cool/arxiv/2510.18162</link>
<guid>https://papers.cool/arxiv/2510.18162</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a method that automatically selects and combines prompting techniques for large language models by mapping user-provided task descriptions to semantically similar task clusters stored in a knowledge base, then generating high-quality prompts without pre‑existing templates. Experiments on 23 BIG‑Bench Extra Hard tasks show the approach outperforms standard prompts and existing automatic prompt‑generation tools in both arithmetic and harmonic mean scores. This work aims to make prompt engineering accessible to non‑experts by streamlining prompt creation.<br /><strong>Summary (CN):</strong> 本文提出一种自动生成大型语言模型提示的方法：通过将用户的任务描述映射到语义相似的任务聚类，并从知识库中选取相应的提示技术组合，生成高质量的提示，而无需预设模板。 在 23 项 BIG‑Bench Extra Hard 任务上的实验表明，该方法在算术平均和调和平均得分上均优于标准提示和已有的自动提示生成工具。 此工作旨在通过标准化提示创建流程，使非专业用户也能有效利用大语言模型。<br /><strong>Keywords:</strong> automatic prompt generation, adaptive prompting, task clustering, large language models, prompt engineering, BIG-Bench Extra Hard, knowledge base<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
Prompt engineering is crucial for achieving reliable and effective outputs from large language models (LLMs), but its design requires specialized knowledge of prompting techniques and a deep understanding of target tasks. To address this challenge, we propose a novel method that adaptively selects task-appropriate prompting techniques based on users' abstract task descriptions and automatically generates high-quality prompts without relying on pre-existing templates or frameworks. The proposed method constructs a knowledge base that associates task clusters, characterized by semantic similarity across diverse tasks, with their corresponding prompting techniques. When users input task descriptions, the system assigns them to the most relevant task cluster and dynamically generates prompts by integrating techniques drawn from the knowledge base. An experimental evaluation of the proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates superior performance compared with standard prompts and existing automatic prompt-generation tools, as measured by both arithmetic and harmonic mean scores. This research establishes a foundation for streamlining and standardizing prompt creation, enabling non-experts to effectively leverage LLMs.
<div><strong>Authors:</strong> Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a method that automatically selects and combines prompting techniques for large language models by mapping user-provided task descriptions to semantically similar task clusters stored in a knowledge base, then generating high-quality prompts without pre‑existing templates. Experiments on 23 BIG‑Bench Extra Hard tasks show the approach outperforms standard prompts and existing automatic prompt‑generation tools in both arithmetic and harmonic mean scores. This work aims to make prompt engineering accessible to non‑experts by streamlining prompt creation.", "summary_cn": "本文提出一种自动生成大型语言模型提示的方法：通过将用户的任务描述映射到语义相似的任务聚类，并从知识库中选取相应的提示技术组合，生成高质量的提示，而无需预设模板。 在 23 项 BIG‑Bench Extra Hard 任务上的实验表明，该方法在算术平均和调和平均得分上均优于标准提示和已有的自动提示生成工具。 此工作旨在通过标准化提示创建流程，使非专业用户也能有效利用大语言模型。", "keywords": "automatic prompt generation, adaptive prompting, task clustering, large language models, prompt engineering, BIG-Bench Extra Hard, knowledge base", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yohei Ikenoue", "Hitomi Tashiro", "Shigeru Kuroyanagi"]}
]]></acme>

<pubDate>2025-10-20T23:28:23+00:00</pubDate>
</item>
<item>
<title>Extracting Rule-based Descriptions of Attention Features in Transformers</title>
<link>https://papers.cool/arxiv/2510.18148</link>
<guid>https://papers.cool/arxiv/2510.18148</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes rule-based descriptions for Sparse Autoencoder (SAE) features extracted from transformer attention layers, moving beyond exemplar-based interpretations. It defines three rule types—skip-gram, absence, and counting rules—and presents an automatic extraction method applied to GPT-2 small, showing that many features can be captured with concise rule sets and revealing early-layer absence and counting behaviors. This work establishes a taxonomy and baseline for future rule-based feature interpretation research.<br /><strong>Summary (CN):</strong> 本文提出为 Transformer 注意力层的稀疏自编码器（SAE）特征提供基于规则的描述，超越仅靠示例进行解释的方法。文中定义了三类规则：跳词（skip-gram）规则、缺失规则和计数规则，并设计了自动提取流程，在 GPT-2 small 上验证，大多数特征可用约 100 条跳词规则描述，同时在早期层发现大量缺失规则并给出少量计数规则示例。该工作为基于规则的特征解释奠定了分类框架和初步基准。<br /><strong>Keywords:</strong> rule-based interpretability, SAE, attention, skip-gram rules, counting rules, transformer, mechanistic interpretability<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
Mechanistic interpretability strives to explain model behavior in terms of bottom-up primitives. The leading paradigm is to express hidden states as a sparse linear combination of basis vectors, called features. However, this only identifies which text sequences (exemplars) activate which features; the actual interpretation of features requires subjective inspection of these exemplars. This paper advocates for a different solution: rule-based descriptions that match token patterns in the input and correspondingly increase or decrease the likelihood of specific output tokens. Specifically, we extract rule-based descriptions of SAE features trained on the outputs of attention layers. While prior work treats the attention layers as an opaque box, we describe how it may naturally be expressed in terms of interactions between input and output features, of which we study three types: (1) skip-gram rules of the form "[Canadian city]... speaks --> English", (2) absence rules of the form "[Montreal]... speaks -/-> English," and (3) counting rules that toggle only when the count of a word exceeds a certain value or the count of another word. Absence and counting rules are not readily discovered by inspection of exemplars, where manual and automatic descriptions often identify misleading or incomplete explanations. We then describe a simple approach to extract these types of rules automatically from a transformer, and apply it to GPT-2 small. We find that a majority of features may be described well with around 100 skip-gram rules, though absence rules are abundant even as early as the first layer (in over a fourth of features). We also isolate a few examples of counting rules. This paper lays the groundwork for future research into rule-based descriptions of features by defining them, showing how they may be extracted, and providing a preliminary taxonomy of some of the behaviors they represent.
<div><strong>Authors:</strong> Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes rule-based descriptions for Sparse Autoencoder (SAE) features extracted from transformer attention layers, moving beyond exemplar-based interpretations. It defines three rule types—skip-gram, absence, and counting rules—and presents an automatic extraction method applied to GPT-2 small, showing that many features can be captured with concise rule sets and revealing early-layer absence and counting behaviors. This work establishes a taxonomy and baseline for future rule-based feature interpretation research.", "summary_cn": "本文提出为 Transformer 注意力层的稀疏自编码器（SAE）特征提供基于规则的描述，超越仅靠示例进行解释的方法。文中定义了三类规则：跳词（skip-gram）规则、缺失规则和计数规则，并设计了自动提取流程，在 GPT-2 small 上验证，大多数特征可用约 100 条跳词规则描述，同时在早期层发现大量缺失规则并给出少量计数规则示例。该工作为基于规则的特征解释奠定了分类框架和初步基准。", "keywords": "rule-based interpretability, SAE, attention, skip-gram rules, counting rules, transformer, mechanistic interpretability", "scoring": {"interpretability": 8, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Dan Friedman", "Adithya Bhaskar", "Alexander Wettig", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-20T22:52:40+00:00</pubDate>
</item>
<item>
<title>LLMs Encode How Difficult Problems Are</title>
<link>https://papers.cool/arxiv/2510.18147</link>
<guid>https://papers.cool/arxiv/2510.18147</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models internally encode problem difficulty in a way that aligns with human judgments, using linear probes across layers and token positions on 60 models. It finds that human-labeled difficulty is strongly decodable and scales with model size, while model-derived difficulty does not, and that steering models toward easier representations reduces hallucinations and improves accuracy, especially during reinforcement learning post‑training. The authors release code for probing and evaluation to promote replication.<br /><strong>Summary (CN):</strong> 本文研究大型语言模型是否内部编码了与人类判断一致的问题难度，通过在60个模型的不同层和位置上使用线性探针进行分析。结果显示，人类标注的难度在模型内部高度可线性解码且随模型规模提升，而模型自身推导的难度信号较弱且扩展性差；将模型表征引导至“更容易”方向可以降低幻觉并提升准确率，尤其在强化学习后训练阶段表现明显。作者公开了探针代码及评估脚本以便复现。<br /><strong>Keywords:</strong> difficulty encoding, linear probes, LLM interpretability, hallucination reduction, reinforcement learning post-training, Easy2HardBench, model scaling<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 8, Safety: 7, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> William Lugoloobi, Chris Russell</div>
Large language models exhibit a puzzling inconsistency: they solve complex problems yet frequently fail on seemingly simpler ones. We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment, and whether this representation tracks generalization during reinforcement learning post-training. We train linear probes across layers and token positions on 60 models, evaluating on mathematical and coding subsets of Easy2HardBench. We find that human-labeled difficulty is strongly linearly decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling, whereas LLM-derived difficulty is substantially weaker and scales poorly. Steering along the difficulty direction reveals that pushing models toward "easier" representations reduces hallucination and improves accuracy. During GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and positively correlates with test accuracy across training steps, while the LLM-difficulty probe degrades and negatively correlates with performance. These results suggest that human annotations provide a stable difficulty signal that RL amplifies, while automated difficulty estimates derived from model performance become misaligned precisely as models improve. We release probe code and evaluation scripts to facilitate replication.
<div><strong>Authors:</strong> William Lugoloobi, Chris Russell</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models internally encode problem difficulty in a way that aligns with human judgments, using linear probes across layers and token positions on 60 models. It finds that human-labeled difficulty is strongly decodable and scales with model size, while model-derived difficulty does not, and that steering models toward easier representations reduces hallucinations and improves accuracy, especially during reinforcement learning post‑training. The authors release code for probing and evaluation to promote replication.", "summary_cn": "本文研究大型语言模型是否内部编码了与人类判断一致的问题难度，通过在60个模型的不同层和位置上使用线性探针进行分析。结果显示，人类标注的难度在模型内部高度可线性解码且随模型规模提升，而模型自身推导的难度信号较弱且扩展性差；将模型表征引导至“更容易”方向可以降低幻觉并提升准确率，尤其在强化学习后训练阶段表现明显。作者公开了探针代码及评估脚本以便复现。", "keywords": "difficulty encoding, linear probes, LLM interpretability, hallucination reduction, reinforcement learning post-training, Easy2HardBench, model scaling", "scoring": {"interpretability": 7, "understanding": 8, "safety": 7, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["William Lugoloobi", "Chris Russell"]}
]]></acme>

<pubDate>2025-10-20T22:48:23+00:00</pubDate>
</item>
<item>
<title>Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment</title>
<link>https://papers.cool/arxiv/2510.18112</link>
<guid>https://papers.cool/arxiv/2510.18112</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether reasoning‑enhanced large language models improve the ability of agents to predict player actions in Dungeons & Dragons and generate Avrae Discord bot commands. Using the FIREBALL dataset, the authors compare a reasoning model (DeepSeek‑R1‑Distill‑LLaMA‑8B) with an instruction‑tuned model (LLaMA‑3.1‑8B‑Instruct) and find that precise prompt instructions dominate performance, with instruction models being sufficient for the task. Their experiments demonstrate that even single‑sentence changes in prompts can substantially affect model outputs.<br /><strong>Summary (CN):</strong> 本文研究了加入推理能力的大型语言模型是否能提升代理预测《龙与地下城》玩家动作并生成 Avrae Discord 机器人指令的效果。使用 FIREBALL 数据集，作者比较了推理模型 DeepSeek‑R1‑Distill‑LLaMA‑8B 与指令微调模型 LLaMA‑3.1‑8B‑Instruct，发现提供精确的提示指令对性能影响更大，指令模型已足以完成该任务。实验表明，即使是提示中的单句修改也会显著改变模型输出。<br /><strong>Keywords:</strong> Dungeons & Dragons, LLM agents, prompt engineering, reasoning, instruction tuning, FIREBALL dataset, Avrae, command generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 5, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Patricia Delafuente, Arya Honraopatil, Lara J. Martin</div>
This paper explores the application of Large Language Models (LLMs) and reasoning to predict Dungeons & Dragons (DnD) player actions and format them as Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model, LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the importance of providing specific instructions to models, that even single sentence changes in prompts can greatly affect the output of models, and that instruct models are sufficient for this task compared to reasoning models.
<div><strong>Authors:</strong> Patricia Delafuente, Arya Honraopatil, Lara J. Martin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether reasoning‑enhanced large language models improve the ability of agents to predict player actions in Dungeons & Dragons and generate Avrae Discord bot commands. Using the FIREBALL dataset, the authors compare a reasoning model (DeepSeek‑R1‑Distill‑LLaMA‑8B) with an instruction‑tuned model (LLaMA‑3.1‑8B‑Instruct) and find that precise prompt instructions dominate performance, with instruction models being sufficient for the task. Their experiments demonstrate that even single‑sentence changes in prompts can substantially affect model outputs.", "summary_cn": "本文研究了加入推理能力的大型语言模型是否能提升代理预测《龙与地下城》玩家动作并生成 Avrae Discord 机器人指令的效果。使用 FIREBALL 数据集，作者比较了推理模型 DeepSeek‑R1‑Distill‑LLaMA‑8B 与指令微调模型 LLaMA‑3.1‑8B‑Instruct，发现提供精确的提示指令对性能影响更大，指令模型已足以完成该任务。实验表明，即使是提示中的单句修改也会显著改变模型输出。", "keywords": "Dungeons & Dragons, LLM agents, prompt engineering, reasoning, instruction tuning, FIREBALL dataset, Avrae, command generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 5, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Patricia Delafuente", "Arya Honraopatil", "Lara J. Martin"]}
]]></acme>

<pubDate>2025-10-20T21:23:23+00:00</pubDate>
</item>
<item>
<title>Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica</title>
<link>https://papers.cool/arxiv/2510.18108</link>
<guid>https://papers.cool/arxiv/2510.18108</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper reports an experimental study evaluating four AI systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) on tasks that emulate lawyers' daily work, using a protocol that measures material correctness, systematic coherence, and argumentative integrity. The domain‑specialized model JusIA consistently outperformed the general‑purpose models, indicating that specialization and theoretically grounded evaluation are crucial for reliable legal AI outputs. The study involves 48 legal professionals and proposes a systematic assessment framework for legal AI.<br /><strong>Summary (CN):</strong> 本文通过实验评估四种 AI 系统（JusIA、ChatGPT Free、ChatGPT Pro 和 Gemini）在模拟律师日常工作任务中的表现，使用包括材料正确性、系统一致性和论证完整性在内的评估协议。结果显示，专门面向法律领域的模型 JusIA 始终优于通用模型，表明领域专业化和理论化评估对获得可靠的法律 AI 输出至关重要。该研究邀请了 48 名法律专业人士参与，并提出了一套系统的法律 AI 评估框架。<br /><strong>Keywords:</strong> legal AI, domain specialization, AI evaluation, jurisprudence, ChatGPT, Gemini, factual correctness, argumentative integrity, AI safety, model benchmarking<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 5, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - other; Primary focus - other<br /><strong>Authors:</strong> Marina Soares Marinho, Daniela Vianna, Livy Real, Altigran da Silva, Gabriela Migliorini</div>
This study presents the Jusbrasil Study on the Use of General-Purpose AIs in Law, proposing an experimental evaluation protocol combining legal theory, such as material correctness, systematic coherence, and argumentative integrity, with empirical assessment by 48 legal professionals. Four systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers' daily work. JusIA, a domain-specialized model, consistently outperformed the general-purpose systems, showing that both domain specialization and a theoretically grounded evaluation are essential for reliable legal AI outputs.
<div><strong>Authors:</strong> Marina Soares Marinho, Daniela Vianna, Livy Real, Altigran da Silva, Gabriela Migliorini</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper reports an experimental study evaluating four AI systems (JusIA, ChatGPT Free, ChatGPT Pro, and Gemini) on tasks that emulate lawyers' daily work, using a protocol that measures material correctness, systematic coherence, and argumentative integrity. The domain‑specialized model JusIA consistently outperformed the general‑purpose models, indicating that specialization and theoretically grounded evaluation are crucial for reliable legal AI outputs. The study involves 48 legal professionals and proposes a systematic assessment framework for legal AI.", "summary_cn": "本文通过实验评估四种 AI 系统（JusIA、ChatGPT Free、ChatGPT Pro 和 Gemini）在模拟律师日常工作任务中的表现，使用包括材料正确性、系统一致性和论证完整性在内的评估协议。结果显示，专门面向法律领域的模型 JusIA 始终优于通用模型，表明领域专业化和理论化评估对获得可靠的法律 AI 输出至关重要。该研究邀请了 48 名法律专业人士参与，并提出了一套系统的法律 AI 评估框架。", "keywords": "legal AI, domain specialization, AI evaluation, jurisprudence, ChatGPT, Gemini, factual correctness, argumentative integrity, AI safety, model benchmarking", "scoring": {"interpretability": 3, "understanding": 6, "safety": 5, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "other", "primary_focus": "other"}, "authors": ["Marina Soares Marinho", "Daniela Vianna", "Livy Real", "Altigran da Silva", "Gabriela Migliorini"]}
]]></acme>

<pubDate>2025-10-20T21:09:50+00:00</pubDate>
</item>
<item>
<title>Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18077</link>
<guid>https://papers.cool/arxiv/2510.18077</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how chain-of-thought prompting improves large language models' ability to translate texts with inter‑sentential dependencies, using the English‑French DiscEvalMT benchmark for pronoun anaphora and lexical cohesion challenges. Experiments with 12 LLMs show that reasoning prompts raise discrimination accuracy to about 90% and COMET translation quality to roughly 92%, with GPT‑4, GPT‑4o and Phi achieving the best results.<br /><strong>Summary (CN):</strong> 本文研究了链式思考（chain‑of‑thought）提示如何提升大型语言模型在涉及跨句依存关系的翻译任务中的表现，使用英法 DiscEvalMT 基准考察代词指代和词汇衔接难题。对 12 种 LLM 进行实验表明，使用推理提示后，判别正确翻译的准确率可达约 90%，而基于 COMET 的翻译质量约为 92%，其中 GPT‑4、GPT‑4o 与 Phi 表现尤为突出。<br /><strong>Keywords:</strong> chain-of-thought, context-aware translation, inter-sentential dependencies, DiscEvalMT, LLM prompting, translation evaluation, COMET, reasoning prompts<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - interpretability<br /><strong>Authors:</strong> Shabnam Ataee, Andrei Popescu-Belis</div>
This paper assesses the capacity of large language models (LLMs) to translate texts that include inter-sentential dependencies. We use the English-French DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing translation challenges either for pronominal anaphora or for lexical cohesion. We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families on two tasks: (1) distinguishing a correct translation from a wrong but plausible one; (2) generating a correct translation. We compare prompts that encourage chain-of-thought reasoning with those that do not. The best models take advantage of reasoning and reach about 90% accuracy on the first task, and COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi standing out. Moreover, we observe a "wise get wiser" effect: the improvements through reasoning are positively correlated with the scores of the models without reasoning.
<div><strong>Authors:</strong> Shabnam Ataee, Andrei Popescu-Belis</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how chain-of-thought prompting improves large language models' ability to translate texts with inter‑sentential dependencies, using the English‑French DiscEvalMT benchmark for pronoun anaphora and lexical cohesion challenges. Experiments with 12 LLMs show that reasoning prompts raise discrimination accuracy to about 90% and COMET translation quality to roughly 92%, with GPT‑4, GPT‑4o and Phi achieving the best results.", "summary_cn": "本文研究了链式思考（chain‑of‑thought）提示如何提升大型语言模型在涉及跨句依存关系的翻译任务中的表现，使用英法 DiscEvalMT 基准考察代词指代和词汇衔接难题。对 12 种 LLM 进行实验表明，使用推理提示后，判别正确翻译的准确率可达约 90%，而基于 COMET 的翻译质量约为 92%，其中 GPT‑4、GPT‑4o 与 Phi 表现尤为突出。", "keywords": "chain-of-thought, context-aware translation, inter-sentential dependencies, DiscEvalMT, LLM prompting, translation evaluation, COMET, reasoning prompts", "scoring": {"interpretability": 4, "understanding": 6, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "interpretability"}, "authors": ["Shabnam Ataee", "Andrei Popescu-Belis"]}
]]></acme>

<pubDate>2025-10-20T20:14:46+00:00</pubDate>
</item>
<item>
<title>Language Models as Semantic Augmenters for Sequential Recommenders</title>
<link>https://papers.cool/arxiv/2510.18046</link>
<guid>https://papers.cool/arxiv/2510.18046</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes LaMAR, a framework that uses large language models in a few‑shot setting to generate auxiliary semantic signals (e.g., inferred usage scenarios, item intents) that augment sequential interaction histories for recommender systems. By enriching the original user-item sequences with these generated contextual cues, LaMAR consistently improves performance on benchmark sequential recommendation tasks and demonstrates high semantic novelty and diversity of the signals.<br /><strong>Summary (CN):</strong> 本文提出 LaMAR 框架，利用大型语言模型（LLM）在少样本条件下生成辅助语义信号（如使用场景推断、物品意图），以丰富序列推荐系统中的用户‑物品交互序列。通过将这些生成的上下文信息加入原始序列，LaMAR 在基准序列推荐任务中显著提升了性能，并展示了信号的高度语义新颖性和多样性。<br /><strong>Keywords:</strong> semantic augmentation, large language models, sequential recommendation, data-centric AI, few-shot prompting<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 3, Technicality: 6, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
Large Language Models (LLMs) excel at capturing latent semantics and contextual relationships across diverse modalities. However, in modeling user behavior from sequential interaction data, performance often suffers when such semantic context is limited or absent. We introduce LaMAR, a LLM-driven semantic enrichment framework designed to enrich such sequences automatically. LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual signals by inferring latent semantic aspects of a user's intent and item relationships from existing metadata. These generated signals, such as inferred usage scenarios, item intents, or thematic summaries, augment the original sequences with greater contextual depth. We demonstrate the utility of this generated resource by integrating it into benchmark sequential modeling tasks, where it consistently improves performance. Further analysis shows that LLM-generated signals exhibit high semantic novelty and diversity, enhancing the representational capacity of the downstream models. This work represents a new data-centric paradigm where LLMs serve as intelligent context generators, contributing a new method for the semi-automatic creation of training data and language resources.
<div><strong>Authors:</strong> Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes LaMAR, a framework that uses large language models in a few‑shot setting to generate auxiliary semantic signals (e.g., inferred usage scenarios, item intents) that augment sequential interaction histories for recommender systems. By enriching the original user-item sequences with these generated contextual cues, LaMAR consistently improves performance on benchmark sequential recommendation tasks and demonstrates high semantic novelty and diversity of the signals.", "summary_cn": "本文提出 LaMAR 框架，利用大型语言模型（LLM）在少样本条件下生成辅助语义信号（如使用场景推断、物品意图），以丰富序列推荐系统中的用户‑物品交互序列。通过将这些生成的上下文信息加入原始序列，LaMAR 在基准序列推荐任务中显著提升了性能，并展示了信号的高度语义新颖性和多样性。", "keywords": "semantic augmentation, large language models, sequential recommendation, data-centric AI, few-shot prompting", "scoring": {"interpretability": 3, "understanding": 5, "safety": 3, "technicality": 6, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Mahsa Valizadeh", "Xiangjue Dong", "Rui Tuo", "James Caverlee"]}
]]></acme>

<pubDate>2025-10-20T19:36:38+00:00</pubDate>
</item>
<item>
<title>From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18030</link>
<guid>https://papers.cool/arxiv/2510.18030</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper revisits global structured pruning for LLMs and introduces GISP-Global Iterative Structured Pruning, a post‑training method that uses loss‑based importance scores aggregated at the structure level to prune attention heads and MLP channels iteratively. By defining importance with a model‑level loss, the approach naturally incorporates task‑specific objectives, achieving lower perplexity and higher downstream accuracy on several LLM families at moderate sparsity levels. Exper demonstrate that the iterative schedule stabilizes accuracy without intermediate fine‑tuning and enables a "prune‑once, deploy‑many" workflow.<br /><strong>Summary (CN):</strong> 本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝方法，通过在结构层面聚合基于损失的重要性分数，对注意力头和 MLP 通道进行迭代剪枝。由于重要性由模型整体损失定义，该方法可自然地结合任务特定目标，在多种LM 上实现更低的困惑度和更高的下游任务精度，并在中等稀疏度下保持准确性而无需中间微调，实现“一次剪枝，多次部署”。<br /><strong>Keywords:</strong> structured pruning, large language models, global pruning, task-aligned pruning, iterative pruning, sparsity, lama, Mistral<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 3, Technicality: 8, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a "prune-once, deploy-many" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.
<div><strong>Authors:</strong> Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper revisits global structured pruning for LLMs and introduces GISP-Global Iterative Structured Pruning, a post‑training method that uses loss‑based importance scores aggregated at the structure level to prune attention heads and MLP channels iteratively. By defining importance with a model‑level loss, the approach naturally incorporates task‑specific objectives, achieving lower perplexity and higher downstream accuracy on several LLM families at moderate sparsity levels. Exper demonstrate that the iterative schedule stabilizes accuracy without intermediate fine‑tuning and enables a \"prune‑once, deploy‑many\" workflow.", "summary_cn": "本文重新审视大语言模型的全局结构化剪枝，提出 GISP‑Global 迭代结构化剪枝方法，通过在结构层面聚合基于损失的重要性分数，对注意力头和 MLP 通道进行迭代剪枝。由于重要性由模型整体损失定义，该方法可自然地结合任务特定目标，在多种LM 上实现更低的困惑度和更高的下游任务精度，并在中等稀疏度下保持准确性而无需中间微调，实现“一次剪枝，多次部署”。", "keywords": "structured pruning, large language models, global pruning, task-aligned pruning, iterative pruning, sparsity,lama, Mistral", "scoring": {"interpretability": 2, "understanding": 6, "safety": 3, "technicality": 8, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"]}
]]></acme>

<pubDate>2025-10-20T19:04:09+00:00</pubDate>
</item>
<item>
<title>Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution</title>
<link>https://papers.cool/arxiv/2510.18019</link>
<guid>https://papers.cool/arxiv/2510.18019</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper shows that current multilingual LLM watermarking methods lose effectiveness after translation, especially in medium‑ and low‑resource languages, due to limited full‑word tokens. It introduces STEAM, a back‑translation‑based detection technique that recovers watermark strength and works with any existing watermarking scheme, improving robustness across tokenizers and languages. Experiments on 17 languages report average gains of +0.19 AUC and +40 %p TPR@1 %.<br /><strong>Summary (CN):</strong> 本文指出现有的多语言 LLM 水印技术在翻译攻击下（尤其是中低资源语言）失效，原因是分词器中对应语言的完整词汇不足导致语义聚类失败。为此提出 STEAM——一种基于回译的检测方法，可恢复水印强度并兼容任意水印方案，在不同分词器和语言上均表现出更强的鲁棒性。实验在 17 种语言上实现了平均 AUC 提升 0.19、TPR@1% 提升 40 %点。<br /><strong>Keywords:</strong> multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization robustness, translation attack, cross-lingual robustness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.
<div><strong>Authors:</strong> Asim Mohamed, Martin Gubri</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper shows that current multilingual LLM watermarking methods lose effectiveness after translation, especially in medium‑ and low‑resource languages, due to limited full‑word tokens. It introduces STEAM, a back‑translation‑based detection technique that recovers watermark strength and works with any existing watermarking scheme, improving robustness across tokenizers and languages. Experiments on 17 languages report average gains of +0.19 AUC and +40 %p TPR@1 %.", "summary_cn": "本文指出现有的多语言 LLM 水印技术在翻译攻击下（尤其是中低资源语言）失效，原因是分词器中对应语言的完整词汇不足导致语义聚类失败。为此提出 STEAM——一种基于回译的检测方法，可恢复水印强度并兼容任意水印方案，在不同分词器和语言上均表现出更强的鲁棒性。实验在 17 种语言上实现了平均 AUC 提升 0.19、TPR@1% 提升 40 %点。", "keywords": "multilingual watermarking, back-translation detection, STEAM, LLM traceability, tokenization robustness, translation attack, cross-lingual robustness", "scoring": {"interpretability": 3, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Asim Mohamed", "Martin Gubri"]}
]]></acme>

<pubDate>2025-10-20T18:51:20+00:00</pubDate>
</item>
<item>
<title>SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone</title>
<link>https://papers.cool/arxiv/2510.17998</link>
<guid>https://papers.cool/arxiv/2510.17998</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> SimBA is a three‑phase framework that uses only raw evaluation scores to simplify analysis of large language‑model benchmarks. It first compares datasets and models (stalk), then discovers a small representative subset that still covers the benchmark (prowl), and finally predicts performance of held‑out models with near‑zero error using this subset (pounce). Experiments on HELM, MMLU, and BigBenchLite show strong dataset‑model relationships and high coverage using only a few percent of the original datasets.<br /><strong>Summary (CN):</strong> SimBA 是一种仅利用原始评估分数的三阶段框架，用于简化大语言模型基准的分析。首先进行数据集与模型比较（stalk），随后发现能够覆盖基准的少量代表性子集（prowl），最后使用该子集预测未见模型的性能，误差几乎为零（pounce）。在 HELM、MMLU 和 BigBenchLite 上的实验表明，数据集与模型之间关系紧密，仅使用极少比例的数据集即可实现至少 95% 的覆盖率。<br /><strong>Keywords:</strong> benchmark analysis, performance matrices, representative subset, model ranking, LM evaluation, dataset coverage, SimBA, predictive performance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
Modern language models are evaluated on large benchmarks, which are difficult to make sense of, especially for model selection. Looking at the raw evaluation numbers themselves using a model-centric lens, we propose SimBA, a three phase framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk, where we conduct dataset & model comparisons, prowl, where we discover a representative subset, and pounce, where we use the representative subset to predict performance on a held-out set of models. Applying SimBA to three popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all three benchmarks, datasets and models relate strongly to one another (stalk). We develop an representative set discovery algorithm which covers a benchmark using raw evaluation scores alone. Using our algorithm, we find that with 6.25% (1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl). Additionally, using just these representative subsets, we can both preserve model ranks and predict performance on a held-out set of models with near zero mean-squared error (pounce). Taken together, SimBA can help model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark. Our code is open source, available at https://github.com/nishantsubramani/simba.
<div><strong>Authors:</strong> Nishant Subramani, Alfredo Gomez, Mona Diab</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "SimBA is a three‑phase framework that uses only raw evaluation scores to simplify analysis of large language‑model benchmarks. It first compares datasets and models (stalk), then discovers a small representative subset that still covers the benchmark (prowl), and finally predicts performance of held‑out models with near‑zero error using this subset (pounce). Experiments on HELM, MMLU, and BigBenchLite show strong dataset‑model relationships and high coverage using only a few percent of the original datasets.", "summary_cn": "SimBA 是一种仅利用原始评估分数的三阶段框架，用于简化大语言模型基准的分析。首先进行数据集与模型比较（stalk），随后发现能够覆盖基准的少量代表性子集（prowl），最后使用该子集预测未见模型的性能，误差几乎为零（pounce）。在 HELM、MMLU 和 BigBenchLite 上的实验表明，数据集与模型之间关系紧密，仅使用极少比例的数据集即可实现至少 95% 的覆盖率。", "keywords": "benchmark analysis, performance matrices, representative subset, model ranking, LM evaluation, dataset coverage, SimBA, predictive performance", "scoring": {"interpretability": 2, "understanding": 6, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Nishant Subramani", "Alfredo Gomez", "Mona Diab"]}
]]></acme>

<pubDate>2025-10-20T18:23:27+00:00</pubDate>
</item>
<item>
<title>Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</title>
<link>https://papers.cool/arxiv/2510.17941</link>
<guid>https://papers.cool/arxiv/2510.17941</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a framework to quantify how deeply implanted facts are believed by large language models, evaluating generalization, robustness to challenge, and similarity to genuine knowledge via linear probes. Experiments show that simple prompting and mechanistic edits rarely achieve deep belief implantation, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides measurable criteria for belief depth, enabling more rigorous assessment of knowledge editing techniques before real‑world deployment.<br /><strong>Summary (CN):</strong> 本文提出了一套衡量大语言模型对植入事实信念深度的框架，评估植入知识在相关上下文中的泛化、对自我审视与直接挑战的鲁棒性，以及通过线性探测器与真实知识的表征相似性。实验表明，简单提示和机制性编辑难以实现深层信念植入，而合成文档微调（Synthetic Document Finetuning）常能成功，但在与基础世界知识冲突的事实上表现脆弱且表征不同。该工作提供了可量化的信念深度标准，为在实际应用中安全地使用知识编辑提供了评估手段。<br /><strong>Keywords:</strong> belief depth, knowledge editing, large language models, synthetic document finetuning, linear probes, factuality, model alignment, interpretability, knowledge injection, robustness<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.
<div><strong>Authors:</strong> Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a framework to quantify how deeply implanted facts are believed by large language models, evaluating generalization, robustness to challenge, and similarity to genuine knowledge via linear probes. Experiments show that simple prompting and mechanistic edits rarely achieve deep belief implantation, while Synthetic Document Finetuning often does, though it struggles with facts that conflict with basic world knowledge. The work provides measurable criteria for belief depth, enabling more rigorous assessment of knowledge editing techniques before real‑world deployment.", "summary_cn": "本文提出了一套衡量大语言模型对植入事实信念深度的框架，评估植入知识在相关上下文中的泛化、对自我审视与直接挑战的鲁棒性，以及通过线性探测器与真实知识的表征相似性。实验表明，简单提示和机制性编辑难以实现深层信念植入，而合成文档微调（Synthetic Document Finetuning）常能成功，但在与基础世界知识冲突的事实上表现脆弱且表征不同。该工作提供了可量化的信念深度标准，为在实际应用中安全地使用知识编辑提供了评估手段。", "keywords": "belief depth, knowledge editing, large language models, synthetic document finetuning, linear probes, factuality, model alignment, interpretability, knowledge injection, robustness", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Stewart Slocum", "Julian Minder", "Clément Dumas", "Henry Sleight", "Ryan Greenblatt", "Samuel Marks", "Rowan Wang"]}
]]></acme>

<pubDate>2025-10-20T16:58:54+00:00</pubDate>
</item>
<item>
<title>AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM</title>
<link>https://papers.cool/arxiv/2510.17934</link>
<guid>https://papers.cool/arxiv/2510.17934</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces AtlasKV, a parametric method for integrating billion‑scale knowledge graphs into large language models with less than 20 GB VRAM, using novel components KG2KV and HiKVP that achieve sub‑linear time and memory complexity. By embedding KG triples directly into the model’s attention mechanism, AtlasKV eliminates the need for external retrievers or long context windows while preserving strong knowledge grounding and generalization. The approach is presented as scalable, effective, and adaptable to new knowledge without retraining.<br /><strong>Summary (CN):</strong> 本文提出 AtlasKV，一种将十亿规模知识图谱以少于 20 GB 显存直接嵌入大语言模型的参数化方法，采用 KG2KV 与 HiKVP 实现次线性时间和内存复杂度。该方法利用模型自带的注意力机制将 KG 三元组内嵌，无需外部检索器或冗长上下文，同时保持稳健的知识对齐与泛化能力，且在加入新知识时无需重新训练。<br /><strong>Keywords:</strong> knowledge graph, parametric knowledge integration, AtlasKV, KG2KV, HiKVP, LLM augmentation, low VRAM, sub-linear memory, factuality, retrieval-augmented generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
Retrieval-augmented generation (RAG) has shown some success in augmenting large language models (LLMs) with external knowledge. However, as a non-parametric knowledge integration paradigm for LLMs, RAG methods heavily rely on external retrieval modules and the retrieved textual context prior. Especially for very large scale knowledge augmentation, they would introduce substantial inference latency due to expensive searches and much longer relevant context. In this paper, we propose a parametric knowledge integration method, called \textbf{AtlasKV}, a scalable, effective, and general way to augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with sub-linear time and memory complexity. It maintains strong knowledge grounding and generalization performance using the LLMs' inherent attention mechanism, and requires no external retrievers, long context priors, or retraining when adapting to new knowledge.
<div><strong>Authors:</strong> Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces AtlasKV, a parametric method for integrating billion‑scale knowledge graphs into large language models with less than 20 GB VRAM, using novel components KG2KV and HiKVP that achieve sub‑linear time and memory complexity. By embedding KG triples directly into the model’s attention mechanism, AtlasKV eliminates the need for external retrievers or long context windows while preserving strong knowledge grounding and generalization. The approach is presented as scalable, effective, and adaptable to new knowledge without retraining.", "summary_cn": "本文提出 AtlasKV，一种将十亿规模知识图谱以少于 20 GB 显存直接嵌入大语言模型的参数化方法，采用 KG2KV 与 HiKVP 实现次线性时间和内存复杂度。该方法利用模型自带的注意力机制将 KG 三元组内嵌，无需外部检索器或冗长上下文，同时保持稳健的知识对齐与泛化能力，且在加入新知识时无需重新训练。", "keywords": "knowledge graph, parametric knowledge integration, AtlasKV, KG2KV, HiKVP, LLM augmentation, low VRAM, sub-linear memory, factuality, retrieval-augmented generation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haoyu Huang", "Hong Ting Tsang", "Jiaxin Bai", "Xi Peng", "Gong Zhang", "Yangqiu Song"]}
]]></acme>

<pubDate>2025-10-20T15:40:14+00:00</pubDate>
</item>
<item>
<title>Diagnosing Representation Dynamics in NER Model Extension</title>
<link>https://papers.cool/arxiv/2510.17930</link>
<guid>https://papers.cool/arxiv/2510.17930</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper studies how a BERT‑based NER system can be extended to new PII entities in noisy spoken language without harming performance on existing semantic entities. Using incremental learning diagnostics, it reveals that location tags are vulnerable due to representation overlap with PII patterns and that the background ‘O’ tag can block learning of new patterns unless its classifier is unfrozen. These findings provide a mechanistic interpretation of feature independence, representation overlap, and O‑tag plasticity in NER model adaptation.<br /><strong>Summary (CN):</strong> 本文研究了在嘈杂口语数据中，将 BERT‑基准的命名实体识别（NER）模型扩展至新的个人身份信息（PII）实体时，如何保持对原有语义实体的性能。通过增量学习诊断，发现位置实体因与 PII 模式存在表征重叠而特别脆弱，同时背景 ‘O’ 标签若保持不变会阻碍新模式的学习，只有在解冻其分类器后才能“释放”这些模式。该工作 mechanistic 地解释了特征独立性、表征重叠以及 O‑标签可塑性在 NER 模型适应过程中的作用。<br /><strong>Keywords:</strong> NER, representation drift, BERT, PII detection, incremental learning, feature independence, O-tag plasticity, semantic overlap, mechanistic interpretability<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
Extending Named Entity Recognition (NER) models to new PII entities in noisy spoken-language data is a common need. We find that jointly fine-tuning a BERT model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII (EMAIL, PHONE) results in minimal degradation for original classes. We investigate this "peaceful coexistence," hypothesizing that the model uses independent semantic vs. morphological feature mechanisms. Using an incremental learning setup as a diagnostic tool, we measure semantic drift and find two key insights. First, the LOC (location) entity is uniquely vulnerable due to a representation overlap with new PII, as it shares pattern-like features (e.g., postal codes). Second, we identify a "reverse O-tag representation drift." The model, initially trained to map PII patterns to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's classifier, allowing the background class to adapt and "release" these patterns. This work provides a mechanistic diagnosis of NER model adaptation, highlighting feature independence, representation overlap, and 'O' tag plasticity.
<div><strong>Authors:</strong> Xirui Zhang, Philippe de La Chevasnerie, Benoit Fabre</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper studies how a BERT‑based NER system can be extended to new PII entities in noisy spoken language without harming performance on existing semantic entities. Using incremental learning diagnostics, it reveals that location tags are vulnerable due to representation overlap with PII patterns and that the background ‘O’ tag can block learning of new patterns unless its classifier is unfrozen. These findings provide a mechanistic interpretation of feature independence, representation overlap, and O‑tag plasticity in NER model adaptation.", "summary_cn": "本文研究了在嘈杂口语数据中，将 BERT‑基准的命名实体识别（NER）模型扩展至新的个人身份信息（PII）实体时，如何保持对原有语义实体的性能。通过增量学习诊断，发现位置实体因与 PII 模式存在表征重叠而特别脆弱，同时背景 ‘O’ 标签若保持不变会阻碍新模式的学习，只有在解冻其分类器后才能“释放”这些模式。该工作 mechanistic 地解释了特征独立性、表征重叠以及 O‑标签可塑性在 NER 模型适应过程中的作用。", "keywords": "NER, representation drift, BERT, PII detection, incremental learning, feature independence, O-tag plasticity, semantic overlap, mechanistic interpretability", "scoring": {"interpretability": 7, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Xirui Zhang", "Philippe de La Chevasnerie", "Benoit Fabre"]}
]]></acme>

<pubDate>2025-10-20T14:53:42+00:00</pubDate>
</item>
<item>
<title>Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs</title>
<link>https://papers.cool/arxiv/2510.17924</link>
<guid>https://papers.cool/arxiv/2510.17924</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a comparative study of various NLP approaches—traditional embeddings, fine‑tuned transformer models, zero‑shot/few‑shot prompting of large language models, and retrieval‑augmented generation—for detecting toxic content in online gaming chats. It evaluates each method on accuracy, processing speed, and computational cost, and proposes a hybrid moderation architecture that reduces human moderator workload while enabling continuous learning. Experiments show that fine‑tuned DistilBERT offers the best accuracy‑cost balance, providing practical guidance for cost‑effective content moderation in dynamic gaming environments.<br /><strong>Summary (CN):</strong> 本文比较了多种自然语言处理方法（传统嵌入、微调的 Transformer 模型、大语言模型的零示例/少示例提示以及检索增强生成）在在线游戏聊天中检测有害言论的效果，并在分类准确率、处理速度和计算成本三个维度进行评估。同时提出了一种混合审查系统架构，通过自动检测减轻人工审查负担并支持持续学习。实验结果表明，微调的 DistilBERT 在准确率‑成本权衡上表现最佳，为在动态游戏环境中部署高效、经济的内容审查系统提供了经验依据。<br /><strong>Keywords:</strong> toxicity detection, gaming chat moderation, embeddings, fine-tuned transformers, large language models, retrieval-augmented generation, cost-performance trade-off, hybrid moderation system<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 6, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - control<br /><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer models, and retrieval-augmented generation (RAG) approaches are evaluated. The evaluation framework assesses three critical dimensions: classification accuracy, processing speed, and computational costs. A hybrid moderation system architecture is proposed that optimizes human moderator workload through automated detection and incorporates continuous learning mechanisms. The experimental results demonstrate significant performance variations across methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs. The findings provide empirical evidence for deploying cost-effective, efficient content moderation systems in dynamic online gaming environments.
<div><strong>Authors:</strong> Yehor Tereshchenko, Mika Hämäläinen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a comparative study of various NLP approaches—traditional embeddings, fine‑tuned transformer models, zero‑shot/few‑shot prompting of large language models, and retrieval‑augmented generation—for detecting toxic content in online gaming chats. It evaluates each method on accuracy, processing speed, and computational cost, and proposes a hybrid moderation architecture that reduces human moderator workload while enabling continuous learning. Experiments show that fine‑tuned DistilBERT offers the best accuracy‑cost balance, providing practical guidance for cost‑effective content moderation in dynamic gaming environments.", "summary_cn": "本文比较了多种自然语言处理方法（传统嵌入、微调的 Transformer 模型、大语言模型的零示例/少示例提示以及检索增强生成）在在线游戏聊天中检测有害言论的效果，并在分类准确率、处理速度和计算成本三个维度进行评估。同时提出了一种混合审查系统架构，通过自动检测减轻人工审查负担并支持持续学习。实验结果表明，微调的 DistilBERT 在准确率‑成本权衡上表现最佳，为在动态游戏环境中部署高效、经济的内容审查系统提供了经验依据。", "keywords": "toxicity detection, gaming chat moderation, embeddings, fine-tuned transformers, large language models, retrieval-augmented generation, cost-performance trade-off, hybrid moderation system", "scoring": {"interpretability": 3, "understanding": 5, "safety": 6, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "control"}, "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"]}
]]></acme>

<pubDate>2025-10-20T08:03:28+00:00</pubDate>
</item>
<item>
<title>Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17922</link>
<guid>https://papers.cool/arxiv/2510.17922</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how different task decomposition approaches for large language models affect performance and cost, identifying six categorization schemes and three key influencing factors. Based on an empirical analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition method, executes it, and verifies results, achieving a Pareto‑optimal balance across multiple benchmarks.<br /><strong>Summary (CN):</strong> 本文系统研究了大型语言模型任务分解方法对性能与成本的影响，提出了六种分类方案并分析了三大关键因素。基于此分析，作者设计了 Select-Then-Decompose 策略，通过动态选择分解方式、执行并验证结果，实现了在多个基准上性能‑成本的 Pareto 最优平衡。<br /><strong>Keywords:</strong> task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, prompting strategy<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - control<br /><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose.
<div><strong>Authors:</strong> Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how different task decomposition approaches for large language models affect performance and cost, identifying six categorization schemes and three key influencing factors. Based on an empirical analysis, it introduces the Select-Then-Decompose strategy, which dynamically selects a decomposition method, executes it, and verifies results, achieving a Pareto‑optimal balance across multiple benchmarks.", "summary_cn": "本文系统研究了大型语言模型任务分解方法对性能与成本的影响，提出了六种分类方案并分析了三大关键因素。基于此分析，作者设计了 Select-Then-Decompose 策略，通过动态选择分解方式、执行并验证结果，实现了在多个基准上性能‑成本的 Pareto 最优平衡。", "keywords": "task decomposition, large language models, adaptive selection, performance-cost tradeoff, verification, prompting strategy", "scoring": {"interpretability": 3, "understanding": 7, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "control"}, "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"]}
]]></acme>

<pubDate>2025-10-20T07:28:15+00:00</pubDate>
</item>
<item>
<title>CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections</title>
<link>https://papers.cool/arxiv/2510.17921</link>
<guid>https://papers.cool/arxiv/2510.17921</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CLAWS, a method that uses attention weights across prompt sections and model outputs to automatically classify mathematical solutions generated by LLMs into typical, creative, and hallucinated categories without human evaluation. CLAWS outperforms five existing white-box detection baselines on several 7-8B reinforcement‑learning‑trained math models across a large benchmark of 4,545 problems from major math contests. This work provides a novel, interpretable approach to assess creativity and detect hallucinations in LLM reasoning tasks.<br /><strong>Summary (CN):</strong> 本文提出 CLAWS 方法，利用注意力权重在提示段落和模型输出之间的分布，自动将 LLM 生成的数学解答划分为典型、创造性和幻觉三类，无需人工评估。实验表明，CLAWS 在多个 7‑8B 规模的 RL 训练数学模型上，针对 4,545 道来自主要数学竞赛的题目，优于五种现有白盒检测基准。该工作提供了一种新颖且可解释的手段来评估创造力并检测 LLM 推理中的幻觉。<br /><strong>Keywords:</strong> creativity detection, LLM, attention window, white-box detection, hallucination, mathematical reasoning, RL fine-tuning, interpretability, model evaluation<br /><strong>Scores:</strong> Interpretability: 7, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - other; Primary focus - interpretability<br /><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a method that defines and classifies mathematical solutions into typical, creative, and hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems collected from 181 math contests (AJHSME, AMC, AIME).
<div><strong>Authors:</strong> Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CLAWS, a method that uses attention weights across prompt sections and model outputs to automatically classify mathematical solutions generated by LLMs into typical, creative, and hallucinated categories without human evaluation. CLAWS outperforms five existing white-box detection baselines on several 7-8B reinforcement‑learning‑trained math models across a large benchmark of 4,545 problems from major math contests. This work provides a novel, interpretable approach to assess creativity and detect hallucinations in LLM reasoning tasks.", "summary_cn": "本文提出 CLAWS 方法，利用注意力权重在提示段落和模型输出之间的分布，自动将 LLM 生成的数学解答划分为典型、创造性和幻觉三类，无需人工评估。实验表明，CLAWS 在多个 7‑8B 规模的 RL 训练数学模型上，针对 4,545 道来自主要数学竞赛的题目，优于五种现有白盒检测基准。该工作提供了一种新颖且可解释的手段来评估创造力并检测 LLM 推理中的幻觉。", "keywords": "creativity detection, LLM, attention window, white-box detection, hallucination, mathematical reasoning, RL fine-tuning, interpretability, model evaluation", "scoring": {"interpretability": 7, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "other", "primary_focus": "interpretability"}, "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"]}
]]></acme>

<pubDate>2025-10-20T06:59:37+00:00</pubDate>
</item>
<item>
<title>JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</title>
<link>https://papers.cool/arxiv/2510.17918</link>
<guid>https://papers.cool/arxiv/2510.17918</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> JT-Safe introduces a data augmentation strategy called Data with World Context (DWC) that enriches pre‑training corpora with real‑world contextual information and industrial‑scenario content to reduce hallucinations and improve trustworthiness of large language models. The authors continue pre‑training a 35‑billion‑parameter model on 1.5 trillion DWC tokens and demonstrate a 1.79 % improvement on safety and trustworthy benchmarks compared with a similar‑scale model. The work highlights the intrinsic role of pre‑training data in LLM safety.<br /><strong>Summary (CN):</strong> JT‑Safe 提出“带世界上下文的数据”(Data with World Context, DWC) 方法，通过在预训练语料中加入真实世界的时空背景和工业场景信息，以降低大语言模型的幻觉并提升可信度。研究者在 35 B 参数的模型上继续使用 1.5 万亿 DWC 令牌进行预训练，并在安全与可信评估基准上相较于同规模模型提升 1.79%。该工作强调了预训练数据对 LLM 安全的根本影响。<br /><strong>Keywords:</strong> LLM safety, hallucination mitigation, pretraining data, world context, data augmentation, trustworthiness, large language models, JT-Safe, DWC, industrial scenario data<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.
<div><strong>Authors:</strong> Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "JT-Safe introduces a data augmentation strategy called Data with World Context (DWC) that enriches pre‑training corpora with real‑world contextual information and industrial‑scenario content to reduce hallucinations and improve trustworthiness of large language models. The authors continue pre‑training a 35‑billion‑parameter model on 1.5 trillion DWC tokens and demonstrate a 1.79 % improvement on safety and trustworthy benchmarks compared with a similar‑scale model. The work highlights the intrinsic role of pre‑training data in LLM safety.", "summary_cn": "JT‑Safe 提出“带世界上下文的数据”(Data with World Context, DWC) 方法，通过在预训练语料中加入真实世界的时空背景和工业场景信息，以降低大语言模型的幻觉并提升可信度。研究者在 35 B 参数的模型上继续使用 1.5 万亿 DWC 令牌进行预训练，并在安全与可信评估基准上相较于同规模模型提升 1.79%。该工作强调了预训练数据对 LLM 安全的根本影响。", "keywords": "LLM safety, hallucination mitigation, pretraining data, world context, data augmentation, trustworthiness, large language models, JT-Safe, DWC, industrial scenario data", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Junlan Feng", "Fanyu Meng", "Chong Long", "Pengyu Cong", "Duqing Wang", "Yan Zheng", "Yuyao Zhang", "Xuanchang Gao", "Ye Yuan", "Yunfei Ma", "Zhijie Ren", "Fan Yang", "Na Wu", "Di Jin", "Chao Deng"]}
]]></acme>

<pubDate>2025-10-20T02:12:49+00:00</pubDate>
</item>
<item>
<title>Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models</title>
<link>https://papers.cool/arxiv/2510.17909</link>
<guid>https://papers.cool/arxiv/2510.17909</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts a mechanistic analysis of literary style in GPT-2 by identifying neurons that distinguish human literary prose from rigid AI-generated text, finding thousands of statistically significant discriminative neurons. Counterintuitively, systematic ablation of the most discriminative neurons improves literary style metrics, revealing a disconnect between observational correlation and causal necessity. These results challenge assumptions in mechanistic interpretability and have implications for AI alignment research.<br /><strong>Summary (CN):</strong> 本文对 GPT-2 中的文学风格进行机械化分析，识别出能区分人类文学散文与僵硬机器生成文本的神经元，发现数千个具有统计显著性的判别神经元。出人意料的是，对最具判别性的神经元进行系统性消融会提升文学风格指标，显示出观察到的相关性与因果必要性之间的脱节。该发现挑战了可解释性研究中的假设，并对 AI 对齐具有启示意义。<br /><strong>Keywords:</strong> mechanistic interpretability, neuron ablation, literary style, GPT-2, correlation vs causation, alignment, neural circuits<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 8<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</div>
We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus, we extract activation patterns from 355 million parameters across 32,768 neurons in late layers. We find 27,122 statistically significant discriminative neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic ablation studies, we discover a paradoxical result: while these neurons correlate with literary text during analysis, removing them often improves rather than degrades generated prose quality. Specifically, ablating 50 high-discriminating neurons yields a 25.7% improvement in literary style metrics. This demonstrates a critical gap between observational correlation and causal necessity in neural networks. Our findings challenge the assumption that neurons which activate on desirable inputs will produce those outputs during generation, with implications for mechanistic interpretability research and AI alignment.
<div><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts a mechanistic analysis of literary style in GPT-2 by identifying neurons that distinguish human literary prose from rigid AI-generated text, finding thousands of statistically significant discriminative neurons. Counterintuitively, systematic ablation of the most discriminative neurons improves literary style metrics, revealing a disconnect between observational correlation and causal necessity. These results challenge assumptions in mechanistic interpretability and have implications for AI alignment research.", "summary_cn": "本文对 GPT-2 中的文学风格进行机械化分析，识别出能区分人类文学散文与僵硬机器生成文本的神经元，发现数千个具有统计显著性的判别神经元。出人意料的是，对最具判别性的神经元进行系统性消融会提升文学风格指标，显示出观察到的相关性与因果必要性之间的脱节。该发现挑战了可解释性研究中的假设，并对 AI 对齐具有启示意义。", "keywords": "mechanistic interpretability, neuron ablation, literary style, GPT-2, correlation vs causation, alignment, neural circuits", "scoring": {"interpretability": 8, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 8}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Tsogt-Ochir Enkhbayar"]}
]]></acme>

<pubDate>2025-10-19T16:13:53+00:00</pubDate>
</item>
<item>
<title>Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review</title>
<link>https://papers.cool/arxiv/2510.17892</link>
<guid>https://papers.cool/arxiv/2510.17892</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This systematic review examines 41 studies on the use of pre‑trained language models for domain‑specific text classification, comparing traditional and transformer‑based approaches and presenting a taxonomy of techniques. It also includes a comparative experiment on biomedical sentence classification using BERT, SciBERT, and BioBERT, and discusses challenges, limitations, and future directions.<br /><strong>Summary (CN):</strong> 本文系统回顾了 41 篇关于预训练语言模型在特定领域文本分类中应用的研究，比较了传统方法与基于 Transformer 的方法，并提出了技术分类体系。文中还对生物医学句子分类任务使用 BERT、SciBERT 与 BioBERT 进行了对比实验，讨论了挑战、局限以及未来发展方向。<br /><strong>Keywords:</strong> pre-trained language models, domain-specific text classification, BERT, SciBERT, BioBERT, systematic review, transformer, taxonomy, NLP, domain adaptation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 1, Technicality: 5, Surprisal: 3<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Zhyar Rzgar K. Rostam, Gábor Kertész</div>
The exponential increase in scientific literature and online information necessitates efficient methods for extracting knowledge from textual data. Natural language processing (NLP) plays a crucial role in addressing this challenge, particularly in text classification tasks. While large language models (LLMs) have achieved remarkable success in NLP, their accuracy can suffer in domain-specific contexts due to specialized vocabulary, unique grammatical structures, and imbalanced data distributions. In this systematic literature review (SLR), we investigate the utilization of pre-trained language models (PLMs) for domain-specific text classification. We systematically review 41 articles published between 2018 and January 2024, adhering to the PRISMA statement (preferred reporting items for systematic reviews and meta-analyses). This review methodology involved rigorous inclusion criteria and a multi-step selection process employing AI-powered tools. We delve into the evolution of text classification techniques and differentiate between traditional and modern approaches. We emphasize transformer-based models and explore the challenges and considerations associated with using LLMs for domain-specific text classification. Furthermore, we categorize existing research based on various PLMs and propose a taxonomy of techniques used in the field. To validate our findings, we conducted a comparative experiment involving BERT, SciBERT, and BioBERT in biomedical sentence classification. Finally, we present a comparative study on the performance of LLMs in text classification tasks across different domains. In addition, we examine recent advancements in PLMs for domain-specific text classification and offer insights into future directions and limitations in this rapidly evolving domain.
<div><strong>Authors:</strong> Zhyar Rzgar K. Rostam, Gábor Kertész</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This systematic review examines 41 studies on the use of pre‑trained language models for domain‑specific text classification, comparing traditional and transformer‑based approaches and presenting a taxonomy of techniques. It also includes a comparative experiment on biomedical sentence classification using BERT, SciBERT, and BioBERT, and discusses challenges, limitations, and future directions.", "summary_cn": "本文系统回顾了 41 篇关于预训练语言模型在特定领域文本分类中应用的研究，比较了传统方法与基于 Transformer 的方法，并提出了技术分类体系。文中还对生物医学句子分类任务使用 BERT、SciBERT 与 BioBERT 进行了对比实验，讨论了挑战、局限以及未来发展方向。", "keywords": "pre-trained language models, domain-specific text classification, BERT, SciBERT, BioBERT, systematic review, transformer, taxonomy, NLP, domain adaptation", "scoring": {"interpretability": 2, "understanding": 4, "safety": 1, "technicality": 5, "surprisal": 3}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Zhyar Rzgar K. Rostam", "Gábor Kertész"]}
]]></acme>

<pubDate>2025-10-18T22:46:53+00:00</pubDate>
</item>
<item>
<title>POPI: Personalizing LLMs via Optimized Natural Language Preference Inference</title>
<link>https://papers.cool/arxiv/2510.17881</link>
<guid>https://papers.cool/arxiv/2510.17881</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces POPI, a framework that learns compact natural-language summaries of individual user preferences through a preference-inference model and uses these summaries to condition a shared LLM, enabling personalized generation without per-user fine-tuning. POPI jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy and lower context overhead, and the learned summaries can be transferred to frozen off-the-shelf models.<br /><strong>Summary (CN):</strong> 本文提出 POPI 框架，通过偏好推断模型将用户的多样化偏好压缩为简洁的自然语言摘要，并以此条件化共享的大语言模型，从而实现无需针对每个用户微调的个性化生成。该方法在强化学习下联合优化偏好推断和生成，两者协同提升个性化准确性并显著降低上下文开销，且生成的摘要可直接迁移至冻结的现成模型。<br /><strong>Keywords:</strong> personalization, LLM, preference inference, reinforcement learning, natural language summary, plug-and-play adaptation, RLHF, DPO, user modeling, zero-shot transfer<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.
<div><strong>Authors:</strong> Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces POPI, a framework that learns compact natural-language summaries of individual user preferences through a preference-inference model and uses these summaries to condition a shared LLM, enabling personalized generation without per-user fine-tuning. POPI jointly optimizes the inference and generation components with reinforcement learning, achieving higher personalization accuracy and lower context overhead, and the learned summaries can be transferred to frozen off-the-shelf models.", "summary_cn": "本文提出 POPI 框架，通过偏好推断模型将用户的多样化偏好压缩为简洁的自然语言摘要，并以此条件化共享的大语言模型，从而实现无需针对每个用户微调的个性化生成。该方法在强化学习下联合优化偏好推断和生成，两者协同提升个性化准确性并显著降低上下文开销，且生成的摘要可直接迁移至冻结的现成模型。", "keywords": "personalization, LLM, preference inference, reinforcement learning, natural language summary, plug-and-play adaptation, RLHF, DPO, user modeling, zero-shot transfer", "scoring": {"interpretability": 3, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Yizhuo Chen", "Xin Liu", "Ruijie Wang", "Zheng Li", "Pei Chen", "Changlong Yu", "Priyanka Nigam", "Meng Jiang", "Bing Yin"]}
]]></acme>

<pubDate>2025-10-17T23:07:57+00:00</pubDate>
</item>
<item>
<title>Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</title>
<link>https://papers.cool/arxiv/2510.17880</link>
<guid>https://papers.cool/arxiv/2510.17880</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates whether large language models (LLMs) use emotion similarly to humans when enforcing fairness through third‑party punishment. Across millions of decisions, LLMs showed stronger emotion‑driven punishment and prioritized emotional responses over cost, unlike humans who balance fairness with cost; reasoning‑oriented models were more cost‑sensitive but still largely emotion driven. The authors suggest LLMs follow a developmental trajectory akin to humans and call for integrating emotion with context‑sensitive reasoning to improve alignment and emotional intelligence.<br /><strong>Summary (CN):</strong> 本文研究大型语言模型（LLM）在通过第三方惩罚执行公平时是否像人类一样受情绪驱动。实验发现，LLM 的惩罚行为更受负面情绪影响，且在决定是否惩罚时更倾向于情绪而非成本，表现出几乎全有或全无的规范执行；相比之下，人类会在公平与成本之间权衡。推理型模型（如 o3-mini、DeepSeek‑R1）对成本更敏感但仍主要受情绪驱动。作者提出 LLM 的情绪使用可能沿着类似人类发展的轨迹，并呼吁在未来模型中结合情绪与情境推理以提升对齐和情绪智能。<br /><strong>Keywords:</strong> emotion-guided decision making, large language models, fairness enforcement, third-party punishment, cost sensitivity, alignment, moral AI, behavioral analysis, LLM evaluation, emotional intelligence<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 6, Technicality: 6, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.
<div><strong>Authors:</strong> Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates whether large language models (LLMs) use emotion similarly to humans when enforcing fairness through third‑party punishment. Across millions of decisions, LLMs showed stronger emotion‑driven punishment and prioritized emotional responses over cost, unlike humans who balance fairness with cost; reasoning‑oriented models were more cost‑sensitive but still largely emotion driven. The authors suggest LLMs follow a developmental trajectory akin to humans and call for integrating emotion with context‑sensitive reasoning to improve alignment and emotional intelligence.", "summary_cn": "本文研究大型语言模型（LLM）在通过第三方惩罚执行公平时是否像人类一样受情绪驱动。实验发现，LLM 的惩罚行为更受负面情绪影响，且在决定是否惩罚时更倾向于情绪而非成本，表现出几乎全有或全无的规范执行；相比之下，人类会在公平与成本之间权衡。推理型模型（如 o3-mini、DeepSeek‑R1）对成本更敏感但仍主要受情绪驱动。作者提出 LLM 的情绪使用可能沿着类似人类发展的轨迹，并呼吁在未来模型中结合情绪与情境推理以提升对齐和情绪智能。", "keywords": "emotion-guided decision making, large language models, fairness enforcement, third-party punishment, cost sensitivity, alignment, moral AI, behavioral analysis, LLM evaluation, emotional intelligence", "scoring": {"interpretability": 3, "understanding": 7, "safety": 6, "technicality": 6, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"]}
]]></acme>

<pubDate>2025-10-17T08:41:36+00:00</pubDate>
</item>
<item>
<title>Modeling Layered Consciousness with Multi-Agent Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17844</link>
<guid>https://papers.cool/arxiv/2510.17844</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a multi‑agent framework called the Psychodynamic Model that aims to simulate layered artificial consciousness—in particular self‑awareness, preconsciousness, and unconsciousness—by drawing on psychoanalytic theory. A Personalization Module combines fixed personality traits with dynamic needs, and the system is fine‑tuned on emotionally rich dialogues; evaluation with an LLM judge shows a 71.2% preference for the fine‑tuned model, indicating improved emotional depth and reduced output variance.<br /><strong>Summary (CN):</strong> 本文提出一种多代理框架（Psychodynamic Model），基于精神分析理论模拟人工意识的层次结构，包括自我意识、前意识和无意识。通过个人化模块将固定特质与动态需求结合，并在情感丰富的对话数据上进行参数高效微调；使用 LLM 评审的实验显示，微调模型获得 71.2% 的偏好，表现出更强的情感深度和更低的输出方差。<br /><strong>Keywords:</strong> consciousness modeling, multi-agent LLM, psychodynamic model, self-awareness, preconsciousness, unconsciousness, personalization module, fine-tuning, emotional dialogue, adaptive cognition<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 3, Technicality: 5, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.
<div><strong>Authors:</strong> Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a multi‑agent framework called the Psychodynamic Model that aims to simulate layered artificial consciousness—in particular self‑awareness, preconsciousness, and unconsciousness—by drawing on psychoanalytic theory. A Personalization Module combines fixed personality traits with dynamic needs, and the system is fine‑tuned on emotionally rich dialogues; evaluation with an LLM judge shows a 71.2% preference for the fine‑tuned model, indicating improved emotional depth and reduced output variance.", "summary_cn": "本文提出一种多代理框架（Psychodynamic Model），基于精神分析理论模拟人工意识的层次结构，包括自我意识、前意识和无意识。通过个人化模块将固定特质与动态需求结合，并在情感丰富的对话数据上进行参数高效微调；使用 LLM 评审的实验显示，微调模型获得 71.2% 的偏好，表现出更强的情感深度和更低的输出方差。", "keywords": "consciousness modeling, multi-agent LLM, psychodynamic model, self-awareness, preconsciousness, unconsciousness, personalization module, fine-tuning, emotional dialogue, adaptive cognition", "scoring": {"interpretability": 3, "understanding": 6, "safety": 3, "technicality": 5, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"]}
]]></acme>

<pubDate>2025-10-10T07:08:34+00:00</pubDate>
</item>
<item>
<title>Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</title>
<link>https://papers.cool/arxiv/2510.18876</link>
<guid>https://papers.cool/arxiv/2510.18876</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Grasp Any Region (GAR), a multimodal LLM architecture that uses a RoI-aligned feature replay mechanism to incorporate global context for precise, region-level visual comprehension and to model interactions among multiple prompts. GAR also comes with a new benchmark, GAR-Bench, which evaluates single‑region understanding as well as multi‑region compositional reasoning, demonstrating state‑of‑the‑art performance on several vision‑language tasks.<br /><strong>Summary (CN):</strong> 本文提出了 Grasp Any Region（GAR）模型，利用 RoI 对齐特征回放技术在多模态大语言模型中引入全局上下文，实现对任意区域的精确视觉理解并建模多个提示之间的交互。作者同时构建了 GAR‑Bench 基准，用于评估单区域理解和多区域组合推理，实验表明 GAR 在多项视觉语言任务上达到了领先水平。<br /><strong>Keywords:</strong> multimodal LLM, region-level visual, RoI-aligned feature replay, compositional reasoning, GAR-Bench, dense captioning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 2, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.
<div><strong>Authors:</strong> Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Grasp Any Region (GAR), a multimodal LLM architecture that uses a RoI-aligned feature replay mechanism to incorporate global context for precise, region-level visual comprehension and to model interactions among multiple prompts. GAR also comes with a new benchmark, GAR-Bench, which evaluates single‑region understanding as well as multi‑region compositional reasoning, demonstrating state‑of‑the‑art performance on several vision‑language tasks.", "summary_cn": "本文提出了 Grasp Any Region（GAR）模型，利用 RoI 对齐特征回放技术在多模态大语言模型中引入全局上下文，实现对任意区域的精确视觉理解并建模多个提示之间的交互。作者同时构建了 GAR‑Bench 基准，用于评估单区域理解和多区域组合推理，实验表明 GAR 在多项视觉语言任务上达到了领先水平。", "keywords": "multimodal LLM, region-level visual, RoI-aligned feature replay, compositional reasoning, GAR-Bench, dense captioning", "scoring": {"interpretability": 2, "understanding": 4, "safety": 2, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"]}
]]></acme>

<pubDate>2025-10-21T17:59:59+00:00</pubDate>
</item>
<item>
<title>Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</title>
<link>https://papers.cool/arxiv/2510.18874</link>
<guid>https://papers.cool/arxiv/2510.18874</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates catastrophic forgetting in language models when adapting to new tasks via supervised fine-tuning (SFT) versus reinforcement learning (RL). Experiments across multiple model families and tasks show that RL causes less forgetting while achieving comparable or better performance, attributed to its mode‑seeking nature enabled by on‑policy data. The authors further demonstrate that approximating on‑policy data can mitigate forgetting efficiently, highlighting a practical strategy for continual learning.<br /><strong>Summary (CN):</strong> 本文研究了语言模型在通过监督微调（SFT）与强化学习（RL）适配新任务时出现的灾难性遗忘现象。实验覆盖多种模型系列和任务，发现 RL 在保持先前知识方面优于 SFT，且性能相当或更佳，这归因于 RL 使用的在策略数据导致的模式寻踪特性。作者进一步验证，使用近似在策略数据即可有效减轻遗忘，为持续学习提供了实用方案。<br /><strong>Keywords:</strong> catastrophic forgetting, reinforcement learning, on-policy data, supervised fine-tuning, language model adaptation, continual learning, KL regularization, mixture model analysis<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.
<div><strong>Authors:</strong> Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates catastrophic forgetting in language models when adapting to new tasks via supervised fine-tuning (SFT) versus reinforcement learning (RL). Experiments across multiple model families and tasks show that RL causes less forgetting while achieving comparable or better performance, attributed to its mode‑seeking nature enabled by on‑policy data. The authors further demonstrate that approximating on‑policy data can mitigate forgetting efficiently, highlighting a practical strategy for continual learning.", "summary_cn": "本文研究了语言模型在通过监督微调（SFT）与强化学习（RL）适配新任务时出现的灾难性遗忘现象。实验覆盖多种模型系列和任务，发现 RL 在保持先前知识方面优于 SFT，且性能相当或更佳，这归因于 RL 使用的在策略数据导致的模式寻踪特性。作者进一步验证，使用近似在策略数据即可有效减轻遗忘，为持续学习提供了实用方案。", "keywords": "catastrophic forgetting, reinforcement learning, on-policy data, supervised fine-tuning, language model adaptation, continual learning, KL regularization, mixture model analysis", "scoring": {"interpretability": 2, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Howard Chen", "Noam Razin", "Karthik Narasimhan", "Danqi Chen"]}
]]></acme>

<pubDate>2025-10-21T17:59:41+00:00</pubDate>
</item>
<item>
<title>See the Text: From Tokenization to Visual Reading</title>
<link>https://papers.cool/arxiv/2510.18840</link>
<guid>https://papers.cool/arxiv/2510.18840</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces SeeTok, a method that renders text as images and feeds them to pretrained multimodal LLMs, leveraging OCR and vision-language alignment to replace subword tokenization. Experiments on three language tasks show that SeeTok matches or exceeds traditional tokenizers while using 4.43× fewer tokens and cutting FLOPs by 70.5%, with added benefits for low‑resource languages, typographic noise robustness, and cross‑lingual generalization. This approach proposes a vision‑centric, human‑like reading paradigm for language models.<br /><strong>Summary (CN):</strong> 本文提出 SeeTok 方法，将文本渲染为图像并使用预训练的多模态 LLM 进行识别，利用 OCR 和视觉‑语言对齐能力取代子词分词。实验表明在三项语言任务上，SeeTok 能够匹配或超越传统分词器，同时使用 4.43 倍更少的 token，计算量削减 70.5%，并在低资源语言、排版噪声鲁棒性以及跨语言泛方面表现出额外提升。该方法倡导一种以视觉为中心、类人阅读的语言模型新范式。<br /><strong>Keywords:</strong> visual tokenization, multimodal LLM, OCR, cross-lingual generalization, token efficiency, low-resource languages, typographic noise robustness, visual reading, language model<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.
<div><strong>Authors:</strong> Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces SeeTok, a method that renders text as images and feeds them to pretrained multimodal LLMs, leveraging OCR and vision-language alignment to replace subword tokenization. Experiments on three language tasks show that SeeTok matches or exceeds traditional tokenizers while using 4.43× fewer tokens and cutting FLOPs by 70.5%, with added benefits for low‑resource languages, typographic noise robustness, and cross‑lingual generalization. This approach proposes a vision‑centric, human‑like reading paradigm for language models.", "summary_cn": "本文提出 SeeTok 方法，将文本渲染为图像并使用预训练的多模态 LLM 进行识别，利用 OCR 和视觉‑语言对齐能力取代子词分词。实验表明在三项语言任务上，SeeTok 能够匹配或超越传统分词器，同时使用 4.43 倍更少的 token，计算量削减 70.5%，并在低资源语言、排版噪声鲁棒性以及跨语言泛方面表现出额外提升。该方法倡导一种以视觉为中心、类人阅读的语言模型新范式。", "keywords": "visual tokenization, multimodal LLM, OCR, cross-lingual generalization, token efficiency, low-resource languages, typographic noise robustness, visual reading, language model", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"]}
]]></acme>

<pubDate>2025-10-21T17:34:48+00:00</pubDate>
</item>
<item>
<title>Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</title>
<link>https://papers.cool/arxiv/2510.18502</link>
<guid>https://papers.cool/arxiv/2510.18502</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a pipeline that combines a vision-language model (VLM) with retrieval-augmented generation (RAG) to perform zero-shot vehicle make and model recognition. The VLM describes vehicle images in textual attributes, retrieves matching vehicle descriptions from a text database, and uses a language model to infer the specific make and model, achieving around a 20% improvement over a CLIP baseline without any image-specific finetuning.<br /><strong>Summary (CN):</strong> 本文提出将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 将车辆图像转化为文本属性，检索匹配的车辆文本描述，并利用语言模型推断具体品牌和型号，在无需图像特定微调的情况下比 CLIP 基线提升约 20%。<br /><strong>Keywords:</strong> vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, text-based reasoning<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.
<div><strong>Authors:</strong> Wei-Chia Chang, Yan-Ann Chen</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a pipeline that combines a vision-language model (VLM) with retrieval-augmented generation (RAG) to perform zero-shot vehicle make and model recognition. The VLM describes vehicle images in textual attributes, retrieves matching vehicle descriptions from a text database, and uses a language model to infer the specific make and model, achieving around a 20% improvement over a CLIP baseline without any image-specific finetuning.", "summary_cn": "本文提出将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流水线，实现零样本车辆品牌和型号识别。VLM 将车辆图像转化为文本属性，检索匹配的车辆文本描述，并利用语言模型推断具体品牌和型号，在无需图像特定微调的情况下比 CLIP 基线提升约 20%。", "keywords": "vehicle model recognition, zero-shot, CLIP, retrieval-augmented generation, vision-language model, text-based reasoning", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Wei-Chia Chang", "Yan-Ann Chen"]}
]]></acme>

<pubDate>2025-10-21T10:39:39+00:00</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents</title>
<link>https://papers.cool/arxiv/2510.18476</link>
<guid>https://papers.cool/arxiv/2510.18476</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a probabilistic framework that maintains and updates a belief distribution over a dialogue partner's latent intentions, using these beliefs to guide the policy of LLM agents in multi‑turn social conversations. Experiments in the SOTOPIA environment demonstrate consistent performance gains over strong baselines, suggesting that intent modeling improves the adaptability and effectiveness of socially intelligent agents.<br /><strong>Summary (CN):</strong> 本文提出了一种概率意图建模框架，实时维护并更新对对话伙伴潜在意图的信念分布，并将该分布作为上下文输入以指导大语言模型（LLM）在多轮社交对话中的策略。 在 SOTOPIA 环境中的实验显示，该方法相较于强基线在整体得分上提升 9.0%（All 场景）和 4.1%（Hard 场景），表明意图建模有助于提升社交智能 LLM 代理的适应性和表现。<br /><strong>Keywords:</strong> probabilistic intent modeling, large language model agents, multi-turn dialogue, belief distribution, social intelligence, latent intentions, uncertainty estimation, adaptive dialogue policies, SOTOPIA benchmark<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 4, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.
<div><strong>Authors:</strong> Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a probabilistic framework that maintains and updates a belief distribution over a dialogue partner's latent intentions, using these beliefs to guide the policy of LLM agents in multi‑turn social conversations. Experiments in the SOTOPIA environment demonstrate consistent performance gains over strong baselines, suggesting that intent modeling improves the adaptability and effectiveness of socially intelligent agents.", "summary_cn": "本文提出了一种概率意图建模框架，实时维护并更新对对话伙伴潜在意图的信念分布，并将该分布作为上下文输入以指导大语言模型（LLM）在多轮社交对话中的策略。 在 SOTOPIA 环境中的实验显示，该方法相较于强基线在整体得分上提升 9.0%（All 场景）和 4.1%（Hard 场景），表明意图建模有助于提升社交智能 LLM 代理的适应性和表现。", "keywords": "probabilistic intent modeling, large language model agents, multi-turn dialogue, belief distribution, social intelligence, latent intentions, uncertainty estimation, adaptive dialogue policies, SOTOPIA benchmark", "scoring": {"interpretability": 3, "understanding": 7, "safety": 4, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"]}
]]></acme>

<pubDate>2025-10-21T09:54:44+00:00</pubDate>
</item>
<item>
<title>CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment</title>
<link>https://papers.cool/arxiv/2510.18471</link>
<guid>https://papers.cool/arxiv/2510.18471</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces CodeRL+, a reinforcement‑learning framework that augments existing RL with verifiable rewards by aligning generated code with its execution semantics via variable‑level trajectory signals. This alignment provides richer feedback than binary pass/fail outcomes, leading to consistent improvements on pass@1 and on downstream coding tasks such as code‑reasoning and test‑output generation. Experiments show that CodeRL+ works across multiple RL algorithms and LLM sizes and probe analyses indicate stronger textual‑semantic coupling.<br /><strong>Summary (CN):</strong> 本文提出 CodeRL+，一种通过变量级执行轨迹将生成的代码与其执行语义对齐的强化学习框架，以提升相较于仅二元通过/失败奖励的学习信号。该语义对齐提供了更细粒度的反馈，使得模型在 pass@1 以及代码推理和测试生成等下游任务上均取得显著提升。实验表明 CodeRL+ 能多种强化学习算法和不同规模的 LLM 上有效运行，探针分析进一步验证了代码文本表示与底层执行语义的耦合度增强。<br /><strong>Keywords:</strong> code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, pass@1, code reasoning, variable-level execution trajectory, LLM fine-tuning, semantic alignment, code correctness<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 5, Technicality: 8, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
While Large Language Models (LLMs) excel at code generation by learning from vast code corpora, a fundamental semantic gap remains between their training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics. Reinforcement Learning with Verifiable Rewards (RLVR) approaches attempt to bridge this gap using outcome rewards from executing test cases. However, solely relying on binary pass/fail signals is inefficient for establishing a well-aligned connection between the textual representation of code and its execution semantics, especially for subtle logical errors within the code. In this paper, we propose CodeRL+, a novel approach that integrates execution semantics alignment into the RLVR training pipeline for code generation. CodeRL+ enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms. Extensive experiments demonstrate that CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. CodeRL+ generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively. CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.
<div><strong>Authors:</strong> Xue Jiang, Yihong Dong, Mengyang Liu, Hongyi Deng, Tian Wang, Yongding Tao, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces CodeRL+, a reinforcement‑learning framework that augments existing RL with verifiable rewards by aligning generated code with its execution semantics via variable‑level trajectory signals. This alignment provides richer feedback than binary pass/fail outcomes, leading to consistent improvements on pass@1 and on downstream coding tasks such as code‑reasoning and test‑output generation. Experiments show that CodeRL+ works across multiple RL algorithms and LLM sizes and probe analyses indicate stronger textual‑semantic coupling.", "summary_cn": "本文提出 CodeRL+，一种通过变量级执行轨迹将生成的代码与其执行语义对齐的强化学习框架，以提升相较于仅二元通过/失败奖励的学习信号。该语义对齐提供了更细粒度的反馈，使得模型在 pass@1 以及代码推理和测试生成等下游任务上均取得显著提升。实验表明 CodeRL+ 能多种强化学习算法和不同规模的 LLM 上有效运行，探针分析进一步验证了代码文本表示与底层执行语义的耦合度增强。", "keywords": "code generation, reinforcement learning, execution semantics alignment, RL with verifiable rewards, pass@1, code reasoning, variable-level execution trajectory, LLM fine-tuning, semantic alignment, code correctness", "scoring": {"interpretability": 3, "understanding": 7, "safety": 5, "technicality": 8, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-21T09:48:06+00:00</pubDate>
</item>
<item>
<title>Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption</title>
<link>https://papers.cool/arxiv/2510.18333</link>
<guid>https://papers.cool/arxiv/2510.18333</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper argues that limited real-world deployment of LLM watermarking stems from misaligned incentives among providers, platforms, and users, identifying four barriers: competitive risk, governance, robustness, and attribution. It reviews model, text, and in‑context watermarking through this lens, proposing incentive‑aligned approaches—particularly in‑context watermarking for trusted parties—to enable practical detection of misuse without harming user experience. Design principles for domain‑specific, incentive‑aligned watermarking and future research directions are also outlined.<br /><strong>Summary (CN):</strong> 本文指出 LLM 水印技术在实际应用中的受限主要来源于提供者、平台和用户之间激励不一致，提出四大障碍：竞争风险、治理、鲁棒性和归属问题。文章从这一视角审视模型水印、文本水印和上下文水印（In‑context watermarking），强调通过激励对齐（尤其是面向可信方的上下文水印）实现对滥用行为的检测，而不影响用户体验，并给出领域特定激励对齐水印的设计原则及未来研究方向。<br /><strong>Keywords:</strong> LLM watermarking, incentive alignment, misuse detection, in-context watermarking, model watermarking, AI safety, provenance, robustness<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 7, Technicality: 6, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</div>
Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.
<div><strong>Authors:</strong> Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper argues that limited real-world deployment of LLM watermarking stems from misaligned incentives among providers, platforms, and users, identifying four barriers: competitive risk, governance, robustness, and attribution. It reviews model, text, and in‑context watermarking through this lens, proposing incentive‑aligned approaches—particularly in‑context watermarking for trusted parties—to enable practical detection of misuse without harming user experience. Design principles for domain‑specific, incentive‑aligned watermarking and future research directions are also outlined.", "summary_cn": "本文指出 LLM 水印技术在实际应用中的受限主要来源于提供者、平台和用户之间激励不一致，提出四大障碍：竞争风险、治理、鲁棒性和归属问题。文章从这一视角审视模型水印、文本水印和上下文水印（In‑context watermarking），强调通过激励对齐（尤其是面向可信方的上下文水印）实现对滥用行为的检测，而不影响用户体验，并给出领域特定激励对齐水印的设计原则及未来研究方向。", "keywords": "LLM watermarking, incentive alignment, misuse detection, in-context watermarking, model watermarking, AI safety, provenance, robustness", "scoring": {"interpretability": 2, "understanding": 5, "safety": 7, "technicality": 6, "surprisal": 5}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Yepeng Liu", "Xuandong Zhao", "Dawn Song", "Gregory W. Wornell", "Yuheng Bu"]}
]]></acme>

<pubDate>2025-10-21T06:34:51+00:00</pubDate>
</item>
<item>
<title>The Impact of Image Resolution on Biomedical Multimodal Large Language Models</title>
<link>https://papers.cool/arxiv/2510.18304</link>
<guid>https://papers.cool/arxiv/2510.18304</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper investigates how image resolution affects the performance of biomedical multimodal large language models (MLLMs). It demonstrates that training and inference at native resolution considerably boost performance across various tasks, while a mismatch between training and inference resolutions severely degrades results. Additionally, mixed‑resolution training is shown to mitigate misalignment effects and balance computational constraints with performance needs.<br /><strong>Summary (CN):</strong> 本文研究了图像分辨率对生物医学多模态大型语言模型（MLLM）性能的影响。结果表明，在原始分辨率下进行训练和推理可显著提升多个任务的表现，而训练‑推理分辨率不匹配则会导致性能大幅下降。此外，混合分辨率训练能够缓解这种不匹配带来的问题，在保证计算资源可接受的前提下提升模型效果。<br /><strong>Keywords:</strong> biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, performance degradation, multimodal large language models, biomedical imaging, resolution misalignment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 4, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.
<div><strong>Authors:</strong> Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper investigates how image resolution affects the performance of biomedical multimodal large language models (MLLMs). It demonstrates that training and inference at native resolution considerably boost performance across various tasks, while a mismatch between training and inference resolutions severely degrades results. Additionally, mixed‑resolution training is shown to mitigate misalignment effects and balance computational constraints with performance needs.", "summary_cn": "本文研究了图像分辨率对生物医学多模态大型语言模型（MLLM）性能的影响。结果表明，在原始分辨率下进行训练和推理可显著提升多个任务的表现，而训练‑推理分辨率不匹配则会导致性能大幅下降。此外，混合分辨率训练能够缓解这种不匹配带来的问题，在保证计算资源可接受的前提下提升模型效果。", "keywords": "biomedical multimodal LLM, image resolution, native-resolution training, mixed-resolution training, performance degradation, multimodal large language models, biomedical imaging, resolution misalignment", "scoring": {"interpretability": 2, "understanding": 6, "safety": 4, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"]}
]]></acme>

<pubDate>2025-10-21T05:19:43+00:00</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://papers.cool/arxiv/2510.18214</link>
<guid>https://papers.cool/arxiv/2510.18214</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Vision Language Safety Understanding (VLSU), a benchmark and evaluation framework that systematically assesses multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world image‑text samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint understanding is required, exposing significant alignment gaps. The study also highlights the trade‑off between over‑blocking and under‑refusing borderline content and provides a testbed for future robust vision‑language safety research.<br /><strong>Summary (CN):</strong> 本文提出了视觉语言安全理解（Vision Language Safety Understanding，VLSU）框架，系统评估多模态基础模型在细粒度安全严重性和图像‑文本组合推理方面的表现，涵盖 17 种安全模式。利用 8,187 条真实图文样本并通过人工标注，作者发现模型在单模态安全信号上能够达到 90% 以上准确率，但在需要联合理解时准确率骤降至 20‑55%，暴露出显著的对齐缺口。研究还指出在边缘内容上出现的过度拦截与不足拒绝的权衡，并提供了用于后续稳健视觉‑语言安全研究的测试平台。<br /><strong>Keywords:</strong> multimodal safety, vision-language models, compositional reasoning, benchmark, VLSU, alignment gaps, over-blocking, under-refusing, safety evaluation, joint understanding<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 8, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - alignment<br /><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
<div><strong>Authors:</strong> Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Vision Language Safety Understanding (VLSU), a benchmark and evaluation framework that systematically assesses multimodal foundation models on fine‑grained safety severity and compositional image‑text reasoning across 17 safety patterns. Using 8,187 real‑world image‑text samples annotated by humans, the authors show that state‑of‑the‑art models drop from >90% accuracy on unimodal cues to 20‑55% when joint understanding is required, exposing significant alignment gaps. The study also highlights the trade‑off between over‑blocking and under‑refusing borderline content and provides a testbed for future robust vision‑language safety research.", "summary_cn": "本文提出了视觉语言安全理解（Vision Language Safety Understanding，VLSU）框架，系统评估多模态基础模型在细粒度安全严重性和图像‑文本组合推理方面的表现，涵盖 17 种安全模式。利用 8,187 条真实图文样本并通过人工标注，作者发现模型在单模态安全信号上能够达到 90% 以上准确率，但在需要联合理解时准确率骤降至 20‑55%，暴露出显著的对齐缺口。研究还指出在边缘内容上出现的过度拦截与不足拒绝的权衡，并提供了用于后续稳健视觉‑语言安全研究的测试平台。", "keywords": "multimodal safety, vision-language models, compositional reasoning, benchmark, VLSU, alignment gaps, over-blocking, under-refusing, safety evaluation, joint understanding", "scoring": {"interpretability": 4, "understanding": 8, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "alignment"}, "authors": ["Shruti Palaskar", "Leon Gatys", "Mona Abdelrahman", "Mar Jacobo", "Larry Lindsey", "Rutika Moharir", "Gunnar Lund", "Yang Xu", "Navid Shiee", "Jeffrey Bigham", "Charles Maalouf", "Joseph Yitan Cheng"]}
]]></acme>

<pubDate>2025-10-21T01:30:31+00:00</pubDate>
</item>
<item>
<title>Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model</title>
<link>https://papers.cool/arxiv/2510.18165</link>
<guid>https://papers.cool/arxiv/2510.18165</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces Saber, a training-free sampling algorithm for diffusion language models that combines adaptive acceleration and backtracking‑enhanced remasking to improve code generation efficiency. By dynamically speeding up sampling as code context builds and allowing reversal of generated tokens, Saber achieves an average 251.4% inference speedup while raising Pass@1 accuracy by 1.9% across several benchmarks.<br /><strong>Summary (CN):</strong> 本文提出 Saber，一种无需再训练的扩散语言模型采样算法，采用自适应加速与回溯增强重新掩码相结合的方式提升代码生成效率。该方法在代码上下文逐渐形成时动态加速采样，并通过回溯机制纠正生成的 token，在多个基准上实现了约 251.4% 的推理加速，同时将 Pass@1 准确率提升约 1.9%。<br /><strong>Keywords:</strong> diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
Diffusion language models (DLMs) are emerging as a powerful and promising alternative to the dominant autoregressive paradigm, offering inherent advantages in parallel generation and bidirectional context modeling. However, the performance of DLMs on code generation tasks, which have stronger structural constraints, is significantly hampered by the critical trade-off between inference speed and output quality. We observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance. In this paper, we introduce efficient Sampling with Adaptive acceleration and Backtracking Enhanced Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to achieve better inference speed and output quality in code generation. Specifically, Saber is motivated by two key insights in the DLM generation process: 1) it can be adaptively accelerated as more of the code context is established; 2) it requires a backtracking mechanism to reverse the generated tokens. Extensive experiments on multiple mainstream code generation benchmarks show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over mainstream DLM sampling methods, meanwhile achieving an average 251.4% inference speedup. By leveraging the inherent advantages of DLMs, our work significantly narrows the performance gap with autoregressive models in code generation.
<div><strong>Authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces Saber, a training-free sampling algorithm for diffusion language models that combines adaptive acceleration and backtracking‑enhanced remasking to improve code generation efficiency. By dynamically speeding up sampling as code context builds and allowing reversal of generated tokens, Saber achieves an average 251.4% inference speedup while raising Pass@1 accuracy by 1.9% across several benchmarks.", "summary_cn": "本文提出 Saber，一种无需再训练的扩散语言模型采样算法，采用自适应加速与回溯增强重新掩码相结合的方式提升代码生成效率。该方法在代码上下文逐渐形成时动态加速采样，并通过回溯机制纠正生成的 token，在多个基准上实现了约 251.4% 的推理加速，同时将 Pass@1 准确率提升约 1.9%。", "keywords": "diffusion language model, code generation, sampling algorithm, adaptive acceleration, backtracking, remasking, inference speedup, Pass@1", "scoring": {"interpretability": 3, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"]}
]]></acme>

<pubDate>2025-10-20T23:38:12+00:00</pubDate>
</item>
<item>
<title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title>
<link>https://papers.cool/arxiv/2510.18123</link>
<guid>https://papers.cool/arxiv/2510.18123</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper presents the first systematic study of safety and security issues in natural-language-based collaborative driving, introducing a taxonomy of attacks such as connection disruption, content spoofing, and multi-connection forgery. To address these threats, the authors propose SafeCoop, an agentic defense pipeline that combines a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by spatial alignment across frames. Evaluations in closed‑loop CARLA simulations across 32 scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection performance under malicious attacks.<br /><strong>Summary (CN):</strong> 本文首次系统性地研究基于自然语言的协同驾驶安全与安全问题，提出了包括连接中断、内容伪造和多连接伪造等攻击策略的分类体系。作者设计了 SafeCoop 防御管线，融合语义防火墙、语言‑感知一致性检查以及跨帧空间对齐的多源共识机制。通过在 CARLA 仿真环境中 32 种关键场景的闭环实验，SafeCoop 在恶意攻击下实现了最高 69.15% 的驾驶得分提升和 67.32% 的 F1 检测率。<br /><strong>Keywords:</strong> collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, CARLA simulation<br /><strong>Scores:</strong> Interpretability: 4, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is https://xiangbogaobarry.github.io/SafeCoop.
<div><strong>Authors:</strong> Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper presents the first systematic study of safety and security issues in natural-language-based collaborative driving, introducing a taxonomy of attacks such as connection disruption, content spoofing, and multi-connection forgery. To address these threats, the authors propose SafeCoop, an agentic defense pipeline that combines a semantic firewall, language‑perception consistency checks, and multi‑source consensus enabled by spatial alignment across frames. Evaluations in closed‑loop CARLA simulations across 32 scenarios show up to 69.15% improvement in driving score and 67.32% F1 detection performance under malicious attacks.", "summary_cn": "本文首次系统性地研究基于自然语言的协同驾驶安全与安全问题，提出了包括连接中断、内容伪造和多连接伪造等攻击策略的分类体系。作者设计了 SafeCoop 防御管线，融合语义防火墙、语言‑感知一致性检查以及跨帧空间对齐的多源共识机制。通过在 CARLA 仿真环境中 32 种关键场景的闭环实验，SafeCoop 在恶意攻击下实现了最高 69.15% 的驾驶得分提升和 67.32% 的 F1 检测率。", "keywords": "collaborative driving, V2X, natural language communication, safety, security, semantic firewall, language-perception consistency, multi-source consensus, CARLA simulation", "scoring": {"interpretability": 4, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"]}
]]></acme>

<pubDate>2025-10-20T21:41:28+00:00</pubDate>
</item>
<item>
<title>SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning</title>
<link>https://papers.cool/arxiv/2510.18095</link>
<guid>https://papers.cool/arxiv/2510.18095</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes SMaRT, a framework that selects, mixes, and reinvents prompting strategies for large language models to improve reasoning, planning, and sequential decision‑making. By treating LLMs as intelligent integrators rather than mere evaluators, SMaRT combines diverse reasoning approaches to produce more robust and higher‑quality solutions, and empirical results show consistent gains over state‑of‑the‑art baselines.<br /><strong>Summary (CN):</strong> 本文提出了 SMaRT 框架，通过选择、混合和重新发明 (Select, Mix, and Reinvent) 提示策略，使大语言模型在推理、规划和序列决策任务中表现更佳。该框架将 LLM 视为智能整合者而非仅仅的评估者，融合多种推理方法以提升鲁棒性和解答质量，实验证明其在多个基准上均优于现有最先进方法。<br /><strong>Keywords:</strong> strategy fusion, LLM reasoning, prompting strategies, planning, sequential decision-making, robustness, chain-of-thought, self-refinement, multi-strategy integration<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 3, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - robustness<br /><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness. We introduce the Select, Mix, and ReinvenT (SMaRT) framework, an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies. Unlike existing methods, which employ LLMs merely as evaluators, SMaRT uses them as intelligent integrators, unlocking the "best of all worlds" across tasks. Extensive empirical evaluations across benchmarks in reasoning, planning, and sequential decision-making highlight the robustness and adaptability of SMaRT. The framework consistently outperforms state-of-the-art baselines in solution quality, constraint adherence, and performance metrics. This work redefines LLM-driven decision-making by pioneering a new paradigm in cross-strategy calibration, unlocking superior outcomes for reasoning systems and advancing the boundaries of self-refining methodologies.
<div><strong>Authors:</strong> Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes SMaRT, a framework that selects, mixes, and reinvents prompting strategies for large language models to improve reasoning, planning, and sequential decision‑making. By treating LLMs as intelligent integrators rather than mere evaluators, SMaRT combines diverse reasoning approaches to produce more robust and higher‑quality solutions, and empirical results show consistent gains over state‑of‑the‑art baselines.", "summary_cn": "本文提出了 SMaRT 框架，通过选择、混合和重新发明 (Select, Mix, and Reinvent) 提示策略，使大语言模型在推理、规划和序列决策任务中表现更佳。该框架将 LLM 视为智能整合者而非仅仅的评估者，融合多种推理方法以提升鲁棒性和解答质量，实验证明其在多个基准上均优于现有最先进方法。", "keywords": "strategy fusion, LLM reasoning, prompting strategies, planning, sequential decision-making, robustness, chain-of-thought, self-refinement, multi-strategy integration", "scoring": {"interpretability": 2, "understanding": 5, "safety": 3, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "robustness"}, "authors": ["Nikhil Verma", "Manasa Bharadwaj", "Wonjun Jang", "Harmanpreet Singh", "Yixiao Wang", "Homa Fashandi", "Chul Lee"]}
]]></acme>

<pubDate>2025-10-20T20:42:24+00:00</pubDate>
</item>
<item>
<title>HouseTour: A Virtual Real Estate A(I)gent</title>
<link>https://papers.cool/arxiv/2510.18054</link>
<guid>https://papers.cool/arxiv/2510.18054</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> HouseTour proposes a method that jointly generates spatially-aware 3D camera trajectories and natural language descriptions for virtual real‑estate tours. It uses a diffusion process constrained by known camera poses to produce smooth video paths, renders novel views with 3D Gaussian splatting, and integrates the trajectory information into a vision‑language model for grounded text generation. The accompanying HouseTour dataset of 1,200 house‑tour videos with pose and reconstruction data enables evaluation of both individual and end‑to‑end performance using a new joint metric.<br /><strong>Summary (CN):</strong> 本文提出 HouseTour 方法，能够同时生成空间感知的 3D 摄像机轨迹和对应的自然语言描述，用于虚拟房产导览。该方法利用受已知相机位姿约束的扩散过程生成平滑视频路径，并通过 3D 高斯喷溅渲染新视角，将轨迹信息融入视觉语言模型以实现基于 3D 场景的文本生成。作者还提供了包含 1,200 段房产导览视频、相机姿态和 3D 重建的 HouseTour 数据集，并使用新提出的联合指标评估单独及端到端的性能。<br /><strong>Keywords:</strong> 3D camera trajectory, diffusion process, Gaussian splatting, vision-language model, real estate video generation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 5, Safety: 2, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.
<div><strong>Authors:</strong> Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "HouseTour proposes a method that jointly generates spatially-aware 3D camera trajectories and natural language descriptions for virtual real‑estate tours. It uses a diffusion process constrained by known camera poses to produce smooth video paths, renders novel views with 3D Gaussian splatting, and integrates the trajectory information into a vision‑language model for grounded text generation. The accompanying HouseTour dataset of 1,200 house‑tour videos with pose and reconstruction data enables evaluation of both individual and end‑to‑end performance using a new joint metric.", "summary_cn": "本文提出 HouseTour 方法，能够同时生成空间感知的 3D 摄像机轨迹和对应的自然语言描述，用于虚拟房产导览。该方法利用受已知相机位姿约束的扩散过程生成平滑视频路径，并通过 3D 高斯喷溅渲染新视角，将轨迹信息融入视觉语言模型以实现基于 3D 场景的文本生成。作者还提供了包含 1,200 段房产导览视频、相机姿态和 3D 重建的 HouseTour 数据集，并使用新提出的联合指标评估单独及端到端的性能。", "keywords": "3D camera trajectory, diffusion process, Gaussian splatting, vision-language model, real estate video generation", "scoring": {"interpretability": 2, "understanding": 5, "safety": 2, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"]}
]]></acme>

<pubDate>2025-10-20T19:47:35+00:00</pubDate>
</item>
<item>
<title>Subject-Event Ontology Without Global Time: Foundations and Execution Semantics</title>
<link>https://papers.cool/arxiv/2510.18040</link>
<guid>https://papers.cool/arxiv/2510.18040</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a formal subject-event ontology for modeling complex dynamic systems without relying on global timestamps, defining causal order through explicit happens-before dependencies and making the ontology executable via a declarative dataflow mechanism. It introduces nine axioms ensuring properties such as monotonicity, acyclicity, and traceability, and demonstrates practical applicability through the Boldsea workflow engine and its semantic language BSL. The framework targets use cases in distributed systems, microservice architectures, DLT platforms, and scenarios with conflicting perspectives among different subjects.<br /><strong>Summary (CN):</strong> 本文提出了一种不依赖全局时间的主体-事件本体形式化，用显式的先行关系定义因果顺序，并通过声明式数据流机制使本体可执行。文中给出九条公理，确保历史单调性、因果无环性和可追溯性，并在 Boldsea 工作流引擎及其 BSL 语言上展示了实际实现。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突的情形。<br /><strong>Keywords:</strong> subject-event ontology, causal order, declarative dataflow, distributed systems, epistemic models, workflow engine, Boldsea, event semantics, multi-perspective, DLT<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 6, Safety: 2, Technicality: 8, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - non-applicable; Primary focus - other<br /><strong>Authors:</strong> Alexander Boldachev</div>
A formalization of a subject-event ontology is proposed for modeling complex dynamic systems without reliance on global time. Key principles: (1) event as an act of fixation - a subject discerns and fixes changes according to models (conceptual templates) available to them; (2) causal order via happens-before - the order of events is defined by explicit dependencies, not timestamps; (3) making the ontology executable via a declarative dataflow mechanism, ensuring determinism; (4) models as epistemic filters - a subject can only fix what falls under its known concepts and properties; (5) presumption of truth - the declarative content of an event is available for computation from the moment of fixation, without external verification. The formalization includes nine axioms (A1-A9), ensuring the correctness of executable ontologies: monotonicity of history (I1), acyclicity of causality (I2), traceability (I3). Special attention is given to the model-based approach (A9): event validation via schemas, actor authorization, automatic construction of causal chains (W3) without global time. Practical applicability is demonstrated on the boldsea system - a workflow engine for executable ontologies, where the theoretical constructs are implemented in BSL (Boldsea Semantic Language). The formalization is applicable to distributed systems, microservice architectures, DLT platforms, and multiperspectivity scenarios (conflicting facts from different subjects).
<div><strong>Authors:</strong> Alexander Boldachev</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a formal subject-event ontology for modeling complex dynamic systems without relying on global timestamps, defining causal order through explicit happens-before dependencies and making the ontology executable via a declarative dataflow mechanism. It introduces nine axioms ensuring properties such as monotonicity, acyclicity, and traceability, and demonstrates practical applicability through the Boldsea workflow engine and its semantic language BSL. The framework targets use cases in distributed systems, microservice architectures, DLT platforms, and scenarios with conflicting perspectives among different subjects.", "summary_cn": "本文提出了一种不依赖全局时间的主体-事件本体形式化，用显式的先行关系定义因果顺序，并通过声明式数据流机制使本体可执行。文中给出九条公理，确保历史单调性、因果无环性和可追溯性，并在 Boldsea 工作流引擎及其 BSL 语言上展示了实际实现。该框架适用于分布式系统、微服务架构、分布式账本技术以及多主体视角冲突的情形。", "keywords": "subject-event ontology, causal order, declarative dataflow, distributed systems, epistemic models, workflow engine, Boldsea, event semantics, multi-perspective, DLT", "scoring": {"interpretability": 3, "understanding": 6, "safety": 2, "technicality": 8, "surprisal": 5}, "category": {"failure_mode_addressed": "non-applicable", "primary_focus": "other"}, "authors": ["Alexander Boldachev"]}
]]></acme>

<pubDate>2025-10-20T19:26:44+00:00</pubDate>
</item>
<item>
<title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>
<link>https://papers.cool/arxiv/2510.17947</link>
<guid>https://papers.cool/arxiv/2510.17947</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces PLAGUE, a plug-and‑play framework that structures multi‑turn jailbreak attacks against large language models into three phases (Primer, Planner, Finisher) inspired by lifelong‑learning agents. Experiments show that red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on strong, safety‑focused models such as OpenAI's o3 and Claude Opus 4.1. The work highlights the importance of plan initialization, context optimization, and continual adaptation for evaluating and exposing model vulnerabilities.<br /><strong>Summary (CN):</strong> 本文提出 PLAGUE 框架，将针对大型语言模型的多轮越狱攻击分为 Primer、Planner、Finisher 三个阶段，借鉴终身学习代理的思路，实现插件式、可适应的攻击生成。实验表明，基于 PLAGUE 的红队代理在 OpenAI o3 和 Claude Opus 4.1 等高安全性模型上将攻击成功率提升超过 30%，达到 81.4% 与 67.3%。该工作强调了计划初始化、上下文优化和持续学习在系统性评估模型漏洞关键作用。<br /><strong>Keywords:</strong> jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, PLAGUE, adversarial exploitation<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - robustness<br /><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.
<div><strong>Authors:</strong> Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces PLAGUE, a plug-and‑play framework that structures multi‑turn jailbreak attacks against large language models into three phases (Primer, Planner, Finisher) inspired by lifelong‑learning agents. Experiments show that red‑teaming agents built with PLAGUE achieve state‑of‑the‑art success rates, improving attack effectiveness by over 30% on strong, safety‑focused models such as OpenAI's o3 and Claude Opus 4.1. The work highlights the importance of plan initialization, context optimization, and continual adaptation for evaluating and exposing model vulnerabilities.", "summary_cn": "本文提出 PLAGUE 框架，将针对大型语言模型的多轮越狱攻击分为 Primer、Planner、Finisher 三个阶段，借鉴终身学习代理的思路，实现插件式、可适应的攻击生成。实验表明，基于 PLAGUE 的红队代理在 OpenAI o3 和 Claude Opus 4.1 等高安全性模型上将攻击成功率提升超过 30%，达到 81.4% 与 67.3%。该工作强调了计划初始化、上下文优化和持续学习在系统性评估模型漏洞关键作用。", "keywords": "jailbreak, multi-turn attack, lifelong learning, LLM safety, red-teaming, prompt engineering, PLAGUE, adversarial exploitation", "scoring": {"interpretability": 2, "understanding": 6, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "robustness"}, "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"]}
]]></acme>

<pubDate>2025-10-20T17:37:03+00:00</pubDate>
</item>
<item>
<title>Interpretability Framework for LLMs in Undergraduate Calculus</title>
<link>https://papers.cool/arxiv/2510.17910</link>
<guid>https://papers.cool/arxiv/2510.17910</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces a quantitative interpretability framework that extracts reasoning flows and semantically labels operations in LLM-generated solutions to undergraduate calculus problems, using metrics such as reasoning complexity, phrase sensitivity, and robustness. Experiments on real Calculus I‑III exams reveal that LLMs often produce fluent but conceptually flawed solutions, with reasoning highly sensitive to prompt phrasing and input variations. The framework enables fine‑grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI‑assisted feedback tools for STEM education.<br /><strong>Summary (CN):</strong> 本文提出了一套量化的可解释性框架，通过提取推理流程并对大语言模型在大学微积分题目中的解答进行语义标注，使用推理复杂度、短语敏感性和鲁棒性等指标进行评估。对实际的微积分 I‑III 考试进行实验后发现，模型常产生表面流畅却概念错误的解答，且推理过程对提示措辞和输入变化极为敏感。该框架能够细粒度诊断推理失误，帮助课程对齐，并为 STEM 教育中的可解释 AI 辅助反馈工具提供设计依据。<br /><strong>Keywords:</strong> interpretability, large language models, calculus education, reasoning analysis, prompt ablation, robustness, pedagogical alignment, AI safety in education<br /><strong>Scores:</strong> Interpretability: 8, Understanding: 7, Safety: 6, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - interpretability<br /><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
Large Language Models (LLMs) are increasingly being used in education, yet their correctness alone does not capture the quality, reliability, or pedagogical validity of their problem-solving behavior, especially in mathematics, where multistep logic, symbolic reasoning, and conceptual clarity are critical. Conventional evaluation methods largely focus on final answer accuracy and overlook the reasoning process. To address this gap, we introduce a novel interpretability framework for analyzing LLM-generated solutions using undergraduate calculus problems as a representative domain. Our approach combines reasoning flow extraction and decomposing solutions into semantically labeled operations and concepts with prompt ablation analysis to assess input salience and output stability. Using structured metrics such as reasoning complexity, phrase sensitivity, and robustness, we evaluated the model behavior on real Calculus I to III university exams. Our findings revealed that LLMs often produce syntactically fluent yet conceptually flawed solutions, with reasoning patterns sensitive to prompt phrasing and input variation. This framework enables fine-grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI-assisted feedback tools. This is the first study to offer a structured, quantitative, and pedagogically grounded framework for interpreting LLM reasoning in mathematics education, laying the foundation for the transparent and responsible deployment of AI in STEM learning environments.
<div><strong>Authors:</strong> Sagnik Dakshit, Sushmita Sinha Roy</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces a quantitative interpretability framework that extracts reasoning flows and semantically labels operations in LLM-generated solutions to undergraduate calculus problems, using metrics such as reasoning complexity, phrase sensitivity, and robustness. Experiments on real Calculus I‑III exams reveal that LLMs often produce fluent but conceptually flawed solutions, with reasoning highly sensitive to prompt phrasing and input variations. The framework enables fine‑grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI‑assisted feedback tools for STEM education.", "summary_cn": "本文提出了一套量化的可解释性框架，通过提取推理流程并对大语言模型在大学微积分题目中的解答进行语义标注，使用推理复杂度、短语敏感性和鲁棒性等指标进行评估。对实际的微积分 I‑III 考试进行实验后发现，模型常产生表面流畅却概念错误的解答，且推理过程对提示措辞和输入变化极为敏感。该框架能够细粒度诊断推理失误，帮助课程对齐，并为 STEM 教育中的可解释 AI 辅助反馈工具提供设计依据。", "keywords": "interpretability, large language models, calculus education, reasoning analysis, prompt ablation, robustness, pedagogical alignment, AI safety in education", "scoring": {"interpretability": 8, "understanding": 7, "safety": 6, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "interpretability"}, "authors": ["Sagnik Dakshit", "Sushmita Sinha Roy"]}
]]></acme>

<pubDate>2025-10-19T17:20:36+00:00</pubDate>
</item>
<item>
<title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title>
<link>https://papers.cool/arxiv/2510.17904</link>
<guid>https://papers.cool/arxiv/2510.17904</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper introduces BreakFun, a jailbreak technique that leverages an LLM's strong tendency to follow structured schemas by embedding a malicious "Trojan Schema" within a three-part prompt, achieving up to 100% attack success on several models. An extensive evaluation on 13 models shows an average 89% success rate, and an ablation study confirms the schema as the primary causal factor. To mitigate this vulnerability, the authors propose the Adversarial Prompt Deconstruction guardrail, which uses a secondary LLM to extract literal text and reveal hidden harmful intent, demonstrating strong defensive performance.<br /><strong>Summary (CN):</strong> 本文提出 BreakFun，一种利用大语言模型（LL）强烈遵循结构化模式倾向的越狱方法，通过在三段式提示中嵌入恶意的“Trojan Schema”（特洛伊模式），迫使模型生成有害内容，在部分模型上实现 100% 攻击成功率，整体平均成功率达 89%。大规模实验和消融研究表明该模式是攻击的核心因素。为应对该，作者设计了 Adversarial Prompt Deconstruction 防护机制，使用二级 LLM 执行“Literal Transcription”（文字转录），提取所有可读文本以揭示隐藏的有害意图，展示出显著的防御效果。<br /><strong>Keywords:</strong> jailbreak, schema exploitation, prompt engineering, adversarial defense, LLM alignment, chain-of-thought, Trojan schema, guardrail<br /><strong>Scores:</strong> Interpretability: 3, Understanding: 7, Safety: 8, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - human-misuse; Primary focus - control<br /><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.
<div><strong>Authors:</strong> Amirkia Rafiei Oskooei, Mehmet S. Aktas</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper introduces BreakFun, a jailbreak technique that leverages an LLM's strong tendency to follow structured schemas by embedding a malicious \"Trojan Schema\" within a three-part prompt, achieving up to 100% attack success on several models. An extensive evaluation on 13 models shows an average 89% success rate, and an ablation study confirms the schema as the primary causal factor. To mitigate this vulnerability, the authors propose the Adversarial Prompt Deconstruction guardrail, which uses a secondary LLM to extract literal text and reveal hidden harmful intent, demonstrating strong defensive performance.", "summary_cn": "本文提出 BreakFun，一种利用大语言模型（LL）强烈遵循结构化模式倾向的越狱方法，通过在三段式提示中嵌入恶意的“Trojan Schema”（特洛伊模式），迫使模型生成有害内容，在部分模型上实现 100% 攻击成功率，整体平均成功率达 89%。大规模实验和消融研究表明该模式是攻击的核心因素。为应对该，作者设计了 Adversarial Prompt Deconstruction 防护机制，使用二级 LLM 执行“Literal Transcription”（文字转录），提取所有可读文本以揭示隐藏的有害意图，展示出显著的防御效果。", "keywords": "jailbreak, schema exploitation, prompt engineering, adversarial defense, LLM alignment, chain-of-thought, Trojan schema, guardrail", "scoring": {"interpretability": 3, "understanding": 7, "safety": 8, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "human-misuse", "primary_focus": "control"}, "authors": ["Amirkia Rafiei Oskooei", "Mehmet S. Aktas"]}
]]></acme>

<pubDate>2025-10-19T11:27:44+00:00</pubDate>
</item>
<item>
<title>Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning</title>
<link>https://papers.cool/arxiv/2510.17900</link>
<guid>https://papers.cool/arxiv/2510.17900</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> This paper introduces a benchmark using Indian public legal examinations to evaluate large language models' competence in legal reasoning, covering both multiple‑choice objective tests and a lawyer‑graded long‑form assessment from the Supreme Court Advocate‑on‑Record exam. Results show frontier models meet historical cut‑offs on objective sections but fall short of top human performance on long‑form reasoning, revealing key reliability failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work provides datasets, protocols, and analysis of where LLMs can assist versus where human lawyers remain essential.<br /><strong>Summary (CN):</strong> 本文提出使用印度公共法律考试作为基准，评估大语言模型在法律推理方面的能力，涵盖客观选择题以及最高法院律师资格考试的长文答题，由律师盲审评分。结果显示，前沿模型在客观题上达到了历史合格线，但在长文推理上仍未超越人类最高分，暴露出程序合规、引用规范以及法庭语气等三类可靠性失效模式。论文提供了数据集、评估协议，并分析了 LLM协助的环节与仍需人工主导的关键法律任务。<br /><strong>Keywords:</strong> Indian legal reasoning, LLM evaluation, legal benchmark, courtroom readiness, multiple-choice exam, long-form legal reasoning, legal AI, exam-based assessment<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 7, Safety: 5, Technicality: 7, Surprisal: 6<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - robustness<br /><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
Large language models (LLMs) are entering legal workflows, yet we lack a jurisdiction-specific framework to assess their baseline competence therein. We use India's public legal examinations as a transparent proxy. Our multi-year benchmark assembles objective screens from top national and state exams and evaluates open and frontier LLMs under real-world exam conditions. To probe beyond multiple-choice questions, we also include a lawyer-graded, paired-blinded study of long-form answers from the Supreme Court's Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded, India-specific yardstick for LLM court-readiness released with datasets and protocols. Our work shows that while frontier systems consistently clear historical cutoffs and often match or exceed recent top-scorer bands on objective exams, none surpasses the human topper on long-form reasoning. Grader notes converge on three reliability failure modes: procedural or format compliance, authority or citation discipline, and forum-appropriate voice and structure. These findings delineate where LLMs can assist (checks, cross-statute consistency, statute and precedent lookups) and where human leadership remains essential: forum-specific drafting and filing, procedural and relief strategy, reconciling authorities and exceptions, and ethical, accountable judgment.
<div><strong>Authors:</strong> Kush Juvekar, Arghya Bhattacharya, Sai Khadloya, Utkarsh Saxena</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "This paper introduces a benchmark using Indian public legal examinations to evaluate large language models' competence in legal reasoning, covering both multiple‑choice objective tests and a lawyer‑graded long‑form assessment from the Supreme Court Advocate‑on‑Record exam. Results show frontier models meet historical cut‑offs on objective sections but fall short of top human performance on long‑form reasoning, revealing key reliability failure modes such as procedural compliance, citation discipline, and appropriate courtroom voice. The work provides datasets, protocols, and analysis of where LLMs can assist versus where human lawyers remain essential.", "summary_cn": "本文提出使用印度公共法律考试作为基准，评估大语言模型在法律推理方面的能力，涵盖客观选择题以及最高法院律师资格考试的长文答题，由律师盲审评分。结果显示，前沿模型在客观题上达到了历史合格线，但在长文推理上仍未超越人类最高分，暴露出程序合规、引用规范以及法庭语气等三类可靠性失效模式。论文提供了数据集、评估协议，并分析了 LLM协助的环节与仍需人工主导的关键法律任务。", "keywords": "Indian legal reasoning, LLM evaluation, legal benchmark, courtroom readiness, multiple-choice exam, long-form legal reasoning, legal AI, exam-based assessment", "scoring": {"interpretability": 2, "understanding": 7, "safety": 5, "technicality": 7, "surprisal": 6}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "robustness"}, "authors": ["Kush Juvekar", "Arghya Bhattacharya", "Sai Khadloya", "Utkarsh Saxena"]}
]]></acme>

<pubDate>2025-10-19T10:04:29+00:00</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://papers.cool/arxiv/2510.17895</link>
<guid>https://papers.cool/arxiv/2510.17895</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a hierarchical federated unlearning framework for large language models that addresses continuous, heterogeneous unlearning requests in decentralized, privacy-sensitive settings. By decoupling unlearning and retention through task-specific adapters and employing a hierarchical merging strategy, the method mitigates inter- and intra-domain interference while preserving model performance. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate effective handling of heterogeneous unlearning demands with superior utility compared to baselines.<br /><strong>Summary (CN):</strong> 本文提出了一种层次化联邦消除（unlearning）框架，用于大语言模型，以应对持续且多样的消除需求以及去中心化、隐私敏感的数据环境。通过任务专用适配器将消除与保留解耦，并采用层合并策略来缓解域间和域内的冲突而在保持模型性能的同时实现有效的知识删除。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够灵活处理多样化的消除请求，并在模型效用上优于现有基线。<br /><strong>Keywords:</strong> federated unlearning, large language models, privacy, hierarchical merging, adapter learning, continual unlearning, decentralized data, model utility<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 7, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - misalignment; Primary focus - control<br /><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
<div><strong>Authors:</strong> Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a hierarchical federated unlearning framework for large language models that addresses continuous, heterogeneous unlearning requests in decentralized, privacy-sensitive settings. By decoupling unlearning and retention through task-specific adapters and employing a hierarchical merging strategy, the method mitigates inter- and intra-domain interference while preserving model performance. Experiments on benchmarks such as WMDP, MUSE, and TOFU demonstrate effective handling of heterogeneous unlearning demands with superior utility compared to baselines.", "summary_cn": "本文提出了一种层次化联邦消除（unlearning）框架，用于大语言模型，以应对持续且多样的消除需求以及去中心化、隐私敏感的数据环境。通过任务专用适配器将消除与保留解耦，并采用层合并策略来缓解域间和域内的冲突而在保持模型性能的同时实现有效的知识删除。实验在 WMDP、MUSE、TOFU 等基准上表明，该方法能够灵活处理多样化的消除请求，并在模型效用上优于现有基线。", "keywords": "federated unlearning, large language models, privacy, hierarchical merging, adapter learning, continual unlearning, decentralized data, model utility", "scoring": {"interpretability": 2, "understanding": 6, "safety": 7, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "misalignment", "primary_focus": "control"}, "authors": ["Yisheng Zhong", "Zhengbang Yang", "Zhuangdi Zhu"]}
]]></acme>

<pubDate>2025-10-19T04:24:51+00:00</pubDate>
</item>
<item>
<title>Metrics and evaluations for computational and sustainable AI efficiency</title>
<link>https://papers.cool/arxiv/2510.17885</link>
<guid>https://papers.cool/arxiv/2510.17885</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper proposes a unified, reproducible methodology for evaluating AI model inference efficiency that jointly measures latency, throughput, energy consumption, and location‑adjusted carbon emissions while keeping accuracy constraints constant. It applies this framework across a range of hardware platforms and software stacks, generating Pareto frontiers that illuminate trade‑offs between accuracy, speed, energy use, and carbon impact, and releases open‑source code for independent verification. This benchmarking approach aims to enable evidence‑based, carbon‑aware decisions for sustainable AI deployment.<br /><strong>Summary (CN):</strong> 本文提出了一种统一且可复现的 AI 推理效率评估方法，在保持准确度约束的前提下同时测量时延、吞吐量、能耗和基于位置的碳排放。作者在多种硬件平台和软件栈上应用该框架，生成展示准确率、速度、能耗和碳足迹权衡的 Pareto 前沿，并公开开源代码供独验证。此基准旨在帮助研究者和实践者做出基于证据的碳意识决策，推动可持续 AI 部署。<br /><strong>Keywords:</strong> computational efficiency, sustainable AI, carbon emissions, latency, throughput, energy consumption, benchmarking, multi-precision, hardware-software stack, AI sustainability<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 6, Safety: 5, Technicality: 7, Surprisal: 5<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
The rapid advancement of Artificial Intelligence (AI) has created unprecedented demands for computational power, yet methods for evaluating the performance, efficiency, and environmental impact of deployed models remain fragmented. Current approaches often fail to provide a holistic view, making it difficult to compare and optimise systems across heterogeneous hardware, software stacks, and numeric precisions. To address this gap, we propose a unified and reproducible methodology for AI model inference that integrates computational and environmental metrics under realistic serving conditions. Our framework provides a pragmatic, carbon-aware evaluation by systematically measuring latency and throughput distributions, energy consumption, and location-adjusted carbon emissions, all while maintaining matched accuracy constraints for valid comparisons. We apply this methodology to multi-precision models across diverse hardware platforms, from data-centre accelerators like the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream software stacks including PyTorch, TensorRT, and ONNX Runtime. By systematically categorising these factors, our work establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying the trade-offs between accuracy, latency, energy, and carbon. The accompanying open-source code enables independent verification and facilitates adoption, empowering researchers and practitioners to make evidence-based decisions for sustainable AI deployment.
<div><strong>Authors:</strong> Hongyuan Liu, Xinyang Liu, Guosheng Hu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper proposes a unified, reproducible methodology for evaluating AI model inference efficiency that jointly measures latency, throughput, energy consumption, and location‑adjusted carbon emissions while keeping accuracy constraints constant. It applies this framework across a range of hardware platforms and software stacks, generating Pareto frontiers that illuminate trade‑offs between accuracy, speed, energy use, and carbon impact, and releases open‑source code for independent verification. This benchmarking approach aims to enable evidence‑based, carbon‑aware decisions for sustainable AI deployment.", "summary_cn": "本文提出了一种统一且可复现的 AI 推理效率评估方法，在保持准确度约束的前提下同时测量时延、吞吐量、能耗和基于位置的碳排放。作者在多种硬件平台和软件栈上应用该框架，生成展示准确率、速度、能耗和碳足迹权衡的 Pareto 前沿，并公开开源代码供独验证。此基准旨在帮助研究者和实践者做出基于证据的碳意识决策，推动可持续 AI 部署。", "keywords": "computational efficiency, sustainable AI, carbon emissions, latency, throughput, energy consumption, benchmarking, multi-precision, hardware-software stack, AI sustainability", "scoring": {"interpretability": 2, "understanding": 6, "safety": 5, "technicality": 7, "surprisal": 5}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"]}
]]></acme>

<pubDate>2025-10-18T03:30:15+00:00</pubDate>
</item>
<item>
<title>Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints</title>
<link>https://papers.cool/arxiv/2510.17882</link>
<guid>https://papers.cool/arxiv/2510.17882</guid>
<content:encoded><![CDATA[
<div><strong>Summary:</strong> The paper conducts large‑scale empirical analysis of over 2.1 million preprints from 2016‑2025 to examine how generative large language models influence scholarly publishing. Using interrupted time‑series, authorship metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission cycles, modestly raise linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields. The authors argue that LLMs as selective catalysts that amplify existing strengths and widen disciplinary divides, calling for governance frameworks to maintain trust fairness.<br /><strong>Summary (CN):</strong> 本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，评估生成式大语言模型对学术出版的影响。通过中断时间序列、作者合作与产出指标、语言特征分析和主题建模，发现 LLM 加速了提交与修订过程，略微提升了语言复杂度，并显著增加了 AI 相关主题，尤其在计算密集型领域更为突出。作者指出，LLM 更像是选择性催化剂，放大了既有优势并扩大学科差距，因而呼吁制定治理框架以维护信任、公平与责任。<br /><strong>Keywords:</strong> generative AI, preprints, scholarly publishing, LLM impact, interrupted time series, topic modeling, linguistic complexity, AI governance<br /><strong>Scores:</strong> Interpretability: 2, Understanding: 4, Safety: 5, Technicality: 7, Surprisal: 7<br /><strong>Categories:</strong> Failure mode - societal-disruption; Primary focus - other<br /><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
Preprint repositories become central infrastructures for scholarly communication. Their expansion transforms how research is circulated and evaluated before journal publication. Generative large language models (LLMs) introduce a further potential disruption by altering how manuscripts are written. While speculation abounds, systematic evidence of whether and how LLMs reshape scientific publishing remains limited. This paper addresses the gap through a large-scale analysis of more than 2.1 million preprints spanning 2016--2025 (115 months) across four major repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a multi-level analytical framework that integrates interrupted time-series models, collaboration and productivity metrics, linguistic profiling, and topic modeling to assess changes in volume, authorship, style, and disciplinary orientation. Our findings reveal that LLMs have accelerated submission and revision cycles, modestly increased linguistic complexity, and disproportionately expanded AI-related topics, while computationally intensive fields benefit more than others. These results show that LLMs act less as universal disruptors than as selective catalysts, amplifying existing strengths and widening disciplinary divides. By documenting these dynamics, the paper provides the first empirical foundation for evaluating the influence of generative AI on academic publishing and highlights the need for governance frameworks that preserve trust, fairness, and accountability in an AI-enabled research ecosystem.
<div><strong>Authors:</strong> Minfeng Qi, Zhongmin Cao, Qin Wang, Ningran Li, Tianqing Zhu</div>
]]></content:encoded>
<acme type="application/json"><![CDATA[
{"summary": "The paper conducts large‑scale empirical analysis of over 2.1 million preprints from 2016‑2025 to examine how generative large language models influence scholarly publishing. Using interrupted time‑series, authorship metrics, linguistic profiling, and topic modeling, it finds that LLMs speed up submission cycles, modestly raise linguistic complexity, and disproportionately boost AI‑related topics, especially in computationally intensive fields. The authors argue that LLMs as selective catalysts that amplify existing strengths and widen disciplinary divides, calling for governance frameworks to maintain trust fairness.", "summary_cn": "本文对 2016‑2025 年期间超过 210 万篇预印本进行大规模实证分析，评估生成式大语言模型对学术出版的影响。通过中断时间序列、作者合作与产出指标、语言特征分析和主题建模，发现 LLM 加速了提交与修订过程，略微提升了语言复杂度，并显著增加了 AI 相关主题，尤其在计算密集型领域更为突出。作者指出，LLM 更像是选择性催化剂，放大了既有优势并扩大学科差距，因而呼吁制定治理框架以维护信任、公平与责任。", "keywords": "generative AI, preprints, scholarly publishing, LLM impact, interrupted time series, topic modeling, linguistic complexity, AI governance", "scoring": {"interpretability": 2, "understanding": 4, "safety": 5, "technicality": 7, "surprisal": 7}, "category": {"failure_mode_addressed": "societal-disruption", "primary_focus": "other"}, "authors": ["Minfeng Qi", "Zhongmin Cao", "Qin Wang", "Ningran Li", "Tianqing Zhu"]}
]]></acme>

<pubDate>2025-10-18T01:37:40+00:00</pubDate>
</item>
</channel>
</rss>